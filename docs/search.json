[
  {
    "objectID": "team.html#organizers-and-instructors",
    "href": "team.html#organizers-and-instructors",
    "title": "Our Team",
    "section": "Organizers and Instructors",
    "text": "Organizers and Instructors\n\n\n\nDr. Eli Holmes\n\n\nNOAA Fisheries\nwebpage • GitHub • ORCID",
    "crumbs": [
      "Welcome",
      "Our Team"
    ]
  },
  {
    "objectID": "setup.html#user-accounts",
    "href": "setup.html#user-accounts",
    "title": "Set-up",
    "section": "User accounts",
    "text": "User accounts\n\nGitHub username\n\nCreate a GitHub account (if you don’t already have one) at https://github.com. Advice for choosing a GitHub username: this is a professional username that you will use in work settings. GitHub accounts are not anonymous; this is for sharing work. Using your real name is common.\nRemember your username and password; you will need to be logged in during the course!\nHere is a video showing the whole process\n\nEarthdata Login account\n\nCreate an Earthdata Login account (if you don’t already have one) at https://urs.earthdata.nasa.gov\nRemember your username and password; you will need to be logged in during the course!",
    "crumbs": [
      "Welcome",
      "Set-up"
    ]
  },
  {
    "objectID": "setup.html#get-on-the-jupyterhub",
    "href": "setup.html#get-on-the-jupyterhub",
    "title": "Set-up",
    "section": "Get on the JupyterHub",
    "text": "Get on the JupyterHub\nOnce you have submitted your GitHub username and have been accepted as a member of the DaskHub team on the nmfs-opensci organization, you can log-into the JupyterHub.\nhttps://dhub.opensci.live/\n\nMake sure to choose an R image or else you will not see the RStudio Server button. Watch a video of the login process and basic JupyterHub orientation.\nhome directory is yours and no one else can see it. To share files, you can connect to a GitHub repository or use the shared directory. Everyone can read and write to this directory. Please don’t delete content that is not your own.",
    "crumbs": [
      "Welcome",
      "Set-up"
    ]
  },
  {
    "objectID": "setup.html#set-up-authentication-to-github",
    "href": "setup.html#set-up-authentication-to-github",
    "title": "Set-up",
    "section": "Set up authentication to GitHub",
    "text": "Set up authentication to GitHub\nYou need to tell GitHub who you are so you can push your local changes up to GitHub. There are a few ways to do this. I am going to show you a way that works on any computer, including a virtual computer like the JupyterHub.\n\nStep 1: Generate a Personal Access Token\nWe are going to generate a classic token.\n\nGo to https://github.com/settings/tokens\nClick Generate new token &gt; Generate new token (classic)\nWhen the pop-up shows up, fill in a description, click the “repo” checkbox, and then scroll to bottom to click “Generate”.\nFor scope, select “repo”.\nSAVE the token. You need it for the next step.\n\n\n\nStep 2: Tell Git who your are\n\nOpen a terminal. In Jupyter Lab, you will see a box labelled “Terminal” on the Launcher window. In RStudio, you will see a tab (usually in lower left) with the label “Terminal”\nPaste these 3 lines of code into the terminal\n\ngit config --global user.email \"&lt;your email&gt;\"\ngit config --global user.name \"&lt;your name&gt;\"\ngit config --global pull.rebase false\ngit config --global credential.helper store\nReplace \"&lt;your email&gt;\" with something like jane.doe@noaa.gov. Replace \"&lt;your name&gt;\" with something like \"Jane Doe\". Notice the quotes.\n\n\nStep 3: Trigger git to ask for your password\nThere are a few ways to do this.\n\nClone a repo, make a change, and then commit and push the change\nClone a private repo\n\nOption b is easiest if you are new to Git and GitHub.\n\nOpen a terminal window\nMake sure you are in the home directory by typing cd ~\nClone a repo and create an RStudio project. File &gt; New Project &gt; Version Control &gt; Git. Paste in this URL https://github.com/nmfs-opensci/github_setup_check and make sure it is creating the repo at ~ (home directory).\nYou will be asked for your GitHub username and password. For the password, enter the PERSONAL ACCESS TOKEN from Step 1.\n\nWatch a video of these 4 steps\nFull instructions with other ways to do this from R",
    "crumbs": [
      "Welcome",
      "Set-up"
    ]
  },
  {
    "objectID": "r-tutorials/newfile.html#hi",
    "href": "r-tutorials/newfile.html#hi",
    "title": "2024 NOAA HackDays",
    "section": "Hi",
    "text": "Hi\nThis is a new file."
  },
  {
    "objectID": "r-tutorials/SDM-lab1.html",
    "href": "r-tutorials/SDM-lab1.html",
    "title": "Species Distribution Modeling",
    "section": "",
    "text": "Save the files to your own folder to work though the examples or follow along but don’t save.",
    "crumbs": [
      "Tutorials",
      "Marine SDMs",
      "SDM 1"
    ]
  },
  {
    "objectID": "r-tutorials/SDM-lab1.html#topics",
    "href": "r-tutorials/SDM-lab1.html#topics",
    "title": "Species Distribution Modeling",
    "section": "Topics",
    "text": "Topics\nOHW marine SDM tutorial\n\nDownload occurence data.\n\nshared/2023-Hackbook/r-tutorials/SDM-lab-spocc.qmd\nshared/2023-Hackbook/r-tutorials/SDM-lab-robis.qmd\n\n\n\nDownload sdmpredictors data\n\nThe spreadsheet with all the variables\nshared/tutorials-marine-sdm/tutorial/Steps_occ_env.Rmd\n\n\n\nFull example\n\nshared/tutorials-marine-sdm/SDM/Turtle_maxnet.Rmd",
    "crumbs": [
      "Tutorials",
      "Marine SDMs",
      "SDM 1"
    ]
  },
  {
    "objectID": "r-tutorials/SDM-lab-robis.html",
    "href": "r-tutorials/SDM-lab-robis.html",
    "title": "SDM lab 1 - Downloading species data with robis",
    "section": "",
    "text": "Here we download from OBIS using the robis package.",
    "crumbs": [
      "Tutorials",
      "Marine SDMs",
      "robis"
    ]
  },
  {
    "objectID": "r-tutorials/SDM-lab-robis.html#set-up",
    "href": "r-tutorials/SDM-lab-robis.html#set-up",
    "title": "SDM lab 1 - Downloading species data with robis",
    "section": "Set up",
    "text": "Set up\n\nhere::i_am(\"r-tutorials/SDM-lab-robis.qmd\")\n\nhere() starts at /Users/eli.holmes/Documents/GitHub/NOAAHackDays\n\ndir_data &lt;- here::here(\"r-tutorials\", \"data\")\n\n\nlibrary(ggplot2)\nlibrary(sf)\n\nLinking to GEOS 3.11.0, GDAL 3.5.3, PROJ 9.1.0; sf_use_s2() is TRUE\n\nlibrary(\"rnaturalearth\")\nlibrary(\"rnaturalearthdata\")\n\n\nAttaching package: 'rnaturalearthdata'\n\n\nThe following object is masked from 'package:rnaturalearth':\n\n    countries110\n\nlibrary(raster)\n\nLoading required package: sp\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.3     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ lubridate 1.9.2     ✔ tibble    3.2.1\n✔ purrr     1.0.1     ✔ tidyr     1.3.0\n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ tidyr::extract() masks raster::extract()\n✖ dplyr::filter()  masks stats::filter()\n✖ dplyr::lag()     masks stats::lag()\n✖ dplyr::select()  masks raster::select()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(robis)\n\n\nAttaching package: 'robis'\n\nThe following object is masked from 'package:raster':\n\n    area",
    "crumbs": [
      "Tutorials",
      "Marine SDMs",
      "robis"
    ]
  },
  {
    "objectID": "r-tutorials/SDM-lab-robis.html#create-a-bounding-box",
    "href": "r-tutorials/SDM-lab-robis.html#create-a-bounding-box",
    "title": "SDM lab 1 - Downloading species data with robis",
    "section": "Create a bounding box",
    "text": "Create a bounding box\n\nbbox &lt;- sf::st_bbox(c(xmin = 41.875, xmax = 65.125, ymax = -0.125, ymin = 32.125), \n                          crs = sf::st_crs(4326))\n\nCreates a sf object with a sfs_POLYGON from which we can get a polygon string. We also use this for cropping with the raster package, while we will need bbox for cropping with the stars package.\n\nextent_polygon &lt;-  bbox %&gt;% sf::st_as_sfc() %&gt;% st_sf()\n\nThen for the robis package we need a bounding box string.\n\nwkt_geometry &lt;- extent_polygon$geometry %&gt;% st_as_text()\n\nMake a map of our region so we know we have the right area.\n\nworld &lt;- rnaturalearth::ne_countries(scale = \"medium\", returnclass = \"sf\")\nggplot(data = world) + geom_sf() +\n  geom_sf(data = extent_polygon, color = \"red\", fill=NA)",
    "crumbs": [
      "Tutorials",
      "Marine SDMs",
      "robis"
    ]
  },
  {
    "objectID": "r-tutorials/SDM-lab-robis.html#get-occurrence-data-from-robis",
    "href": "r-tutorials/SDM-lab-robis.html#get-occurrence-data-from-robis",
    "title": "SDM lab 1 - Downloading species data with robis",
    "section": "Get occurrence data from robis",
    "text": "Get occurrence data from robis\nWe will download data for four sea turtles found in the Arabian sea and save to one file. We will use the occurrence() function in the robis package.\n\nspp &lt;- c(\"Chelonia mydas\", \"Caretta caretta\", \"Eretmochelys imbricata\", \"Lepidochelys olivacea\", \"Natator depressus\", \"Dermochelys coriacea\")\n\nobs &lt;- robis::occurrence(spp, startdate = as.Date(\"2000-01-01\"), geometry = wkt_geometry)\n\nThis has many columns that we don’t need. We reduced to fewer columns.\n\ncols.to.use &lt;- c(\"occurrenceID\", \"scientificName\", \n                 \"dateIdentified\", \"eventDate\", \n                 \"decimalLatitude\", \"decimalLongitude\", \"coordinateUncertaintyInMeters\",\n                 \"individualCount\",\"lifeStage\", \"sex\",\n                 \"bathymetry\",  \"shoredistance\", \"sst\", \"sss\")\nobs &lt;- obs[,cols.to.use]\n\nWe also added a cleaner date with YYYY-MM-DD format.\n\nobs$date &lt;- as.Date(obs$eventDate)",
    "crumbs": [
      "Tutorials",
      "Marine SDMs",
      "robis"
    ]
  },
  {
    "objectID": "r-tutorials/SDM-lab-robis.html#save-our-data",
    "href": "r-tutorials/SDM-lab-robis.html#save-our-data",
    "title": "SDM lab 1 - Downloading species data with robis",
    "section": "Save our data",
    "text": "Save our data\nSet up the file names\n\ndir_data &lt;- here::here(\"data\")\nfil &lt;- here::here(\"data\", \"io-sea-turtles.csv\")\nreadr::write_csv(obs, file=fil)\n\nLater we can reload our data as\n\nfil &lt;- here::here(\"r-tutorials\", \"data\", \"io-sea-turtles.csv\")\nobs &lt;- read.csv(fil)\n\n\nClean the data\nSelect species.\n\n# subset the occurrences to include just those in the water\nobs &lt;- obs %&gt;% \n  subset(bathymetry &gt; 0 & shoredistance &gt; 0 & coordinateUncertaintyInMeters &lt; 200)\n\n# seeing how often each species occurs\ntable(obs$scientificName) \n\n\nCaretta caretta  Chelonia mydas \n           5141            7060",
    "crumbs": [
      "Tutorials",
      "Marine SDMs",
      "robis"
    ]
  },
  {
    "objectID": "r-tutorials/05-r-geospatial.html",
    "href": "r-tutorials/05-r-geospatial.html",
    "title": "Geospatial in R - lab 1",
    "section": "",
    "text": "require(sf)\nrequire(mapview)\nrequire(readr)\nrequire(readxl)\nhere::i_am(\"r-tutorials/05-r-geospatial.qmd\")\ndir_data &lt;- here::here(\"r-tutorials\", \"data\")\nIn this lab, you will learn basic skills of working with points. We will store our points in data frames. All our points data frames will have these columns:\nBut the columns names are often shortened to lon, lat or lng, latd or anything else.\nIn addition they will have other info in other columns.",
    "crumbs": [
      "Tutorials",
      "R and RStudio Basics",
      "Maps!"
    ]
  },
  {
    "objectID": "r-tutorials/05-r-geospatial.html#create-a-data-frame-of-points",
    "href": "r-tutorials/05-r-geospatial.html#create-a-data-frame-of-points",
    "title": "Geospatial in R - lab 1",
    "section": "Create a data frame of points",
    "text": "Create a data frame of points\n\nlibrary(mapview)\n  \n# Create example data of points\nlon &lt;- c(85.21, 80.23, 77.28)\nlat = c(25.59, 12.99, 28.56)\nnames = c(\"Patna\", \"Chennai\", \"New Delhi\")\n  \n# Create a data frame with the point data\ndf &lt;- data.frame(lon, lat, names)",
    "crumbs": [
      "Tutorials",
      "R and RStudio Basics",
      "Maps!"
    ]
  },
  {
    "objectID": "r-tutorials/05-r-geospatial.html#convert-to-a-spatial-points-data-frame",
    "href": "r-tutorials/05-r-geospatial.html#convert-to-a-spatial-points-data-frame",
    "title": "Geospatial in R - lab 1",
    "section": "Convert to a spatial points data frame",
    "text": "Convert to a spatial points data frame\n\n\n\n\n\n\nCore skill\n\n\n\nConvert data frame with latitude and longitude columns to a geospatial object with a geometry column and coordinate system. We are setting the coordinate system to WGS 84 with crs = 4326.\n\nsf::st_as_sf() function\n\n\n\nThis is a special data frame where the location data is converted to a single point object.\n\ncities3 &lt;- sf::st_as_sf(\n    df, # the data frame\n    coords = c(\"lon\", \"lat\"), # what are the x and y dimension names\n    crs = 4326)\n\nLook at the class of the object\n\nclass(cities3)\n\n[1] \"sf\"         \"data.frame\"",
    "crumbs": [
      "Tutorials",
      "R and RStudio Basics",
      "Maps!"
    ]
  },
  {
    "objectID": "r-tutorials/05-r-geospatial.html#plot-the-points",
    "href": "r-tutorials/05-r-geospatial.html#plot-the-points",
    "title": "Geospatial in R - lab 1",
    "section": "Plot the points",
    "text": "Plot the points\n\nplot(cities3)\n\n\n\n\nIt plotted but it is not very useful. Let’s use the helper package mapview. That’s more useful.\n\nmapview::mapview(cities3, label = cities3$names)",
    "crumbs": [
      "Tutorials",
      "R and RStudio Basics",
      "Maps!"
    ]
  },
  {
    "objectID": "r-tutorials/05-r-geospatial.html#read-in-points-from-files",
    "href": "r-tutorials/05-r-geospatial.html#read-in-points-from-files",
    "title": "Geospatial in R - lab 1",
    "section": "Read in points from files",
    "text": "Read in points from files\n\n\n\n\n\n\nCore skill\n\n\n\nRead in tabular data with latitude, longitude into a data frame.\n\nreadr::read_csv() or readxl::read_excel()",
    "crumbs": [
      "Tutorials",
      "R and RStudio Basics",
      "Maps!"
    ]
  },
  {
    "objectID": "r-tutorials/05-r-geospatial.html#from-a-csv-file",
    "href": "r-tutorials/05-r-geospatial.html#from-a-csv-file",
    "title": "Geospatial in R - lab 1",
    "section": "from a csv file",
    "text": "from a csv file\nHere I use a URL to a csv file. However I could use fil &lt;- file.path(\"data\", \"india_tide_guages.csv\") since I have the data file in a directory data in the same folder as my Quarto file (or RMarkdown or R script).\n\nfil &lt;- here::here(\"r-tutorials\", \"data\", \"india_tide_guages.csv\")\ndf2 &lt;- readr::read_csv(fil, show_col_types = FALSE)\n\nConvert to spatial data frame. Notice, I had to change the latitude and longitude to match the columns names in the dataframe.\n\nsdf &lt;- sf::st_as_sf(\n    df2, \n    coords = c(\"Longitude\", \"Latitude\"), # what are the x and y dimension names\n    crs = 4326)\n\nMap. You can click on the points to get more info.\n\nmapview::mapview(sdf)\n\n\n\n\n\nIf you want state labels, you need to only have the geometry and label columns in the dataframe.\n\nsdf2 &lt;- sdf %&gt;% select(geometry, State)\nmapview::mapview(sdf2, label = sdf2$State)\n\n\nfrom Excel file\n\nfil &lt;- here::here(\"r-tutorials\", \"data\", \"india_tide_guages.xlsx\")\ndf3 &lt;- readxl::read_excel(fil, sheet = \"Kerala\")\n\nConvert to spatial points.\n\nsdf = sf::st_as_sf(\n    df3, \n    coords = c(\"Longitude\", \"Latitude\"), \n    crs = 4326)\n\n\nmapview::mapview(sdf)",
    "crumbs": [
      "Tutorials",
      "R and RStudio Basics",
      "Maps!"
    ]
  },
  {
    "objectID": "r-tutorials/05-r-geospatial.html#using-ggplot2",
    "href": "r-tutorials/05-r-geospatial.html#using-ggplot2",
    "title": "Geospatial in R - lab 1",
    "section": "Using ggplot2",
    "text": "Using ggplot2\nHere is a gallery of some basic plots you can make. There are many ways to make maps with ggplot2. I will use a single approach that is fairly flexible.\n\n\n\n\n\n\nCore skill\n\n\n\nCreate a base world and India map from rnaturalearth. Plot with ggplot2.\n\nne_countries()\nggplot() + geom_sf()\ncoord_sf()\n\n\n\n\nIndia alone\n\nlibrary(ggplot2)\nlibrary(sf)\nlibrary(rnaturalearth)\nindia_sf &lt;- ne_countries(country = \"India\", scale = \"medium\", returnclass = \"sf\")\nbasemap &lt;- ggplot() + \n  geom_sf(data = india_sf, color = \"black\", size = 2, fill=\"green\") +\n  coord_sf(xlim = c(58, 98), ylim = c(6, 30))\nbasemap\n\n\n\n\n\nfil &lt;- here::here(\"r-tutorials\", \"data\", \"india.jpeg\")\nggsave(filename = fil, plot = basemap, device = \"jpeg\")\n\nSaving 7 x 5 in image\n\n\n\n\n\n\n\n\nCore skill\n\n\n\nAdd points to a plot.\n\ngeom_sf(data=points_df)\n\n\n\nAdd points\n\nbasemap + \n  geom_sf(data = cities3, aes(color = names), size = 3) +\n  theme_void()\n\nCoordinate system already present. Adding new coordinate system, which will\nreplace the existing one.\n\n\n\n\n\n\n\nThe world\n\nlibrary(ggplot2)\nlibrary(sf)\nlibrary(rnaturalearth)\nworld_sf &lt;- ne_countries(scale = \"medium\", returnclass = \"sf\")\nbasemap &lt;- ggplot() + \n  geom_sf(data = world_sf, color = \"black\", size = 0.2, fill=\"lightblue\")\nbasemap\n\n\n\n\nAdd points\n\nbasemap + \n  geom_sf(data = cities3, aes(color = names), size = 1)\n\n\n\n\nZoom in\n\nlibrary(ggplot2)\nlibrary(sf)\nlibrary(rnaturalearth)\nworld_sf &lt;- ne_countries(scale = \"medium\", returnclass = \"sf\")\nbasemap &lt;- ggplot() + \n  geom_sf(data = world_sf, color = \"black\", size = 0.2, fill=\"lightblue\") +\n  coord_sf(xlim = c(58, 98), ylim = c(0, 30))\nbasemap\n\n\n\n\nAdd points\n\nbasemap + \n  geom_sf(data = cities3, aes(color = names), size = 2) +\n  coord_sf(xlim = c(58, 98), ylim = c(0, 30))\n\nCoordinate system already present. Adding new coordinate system, which will\nreplace the existing one.",
    "crumbs": [
      "Tutorials",
      "R and RStudio Basics",
      "Maps!"
    ]
  },
  {
    "objectID": "r-tutorials/05-r-geospatial.html#change-the-projection",
    "href": "r-tutorials/05-r-geospatial.html#change-the-projection",
    "title": "Geospatial in R - lab 1",
    "section": "Change the projection",
    "text": "Change the projection\n\n\n\n\n\n\nCore skill\n\n\n\nApply a coordinate reference system to a sf object.\n\nst_transform(sf_object, crs=crs)\n\nCommon CRS’s\n\ncrs = 4326 WGS 84\nRobinson crs = \"+proj=robin +lon_0=0 +x_0=0 +y_0=0 +ellps=WGS84 +datum=WGS84 +units=m +no_defs\"\nGlobe crs = \"+proj=laea +lon_0=77 +lat_0=20 +ellps=WGS84 +no_defs\"\n\n\n\nMake a world in Robinson coord system.\n\nlibrary(ggplot2)\nlibrary(sf)\nlibrary(rnaturalearth)\nworld_sf &lt;- ne_countries(scale = \"medium\", returnclass = \"sf\")\ncrs &lt;- \"+proj=robin +lon_0=0 +x_0=0 +y_0=0 +ellps=WGS84 +datum=WGS84 +units=m +no_defs\" \nworld_crs &lt;- st_transform(world_sf, crs=crs)\nbasemap &lt;- ggplot() +\n  geom_sf(data = world_crs, color = \"black\", size = 0.2, fill=\"lightblue\") +\n  theme_minimal()\nbasemap\n\n\n\n\nAdd points.\n\ncities3_robin &lt;- st_transform(cities3, crs=crs)\nbasemap + \n  geom_sf(data = cities3_robin, aes(color = names), size = 2)\n\n\n\n\nMake a globe.\n\nlibrary(ggplot2)\nlibrary(sf)\nlibrary(rnaturalearth)\nworld_sf &lt;- ne_countries(scale = \"medium\", returnclass = \"sf\")\ncrs &lt;- \"+proj=laea +lon_0=77 +lat_0=20 +ellps=WGS84 +no_defs\"\nworld_crs &lt;- sf::st_transform(world_sf, crs)\nbasemap &lt;- ggplot() +\n  geom_sf(data = world_crs, color = \"black\", size = 0.2, fill=\"lightblue\") +\n  theme_minimal()\nbasemap\n\n\n\n\nAdd points.\n\ncities3_crs &lt;- st_transform(cities3, crs=crs)\nbasemap + \n  geom_sf(data = cities3_crs, aes(color = names), size = 2)\n\n\n\n\nAdd a circle around the globe.\n\nlibrary(ggplot2)\nlibrary(sf)\nlibrary(rnaturalearth)\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nworld_sf &lt;- ne_countries(scale = \"medium\", returnclass = \"sf\")\ncrs &lt;- \"+proj=laea +lon_0=77 +lat_0=20 +ellps=WGS84 +no_defs\"\nworld_crs &lt;- sf::st_transform(world_sf, crs)\n\nsphere &lt;- st_graticule(ndiscr = 10000, margin = 10e-6) %&gt;%\n  st_transform(crs = crs) %&gt;%\n  st_convex_hull() %&gt;%\n  summarise(geometry = st_union(geometry))\n\nbasemap &lt;- ggplot()  +\n  geom_sf(data = sphere, fill = \"#D8F4FF\", alpha = 0.7) +\n  geom_sf(data = world_crs, fill=\"grey\") +\n  theme_bw()\nbasemap\n\n\n\n\nAdd points and remove legend.\n\n# Add crs\ncities3_crs &lt;- st_transform(cities3, crs=crs)\nbasemap + \n  geom_sf(data = cities3_crs, aes(color = names), size = 1) +\n  theme(legend.position = \"none\")",
    "crumbs": [
      "Tutorials",
      "R and RStudio Basics",
      "Maps!"
    ]
  },
  {
    "objectID": "r-tutorials/05-r-geospatial.html#getting-help-from-chatgpt",
    "href": "r-tutorials/05-r-geospatial.html#getting-help-from-chatgpt",
    "title": "Geospatial in R - lab 1",
    "section": "Getting help from ChatGPT",
    "text": "Getting help from ChatGPT\nUnfortunately, ChatGPT often gets confused with mapping and gives you code that doesn’t fully work. This can be hard for beginners (and experts) to debug.\nTry telling it\n\nUse only the sf, rnaturalearth, and ggplot2 packages\nWork in steps, “Make a sf points object and name it sf_points”, “Using sf_points”, add these points to a map of the world.”",
    "crumbs": [
      "Tutorials",
      "R and RStudio Basics",
      "Maps!"
    ]
  },
  {
    "objectID": "r-tutorials/05-r-geospatial.html#your-turn",
    "href": "r-tutorials/05-r-geospatial.html#your-turn",
    "title": "Geospatial in R - lab 1",
    "section": "Your Turn!",
    "text": "Your Turn!\nMake some maps using mapview of your own data, data in the “r-tutorials/data” directory or data you can find on-line.\nTry the layer feature to change the base map.",
    "crumbs": [
      "Tutorials",
      "R and RStudio Basics",
      "Maps!"
    ]
  },
  {
    "objectID": "r-tutorials/05-r-geospatial.html#advanced-programmers",
    "href": "r-tutorials/05-r-geospatial.html#advanced-programmers",
    "title": "Geospatial in R - lab 1",
    "section": "Advanced programmers",
    "text": "Advanced programmers\nTry using customizing mapview to create some pretty maps of the tide guage data!\nHere are some ideas\n\nhttps://www.paulamoraga.com/book-spatial/making-maps-with-r.html#mapview\nArticles tab here https://r-spatial.github.io/mapview/index.html\nThis shows a nicer example of maps with ggplot https://r-spatial.org/r/2018/10/25/ggplot2-sf.html.\nHere are some examples of maps I made in R. Can you adapt the globe example to show India and add the tide guage points? https://eeholmes.github.io/30Maps/",
    "crumbs": [
      "Tutorials",
      "R and RStudio Basics",
      "Maps!"
    ]
  },
  {
    "objectID": "r-tutorials/04-r-ggplot2.html",
    "href": "r-tutorials/04-r-ggplot2.html",
    "title": "R - ggplot2",
    "section": "",
    "text": "This tutorial will give an introduction to plotting with the ggplot2 package, which is the modern way to make general plots with R. I will talk tomorrow about plots of spatial data.\nThis is oriented to those with little or no experience using ggplot2 or those who have tried it and gotten frustrated. The goal is to get you started. Once you have the basics, there are a many many ggplot tutorials online – though personally the basics are all I need.\nThe goal is to give you enough of an understanding of ggplot2 so that you can create prompts for ChatGPT. You need to know the basics of ggplot2 to understand ChatGPT’s code and know what sort of things to ask ChatGPT. But if you are lost, you can always tell it what you want and ask it to ask you questions.",
    "crumbs": [
      "Tutorials",
      "R and RStudio Basics",
      "Plotting with ggplot2"
    ]
  },
  {
    "objectID": "r-tutorials/04-r-ggplot2.html#load-the-library",
    "href": "r-tutorials/04-r-ggplot2.html#load-the-library",
    "title": "R - ggplot2",
    "section": "Load the library",
    "text": "Load the library\n\nlibrary(ggplot2)",
    "crumbs": [
      "Tutorials",
      "R and RStudio Basics",
      "Plotting with ggplot2"
    ]
  },
  {
    "objectID": "r-tutorials/04-r-ggplot2.html#basic-x-versus-y-plot",
    "href": "r-tutorials/04-r-ggplot2.html#basic-x-versus-y-plot",
    "title": "R - ggplot2",
    "section": "Basic x versus y plot",
    "text": "Basic x versus y plot\n\nWorkflow\nLet’s say you wanted to make a simple x versus y plot. Here’s your ggplot() workflow.\n\nDecide on x.\nDecide on y.\nMake a data frame with x and y.\nCreate the plot object with a call to ggplot() to tell it the data frame and then what the x and y to use: aes(x=..., y=...).\nAdd points or lines to the plot with geom_line() or geom_point().\n\n\n\nCreate data frame\n\nval &lt;- mtcars$mpg\nx &lt;- mtcars$hp\ndf &lt;- data.frame(x=x, val=val)\n\n\n\nggplot call\n\np1 &lt;- ggplot(df, aes(x=x, y=val)) + \n        geom_point() # plot a line\np1 # plot it\n\n\n\n\np1 is a ggplot object.\n\nclass(p1)\n\n[1] \"gg\"     \"ggplot\"\n\n\nWe can add elements to a ggplot object\n\np1 + geom_line()\n\n\n\n\n\n\n\n\n\n\nAn x y plot\n\n\n\nggplot(df, aes(x=x, y=val)) +\n geom_point() +\n geom_line()",
    "crumbs": [
      "Tutorials",
      "R and RStudio Basics",
      "Plotting with ggplot2"
    ]
  },
  {
    "objectID": "r-tutorials/04-r-ggplot2.html#changing-the-whole-look",
    "href": "r-tutorials/04-r-ggplot2.html#changing-the-whole-look",
    "title": "R - ggplot2",
    "section": "Changing the whole look",
    "text": "Changing the whole look\nggplot uses themes to set the look of your plot and you can change the whole look by setting a different theme. You can also just tweak one element of the plot’s existing theme. Note because we fixed the line and point colors, we override some theme elements (eg, line colors). See ?theme_bw to see all the themes. See ?theme to learn how to change one element of your plot design.\n\np1 + theme_classic()",
    "crumbs": [
      "Tutorials",
      "R and RStudio Basics",
      "Plotting with ggplot2"
    ]
  },
  {
    "objectID": "r-tutorials/04-r-ggplot2.html#using-chatgpt",
    "href": "r-tutorials/04-r-ggplot2.html#using-chatgpt",
    "title": "R - ggplot2",
    "section": "Using ChatGPT",
    "text": "Using ChatGPT\n\nBoxplots\nNow that you see the basic structure of a simple x versus y plot, let’s use ChatGPT to help us make plots.\n\n\n\n\n\n\nPrompt\n\n\n\nI have a data frame with species, gender and bill length. I want to make a boxplot by species with ggplot2 where the genders are separate and side by side.\n\n\nThis what ChatGPT gave me. Try using this prompt and adding more info until you get a plot you like.\n\n# Load the ggplot2 library\nlibrary(ggplot2)\n\n# Sample data frame (replace with your actual data)\ndata &lt;- data.frame(\n  species = c(\"Species A\", \"Species A\", \"Species B\", \"Species B\", \"Species C\", \"Species C\"),\n  gender = c(\"Male\", \"Female\", \"Male\", \"Female\", \"Male\", \"Female\"),\n  bill_length = c(10.2, 9.8, 11.5, 10.7, 9.3, 9.9)\n)\n\n# Create the side-by-side boxplot\nggplot(data, aes(x = species, y = bill_length, fill = gender)) +\n  geom_boxplot(position = position_dodge(width = 0.8)) +\n  labs(title = \"Boxplot of Bill Length by Species and Gender\",\n       x = \"Species\",\n       y = \"Bill Length\") +\n  scale_fill_manual(values = c(\"Male\" = \"blue\", \"Female\" = \"red\"))\n\n\n\n\n\n\nAdding trends\n\n\n\n\n\n\nPrompt\n\n\n\nI have a data frame with value and time for 3 regions. I want to make a scatter plot of time versus value for each region and add a linear regression to each. I would like each region to be in a separate panel. Show only the code. Please make a sample data frame that I can use to run the code.\n\n\nChatGPT gives:\n\n# Load the required libraries\nlibrary(ggplot2)\nlibrary(dplyr)\n\n# Create a sample data frame\ndf &lt;- data.frame(\n  Region = rep(c(\"Region A\", \"Region B\", \"Region C\"), each = 100),\n  Time = rep(seq(1, 100), times = 3),\n  Value = c(rnorm(100, mean = 50, sd = 10),\n            rnorm(100, mean = 70, sd = 12),\n            rnorm(100, mean = 60, sd = 15))\n)\n\n# Create the scatter plot with separate panels for each region\nggplot(df, aes(x = Time, y = Value, color = Region)) +\n  geom_point() +\n  geom_smooth(method = \"lm\") +\n  labs(title = \"Scatter Plot with Linear Regression Lines\",\n       x = \"Time\",\n       y = \"Value\") +\n  facet_wrap(~Region)\n\n\n\n\n\n\nGiving your data to ChatGPT\nYou can paste in the top of your data (if it is csv) and ChatGPT can figure out the structure. Then you can say what you want to do with this data.\nDemo",
    "crumbs": [
      "Tutorials",
      "R and RStudio Basics",
      "Plotting with ggplot2"
    ]
  },
  {
    "objectID": "r-tutorials/04-r-ggplot2.html#arranging-plots-into-grids",
    "href": "r-tutorials/04-r-ggplot2.html#arranging-plots-into-grids",
    "title": "R - ggplot2",
    "section": "Arranging plots into grids",
    "text": "Arranging plots into grids\n\nDynamically\nggplot() will dynamically break your data into plots for you using facet_wrap().\n\ndf1 &lt;- data.frame(t=1:10, val=10:1, name=\"df1\")\ndf2 &lt;- data.frame(t=1:20, val=1:20+100, name=\"df2\")\ndf &lt;- rbind(df1, df2)\np1 &lt;- ggplot(df, aes(x=t, y=val, col=name)) + \n  geom_line() +\n  facet_wrap(~name)\np1\n\n\n\n\nWe can “free” the scales.\n\np1 + facet_wrap(~name, scales=\"free\")\n\n\n\n\n\n\nManually\nWe can arrange plots into a grid manually with gridExtra::grid.arrange() (from the gridExtra package).\n\nlibrary(gridExtra)\ndf &lt;- data.frame(t = 1:10, val = rnorm(10))\np1 &lt;- ggplot(df, aes(x=t, y=val)) + geom_line() + geom_point()\np2 &lt;- ggplot(df, aes(x=t, y=val+100)) + geom_line()\np3 &lt;- ggplot(df, aes(x=t, y=100*val)) + geom_point()\ndf2 &lt;- data.frame(t = 1:4, val = 1:4, se=.1*(1:4))\np4 &lt;- ggplot(df2, aes(x=t, y=val)) + geom_col()\ngridExtra::grid.arrange(p1, p2, p3, p4)\n\n\n\n\nInstead of gridExtra and grid.arrange(), check out the patchwork package. It does similar jobs as grid.arrange() but is better.\nLet’s do two plots in one column but add new info to plot 1.\n\ngridExtra::grid.arrange(p1+geom_smooth(method=\"lm\"), \n                        p4+geom_errorbar(aes(ymin=val-se, ymax=val+se), width=0.3), ncol=1)\n\n`geom_smooth()` using formula = 'y ~ x'",
    "crumbs": [
      "Tutorials",
      "R and RStudio Basics",
      "Plotting with ggplot2"
    ]
  },
  {
    "objectID": "r-tutorials/04-r-ggplot2.html#getting-your-data-in-the-right-format",
    "href": "r-tutorials/04-r-ggplot2.html#getting-your-data-in-the-right-format",
    "title": "R - ggplot2",
    "section": "Getting your data in the right format",
    "text": "Getting your data in the right format\nYou need your data in long format.\nLet’s say you had temperature data for 3 rivers. It would be common to have your data like this. This is wide format.\nDate, River1, River2, River3\n1, 45, 35, 20\n2, 25, 30, 21\nYou need to get this in long format like so.\nDate, River, Temperature\n1, River1, 45\n1, River2, 35\n1. River3, 20\n2, River1, 25\n2, River2, 30\n2, River3, 21\nYou do this with code not manually! This is hard to remember how to do, so just ask ChatGPT. Pass the data into ChatGPT and ask it how to put it in long format.\n\n\n\n\n\n\nPrompt\n\n\n\nhow can I read a text string of my data in\n\n\n\n# Your data as a text string\ndata_string &lt;- \"Date, River1, River2, River3\n1, 45, 35, 20\n2, 25, 30, 21\"\n\n# Use read.table() to read the data from the text string\ndf &lt;- read.table(text = data_string, header = TRUE, sep = \",\")\n\n# Print the resulting data frame\nprint(df)\n\n  Date River1 River2 River3\n1    1     45     35     20\n2    2     25     30     21\n\n\n\n\n\n\n\n\nPrompt\n\n\n\nHow do I convert that to long format\n\n\n\nlibrary(tidyr)\ndf_long &lt;- gather(df, key = \"River\", value = \"Value\", -Date)\ndf_long\n\n  Date  River Value\n1    1 River1    45\n2    2 River1    25\n3    1 River2    35\n4    2 River2    30\n5    1 River3    20\n6    2 River3    21",
    "crumbs": [
      "Tutorials",
      "R and RStudio Basics",
      "Plotting with ggplot2"
    ]
  },
  {
    "objectID": "r-tutorials/04-r-ggplot2.html#creating-plot-templates",
    "href": "r-tutorials/04-r-ggplot2.html#creating-plot-templates",
    "title": "R - ggplot2",
    "section": "Creating plot templates",
    "text": "Creating plot templates\nIf you are creating plots with the same features over and over, you can hold the features in a list and add that on to your plot.\nExample where I want all my plots to have red points and a regression line:\n\np3 &lt;- geom_point(col=\"red\")\np2 &lt;- geom_smooth(method=\"lm\")\np4 &lt;- xlab(\"MPG\")\np5 &lt;- ylab(\"HP\")\n# pt is my template\npt &lt;- list(p2, p3, p4, p5)\n\np1 &lt;- ggplot(mtcars, aes(x=mpg, y=hp)) + pt\np2 &lt;- ggplot(mpg, aes(x=hwy,y=cyl)) + pt\ngrid.arrange(p1, p2)",
    "crumbs": [
      "Tutorials",
      "R and RStudio Basics",
      "Plotting with ggplot2"
    ]
  },
  {
    "objectID": "python-notebooks/04_Combining_Data.html#loading-data",
    "href": "python-notebooks/04_Combining_Data.html#loading-data",
    "title": "Combine, clean, and export the data",
    "section": "Loading data",
    "text": "Loading data\nBegin by loading all the data into disk\n\nimport xarray as xr\nimport pandas as pd\nimport numpy as np\nfrom datetime import datetime, timedelta\nimport matplotlib.pyplot as plt\n\n\n# era5 = xr.combine_by_coords([xr.open_mfdataset('demonstrated data/era5/eastward_wind_at_10_metres/*.nc'),\n#                              xr.open_mfdataset('demonstrated data/era5/northward_wind_at_10_metres/*.nc')])\n\nera5 = xr.open_mfdataset('demonstrated data/era5/*/*.nc')\nsalinity = xr.open_dataset('demonstrated data/salinity_at_0_49m.nc')\nsst = xr.open_mfdataset('demonstrated data/earth_data/*/*.nc')['analysed_sst'] # we only have one collection in earth_data directory currently",
    "crumbs": [
      "Python Tutorials",
      "IO Zarr Database",
      "Combine, clean, and export the data"
    ]
  },
  {
    "objectID": "python-notebooks/04_Combining_Data.html#process-era5-data-and-calculate-speeddirection",
    "href": "python-notebooks/04_Combining_Data.html#process-era5-data-and-calculate-speeddirection",
    "title": "Combine, clean, and export the data",
    "section": "Process ERA5 data and calculate speed/direction",
    "text": "Process ERA5 data and calculate speed/direction\n\n# adding calculated variables (direction and speed)\nera5 = era5.assign(speed = np.sqrt(era5.eastward_wind_at_10_metres**2 + era5.northward_wind_at_10_metres**2))\nera5 = era5.assign(direction = np.rad2deg(np.arctan2(era5.northward_wind_at_10_metres, era5.eastward_wind_at_10_metres)))\n\n# coarsen ()\nera5 = era5.coarsen(time0=24).mean(keep_attrs=True).compute()\n\n\nera5 = era5.rename({'time0': 'time', \n                    'eastward_wind_at_10_metres': 'u_wind', \n                    'northward_wind_at_10_metres': 'v_wind'})\n\n\n# fix time coordinate by resetting to 12AM\nera5 = era5.assign_coords(time=np.arange('2003-01-01', '2003-03-01', timedelta(days=1), dtype='datetime64[ns]')) # again, right-exclusive\n\n\nera5\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt;\nDimensions:    (time: 59, lat: 81, lon: 81)\nCoordinates:\n  * lon        (lon) float32 60.0 60.25 60.5 60.75 ... 79.25 79.5 79.75 80.0\n  * lat        (lat) float32 25.0 24.75 24.5 24.25 24.0 ... 5.75 5.5 5.25 5.0\n  * time       (time) datetime64[ns] 2003-01-01 2003-01-02 ... 2003-02-28\nData variables:\n    u_wind     (time, lat, lon) float32 3.128 2.865 2.555 ... -3.737 -4.479\n    v_wind     (time, lat, lon) float32 0.6016 0.6589 0.6068 ... -1.914 -2.474\n    speed      (time, lat, lon) float32 3.945 3.645 3.368 ... 3.216 4.227 5.153\n    direction  (time, lat, lon) float32 22.31 17.58 13.01 ... -153.4 -151.6\nAttributes:\n    source:       Reanalysis\n    institution:  ECMWF\n    tilte:        ERA5 forecastsxarray.DatasetDimensions:time: 59lat: 81lon: 81Coordinates: (3)lon(lon)float3260.0 60.25 60.5 ... 79.5 79.75 80.0standard_name :longitudelong_name :longitudeunits :degrees_eastarray([60.  , 60.25, 60.5 , 60.75, 61.  , 61.25, 61.5 , 61.75, 62.  , 62.25,\n       62.5 , 62.75, 63.  , 63.25, 63.5 , 63.75, 64.  , 64.25, 64.5 , 64.75,\n       65.  , 65.25, 65.5 , 65.75, 66.  , 66.25, 66.5 , 66.75, 67.  , 67.25,\n       67.5 , 67.75, 68.  , 68.25, 68.5 , 68.75, 69.  , 69.25, 69.5 , 69.75,\n       70.  , 70.25, 70.5 , 70.75, 71.  , 71.25, 71.5 , 71.75, 72.  , 72.25,\n       72.5 , 72.75, 73.  , 73.25, 73.5 , 73.75, 74.  , 74.25, 74.5 , 74.75,\n       75.  , 75.25, 75.5 , 75.75, 76.  , 76.25, 76.5 , 76.75, 77.  , 77.25,\n       77.5 , 77.75, 78.  , 78.25, 78.5 , 78.75, 79.  , 79.25, 79.5 , 79.75,\n       80.  ], dtype=float32)lat(lat)float3225.0 24.75 24.5 ... 5.5 5.25 5.0standard_name :latitudelong_name :latitudeunits :degrees_northarray([25.  , 24.75, 24.5 , 24.25, 24.  , 23.75, 23.5 , 23.25, 23.  , 22.75,\n       22.5 , 22.25, 22.  , 21.75, 21.5 , 21.25, 21.  , 20.75, 20.5 , 20.25,\n       20.  , 19.75, 19.5 , 19.25, 19.  , 18.75, 18.5 , 18.25, 18.  , 17.75,\n       17.5 , 17.25, 17.  , 16.75, 16.5 , 16.25, 16.  , 15.75, 15.5 , 15.25,\n       15.  , 14.75, 14.5 , 14.25, 14.  , 13.75, 13.5 , 13.25, 13.  , 12.75,\n       12.5 , 12.25, 12.  , 11.75, 11.5 , 11.25, 11.  , 10.75, 10.5 , 10.25,\n       10.  ,  9.75,  9.5 ,  9.25,  9.  ,  8.75,  8.5 ,  8.25,  8.  ,  7.75,\n        7.5 ,  7.25,  7.  ,  6.75,  6.5 ,  6.25,  6.  ,  5.75,  5.5 ,  5.25,\n        5.  ], dtype=float32)time(time)datetime64[ns]2003-01-01 ... 2003-02-28array(['2003-01-01T00:00:00.000000000', '2003-01-02T00:00:00.000000000',\n       '2003-01-03T00:00:00.000000000', '2003-01-04T00:00:00.000000000',\n       '2003-01-05T00:00:00.000000000', '2003-01-06T00:00:00.000000000',\n       '2003-01-07T00:00:00.000000000', '2003-01-08T00:00:00.000000000',\n       '2003-01-09T00:00:00.000000000', '2003-01-10T00:00:00.000000000',\n       '2003-01-11T00:00:00.000000000', '2003-01-12T00:00:00.000000000',\n       '2003-01-13T00:00:00.000000000', '2003-01-14T00:00:00.000000000',\n       '2003-01-15T00:00:00.000000000', '2003-01-16T00:00:00.000000000',\n       '2003-01-17T00:00:00.000000000', '2003-01-18T00:00:00.000000000',\n       '2003-01-19T00:00:00.000000000', '2003-01-20T00:00:00.000000000',\n       '2003-01-21T00:00:00.000000000', '2003-01-22T00:00:00.000000000',\n       '2003-01-23T00:00:00.000000000', '2003-01-24T00:00:00.000000000',\n       '2003-01-25T00:00:00.000000000', '2003-01-26T00:00:00.000000000',\n       '2003-01-27T00:00:00.000000000', '2003-01-28T00:00:00.000000000',\n       '2003-01-29T00:00:00.000000000', '2003-01-30T00:00:00.000000000',\n       '2003-01-31T00:00:00.000000000', '2003-02-01T00:00:00.000000000',\n       '2003-02-02T00:00:00.000000000', '2003-02-03T00:00:00.000000000',\n       '2003-02-04T00:00:00.000000000', '2003-02-05T00:00:00.000000000',\n       '2003-02-06T00:00:00.000000000', '2003-02-07T00:00:00.000000000',\n       '2003-02-08T00:00:00.000000000', '2003-02-09T00:00:00.000000000',\n       '2003-02-10T00:00:00.000000000', '2003-02-11T00:00:00.000000000',\n       '2003-02-12T00:00:00.000000000', '2003-02-13T00:00:00.000000000',\n       '2003-02-14T00:00:00.000000000', '2003-02-15T00:00:00.000000000',\n       '2003-02-16T00:00:00.000000000', '2003-02-17T00:00:00.000000000',\n       '2003-02-18T00:00:00.000000000', '2003-02-19T00:00:00.000000000',\n       '2003-02-20T00:00:00.000000000', '2003-02-21T00:00:00.000000000',\n       '2003-02-22T00:00:00.000000000', '2003-02-23T00:00:00.000000000',\n       '2003-02-24T00:00:00.000000000', '2003-02-25T00:00:00.000000000',\n       '2003-02-26T00:00:00.000000000', '2003-02-27T00:00:00.000000000',\n       '2003-02-28T00:00:00.000000000'], dtype='datetime64[ns]')Data variables: (4)u_wind(time, lat, lon)float323.128 2.865 2.555 ... -3.737 -4.479standard_name :eastward_windunits :m s**-1long_name :10 metre U wind componentnameECMWF :10 metre U wind componentshortNameECMWF :10unameCDM :10_metre_U_wind_component_surfaceproduct_type :analysisarray([[[ 3.1276042 ,  2.8645833 ,  2.5546875 , ...,  0.8776042 ,\n          0.9036458 ,  0.8072917 ],\n        [ 3.1223958 ,  2.8854167 ,  2.5703125 , ...,  0.6015625 ,\n          0.6380208 ,  0.6848958 ],\n        [ 2.5026042 ,  2.2864583 ,  2.0598958 , ...,  0.3828125 ,\n          0.390625  ,  0.45833334],\n        ...,\n        [-2.7916667 , -2.8932292 , -3.0182292 , ...,  0.04947917,\n          0.21614583,  0.23177083],\n        [-2.6380208 , -2.7317708 , -2.8567708 , ..., -0.02604167,\n          0.06510416,  0.05729167],\n        [-2.53125   , -2.6354167 , -2.7682292 , ..., -0.19270833,\n         -0.09114584, -0.07291666]],\n\n       [[-0.41666666, -0.7369792 , -1.0208334 , ..., -1.4401041 ,\n         -1.5677084 , -1.5677084 ],\n        [ 0.12239584, -0.13541667, -0.39583334, ..., -1.4348959 ,\n         -1.453125  , -1.3541666 ],\n        [ 0.6953125 ,  0.5234375 ,  0.44791666, ..., -1.3515625 ,\n         -1.2317709 , -1.140625  ],\n...\n        [-3.0833333 , -3.0286458 , -2.9739583 , ..., -0.8567708 ,\n         -1.21875   , -1.7447916 ],\n        [-3.0364583 , -2.9973958 , -2.9401042 , ..., -0.9322917 ,\n         -1.3854166 , -1.9401041 ],\n        [-3.0026042 , -2.9739583 , -2.9322917 , ..., -1.0442709 ,\n         -1.4401041 , -1.8255209 ]],\n\n       [[ 8.1119795 ,  8.020833  ,  7.950521  , ..., -1.0338541 ,\n         -1.0494791 , -1.0703125 ],\n        [ 7.78125   ,  7.7890625 ,  7.734375  , ..., -0.7942708 ,\n         -0.7135417 , -0.6015625 ],\n        [ 7.205729  ,  7.294271  ,  7.325521  , ..., -0.5026042 ,\n         -0.38020834, -0.29166666],\n        ...,\n        [-1.4270834 , -1.3619791 , -1.3125    , ..., -1.6119791 ,\n         -2.5520833 , -3.8203125 ],\n        [-1.2604166 , -1.2005209 , -1.1328125 , ..., -2.3958333 ,\n         -3.390625  , -4.5390625 ],\n        [-1.0625    , -1.0208334 , -0.9739583 , ..., -2.8697917 ,\n         -3.7369792 , -4.4791665 ]]], dtype=float32)v_wind(time, lat, lon)float320.6016 0.6589 ... -1.914 -2.474standard_name :northward_windunits :m s**-1long_name :10 metre V wind componentnameECMWF :10 metre V wind componentshortNameECMWF :10vnameCDM :10_metre_V_wind_component_surfaceproduct_type :analysisarray([[[ 6.0156250e-01,  6.5885419e-01,  6.0677081e-01, ...,\n         -1.3671875e+00, -1.1848959e+00, -8.8020831e-01],\n        [ 1.0729166e+00,  1.1536459e+00,  1.0807291e+00, ...,\n         -1.4791666e+00, -1.3463541e+00, -1.1796875e+00],\n        [ 1.1510416e+00,  1.2838541e+00,  1.3229166e+00, ...,\n         -1.5963541e+00, -1.4869791e+00, -1.3489584e+00],\n        ...,\n        [-3.6770833e+00, -3.8229167e+00, -3.9713542e+00, ...,\n         -2.3437500e+00, -1.3020834e+00, -3.7760416e-01],\n        [-3.7083333e+00, -3.8255208e+00, -3.9088542e+00, ...,\n         -2.3593750e+00, -1.5390625e+00, -7.4739581e-01],\n        [-3.7395833e+00, -3.7526042e+00, -3.7812500e+00, ...,\n         -2.4817708e+00, -1.6953125e+00, -1.1484375e+00]],\n\n       [[-1.5208334e+00, -1.7005209e+00, -1.9192709e+00, ...,\n         -3.9062500e-01, -4.2708334e-01, -4.9218750e-01],\n        [-1.9010416e+00, -1.9583334e+00, -2.1458333e+00, ...,\n         -5.0260419e-01, -5.8593750e-01, -5.0260419e-01],\n        [-2.2838542e+00, -2.2473958e+00, -2.2109375e+00, ...,\n         -4.9479166e-01, -6.6145831e-01, -6.0937500e-01],\n...\n        [-4.0729165e+00, -4.1276040e+00, -4.1770835e+00, ...,\n         -2.1171875e+00, -1.6562500e+00, -1.3151041e+00],\n        [-4.0885415e+00, -4.1484375e+00, -4.2005210e+00, ...,\n         -2.4244792e+00, -2.1119792e+00, -1.8515625e+00],\n        [-4.0625000e+00, -4.1197915e+00, -4.1614585e+00, ...,\n         -2.7031250e+00, -2.4166667e+00, -2.2708333e+00]],\n\n       [[-1.9791667e-01,  5.2083335e-03,  1.5625000e-01, ...,\n         -1.2760417e-01, -1.6145833e-01, -1.6927083e-01],\n        [-7.9687500e-01, -5.9895831e-01, -4.3750000e-01, ...,\n          5.4687500e-02,  4.6875000e-02,  1.5364583e-01],\n        [-1.2994791e+00, -1.0937500e+00, -9.4010419e-01, ...,\n          2.2395833e-01,  2.9166666e-01,  3.8020834e-01],\n        ...,\n        [-3.0625000e+00, -3.0781250e+00, -3.1197917e+00, ...,\n          5.4687500e-01,  1.2500000e-01, -8.0729169e-01],\n        [-3.1614583e+00, -3.2005208e+00, -3.2291667e+00, ...,\n         -4.1666666e-01, -1.0598959e+00, -1.8958334e+00],\n        [-3.2526042e+00, -3.3255208e+00, -3.3750000e+00, ...,\n         -1.3333334e+00, -1.9140625e+00, -2.4739583e+00]]], dtype=float32)speed(time, lat, lon)float323.945 3.645 3.368 ... 4.227 5.153array([[[3.9451437, 3.6454449, 3.36847  , ..., 1.9201735, 1.8865218,\n         1.7468839],\n        [4.1419544, 3.8504055, 3.444007 , ..., 1.9789158, 1.9153827,\n         1.8310242],\n        [3.6950111, 3.4435928, 3.1019478, ..., 1.9867468, 1.9142274,\n         1.8317447],\n        ...,\n        [4.7469373, 4.912411 , 5.092304 , ..., 2.5135314, 1.6478343,\n         1.3388939],\n        [4.676735 , 4.8191776, 4.95795  , ..., 2.5241547, 1.7888355,\n         1.2424096],\n        [4.636933 , 4.7195234, 4.826424 , ..., 2.6338756, 1.8901653,\n         1.4170161]],\n\n       [[2.3318148, 2.4910347, 2.7374954, ..., 1.7411982, 1.8597933,\n         1.8728681],\n        [2.414438 , 2.5525272, 2.7947886, ..., 1.7888421, 1.8435212,\n         1.7328191],\n        [2.7723866, 2.8588107, 2.8829317, ..., 1.7411242, 1.7196132,\n         1.6104554],\n...\n        [5.146253 , 5.1557384, 5.1632495, ..., 2.8367107, 2.591439 ,\n         2.534239 ],\n        [5.1336794, 5.1560197, 5.166705 , ..., 3.0493386, 2.972432 ,\n         3.014616 ],\n        [5.0962915, 5.1267576, 5.1389093, ..., 3.263656 , 3.196616 ,\n         3.23878  ]],\n\n       [[8.189649 , 8.106084 , 8.037755 , ..., 2.1850507, 2.2752473,\n         2.2316103],\n        [7.892883 , 7.890354 , 7.823316 , ..., 2.3158002, 2.3716338,\n         2.317237 ],\n        [7.391702 , 7.4489727, 7.4578147, ..., 2.337489 , 2.231067 ,\n         2.291389 ],\n        ...,\n        [3.4753053, 3.467406 , 3.4899578, ..., 1.9244485, 2.6942132,\n         4.00572  ],\n        [3.5153275, 3.533966 , 3.542756 , ..., 2.549701 , 3.6089706,\n         4.9639993],\n        [3.5520408, 3.6114044, 3.6468   , ..., 3.2160883, 4.226849 ,\n         5.1525497]]], dtype=float32)direction(time, lat, lon)float3222.31 17.58 13.01 ... -153.4 -151.6standard_name :northward_windunits :m s**-1long_name :10 metre V wind componentnameECMWF :10 metre V wind componentshortNameECMWF :10vnameCDM :10_metre_V_wind_component_surfaceproduct_type :analysisarray([[[  22.31256  ,   17.577126 ,   13.005119 , ...,  -67.38259  ,\n          -47.755325 ,  -33.74233  ],\n        [  30.220772 ,   29.43544  ,   26.357203 , ...,  -78.10914  ,\n          -64.685646 ,  -49.11764  ],\n        [  35.783054 ,   37.048702 ,   35.60695  , ...,  -85.95185  ,\n          -86.35613  ,  -70.15734  ],\n        ...,\n        [-126.93241  , -126.620476 , -126.65524  , ...,  -89.24673  ,\n          -80.60198  ,   -4.201843 ],\n        [-125.02897  , -124.928856 , -125.43902  , ...,  -90.54617  ,\n          -84.9731   ,  -69.18275  ],\n        [-123.4584   , -124.29117  , -125.278015 , ...,  -93.996056 ,\n          -90.138794 ,  -88.29269  ]],\n\n       [[ -72.070206 ,  -85.1071   ,  -93.38763  , ...,  -58.74728  ,\n          -60.06917  ,  -58.698284 ],\n        [ -80.372795 ,  -90.8068   ,  -96.67983  , ...,  -58.435246 ,\n          -56.67389  ,  -58.751236 ],\n        [ -80.680115 ,  -86.12191  ,  -88.40796  , ...,  -59.58118  ,\n          -53.925976 ,  -70.68571  ],\n...\n        [-127.28055  , -126.49218  , -125.74938  , ...,  -71.24183  ,\n          -79.70399  , -133.6834   ],\n        [-126.68684  , -125.9416   , -125.15563  , ..., -114.46243  ,\n         -107.83289  , -131.2433   ],\n        [-126.467865 , -125.800575 , -125.19381  , ..., -113.44105  ,\n         -120.908356 , -126.25626  ]],\n\n       [[  -0.9108308,    0.405311 ,    1.362003 , ...,  -42.883907 ,\n          -38.677364 ,  -36.70583  ],\n        [  -5.2671247,   -3.8980722,   -2.9317954, ...,  -26.818964 ,\n          -36.38267  ,   -2.3517277],\n        [  -9.45855  ,   -7.810347 ,   -6.7994595, ...,    9.280067 ,\n            2.028537 ,   22.812065 ],\n        ...,\n        [-115.06766  , -113.99908  , -113.05724  , ...,   26.856642 ,\n           10.176036 ,  -94.912285 ],\n        [-111.756325 , -110.610634 , -109.46108  , ...,  -81.22135  ,\n         -133.58519  , -158.10306  ],\n        [-108.30612  , -107.21555  , -106.26769  , ..., -140.94438  ,\n         -153.4189   , -151.60954  ]]], dtype=float32)Indexes: (3)lonPandasIndexPandasIndex(Index([ 60.0, 60.25,  60.5, 60.75,  61.0, 61.25,  61.5, 61.75,  62.0, 62.25,\n        62.5, 62.75,  63.0, 63.25,  63.5, 63.75,  64.0, 64.25,  64.5, 64.75,\n        65.0, 65.25,  65.5, 65.75,  66.0, 66.25,  66.5, 66.75,  67.0, 67.25,\n        67.5, 67.75,  68.0, 68.25,  68.5, 68.75,  69.0, 69.25,  69.5, 69.75,\n        70.0, 70.25,  70.5, 70.75,  71.0, 71.25,  71.5, 71.75,  72.0, 72.25,\n        72.5, 72.75,  73.0, 73.25,  73.5, 73.75,  74.0, 74.25,  74.5, 74.75,\n        75.0, 75.25,  75.5, 75.75,  76.0, 76.25,  76.5, 76.75,  77.0, 77.25,\n        77.5, 77.75,  78.0, 78.25,  78.5, 78.75,  79.0, 79.25,  79.5, 79.75,\n        80.0],\n      dtype='float32', name='lon'))latPandasIndexPandasIndex(Index([ 25.0, 24.75,  24.5, 24.25,  24.0, 23.75,  23.5, 23.25,  23.0, 22.75,\n        22.5, 22.25,  22.0, 21.75,  21.5, 21.25,  21.0, 20.75,  20.5, 20.25,\n        20.0, 19.75,  19.5, 19.25,  19.0, 18.75,  18.5, 18.25,  18.0, 17.75,\n        17.5, 17.25,  17.0, 16.75,  16.5, 16.25,  16.0, 15.75,  15.5, 15.25,\n        15.0, 14.75,  14.5, 14.25,  14.0, 13.75,  13.5, 13.25,  13.0, 12.75,\n        12.5, 12.25,  12.0, 11.75,  11.5, 11.25,  11.0, 10.75,  10.5, 10.25,\n        10.0,  9.75,   9.5,  9.25,   9.0,  8.75,   8.5,  8.25,   8.0,  7.75,\n         7.5,  7.25,   7.0,  6.75,   6.5,  6.25,   6.0,  5.75,   5.5,  5.25,\n         5.0],\n      dtype='float32', name='lat'))timePandasIndexPandasIndex(DatetimeIndex(['2003-01-01', '2003-01-02', '2003-01-03', '2003-01-04',\n               '2003-01-05', '2003-01-06', '2003-01-07', '2003-01-08',\n               '2003-01-09', '2003-01-10', '2003-01-11', '2003-01-12',\n               '2003-01-13', '2003-01-14', '2003-01-15', '2003-01-16',\n               '2003-01-17', '2003-01-18', '2003-01-19', '2003-01-20',\n               '2003-01-21', '2003-01-22', '2003-01-23', '2003-01-24',\n               '2003-01-25', '2003-01-26', '2003-01-27', '2003-01-28',\n               '2003-01-29', '2003-01-30', '2003-01-31', '2003-02-01',\n               '2003-02-02', '2003-02-03', '2003-02-04', '2003-02-05',\n               '2003-02-06', '2003-02-07', '2003-02-08', '2003-02-09',\n               '2003-02-10', '2003-02-11', '2003-02-12', '2003-02-13',\n               '2003-02-14', '2003-02-15', '2003-02-16', '2003-02-17',\n               '2003-02-18', '2003-02-19', '2003-02-20', '2003-02-21',\n               '2003-02-22', '2003-02-23', '2003-02-24', '2003-02-25',\n               '2003-02-26', '2003-02-27', '2003-02-28'],\n              dtype='datetime64[ns]', name='time', freq=None))Attributes: (3)source :Reanalysisinstitution :ECMWFtilte :ERA5 forecasts",
    "crumbs": [
      "Python Tutorials",
      "IO Zarr Database",
      "Combine, clean, and export the data"
    ]
  },
  {
    "objectID": "python-notebooks/04_Combining_Data.html#process-mur-data",
    "href": "python-notebooks/04_Combining_Data.html#process-mur-data",
    "title": "Combine, clean, and export the data",
    "section": "Process MUR data",
    "text": "Process MUR data\n\nsst\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray 'analysed_sst' (time: 59, lat: 80, lon: 80)&gt;\ndask.array&lt;concatenate, shape=(59, 80, 80), dtype=float32, chunksize=(31, 80, 80), chunktype=numpy.ndarray&gt;\nCoordinates:\n  * time     (time) datetime64[ns] 2003-01-01T09:00:00 ... 2003-02-28T09:00:00\n  * lat      (lat) float32 5.125 5.375 5.625 5.875 ... 24.12 24.38 24.62 24.88\n  * lon      (lon) float32 60.12 60.38 60.62 60.88 ... 79.12 79.38 79.62 79.88\nAttributes:\n    long_name:              analysed sea surface temperature\n    standard_name:          sea_surface_foundation_temperature\n    coverage_content_type:  physicalMeasurement\n    units:                  kelvin\n    valid_min:              -32767\n    valid_max:              32767\n    comment:                \"Final\" version using Multi-Resolution Variationa...\n    source:                 MODIS_T-JPL, MODIS_A-JPL, AMSRE-REMSS, AVHRR17_G-...xarray.DataArray'analysed_sst'time: 59lat: 80lon: 80dask.array&lt;chunksize=(31, 80, 80), meta=np.ndarray&gt;\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n1.44 MiB\n775.00 kiB\n\n\nShape\n(59, 80, 80)\n(31, 80, 80)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\nCoordinates: (3)time(time)datetime64[ns]2003-01-01T09:00:00 ... 2003-02-...long_name :reference time of sst fieldstandard_name :timecoverage_content_type :coordinateaxis :Tcomment :Nominal time of analyzed fieldsarray(['2003-01-01T09:00:00.000000000', '2003-01-02T09:00:00.000000000',\n       '2003-01-03T09:00:00.000000000', '2003-01-04T09:00:00.000000000',\n       '2003-01-05T09:00:00.000000000', '2003-01-06T09:00:00.000000000',\n       '2003-01-07T09:00:00.000000000', '2003-01-08T09:00:00.000000000',\n       '2003-01-09T09:00:00.000000000', '2003-01-10T09:00:00.000000000',\n       '2003-01-11T09:00:00.000000000', '2003-01-12T09:00:00.000000000',\n       '2003-01-13T09:00:00.000000000', '2003-01-14T09:00:00.000000000',\n       '2003-01-15T09:00:00.000000000', '2003-01-16T09:00:00.000000000',\n       '2003-01-17T09:00:00.000000000', '2003-01-18T09:00:00.000000000',\n       '2003-01-19T09:00:00.000000000', '2003-01-20T09:00:00.000000000',\n       '2003-01-21T09:00:00.000000000', '2003-01-22T09:00:00.000000000',\n       '2003-01-23T09:00:00.000000000', '2003-01-24T09:00:00.000000000',\n       '2003-01-25T09:00:00.000000000', '2003-01-26T09:00:00.000000000',\n       '2003-01-27T09:00:00.000000000', '2003-01-28T09:00:00.000000000',\n       '2003-01-29T09:00:00.000000000', '2003-01-30T09:00:00.000000000',\n       '2003-01-31T09:00:00.000000000', '2003-02-01T09:00:00.000000000',\n       '2003-02-02T09:00:00.000000000', '2003-02-03T09:00:00.000000000',\n       '2003-02-04T09:00:00.000000000', '2003-02-05T09:00:00.000000000',\n       '2003-02-06T09:00:00.000000000', '2003-02-07T09:00:00.000000000',\n       '2003-02-08T09:00:00.000000000', '2003-02-09T09:00:00.000000000',\n       '2003-02-10T09:00:00.000000000', '2003-02-11T09:00:00.000000000',\n       '2003-02-12T09:00:00.000000000', '2003-02-13T09:00:00.000000000',\n       '2003-02-14T09:00:00.000000000', '2003-02-15T09:00:00.000000000',\n       '2003-02-16T09:00:00.000000000', '2003-02-17T09:00:00.000000000',\n       '2003-02-18T09:00:00.000000000', '2003-02-19T09:00:00.000000000',\n       '2003-02-20T09:00:00.000000000', '2003-02-21T09:00:00.000000000',\n       '2003-02-22T09:00:00.000000000', '2003-02-23T09:00:00.000000000',\n       '2003-02-24T09:00:00.000000000', '2003-02-25T09:00:00.000000000',\n       '2003-02-26T09:00:00.000000000', '2003-02-27T09:00:00.000000000',\n       '2003-02-28T09:00:00.000000000'], dtype='datetime64[ns]')lat(lat)float325.125 5.375 5.625 ... 24.62 24.88long_name :latitudestandard_name :latitudecoverage_content_type :coordinateaxis :Yunits :degrees_northvalid_min :-90.0valid_max :90.0comment :geolocations inherited from the input data without correctionarray([ 5.125,  5.375,  5.625,  5.875,  6.125,  6.375,  6.625,  6.875,  7.125,\n        7.375,  7.625,  7.875,  8.125,  8.375,  8.625,  8.875,  9.125,  9.375,\n        9.625,  9.875, 10.125, 10.375, 10.625, 10.875, 11.125, 11.375, 11.625,\n       11.875, 12.125, 12.375, 12.625, 12.875, 13.125, 13.375, 13.625, 13.875,\n       14.125, 14.375, 14.625, 14.875, 15.125, 15.375, 15.625, 15.875, 16.125,\n       16.375, 16.625, 16.875, 17.125, 17.375, 17.625, 17.875, 18.125, 18.375,\n       18.625, 18.875, 19.125, 19.375, 19.625, 19.875, 20.125, 20.375, 20.625,\n       20.875, 21.125, 21.375, 21.625, 21.875, 22.125, 22.375, 22.625, 22.875,\n       23.125, 23.375, 23.625, 23.875, 24.125, 24.375, 24.625, 24.875],\n      dtype=float32)lon(lon)float3260.12 60.38 60.62 ... 79.62 79.88long_name :longitudestandard_name :longitudecoverage_content_type :coordinateaxis :Xunits :degrees_eastvalid_min :-180.0valid_max :180.0comment :geolocations inherited from the input data without correctionarray([60.125, 60.375, 60.625, 60.875, 61.125, 61.375, 61.625, 61.875, 62.125,\n       62.375, 62.625, 62.875, 63.125, 63.375, 63.625, 63.875, 64.125, 64.375,\n       64.625, 64.875, 65.125, 65.375, 65.625, 65.875, 66.125, 66.375, 66.625,\n       66.875, 67.125, 67.375, 67.625, 67.875, 68.125, 68.375, 68.625, 68.875,\n       69.125, 69.375, 69.625, 69.875, 70.125, 70.375, 70.625, 70.875, 71.125,\n       71.375, 71.625, 71.875, 72.125, 72.375, 72.625, 72.875, 73.125, 73.375,\n       73.625, 73.875, 74.125, 74.375, 74.625, 74.875, 75.125, 75.375, 75.625,\n       75.875, 76.125, 76.375, 76.625, 76.875, 77.125, 77.375, 77.625, 77.875,\n       78.125, 78.375, 78.625, 78.875, 79.125, 79.375, 79.625, 79.875],\n      dtype=float32)Indexes: (3)timePandasIndexPandasIndex(DatetimeIndex(['2003-01-01 09:00:00', '2003-01-02 09:00:00',\n               '2003-01-03 09:00:00', '2003-01-04 09:00:00',\n               '2003-01-05 09:00:00', '2003-01-06 09:00:00',\n               '2003-01-07 09:00:00', '2003-01-08 09:00:00',\n               '2003-01-09 09:00:00', '2003-01-10 09:00:00',\n               '2003-01-11 09:00:00', '2003-01-12 09:00:00',\n               '2003-01-13 09:00:00', '2003-01-14 09:00:00',\n               '2003-01-15 09:00:00', '2003-01-16 09:00:00',\n               '2003-01-17 09:00:00', '2003-01-18 09:00:00',\n               '2003-01-19 09:00:00', '2003-01-20 09:00:00',\n               '2003-01-21 09:00:00', '2003-01-22 09:00:00',\n               '2003-01-23 09:00:00', '2003-01-24 09:00:00',\n               '2003-01-25 09:00:00', '2003-01-26 09:00:00',\n               '2003-01-27 09:00:00', '2003-01-28 09:00:00',\n               '2003-01-29 09:00:00', '2003-01-30 09:00:00',\n               '2003-01-31 09:00:00', '2003-02-01 09:00:00',\n               '2003-02-02 09:00:00', '2003-02-03 09:00:00',\n               '2003-02-04 09:00:00', '2003-02-05 09:00:00',\n               '2003-02-06 09:00:00', '2003-02-07 09:00:00',\n               '2003-02-08 09:00:00', '2003-02-09 09:00:00',\n               '2003-02-10 09:00:00', '2003-02-11 09:00:00',\n               '2003-02-12 09:00:00', '2003-02-13 09:00:00',\n               '2003-02-14 09:00:00', '2003-02-15 09:00:00',\n               '2003-02-16 09:00:00', '2003-02-17 09:00:00',\n               '2003-02-18 09:00:00', '2003-02-19 09:00:00',\n               '2003-02-20 09:00:00', '2003-02-21 09:00:00',\n               '2003-02-22 09:00:00', '2003-02-23 09:00:00',\n               '2003-02-24 09:00:00', '2003-02-25 09:00:00',\n               '2003-02-26 09:00:00', '2003-02-27 09:00:00',\n               '2003-02-28 09:00:00'],\n              dtype='datetime64[ns]', name='time', freq=None))latPandasIndexPandasIndex(Index([ 5.125,  5.375,  5.625,  5.875,  6.125,  6.375,  6.625,  6.875,  7.125,\n        7.375,  7.625,  7.875,  8.125,  8.375,  8.625,  8.875,  9.125,  9.375,\n        9.625,  9.875, 10.125, 10.375, 10.625, 10.875, 11.125, 11.375, 11.625,\n       11.875, 12.125, 12.375, 12.625, 12.875, 13.125, 13.375, 13.625, 13.875,\n       14.125, 14.375, 14.625, 14.875, 15.125, 15.375, 15.625, 15.875, 16.125,\n       16.375, 16.625, 16.875, 17.125, 17.375, 17.625, 17.875, 18.125, 18.375,\n       18.625, 18.875, 19.125, 19.375, 19.625, 19.875, 20.125, 20.375, 20.625,\n       20.875, 21.125, 21.375, 21.625, 21.875, 22.125, 22.375, 22.625, 22.875,\n       23.125, 23.375, 23.625, 23.875, 24.125, 24.375, 24.625, 24.875],\n      dtype='float32', name='lat'))lonPandasIndexPandasIndex(Index([60.125, 60.375, 60.625, 60.875, 61.125, 61.375, 61.625, 61.875, 62.125,\n       62.375, 62.625, 62.875, 63.125, 63.375, 63.625, 63.875, 64.125, 64.375,\n       64.625, 64.875, 65.125, 65.375, 65.625, 65.875, 66.125, 66.375, 66.625,\n       66.875, 67.125, 67.375, 67.625, 67.875, 68.125, 68.375, 68.625, 68.875,\n       69.125, 69.375, 69.625, 69.875, 70.125, 70.375, 70.625, 70.875, 71.125,\n       71.375, 71.625, 71.875, 72.125, 72.375, 72.625, 72.875, 73.125, 73.375,\n       73.625, 73.875, 74.125, 74.375, 74.625, 74.875, 75.125, 75.375, 75.625,\n       75.875, 76.125, 76.375, 76.625, 76.875, 77.125, 77.375, 77.625, 77.875,\n       78.125, 78.375, 78.625, 78.875, 79.125, 79.375, 79.625, 79.875],\n      dtype='float32', name='lon'))Attributes: (8)long_name :analysed sea surface temperaturestandard_name :sea_surface_foundation_temperaturecoverage_content_type :physicalMeasurementunits :kelvinvalid_min :-32767valid_max :32767comment :\"Final\" version using Multi-Resolution Variational Analysis (MRVA) method for interpolationsource :MODIS_T-JPL, MODIS_A-JPL, AMSRE-REMSS, AVHRR17_G-ACSPO, iQUAM-NOAA/NESDIS, Ice_Conc-OSISAF\n\n\n\nInterpolation\nYou can notice that the MUR sea surface temperature data, while having the same 0.25x0.25 deg spatial resolution as the ERA5 data, has a different point offset and we cannot combine them together without some interpolation. Xarray’s interp_like() function is designed to help us achieve this feat easily!\nDisclaimer: Interpolation may remove important outlier data (as you can see in the map below). Make sure to choose interpolation type wisely.\nfix time\nSince we don’t want to interpolate time (as time is already averaged over the day), let’s fix it to center at 12AM\n\nsst = sst.assign_coords(time=np.arange('2003-01-01', '2003-03-01', timedelta(days=1), dtype='datetime64[ns]'))\n\n\nsst_interp = sst.interp_like(era5)\n\nDISCLAIMER: make sure that the three coordinate names are identical on both datasets for xarray to infer correctly\n\n# verify before and after interpolation\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 8)) # 2 columns & 1 row, width, height \nsst.isel(time=0).plot.imshow(ax=ax1)\nax1.set_title('before interpolation')\nax1.set_aspect('equal')\n\nsst_interp.isel(time=0).plot.imshow(ax=ax2)\nax2.set_title('after interpolation')\nax2.set_aspect('equal')",
    "crumbs": [
      "Python Tutorials",
      "IO Zarr Database",
      "Combine, clean, and export the data"
    ]
  },
  {
    "objectID": "python-notebooks/04_Combining_Data.html#process-copernicus-salinity-data",
    "href": "python-notebooks/04_Combining_Data.html#process-copernicus-salinity-data",
    "title": "Combine, clean, and export the data",
    "section": "Process Copernicus Salinity data",
    "text": "Process Copernicus Salinity data\n\nsalinity # also need interpolation\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt;\nDimensions:    (longitude: 241, latitude: 241, time: 59)\nCoordinates:\n  * longitude  (longitude) float32 60.0 60.08 60.17 60.25 ... 79.83 79.92 80.0\n  * latitude   (latitude) float32 5.0 5.083 5.167 5.25 ... 24.83 24.92 25.0\n  * time       (time) datetime64[ns] 2003-01-01T12:00:00 ... 2003-02-28T12:00:00\nData variables:\n    so         (time, latitude, longitude) float32 ...xarray.DatasetDimensions:longitude: 241latitude: 241time: 59Coordinates: (3)longitude(longitude)float3260.0 60.08 60.17 ... 79.92 80.0valid_min :-180.0valid_max :179.91667step :0.08332825units :degrees_eastunit_long :Degrees Eastlong_name :Longitudestandard_name :longitudeaxis :X_ChunkSizes :4320array([60.      , 60.083332, 60.166668, ..., 79.833336, 79.916664, 80.      ],\n      dtype=float32)latitude(latitude)float325.0 5.083 5.167 ... 24.92 25.0valid_min :-80.0valid_max :90.0step :0.08333588units :degrees_northunit_long :Degrees Northlong_name :Latitudestandard_name :latitudeaxis :Y_ChunkSizes :2041array([ 5.      ,  5.083333,  5.166667, ..., 24.833334, 24.916666, 25.      ],\n      dtype=float32)time(time)datetime64[ns]2003-01-01T12:00:00 ... 2003-02-...long_name :Time (hours since 1950-01-01)standard_name :timeaxis :T_ChunkSizes :1024array(['2003-01-01T12:00:00.000000000', '2003-01-02T12:00:00.000000000',\n       '2003-01-03T12:00:00.000000000', '2003-01-04T12:00:00.000000000',\n       '2003-01-05T12:00:00.000000000', '2003-01-06T12:00:00.000000000',\n       '2003-01-07T12:00:00.000000000', '2003-01-08T12:00:00.000000000',\n       '2003-01-09T12:00:00.000000000', '2003-01-10T12:00:00.000000000',\n       '2003-01-11T12:00:00.000000000', '2003-01-12T12:00:00.000000000',\n       '2003-01-13T12:00:00.000000000', '2003-01-14T12:00:00.000000000',\n       '2003-01-15T12:00:00.000000000', '2003-01-16T12:00:00.000000000',\n       '2003-01-17T12:00:00.000000000', '2003-01-18T12:00:00.000000000',\n       '2003-01-19T12:00:00.000000000', '2003-01-20T12:00:00.000000000',\n       '2003-01-21T12:00:00.000000000', '2003-01-22T12:00:00.000000000',\n       '2003-01-23T12:00:00.000000000', '2003-01-24T12:00:00.000000000',\n       '2003-01-25T12:00:00.000000000', '2003-01-26T12:00:00.000000000',\n       '2003-01-27T12:00:00.000000000', '2003-01-28T12:00:00.000000000',\n       '2003-01-29T12:00:00.000000000', '2003-01-30T12:00:00.000000000',\n       '2003-01-31T12:00:00.000000000', '2003-02-01T12:00:00.000000000',\n       '2003-02-02T12:00:00.000000000', '2003-02-03T12:00:00.000000000',\n       '2003-02-04T12:00:00.000000000', '2003-02-05T12:00:00.000000000',\n       '2003-02-06T12:00:00.000000000', '2003-02-07T12:00:00.000000000',\n       '2003-02-08T12:00:00.000000000', '2003-02-09T12:00:00.000000000',\n       '2003-02-10T12:00:00.000000000', '2003-02-11T12:00:00.000000000',\n       '2003-02-12T12:00:00.000000000', '2003-02-13T12:00:00.000000000',\n       '2003-02-14T12:00:00.000000000', '2003-02-15T12:00:00.000000000',\n       '2003-02-16T12:00:00.000000000', '2003-02-17T12:00:00.000000000',\n       '2003-02-18T12:00:00.000000000', '2003-02-19T12:00:00.000000000',\n       '2003-02-20T12:00:00.000000000', '2003-02-21T12:00:00.000000000',\n       '2003-02-22T12:00:00.000000000', '2003-02-23T12:00:00.000000000',\n       '2003-02-24T12:00:00.000000000', '2003-02-25T12:00:00.000000000',\n       '2003-02-26T12:00:00.000000000', '2003-02-27T12:00:00.000000000',\n       '2003-02-28T12:00:00.000000000'], dtype='datetime64[ns]')Data variables: (1)so(time, latitude, longitude)float32...long_name :Salinitystandard_name :sea_water_salinityunits :1e-3unit_long :Practical Salinity Unitvalid_min :1valid_max :28336cell_methods :area: mean_ChunkSizes :[  1   7 341 720][3426779 values with dtype=float32]Indexes: (3)longitudePandasIndexPandasIndex(Index([             60.0, 60.08333206176758, 60.16666793823242,\n                   60.25, 60.33333206176758, 60.41666793823242,\n                    60.5, 60.58333206176758, 60.66666793823242,\n                   60.75,\n       ...\n                   79.25, 79.33333587646484, 79.41666412353516,\n                    79.5, 79.58333587646484, 79.66666412353516,\n                   79.75, 79.83333587646484, 79.91666412353516,\n                    80.0],\n      dtype='float32', name='longitude', length=241))latitudePandasIndexPandasIndex(Index([              5.0, 5.083333492279053, 5.166666507720947,\n                    5.25, 5.333333492279053, 5.416666507720947,\n                     5.5, 5.583333492279053, 5.666666507720947,\n                    5.75,\n       ...\n                   24.25, 24.33333396911621, 24.41666603088379,\n                    24.5, 24.58333396911621, 24.66666603088379,\n                   24.75, 24.83333396911621, 24.91666603088379,\n                    25.0],\n      dtype='float32', name='latitude', length=241))timePandasIndexPandasIndex(DatetimeIndex(['2003-01-01 12:00:00', '2003-01-02 12:00:00',\n               '2003-01-03 12:00:00', '2003-01-04 12:00:00',\n               '2003-01-05 12:00:00', '2003-01-06 12:00:00',\n               '2003-01-07 12:00:00', '2003-01-08 12:00:00',\n               '2003-01-09 12:00:00', '2003-01-10 12:00:00',\n               '2003-01-11 12:00:00', '2003-01-12 12:00:00',\n               '2003-01-13 12:00:00', '2003-01-14 12:00:00',\n               '2003-01-15 12:00:00', '2003-01-16 12:00:00',\n               '2003-01-17 12:00:00', '2003-01-18 12:00:00',\n               '2003-01-19 12:00:00', '2003-01-20 12:00:00',\n               '2003-01-21 12:00:00', '2003-01-22 12:00:00',\n               '2003-01-23 12:00:00', '2003-01-24 12:00:00',\n               '2003-01-25 12:00:00', '2003-01-26 12:00:00',\n               '2003-01-27 12:00:00', '2003-01-28 12:00:00',\n               '2003-01-29 12:00:00', '2003-01-30 12:00:00',\n               '2003-01-31 12:00:00', '2003-02-01 12:00:00',\n               '2003-02-02 12:00:00', '2003-02-03 12:00:00',\n               '2003-02-04 12:00:00', '2003-02-05 12:00:00',\n               '2003-02-06 12:00:00', '2003-02-07 12:00:00',\n               '2003-02-08 12:00:00', '2003-02-09 12:00:00',\n               '2003-02-10 12:00:00', '2003-02-11 12:00:00',\n               '2003-02-12 12:00:00', '2003-02-13 12:00:00',\n               '2003-02-14 12:00:00', '2003-02-15 12:00:00',\n               '2003-02-16 12:00:00', '2003-02-17 12:00:00',\n               '2003-02-18 12:00:00', '2003-02-19 12:00:00',\n               '2003-02-20 12:00:00', '2003-02-21 12:00:00',\n               '2003-02-22 12:00:00', '2003-02-23 12:00:00',\n               '2003-02-24 12:00:00', '2003-02-25 12:00:00',\n               '2003-02-26 12:00:00', '2003-02-27 12:00:00',\n               '2003-02-28 12:00:00'],\n              dtype='datetime64[ns]', name='time', freq=None))Attributes: (0)\n\n\n\nInterpolate\nYou can notice that the coordinates are not the same to era5 dataset, so let’s change it\n\nsalinity = salinity.rename({'longitude': 'lon', 'latitude': 'lat'})\n\n\n# fix time frame, again right-exclusive\nsalinity = salinity.assign_coords(time=np.arange('2003-01-01', '2003-03-01', timedelta(days=1), dtype='datetime64[ns]'))\n\n\nsalinity\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt;\nDimensions:  (lon: 241, lat: 241, time: 59)\nCoordinates:\n  * lon      (lon) float32 60.0 60.08 60.17 60.25 ... 79.75 79.83 79.92 80.0\n  * lat      (lat) float32 5.0 5.083 5.167 5.25 5.333 ... 24.75 24.83 24.92 25.0\n  * time     (time) datetime64[ns] 2003-01-01 2003-01-02 ... 2003-02-28\nData variables:\n    so       (time, lat, lon) float32 ...xarray.DatasetDimensions:lon: 241lat: 241time: 59Coordinates: (3)lon(lon)float3260.0 60.08 60.17 ... 79.92 80.0valid_min :-180.0valid_max :179.91667step :0.08332825units :degrees_eastunit_long :Degrees Eastlong_name :Longitudestandard_name :longitudeaxis :X_ChunkSizes :4320array([60.      , 60.083332, 60.166668, ..., 79.833336, 79.916664, 80.      ],\n      dtype=float32)lat(lat)float325.0 5.083 5.167 ... 24.92 25.0valid_min :-80.0valid_max :90.0step :0.08333588units :degrees_northunit_long :Degrees Northlong_name :Latitudestandard_name :latitudeaxis :Y_ChunkSizes :2041array([ 5.      ,  5.083333,  5.166667, ..., 24.833334, 24.916666, 25.      ],\n      dtype=float32)time(time)datetime64[ns]2003-01-01 ... 2003-02-28array(['2003-01-01T00:00:00.000000000', '2003-01-02T00:00:00.000000000',\n       '2003-01-03T00:00:00.000000000', '2003-01-04T00:00:00.000000000',\n       '2003-01-05T00:00:00.000000000', '2003-01-06T00:00:00.000000000',\n       '2003-01-07T00:00:00.000000000', '2003-01-08T00:00:00.000000000',\n       '2003-01-09T00:00:00.000000000', '2003-01-10T00:00:00.000000000',\n       '2003-01-11T00:00:00.000000000', '2003-01-12T00:00:00.000000000',\n       '2003-01-13T00:00:00.000000000', '2003-01-14T00:00:00.000000000',\n       '2003-01-15T00:00:00.000000000', '2003-01-16T00:00:00.000000000',\n       '2003-01-17T00:00:00.000000000', '2003-01-18T00:00:00.000000000',\n       '2003-01-19T00:00:00.000000000', '2003-01-20T00:00:00.000000000',\n       '2003-01-21T00:00:00.000000000', '2003-01-22T00:00:00.000000000',\n       '2003-01-23T00:00:00.000000000', '2003-01-24T00:00:00.000000000',\n       '2003-01-25T00:00:00.000000000', '2003-01-26T00:00:00.000000000',\n       '2003-01-27T00:00:00.000000000', '2003-01-28T00:00:00.000000000',\n       '2003-01-29T00:00:00.000000000', '2003-01-30T00:00:00.000000000',\n       '2003-01-31T00:00:00.000000000', '2003-02-01T00:00:00.000000000',\n       '2003-02-02T00:00:00.000000000', '2003-02-03T00:00:00.000000000',\n       '2003-02-04T00:00:00.000000000', '2003-02-05T00:00:00.000000000',\n       '2003-02-06T00:00:00.000000000', '2003-02-07T00:00:00.000000000',\n       '2003-02-08T00:00:00.000000000', '2003-02-09T00:00:00.000000000',\n       '2003-02-10T00:00:00.000000000', '2003-02-11T00:00:00.000000000',\n       '2003-02-12T00:00:00.000000000', '2003-02-13T00:00:00.000000000',\n       '2003-02-14T00:00:00.000000000', '2003-02-15T00:00:00.000000000',\n       '2003-02-16T00:00:00.000000000', '2003-02-17T00:00:00.000000000',\n       '2003-02-18T00:00:00.000000000', '2003-02-19T00:00:00.000000000',\n       '2003-02-20T00:00:00.000000000', '2003-02-21T00:00:00.000000000',\n       '2003-02-22T00:00:00.000000000', '2003-02-23T00:00:00.000000000',\n       '2003-02-24T00:00:00.000000000', '2003-02-25T00:00:00.000000000',\n       '2003-02-26T00:00:00.000000000', '2003-02-27T00:00:00.000000000',\n       '2003-02-28T00:00:00.000000000'], dtype='datetime64[ns]')Data variables: (1)so(time, lat, lon)float32...long_name :Salinitystandard_name :sea_water_salinityunits :1e-3unit_long :Practical Salinity Unitvalid_min :1valid_max :28336cell_methods :area: mean_ChunkSizes :[  1   7 341 720][3426779 values with dtype=float32]Indexes: (3)lonPandasIndexPandasIndex(Index([             60.0, 60.08333206176758, 60.16666793823242,\n                   60.25, 60.33333206176758, 60.41666793823242,\n                    60.5, 60.58333206176758, 60.66666793823242,\n                   60.75,\n       ...\n                   79.25, 79.33333587646484, 79.41666412353516,\n                    79.5, 79.58333587646484, 79.66666412353516,\n                   79.75, 79.83333587646484, 79.91666412353516,\n                    80.0],\n      dtype='float32', name='lon', length=241))latPandasIndexPandasIndex(Index([              5.0, 5.083333492279053, 5.166666507720947,\n                    5.25, 5.333333492279053, 5.416666507720947,\n                     5.5, 5.583333492279053, 5.666666507720947,\n                    5.75,\n       ...\n                   24.25, 24.33333396911621, 24.41666603088379,\n                    24.5, 24.58333396911621, 24.66666603088379,\n                   24.75, 24.83333396911621, 24.91666603088379,\n                    25.0],\n      dtype='float32', name='lat', length=241))timePandasIndexPandasIndex(DatetimeIndex(['2003-01-01', '2003-01-02', '2003-01-03', '2003-01-04',\n               '2003-01-05', '2003-01-06', '2003-01-07', '2003-01-08',\n               '2003-01-09', '2003-01-10', '2003-01-11', '2003-01-12',\n               '2003-01-13', '2003-01-14', '2003-01-15', '2003-01-16',\n               '2003-01-17', '2003-01-18', '2003-01-19', '2003-01-20',\n               '2003-01-21', '2003-01-22', '2003-01-23', '2003-01-24',\n               '2003-01-25', '2003-01-26', '2003-01-27', '2003-01-28',\n               '2003-01-29', '2003-01-30', '2003-01-31', '2003-02-01',\n               '2003-02-02', '2003-02-03', '2003-02-04', '2003-02-05',\n               '2003-02-06', '2003-02-07', '2003-02-08', '2003-02-09',\n               '2003-02-10', '2003-02-11', '2003-02-12', '2003-02-13',\n               '2003-02-14', '2003-02-15', '2003-02-16', '2003-02-17',\n               '2003-02-18', '2003-02-19', '2003-02-20', '2003-02-21',\n               '2003-02-22', '2003-02-23', '2003-02-24', '2003-02-25',\n               '2003-02-26', '2003-02-27', '2003-02-28'],\n              dtype='datetime64[ns]', name='time', freq=None))Attributes: (0)\n\n\n\nsalinity_interp = salinity.interp_like(era5)\n\n\n# verify before and after interpolation\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 8)) # 2 columns & 1 row, width, height \nsalinity['so'].isel(time=0).plot.imshow(ax=ax1)\nax1.set_title('before interpolation')\nax1.set_aspect('equal')\n\nsalinity_interp['so'].isel(time=0).plot.imshow(ax=ax2)\nax2.set_title('after interpolation')\nax2.set_aspect('equal')",
    "crumbs": [
      "Python Tutorials",
      "IO Zarr Database",
      "Combine, clean, and export the data"
    ]
  },
  {
    "objectID": "python-notebooks/04_Combining_Data.html#combine-files-together",
    "href": "python-notebooks/04_Combining_Data.html#combine-files-together",
    "title": "Combine, clean, and export the data",
    "section": "Combine files together",
    "text": "Combine files together\nDue to different attributes in each dataset coordinates, we cannot combine all datasets together by coords by simply using the combine_by_coords() function in xarray. Issue is replicated below:\n\nfinal_ds = xr.combine_by_coords([era5, salinity_interp, sst_interp])\n\n\nfinal_ds\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt;\nDimensions:       (time: 59, lat: 81, lon: 81)\nCoordinates:\n  * time          (time) datetime64[ns] 2003-01-01 2003-01-02 ... 2003-02-28\n  * lat           (lat) float32 25.0 24.75 24.5 24.25 24.0 ... 5.75 5.5 5.25 5.0\n  * lon           (lon) float32 60.0 60.25 60.5 60.75 ... 79.25 79.5 79.75 80.0\nData variables:\n    analysed_sst  (time, lat, lon) float32 dask.array&lt;chunksize=(59, 81, 81), meta=np.ndarray&gt;\n    u_wind        (time, lat, lon) float32 3.128 2.865 2.555 ... -3.737 -4.479\n    v_wind        (time, lat, lon) float32 0.6016 0.6589 ... -1.914 -2.474\n    speed         (time, lat, lon) float32 3.945 3.645 3.368 ... 4.227 5.153\n    direction     (time, lat, lon) float32 22.31 17.58 13.01 ... -153.4 -151.6\n    so            (time, lat, lon) float64 36.55 36.54 36.54 ... 34.0 34.08\nAttributes:\n    source:       Reanalysis\n    institution:  ECMWF\n    tilte:        ERA5 forecastsxarray.DatasetDimensions:time: 59lat: 81lon: 81Coordinates: (3)time(time)datetime64[ns]2003-01-01 ... 2003-02-28array(['2003-01-01T00:00:00.000000000', '2003-01-02T00:00:00.000000000',\n       '2003-01-03T00:00:00.000000000', '2003-01-04T00:00:00.000000000',\n       '2003-01-05T00:00:00.000000000', '2003-01-06T00:00:00.000000000',\n       '2003-01-07T00:00:00.000000000', '2003-01-08T00:00:00.000000000',\n       '2003-01-09T00:00:00.000000000', '2003-01-10T00:00:00.000000000',\n       '2003-01-11T00:00:00.000000000', '2003-01-12T00:00:00.000000000',\n       '2003-01-13T00:00:00.000000000', '2003-01-14T00:00:00.000000000',\n       '2003-01-15T00:00:00.000000000', '2003-01-16T00:00:00.000000000',\n       '2003-01-17T00:00:00.000000000', '2003-01-18T00:00:00.000000000',\n       '2003-01-19T00:00:00.000000000', '2003-01-20T00:00:00.000000000',\n       '2003-01-21T00:00:00.000000000', '2003-01-22T00:00:00.000000000',\n       '2003-01-23T00:00:00.000000000', '2003-01-24T00:00:00.000000000',\n       '2003-01-25T00:00:00.000000000', '2003-01-26T00:00:00.000000000',\n       '2003-01-27T00:00:00.000000000', '2003-01-28T00:00:00.000000000',\n       '2003-01-29T00:00:00.000000000', '2003-01-30T00:00:00.000000000',\n       '2003-01-31T00:00:00.000000000', '2003-02-01T00:00:00.000000000',\n       '2003-02-02T00:00:00.000000000', '2003-02-03T00:00:00.000000000',\n       '2003-02-04T00:00:00.000000000', '2003-02-05T00:00:00.000000000',\n       '2003-02-06T00:00:00.000000000', '2003-02-07T00:00:00.000000000',\n       '2003-02-08T00:00:00.000000000', '2003-02-09T00:00:00.000000000',\n       '2003-02-10T00:00:00.000000000', '2003-02-11T00:00:00.000000000',\n       '2003-02-12T00:00:00.000000000', '2003-02-13T00:00:00.000000000',\n       '2003-02-14T00:00:00.000000000', '2003-02-15T00:00:00.000000000',\n       '2003-02-16T00:00:00.000000000', '2003-02-17T00:00:00.000000000',\n       '2003-02-18T00:00:00.000000000', '2003-02-19T00:00:00.000000000',\n       '2003-02-20T00:00:00.000000000', '2003-02-21T00:00:00.000000000',\n       '2003-02-22T00:00:00.000000000', '2003-02-23T00:00:00.000000000',\n       '2003-02-24T00:00:00.000000000', '2003-02-25T00:00:00.000000000',\n       '2003-02-26T00:00:00.000000000', '2003-02-27T00:00:00.000000000',\n       '2003-02-28T00:00:00.000000000'], dtype='datetime64[ns]')lat(lat)float3225.0 24.75 24.5 ... 5.5 5.25 5.0standard_name :latitudelong_name :latitudeunits :degrees_northarray([25.  , 24.75, 24.5 , 24.25, 24.  , 23.75, 23.5 , 23.25, 23.  , 22.75,\n       22.5 , 22.25, 22.  , 21.75, 21.5 , 21.25, 21.  , 20.75, 20.5 , 20.25,\n       20.  , 19.75, 19.5 , 19.25, 19.  , 18.75, 18.5 , 18.25, 18.  , 17.75,\n       17.5 , 17.25, 17.  , 16.75, 16.5 , 16.25, 16.  , 15.75, 15.5 , 15.25,\n       15.  , 14.75, 14.5 , 14.25, 14.  , 13.75, 13.5 , 13.25, 13.  , 12.75,\n       12.5 , 12.25, 12.  , 11.75, 11.5 , 11.25, 11.  , 10.75, 10.5 , 10.25,\n       10.  ,  9.75,  9.5 ,  9.25,  9.  ,  8.75,  8.5 ,  8.25,  8.  ,  7.75,\n        7.5 ,  7.25,  7.  ,  6.75,  6.5 ,  6.25,  6.  ,  5.75,  5.5 ,  5.25,\n        5.  ], dtype=float32)lon(lon)float3260.0 60.25 60.5 ... 79.5 79.75 80.0standard_name :longitudelong_name :longitudeunits :degrees_eastarray([60.  , 60.25, 60.5 , 60.75, 61.  , 61.25, 61.5 , 61.75, 62.  , 62.25,\n       62.5 , 62.75, 63.  , 63.25, 63.5 , 63.75, 64.  , 64.25, 64.5 , 64.75,\n       65.  , 65.25, 65.5 , 65.75, 66.  , 66.25, 66.5 , 66.75, 67.  , 67.25,\n       67.5 , 67.75, 68.  , 68.25, 68.5 , 68.75, 69.  , 69.25, 69.5 , 69.75,\n       70.  , 70.25, 70.5 , 70.75, 71.  , 71.25, 71.5 , 71.75, 72.  , 72.25,\n       72.5 , 72.75, 73.  , 73.25, 73.5 , 73.75, 74.  , 74.25, 74.5 , 74.75,\n       75.  , 75.25, 75.5 , 75.75, 76.  , 76.25, 76.5 , 76.75, 77.  , 77.25,\n       77.5 , 77.75, 78.  , 78.25, 78.5 , 78.75, 79.  , 79.25, 79.5 , 79.75,\n       80.  ], dtype=float32)Data variables: (6)analysed_sst(time, lat, lon)float32dask.array&lt;chunksize=(59, 81, 81), meta=np.ndarray&gt;long_name :analysed sea surface temperaturestandard_name :sea_surface_foundation_temperaturecoverage_content_type :physicalMeasurementunits :kelvinvalid_min :-32767valid_max :32767comment :\"Final\" version using Multi-Resolution Variational Analysis (MRVA) method for interpolationsource :MODIS_T-JPL, MODIS_A-JPL, AMSRE-REMSS, AVHRR17_G-ACSPO, iQUAM-NOAA/NESDIS, Ice_Conc-OSISAF\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n1.48 MiB\n1.48 MiB\n\n\nShape\n(59, 81, 81)\n(59, 81, 81)\n\n\nDask graph\n1 chunks in 25 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\nu_wind(time, lat, lon)float323.128 2.865 2.555 ... -3.737 -4.479standard_name :eastward_windunits :m s**-1long_name :10 metre U wind componentnameECMWF :10 metre U wind componentshortNameECMWF :10unameCDM :10_metre_U_wind_component_surfaceproduct_type :analysisarray([[[ 3.1276042 ,  2.8645833 ,  2.5546875 , ...,  0.8776042 ,\n          0.9036458 ,  0.8072917 ],\n        [ 3.1223958 ,  2.8854167 ,  2.5703125 , ...,  0.6015625 ,\n          0.6380208 ,  0.6848958 ],\n        [ 2.5026042 ,  2.2864583 ,  2.0598958 , ...,  0.3828125 ,\n          0.390625  ,  0.45833334],\n        ...,\n        [-2.7916667 , -2.8932292 , -3.0182292 , ...,  0.04947917,\n          0.21614583,  0.23177083],\n        [-2.6380208 , -2.7317708 , -2.8567708 , ..., -0.02604167,\n          0.06510416,  0.05729167],\n        [-2.53125   , -2.6354167 , -2.7682292 , ..., -0.19270833,\n         -0.09114584, -0.07291666]],\n\n       [[-0.41666666, -0.7369792 , -1.0208334 , ..., -1.4401041 ,\n         -1.5677084 , -1.5677084 ],\n        [ 0.12239584, -0.13541667, -0.39583334, ..., -1.4348959 ,\n         -1.453125  , -1.3541666 ],\n        [ 0.6953125 ,  0.5234375 ,  0.44791666, ..., -1.3515625 ,\n         -1.2317709 , -1.140625  ],\n...\n        [-3.0833333 , -3.0286458 , -2.9739583 , ..., -0.8567708 ,\n         -1.21875   , -1.7447916 ],\n        [-3.0364583 , -2.9973958 , -2.9401042 , ..., -0.9322917 ,\n         -1.3854166 , -1.9401041 ],\n        [-3.0026042 , -2.9739583 , -2.9322917 , ..., -1.0442709 ,\n         -1.4401041 , -1.8255209 ]],\n\n       [[ 8.1119795 ,  8.020833  ,  7.950521  , ..., -1.0338541 ,\n         -1.0494791 , -1.0703125 ],\n        [ 7.78125   ,  7.7890625 ,  7.734375  , ..., -0.7942708 ,\n         -0.7135417 , -0.6015625 ],\n        [ 7.205729  ,  7.294271  ,  7.325521  , ..., -0.5026042 ,\n         -0.38020834, -0.29166666],\n        ...,\n        [-1.4270834 , -1.3619791 , -1.3125    , ..., -1.6119791 ,\n         -2.5520833 , -3.8203125 ],\n        [-1.2604166 , -1.2005209 , -1.1328125 , ..., -2.3958333 ,\n         -3.390625  , -4.5390625 ],\n        [-1.0625    , -1.0208334 , -0.9739583 , ..., -2.8697917 ,\n         -3.7369792 , -4.4791665 ]]], dtype=float32)v_wind(time, lat, lon)float320.6016 0.6589 ... -1.914 -2.474standard_name :northward_windunits :m s**-1long_name :10 metre V wind componentnameECMWF :10 metre V wind componentshortNameECMWF :10vnameCDM :10_metre_V_wind_component_surfaceproduct_type :analysisarray([[[ 6.0156250e-01,  6.5885419e-01,  6.0677081e-01, ...,\n         -1.3671875e+00, -1.1848959e+00, -8.8020831e-01],\n        [ 1.0729166e+00,  1.1536459e+00,  1.0807291e+00, ...,\n         -1.4791666e+00, -1.3463541e+00, -1.1796875e+00],\n        [ 1.1510416e+00,  1.2838541e+00,  1.3229166e+00, ...,\n         -1.5963541e+00, -1.4869791e+00, -1.3489584e+00],\n        ...,\n        [-3.6770833e+00, -3.8229167e+00, -3.9713542e+00, ...,\n         -2.3437500e+00, -1.3020834e+00, -3.7760416e-01],\n        [-3.7083333e+00, -3.8255208e+00, -3.9088542e+00, ...,\n         -2.3593750e+00, -1.5390625e+00, -7.4739581e-01],\n        [-3.7395833e+00, -3.7526042e+00, -3.7812500e+00, ...,\n         -2.4817708e+00, -1.6953125e+00, -1.1484375e+00]],\n\n       [[-1.5208334e+00, -1.7005209e+00, -1.9192709e+00, ...,\n         -3.9062500e-01, -4.2708334e-01, -4.9218750e-01],\n        [-1.9010416e+00, -1.9583334e+00, -2.1458333e+00, ...,\n         -5.0260419e-01, -5.8593750e-01, -5.0260419e-01],\n        [-2.2838542e+00, -2.2473958e+00, -2.2109375e+00, ...,\n         -4.9479166e-01, -6.6145831e-01, -6.0937500e-01],\n...\n        [-4.0729165e+00, -4.1276040e+00, -4.1770835e+00, ...,\n         -2.1171875e+00, -1.6562500e+00, -1.3151041e+00],\n        [-4.0885415e+00, -4.1484375e+00, -4.2005210e+00, ...,\n         -2.4244792e+00, -2.1119792e+00, -1.8515625e+00],\n        [-4.0625000e+00, -4.1197915e+00, -4.1614585e+00, ...,\n         -2.7031250e+00, -2.4166667e+00, -2.2708333e+00]],\n\n       [[-1.9791667e-01,  5.2083335e-03,  1.5625000e-01, ...,\n         -1.2760417e-01, -1.6145833e-01, -1.6927083e-01],\n        [-7.9687500e-01, -5.9895831e-01, -4.3750000e-01, ...,\n          5.4687500e-02,  4.6875000e-02,  1.5364583e-01],\n        [-1.2994791e+00, -1.0937500e+00, -9.4010419e-01, ...,\n          2.2395833e-01,  2.9166666e-01,  3.8020834e-01],\n        ...,\n        [-3.0625000e+00, -3.0781250e+00, -3.1197917e+00, ...,\n          5.4687500e-01,  1.2500000e-01, -8.0729169e-01],\n        [-3.1614583e+00, -3.2005208e+00, -3.2291667e+00, ...,\n         -4.1666666e-01, -1.0598959e+00, -1.8958334e+00],\n        [-3.2526042e+00, -3.3255208e+00, -3.3750000e+00, ...,\n         -1.3333334e+00, -1.9140625e+00, -2.4739583e+00]]], dtype=float32)speed(time, lat, lon)float323.945 3.645 3.368 ... 4.227 5.153array([[[3.9451437, 3.6454449, 3.36847  , ..., 1.9201735, 1.8865218,\n         1.7468839],\n        [4.1419544, 3.8504055, 3.444007 , ..., 1.9789158, 1.9153827,\n         1.8310242],\n        [3.6950111, 3.4435928, 3.1019478, ..., 1.9867468, 1.9142274,\n         1.8317447],\n        ...,\n        [4.7469373, 4.912411 , 5.092304 , ..., 2.5135314, 1.6478343,\n         1.3388939],\n        [4.676735 , 4.8191776, 4.95795  , ..., 2.5241547, 1.7888355,\n         1.2424096],\n        [4.636933 , 4.7195234, 4.826424 , ..., 2.6338756, 1.8901653,\n         1.4170161]],\n\n       [[2.3318148, 2.4910347, 2.7374954, ..., 1.7411982, 1.8597933,\n         1.8728681],\n        [2.414438 , 2.5525272, 2.7947886, ..., 1.7888421, 1.8435212,\n         1.7328191],\n        [2.7723866, 2.8588107, 2.8829317, ..., 1.7411242, 1.7196132,\n         1.6104554],\n...\n        [5.146253 , 5.1557384, 5.1632495, ..., 2.8367107, 2.591439 ,\n         2.534239 ],\n        [5.1336794, 5.1560197, 5.166705 , ..., 3.0493386, 2.972432 ,\n         3.014616 ],\n        [5.0962915, 5.1267576, 5.1389093, ..., 3.263656 , 3.196616 ,\n         3.23878  ]],\n\n       [[8.189649 , 8.106084 , 8.037755 , ..., 2.1850507, 2.2752473,\n         2.2316103],\n        [7.892883 , 7.890354 , 7.823316 , ..., 2.3158002, 2.3716338,\n         2.317237 ],\n        [7.391702 , 7.4489727, 7.4578147, ..., 2.337489 , 2.231067 ,\n         2.291389 ],\n        ...,\n        [3.4753053, 3.467406 , 3.4899578, ..., 1.9244485, 2.6942132,\n         4.00572  ],\n        [3.5153275, 3.533966 , 3.542756 , ..., 2.549701 , 3.6089706,\n         4.9639993],\n        [3.5520408, 3.6114044, 3.6468   , ..., 3.2160883, 4.226849 ,\n         5.1525497]]], dtype=float32)direction(time, lat, lon)float3222.31 17.58 13.01 ... -153.4 -151.6standard_name :northward_windunits :m s**-1long_name :10 metre V wind componentnameECMWF :10 metre V wind componentshortNameECMWF :10vnameCDM :10_metre_V_wind_component_surfaceproduct_type :analysisarray([[[  22.31256  ,   17.577126 ,   13.005119 , ...,  -67.38259  ,\n          -47.755325 ,  -33.74233  ],\n        [  30.220772 ,   29.43544  ,   26.357203 , ...,  -78.10914  ,\n          -64.685646 ,  -49.11764  ],\n        [  35.783054 ,   37.048702 ,   35.60695  , ...,  -85.95185  ,\n          -86.35613  ,  -70.15734  ],\n        ...,\n        [-126.93241  , -126.620476 , -126.65524  , ...,  -89.24673  ,\n          -80.60198  ,   -4.201843 ],\n        [-125.02897  , -124.928856 , -125.43902  , ...,  -90.54617  ,\n          -84.9731   ,  -69.18275  ],\n        [-123.4584   , -124.29117  , -125.278015 , ...,  -93.996056 ,\n          -90.138794 ,  -88.29269  ]],\n\n       [[ -72.070206 ,  -85.1071   ,  -93.38763  , ...,  -58.74728  ,\n          -60.06917  ,  -58.698284 ],\n        [ -80.372795 ,  -90.8068   ,  -96.67983  , ...,  -58.435246 ,\n          -56.67389  ,  -58.751236 ],\n        [ -80.680115 ,  -86.12191  ,  -88.40796  , ...,  -59.58118  ,\n          -53.925976 ,  -70.68571  ],\n...\n        [-127.28055  , -126.49218  , -125.74938  , ...,  -71.24183  ,\n          -79.70399  , -133.6834   ],\n        [-126.68684  , -125.9416   , -125.15563  , ..., -114.46243  ,\n         -107.83289  , -131.2433   ],\n        [-126.467865 , -125.800575 , -125.19381  , ..., -113.44105  ,\n         -120.908356 , -126.25626  ]],\n\n       [[  -0.9108308,    0.405311 ,    1.362003 , ...,  -42.883907 ,\n          -38.677364 ,  -36.70583  ],\n        [  -5.2671247,   -3.8980722,   -2.9317954, ...,  -26.818964 ,\n          -36.38267  ,   -2.3517277],\n        [  -9.45855  ,   -7.810347 ,   -6.7994595, ...,    9.280067 ,\n            2.028537 ,   22.812065 ],\n        ...,\n        [-115.06766  , -113.99908  , -113.05724  , ...,   26.856642 ,\n           10.176036 ,  -94.912285 ],\n        [-111.756325 , -110.610634 , -109.46108  , ...,  -81.22135  ,\n         -133.58519  , -158.10306  ],\n        [-108.30612  , -107.21555  , -106.26769  , ..., -140.94438  ,\n         -153.4189   , -151.60954  ]]], dtype=float32)so(time, lat, lon)float6436.55 36.54 36.54 ... 34.0 34.08long_name :Salinitystandard_name :sea_water_salinityunits :1e-3unit_long :Practical Salinity Unitvalid_min :1valid_max :28336cell_methods :area: mean_ChunkSizes :[  1   7 341 720]array([[[36.55201721, 36.54133606, 36.53675842, ...,         nan,\n                 nan,         nan],\n        [36.5459137 , 36.57948685, 36.59321976, ...,         nan,\n                 nan,         nan],\n        [36.62831497, 36.71834564, 36.75344086, ...,         nan,\n                 nan,         nan],\n        ...,\n        [35.49760437, 35.48692322, 35.45030212, ..., 32.78908539,\n         32.854702  , 32.9493103 ],\n        [35.4884491 , 35.43046188, 35.37553024, ..., 34.19446564,\n         34.28754425, 34.29059601],\n        [35.39994431, 35.37400436, 35.36179733, ..., 33.71685028,\n         33.60850906, 33.68022537]],\n\n       [[36.5459137 , 36.53370667, 36.51997375, ...,         nan,\n                 nan,         nan],\n        [36.55659485, 36.57033157, 36.57338333, ...,         nan,\n                 nan,         nan],\n        [36.62373734, 36.71987152, 36.76717377, ...,         nan,\n                 nan,         nan],\n...\n        [35.06271744, 35.02761841, 34.97268677, ..., 33.4818573 ,\n         33.47270203, 33.48796082],\n        [35.18326187, 35.16342545, 35.1084938 , ..., 33.60698318,\n         33.68175125, 33.7733078 ],\n        [35.24582672, 35.19089508, 35.13595963, ..., 33.75041962,\n         33.87859726, 33.99151611]],\n\n       [[36.590168  , 36.59474564, 36.58864212, ...,         nan,\n                 nan,         nan],\n        [36.56117249, 36.57643509, 36.59321976, ...,         nan,\n                 nan,         nan],\n        [36.7000351 , 36.61000443, 36.57796097, ...,         nan,\n                 nan,         nan],\n        ...,\n        [35.08407974, 35.06729507, 35.01541138, ..., 33.4299736 ,\n         33.41471481, 33.44981384],\n        [35.21378326, 35.19547272, 35.14206314, ..., 33.6420784 ,\n         33.72447968, 33.8465538 ],\n        [35.24277496, 35.18326187, 35.13290787, ..., 33.8831749 ,\n         34.00067139, 34.08307266]]])Indexes: (3)timePandasIndexPandasIndex(DatetimeIndex(['2003-01-01', '2003-01-02', '2003-01-03', '2003-01-04',\n               '2003-01-05', '2003-01-06', '2003-01-07', '2003-01-08',\n               '2003-01-09', '2003-01-10', '2003-01-11', '2003-01-12',\n               '2003-01-13', '2003-01-14', '2003-01-15', '2003-01-16',\n               '2003-01-17', '2003-01-18', '2003-01-19', '2003-01-20',\n               '2003-01-21', '2003-01-22', '2003-01-23', '2003-01-24',\n               '2003-01-25', '2003-01-26', '2003-01-27', '2003-01-28',\n               '2003-01-29', '2003-01-30', '2003-01-31', '2003-02-01',\n               '2003-02-02', '2003-02-03', '2003-02-04', '2003-02-05',\n               '2003-02-06', '2003-02-07', '2003-02-08', '2003-02-09',\n               '2003-02-10', '2003-02-11', '2003-02-12', '2003-02-13',\n               '2003-02-14', '2003-02-15', '2003-02-16', '2003-02-17',\n               '2003-02-18', '2003-02-19', '2003-02-20', '2003-02-21',\n               '2003-02-22', '2003-02-23', '2003-02-24', '2003-02-25',\n               '2003-02-26', '2003-02-27', '2003-02-28'],\n              dtype='datetime64[ns]', name='time', freq=None))latPandasIndexPandasIndex(Index([ 25.0, 24.75,  24.5, 24.25,  24.0, 23.75,  23.5, 23.25,  23.0, 22.75,\n        22.5, 22.25,  22.0, 21.75,  21.5, 21.25,  21.0, 20.75,  20.5, 20.25,\n        20.0, 19.75,  19.5, 19.25,  19.0, 18.75,  18.5, 18.25,  18.0, 17.75,\n        17.5, 17.25,  17.0, 16.75,  16.5, 16.25,  16.0, 15.75,  15.5, 15.25,\n        15.0, 14.75,  14.5, 14.25,  14.0, 13.75,  13.5, 13.25,  13.0, 12.75,\n        12.5, 12.25,  12.0, 11.75,  11.5, 11.25,  11.0, 10.75,  10.5, 10.25,\n        10.0,  9.75,   9.5,  9.25,   9.0,  8.75,   8.5,  8.25,   8.0,  7.75,\n         7.5,  7.25,   7.0,  6.75,   6.5,  6.25,   6.0,  5.75,   5.5,  5.25,\n         5.0],\n      dtype='float32', name='lat'))lonPandasIndexPandasIndex(Index([ 60.0, 60.25,  60.5, 60.75,  61.0, 61.25,  61.5, 61.75,  62.0, 62.25,\n        62.5, 62.75,  63.0, 63.25,  63.5, 63.75,  64.0, 64.25,  64.5, 64.75,\n        65.0, 65.25,  65.5, 65.75,  66.0, 66.25,  66.5, 66.75,  67.0, 67.25,\n        67.5, 67.75,  68.0, 68.25,  68.5, 68.75,  69.0, 69.25,  69.5, 69.75,\n        70.0, 70.25,  70.5, 70.75,  71.0, 71.25,  71.5, 71.75,  72.0, 72.25,\n        72.5, 72.75,  73.0, 73.25,  73.5, 73.75,  74.0, 74.25,  74.5, 74.75,\n        75.0, 75.25,  75.5, 75.75,  76.0, 76.25,  76.5, 76.75,  77.0, 77.25,\n        77.5, 77.75,  78.0, 78.25,  78.5, 78.75,  79.0, 79.25,  79.5, 79.75,\n        80.0],\n      dtype='float32', name='lon'))Attributes: (3)source :Reanalysisinstitution :ECMWFtilte :ERA5 forecasts",
    "crumbs": [
      "Python Tutorials",
      "IO Zarr Database",
      "Combine, clean, and export the data"
    ]
  },
  {
    "objectID": "python-notebooks/04_Combining_Data.html#adding-metadata-and-rename-variables",
    "href": "python-notebooks/04_Combining_Data.html#adding-metadata-and-rename-variables",
    "title": "Combine, clean, and export the data",
    "section": "Adding metadata and rename variables",
    "text": "Adding metadata and rename variables\nMetadata is one aspect we also need to address. Correct metadata is vital especially when we want to share our final dataset for others to use, or when we want to graph and feed data for other tools to work on. As we combine and process variables into the final dataset, some of the metadata may be lost, so we need to make sure that their metadata is reserved and resolve any conflict for the export process to proceed smoothly.\n\nfinal_ds['speed'].attrs = {\n    'units': 'm s**-1',\n    'long_name': '10 metre absolute speed'\n}\n\nfinal_ds['direction'].attrs = {\n    'units': '°C',\n    'long_name': '10 metre wind direction'\n}\n\n\nfinal_ds.time.attrs = salinity.time.attrs\n\n\n# you do not need to add everything in here, but it's an example template\n\nfinal_ds.attrs = {\n    'title': 'Sample of Climate Data for Coastal Upwelling Machine Learning Project in Indian Ocean',\n    'summary': \"Daily mean of 0.25 x 0.25 degrees gridded data from multiple climate variables that may influence the patterns of coastal upwelling in the focused area\",\n    'creator_name': 'Minh Phan',\n    'creator_email': 'minhphan@uw.edu',\n    'creator_type': 'person',\n    'source': 'OSCAR, ERA5 Reanalysis, Copernicus Climate Change Service (C3S), Copernicus Marine Environment Monitoring Service (CMEMS)',\n    'geospatial_lat_min': float(final_ds.lat.min().values),\n    'geospatial_lat_max': float(final_ds.lat.max().values),\n    'geospatial_lat_units': 'degrees_north',\n    'geospatial_lat_resolution': 0.25,\n    'geospatial_lon_min': float(final_ds.lon.min().values),\n    'geospatial_lon_max': float(final_ds.lon.max().values),\n    'geospatial_lon_units': 'degrees_east',\n    'geospatial_lon_resolution': 0.25,\n    'time_coverage_start': '2000-01-01T00:00:00',\n    'time_coverage_end': '2002-12-31T23:59:59',\n    'date_created': datetime.today().strftime('%Y-%d-%d')\n}\n\n\nfinal_ds\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt;\nDimensions:       (time: 59, lat: 81, lon: 81)\nCoordinates:\n  * time          (time) datetime64[ns] 2003-01-01 2003-01-02 ... 2003-02-28\n  * lat           (lat) float32 25.0 24.75 24.5 24.25 24.0 ... 5.75 5.5 5.25 5.0\n  * lon           (lon) float32 60.0 60.25 60.5 60.75 ... 79.25 79.5 79.75 80.0\nData variables:\n    analysed_sst  (time, lat, lon) float32 dask.array&lt;chunksize=(59, 81, 81), meta=np.ndarray&gt;\n    u_wind        (time, lat, lon) float32 3.128 2.865 2.555 ... -3.737 -4.479\n    v_wind        (time, lat, lon) float32 0.6016 0.6589 ... -1.914 -2.474\n    speed         (time, lat, lon) float32 3.945 3.645 3.368 ... 4.227 5.153\n    direction     (time, lat, lon) float32 22.31 17.58 13.01 ... -153.4 -151.6\n    so            (time, lat, lon) float64 36.55 36.54 36.54 ... 34.0 34.08\nAttributes: (12/17)\n    title:                      Sample of Climate Data for Coastal Upwelling ...\n    summary:                    Daily mean of 0.25 x 0.25 degrees gridded dat...\n    creator_name:               Minh Phan\n    creator_email:              minhphan@uw.edu\n    creator_type:               person\n    source:                     OSCAR, ERA5 Reanalysis, Copernicus Climate Ch...\n    ...                         ...\n    geospatial_lon_max:         80.0\n    geospatial_lon_units:       degrees_east\n    geospatial_lon_resolution:  0.25\n    time_coverage_start:        2000-01-01T00:00:00\n    time_coverage_end:          2002-12-31T23:59:59\n    date_created:               2023-14-14xarray.DatasetDimensions:time: 59lat: 81lon: 81Coordinates: (3)time(time)datetime64[ns]2003-01-01 ... 2003-02-28array(['2003-01-01T00:00:00.000000000', '2003-01-02T00:00:00.000000000',\n       '2003-01-03T00:00:00.000000000', '2003-01-04T00:00:00.000000000',\n       '2003-01-05T00:00:00.000000000', '2003-01-06T00:00:00.000000000',\n       '2003-01-07T00:00:00.000000000', '2003-01-08T00:00:00.000000000',\n       '2003-01-09T00:00:00.000000000', '2003-01-10T00:00:00.000000000',\n       '2003-01-11T00:00:00.000000000', '2003-01-12T00:00:00.000000000',\n       '2003-01-13T00:00:00.000000000', '2003-01-14T00:00:00.000000000',\n       '2003-01-15T00:00:00.000000000', '2003-01-16T00:00:00.000000000',\n       '2003-01-17T00:00:00.000000000', '2003-01-18T00:00:00.000000000',\n       '2003-01-19T00:00:00.000000000', '2003-01-20T00:00:00.000000000',\n       '2003-01-21T00:00:00.000000000', '2003-01-22T00:00:00.000000000',\n       '2003-01-23T00:00:00.000000000', '2003-01-24T00:00:00.000000000',\n       '2003-01-25T00:00:00.000000000', '2003-01-26T00:00:00.000000000',\n       '2003-01-27T00:00:00.000000000', '2003-01-28T00:00:00.000000000',\n       '2003-01-29T00:00:00.000000000', '2003-01-30T00:00:00.000000000',\n       '2003-01-31T00:00:00.000000000', '2003-02-01T00:00:00.000000000',\n       '2003-02-02T00:00:00.000000000', '2003-02-03T00:00:00.000000000',\n       '2003-02-04T00:00:00.000000000', '2003-02-05T00:00:00.000000000',\n       '2003-02-06T00:00:00.000000000', '2003-02-07T00:00:00.000000000',\n       '2003-02-08T00:00:00.000000000', '2003-02-09T00:00:00.000000000',\n       '2003-02-10T00:00:00.000000000', '2003-02-11T00:00:00.000000000',\n       '2003-02-12T00:00:00.000000000', '2003-02-13T00:00:00.000000000',\n       '2003-02-14T00:00:00.000000000', '2003-02-15T00:00:00.000000000',\n       '2003-02-16T00:00:00.000000000', '2003-02-17T00:00:00.000000000',\n       '2003-02-18T00:00:00.000000000', '2003-02-19T00:00:00.000000000',\n       '2003-02-20T00:00:00.000000000', '2003-02-21T00:00:00.000000000',\n       '2003-02-22T00:00:00.000000000', '2003-02-23T00:00:00.000000000',\n       '2003-02-24T00:00:00.000000000', '2003-02-25T00:00:00.000000000',\n       '2003-02-26T00:00:00.000000000', '2003-02-27T00:00:00.000000000',\n       '2003-02-28T00:00:00.000000000'], dtype='datetime64[ns]')lat(lat)float3225.0 24.75 24.5 ... 5.5 5.25 5.0standard_name :latitudelong_name :latitudeunits :degrees_northarray([25.  , 24.75, 24.5 , 24.25, 24.  , 23.75, 23.5 , 23.25, 23.  , 22.75,\n       22.5 , 22.25, 22.  , 21.75, 21.5 , 21.25, 21.  , 20.75, 20.5 , 20.25,\n       20.  , 19.75, 19.5 , 19.25, 19.  , 18.75, 18.5 , 18.25, 18.  , 17.75,\n       17.5 , 17.25, 17.  , 16.75, 16.5 , 16.25, 16.  , 15.75, 15.5 , 15.25,\n       15.  , 14.75, 14.5 , 14.25, 14.  , 13.75, 13.5 , 13.25, 13.  , 12.75,\n       12.5 , 12.25, 12.  , 11.75, 11.5 , 11.25, 11.  , 10.75, 10.5 , 10.25,\n       10.  ,  9.75,  9.5 ,  9.25,  9.  ,  8.75,  8.5 ,  8.25,  8.  ,  7.75,\n        7.5 ,  7.25,  7.  ,  6.75,  6.5 ,  6.25,  6.  ,  5.75,  5.5 ,  5.25,\n        5.  ], dtype=float32)lon(lon)float3260.0 60.25 60.5 ... 79.5 79.75 80.0standard_name :longitudelong_name :longitudeunits :degrees_eastarray([60.  , 60.25, 60.5 , 60.75, 61.  , 61.25, 61.5 , 61.75, 62.  , 62.25,\n       62.5 , 62.75, 63.  , 63.25, 63.5 , 63.75, 64.  , 64.25, 64.5 , 64.75,\n       65.  , 65.25, 65.5 , 65.75, 66.  , 66.25, 66.5 , 66.75, 67.  , 67.25,\n       67.5 , 67.75, 68.  , 68.25, 68.5 , 68.75, 69.  , 69.25, 69.5 , 69.75,\n       70.  , 70.25, 70.5 , 70.75, 71.  , 71.25, 71.5 , 71.75, 72.  , 72.25,\n       72.5 , 72.75, 73.  , 73.25, 73.5 , 73.75, 74.  , 74.25, 74.5 , 74.75,\n       75.  , 75.25, 75.5 , 75.75, 76.  , 76.25, 76.5 , 76.75, 77.  , 77.25,\n       77.5 , 77.75, 78.  , 78.25, 78.5 , 78.75, 79.  , 79.25, 79.5 , 79.75,\n       80.  ], dtype=float32)Data variables: (6)analysed_sst(time, lat, lon)float32dask.array&lt;chunksize=(59, 81, 81), meta=np.ndarray&gt;long_name :analysed sea surface temperaturestandard_name :sea_surface_foundation_temperaturecoverage_content_type :physicalMeasurementunits :kelvinvalid_min :-32767valid_max :32767comment :\"Final\" version using Multi-Resolution Variational Analysis (MRVA) method for interpolationsource :MODIS_T-JPL, MODIS_A-JPL, AMSRE-REMSS, AVHRR17_G-ACSPO, iQUAM-NOAA/NESDIS, Ice_Conc-OSISAF\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n1.48 MiB\n1.48 MiB\n\n\nShape\n(59, 81, 81)\n(59, 81, 81)\n\n\nDask graph\n1 chunks in 25 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\nu_wind(time, lat, lon)float323.128 2.865 2.555 ... -3.737 -4.479standard_name :eastward_windunits :m s**-1long_name :10 metre U wind componentnameECMWF :10 metre U wind componentshortNameECMWF :10unameCDM :10_metre_U_wind_component_surfaceproduct_type :analysisarray([[[ 3.1276042 ,  2.8645833 ,  2.5546875 , ...,  0.8776042 ,\n          0.9036458 ,  0.8072917 ],\n        [ 3.1223958 ,  2.8854167 ,  2.5703125 , ...,  0.6015625 ,\n          0.6380208 ,  0.6848958 ],\n        [ 2.5026042 ,  2.2864583 ,  2.0598958 , ...,  0.3828125 ,\n          0.390625  ,  0.45833334],\n        ...,\n        [-2.7916667 , -2.8932292 , -3.0182292 , ...,  0.04947917,\n          0.21614583,  0.23177083],\n        [-2.6380208 , -2.7317708 , -2.8567708 , ..., -0.02604167,\n          0.06510416,  0.05729167],\n        [-2.53125   , -2.6354167 , -2.7682292 , ..., -0.19270833,\n         -0.09114584, -0.07291666]],\n\n       [[-0.41666666, -0.7369792 , -1.0208334 , ..., -1.4401041 ,\n         -1.5677084 , -1.5677084 ],\n        [ 0.12239584, -0.13541667, -0.39583334, ..., -1.4348959 ,\n         -1.453125  , -1.3541666 ],\n        [ 0.6953125 ,  0.5234375 ,  0.44791666, ..., -1.3515625 ,\n         -1.2317709 , -1.140625  ],\n...\n        [-3.0833333 , -3.0286458 , -2.9739583 , ..., -0.8567708 ,\n         -1.21875   , -1.7447916 ],\n        [-3.0364583 , -2.9973958 , -2.9401042 , ..., -0.9322917 ,\n         -1.3854166 , -1.9401041 ],\n        [-3.0026042 , -2.9739583 , -2.9322917 , ..., -1.0442709 ,\n         -1.4401041 , -1.8255209 ]],\n\n       [[ 8.1119795 ,  8.020833  ,  7.950521  , ..., -1.0338541 ,\n         -1.0494791 , -1.0703125 ],\n        [ 7.78125   ,  7.7890625 ,  7.734375  , ..., -0.7942708 ,\n         -0.7135417 , -0.6015625 ],\n        [ 7.205729  ,  7.294271  ,  7.325521  , ..., -0.5026042 ,\n         -0.38020834, -0.29166666],\n        ...,\n        [-1.4270834 , -1.3619791 , -1.3125    , ..., -1.6119791 ,\n         -2.5520833 , -3.8203125 ],\n        [-1.2604166 , -1.2005209 , -1.1328125 , ..., -2.3958333 ,\n         -3.390625  , -4.5390625 ],\n        [-1.0625    , -1.0208334 , -0.9739583 , ..., -2.8697917 ,\n         -3.7369792 , -4.4791665 ]]], dtype=float32)v_wind(time, lat, lon)float320.6016 0.6589 ... -1.914 -2.474standard_name :northward_windunits :m s**-1long_name :10 metre V wind componentnameECMWF :10 metre V wind componentshortNameECMWF :10vnameCDM :10_metre_V_wind_component_surfaceproduct_type :analysisarray([[[ 6.0156250e-01,  6.5885419e-01,  6.0677081e-01, ...,\n         -1.3671875e+00, -1.1848959e+00, -8.8020831e-01],\n        [ 1.0729166e+00,  1.1536459e+00,  1.0807291e+00, ...,\n         -1.4791666e+00, -1.3463541e+00, -1.1796875e+00],\n        [ 1.1510416e+00,  1.2838541e+00,  1.3229166e+00, ...,\n         -1.5963541e+00, -1.4869791e+00, -1.3489584e+00],\n        ...,\n        [-3.6770833e+00, -3.8229167e+00, -3.9713542e+00, ...,\n         -2.3437500e+00, -1.3020834e+00, -3.7760416e-01],\n        [-3.7083333e+00, -3.8255208e+00, -3.9088542e+00, ...,\n         -2.3593750e+00, -1.5390625e+00, -7.4739581e-01],\n        [-3.7395833e+00, -3.7526042e+00, -3.7812500e+00, ...,\n         -2.4817708e+00, -1.6953125e+00, -1.1484375e+00]],\n\n       [[-1.5208334e+00, -1.7005209e+00, -1.9192709e+00, ...,\n         -3.9062500e-01, -4.2708334e-01, -4.9218750e-01],\n        [-1.9010416e+00, -1.9583334e+00, -2.1458333e+00, ...,\n         -5.0260419e-01, -5.8593750e-01, -5.0260419e-01],\n        [-2.2838542e+00, -2.2473958e+00, -2.2109375e+00, ...,\n         -4.9479166e-01, -6.6145831e-01, -6.0937500e-01],\n...\n        [-4.0729165e+00, -4.1276040e+00, -4.1770835e+00, ...,\n         -2.1171875e+00, -1.6562500e+00, -1.3151041e+00],\n        [-4.0885415e+00, -4.1484375e+00, -4.2005210e+00, ...,\n         -2.4244792e+00, -2.1119792e+00, -1.8515625e+00],\n        [-4.0625000e+00, -4.1197915e+00, -4.1614585e+00, ...,\n         -2.7031250e+00, -2.4166667e+00, -2.2708333e+00]],\n\n       [[-1.9791667e-01,  5.2083335e-03,  1.5625000e-01, ...,\n         -1.2760417e-01, -1.6145833e-01, -1.6927083e-01],\n        [-7.9687500e-01, -5.9895831e-01, -4.3750000e-01, ...,\n          5.4687500e-02,  4.6875000e-02,  1.5364583e-01],\n        [-1.2994791e+00, -1.0937500e+00, -9.4010419e-01, ...,\n          2.2395833e-01,  2.9166666e-01,  3.8020834e-01],\n        ...,\n        [-3.0625000e+00, -3.0781250e+00, -3.1197917e+00, ...,\n          5.4687500e-01,  1.2500000e-01, -8.0729169e-01],\n        [-3.1614583e+00, -3.2005208e+00, -3.2291667e+00, ...,\n         -4.1666666e-01, -1.0598959e+00, -1.8958334e+00],\n        [-3.2526042e+00, -3.3255208e+00, -3.3750000e+00, ...,\n         -1.3333334e+00, -1.9140625e+00, -2.4739583e+00]]], dtype=float32)speed(time, lat, lon)float323.945 3.645 3.368 ... 4.227 5.153units :m s**-1long_name :10 metre absolute speedarray([[[3.9451437, 3.6454449, 3.36847  , ..., 1.9201735, 1.8865218,\n         1.7468839],\n        [4.1419544, 3.8504055, 3.444007 , ..., 1.9789158, 1.9153827,\n         1.8310242],\n        [3.6950111, 3.4435928, 3.1019478, ..., 1.9867468, 1.9142274,\n         1.8317447],\n        ...,\n        [4.7469373, 4.912411 , 5.092304 , ..., 2.5135314, 1.6478343,\n         1.3388939],\n        [4.676735 , 4.8191776, 4.95795  , ..., 2.5241547, 1.7888355,\n         1.2424096],\n        [4.636933 , 4.7195234, 4.826424 , ..., 2.6338756, 1.8901653,\n         1.4170161]],\n\n       [[2.3318148, 2.4910347, 2.7374954, ..., 1.7411982, 1.8597933,\n         1.8728681],\n        [2.414438 , 2.5525272, 2.7947886, ..., 1.7888421, 1.8435212,\n         1.7328191],\n        [2.7723866, 2.8588107, 2.8829317, ..., 1.7411242, 1.7196132,\n         1.6104554],\n...\n        [5.146253 , 5.1557384, 5.1632495, ..., 2.8367107, 2.591439 ,\n         2.534239 ],\n        [5.1336794, 5.1560197, 5.166705 , ..., 3.0493386, 2.972432 ,\n         3.014616 ],\n        [5.0962915, 5.1267576, 5.1389093, ..., 3.263656 , 3.196616 ,\n         3.23878  ]],\n\n       [[8.189649 , 8.106084 , 8.037755 , ..., 2.1850507, 2.2752473,\n         2.2316103],\n        [7.892883 , 7.890354 , 7.823316 , ..., 2.3158002, 2.3716338,\n         2.317237 ],\n        [7.391702 , 7.4489727, 7.4578147, ..., 2.337489 , 2.231067 ,\n         2.291389 ],\n        ...,\n        [3.4753053, 3.467406 , 3.4899578, ..., 1.9244485, 2.6942132,\n         4.00572  ],\n        [3.5153275, 3.533966 , 3.542756 , ..., 2.549701 , 3.6089706,\n         4.9639993],\n        [3.5520408, 3.6114044, 3.6468   , ..., 3.2160883, 4.226849 ,\n         5.1525497]]], dtype=float32)direction(time, lat, lon)float3222.31 17.58 13.01 ... -153.4 -151.6units :°Clong_name :10 metre wind directionarray([[[  22.31256  ,   17.577126 ,   13.005119 , ...,  -67.38259  ,\n          -47.755325 ,  -33.74233  ],\n        [  30.220772 ,   29.43544  ,   26.357203 , ...,  -78.10914  ,\n          -64.685646 ,  -49.11764  ],\n        [  35.783054 ,   37.048702 ,   35.60695  , ...,  -85.95185  ,\n          -86.35613  ,  -70.15734  ],\n        ...,\n        [-126.93241  , -126.620476 , -126.65524  , ...,  -89.24673  ,\n          -80.60198  ,   -4.201843 ],\n        [-125.02897  , -124.928856 , -125.43902  , ...,  -90.54617  ,\n          -84.9731   ,  -69.18275  ],\n        [-123.4584   , -124.29117  , -125.278015 , ...,  -93.996056 ,\n          -90.138794 ,  -88.29269  ]],\n\n       [[ -72.070206 ,  -85.1071   ,  -93.38763  , ...,  -58.74728  ,\n          -60.06917  ,  -58.698284 ],\n        [ -80.372795 ,  -90.8068   ,  -96.67983  , ...,  -58.435246 ,\n          -56.67389  ,  -58.751236 ],\n        [ -80.680115 ,  -86.12191  ,  -88.40796  , ...,  -59.58118  ,\n          -53.925976 ,  -70.68571  ],\n...\n        [-127.28055  , -126.49218  , -125.74938  , ...,  -71.24183  ,\n          -79.70399  , -133.6834   ],\n        [-126.68684  , -125.9416   , -125.15563  , ..., -114.46243  ,\n         -107.83289  , -131.2433   ],\n        [-126.467865 , -125.800575 , -125.19381  , ..., -113.44105  ,\n         -120.908356 , -126.25626  ]],\n\n       [[  -0.9108308,    0.405311 ,    1.362003 , ...,  -42.883907 ,\n          -38.677364 ,  -36.70583  ],\n        [  -5.2671247,   -3.8980722,   -2.9317954, ...,  -26.818964 ,\n          -36.38267  ,   -2.3517277],\n        [  -9.45855  ,   -7.810347 ,   -6.7994595, ...,    9.280067 ,\n            2.028537 ,   22.812065 ],\n        ...,\n        [-115.06766  , -113.99908  , -113.05724  , ...,   26.856642 ,\n           10.176036 ,  -94.912285 ],\n        [-111.756325 , -110.610634 , -109.46108  , ...,  -81.22135  ,\n         -133.58519  , -158.10306  ],\n        [-108.30612  , -107.21555  , -106.26769  , ..., -140.94438  ,\n         -153.4189   , -151.60954  ]]], dtype=float32)so(time, lat, lon)float6436.55 36.54 36.54 ... 34.0 34.08long_name :Salinitystandard_name :sea_water_salinityunits :1e-3unit_long :Practical Salinity Unitvalid_min :1valid_max :28336cell_methods :area: mean_ChunkSizes :[  1   7 341 720]array([[[36.55201721, 36.54133606, 36.53675842, ...,         nan,\n                 nan,         nan],\n        [36.5459137 , 36.57948685, 36.59321976, ...,         nan,\n                 nan,         nan],\n        [36.62831497, 36.71834564, 36.75344086, ...,         nan,\n                 nan,         nan],\n        ...,\n        [35.49760437, 35.48692322, 35.45030212, ..., 32.78908539,\n         32.854702  , 32.9493103 ],\n        [35.4884491 , 35.43046188, 35.37553024, ..., 34.19446564,\n         34.28754425, 34.29059601],\n        [35.39994431, 35.37400436, 35.36179733, ..., 33.71685028,\n         33.60850906, 33.68022537]],\n\n       [[36.5459137 , 36.53370667, 36.51997375, ...,         nan,\n                 nan,         nan],\n        [36.55659485, 36.57033157, 36.57338333, ...,         nan,\n                 nan,         nan],\n        [36.62373734, 36.71987152, 36.76717377, ...,         nan,\n                 nan,         nan],\n...\n        [35.06271744, 35.02761841, 34.97268677, ..., 33.4818573 ,\n         33.47270203, 33.48796082],\n        [35.18326187, 35.16342545, 35.1084938 , ..., 33.60698318,\n         33.68175125, 33.7733078 ],\n        [35.24582672, 35.19089508, 35.13595963, ..., 33.75041962,\n         33.87859726, 33.99151611]],\n\n       [[36.590168  , 36.59474564, 36.58864212, ...,         nan,\n                 nan,         nan],\n        [36.56117249, 36.57643509, 36.59321976, ...,         nan,\n                 nan,         nan],\n        [36.7000351 , 36.61000443, 36.57796097, ...,         nan,\n                 nan,         nan],\n        ...,\n        [35.08407974, 35.06729507, 35.01541138, ..., 33.4299736 ,\n         33.41471481, 33.44981384],\n        [35.21378326, 35.19547272, 35.14206314, ..., 33.6420784 ,\n         33.72447968, 33.8465538 ],\n        [35.24277496, 35.18326187, 35.13290787, ..., 33.8831749 ,\n         34.00067139, 34.08307266]]])Indexes: (3)timePandasIndexPandasIndex(DatetimeIndex(['2003-01-01', '2003-01-02', '2003-01-03', '2003-01-04',\n               '2003-01-05', '2003-01-06', '2003-01-07', '2003-01-08',\n               '2003-01-09', '2003-01-10', '2003-01-11', '2003-01-12',\n               '2003-01-13', '2003-01-14', '2003-01-15', '2003-01-16',\n               '2003-01-17', '2003-01-18', '2003-01-19', '2003-01-20',\n               '2003-01-21', '2003-01-22', '2003-01-23', '2003-01-24',\n               '2003-01-25', '2003-01-26', '2003-01-27', '2003-01-28',\n               '2003-01-29', '2003-01-30', '2003-01-31', '2003-02-01',\n               '2003-02-02', '2003-02-03', '2003-02-04', '2003-02-05',\n               '2003-02-06', '2003-02-07', '2003-02-08', '2003-02-09',\n               '2003-02-10', '2003-02-11', '2003-02-12', '2003-02-13',\n               '2003-02-14', '2003-02-15', '2003-02-16', '2003-02-17',\n               '2003-02-18', '2003-02-19', '2003-02-20', '2003-02-21',\n               '2003-02-22', '2003-02-23', '2003-02-24', '2003-02-25',\n               '2003-02-26', '2003-02-27', '2003-02-28'],\n              dtype='datetime64[ns]', name='time', freq=None))latPandasIndexPandasIndex(Index([ 25.0, 24.75,  24.5, 24.25,  24.0, 23.75,  23.5, 23.25,  23.0, 22.75,\n        22.5, 22.25,  22.0, 21.75,  21.5, 21.25,  21.0, 20.75,  20.5, 20.25,\n        20.0, 19.75,  19.5, 19.25,  19.0, 18.75,  18.5, 18.25,  18.0, 17.75,\n        17.5, 17.25,  17.0, 16.75,  16.5, 16.25,  16.0, 15.75,  15.5, 15.25,\n        15.0, 14.75,  14.5, 14.25,  14.0, 13.75,  13.5, 13.25,  13.0, 12.75,\n        12.5, 12.25,  12.0, 11.75,  11.5, 11.25,  11.0, 10.75,  10.5, 10.25,\n        10.0,  9.75,   9.5,  9.25,   9.0,  8.75,   8.5,  8.25,   8.0,  7.75,\n         7.5,  7.25,   7.0,  6.75,   6.5,  6.25,   6.0,  5.75,   5.5,  5.25,\n         5.0],\n      dtype='float32', name='lat'))lonPandasIndexPandasIndex(Index([ 60.0, 60.25,  60.5, 60.75,  61.0, 61.25,  61.5, 61.75,  62.0, 62.25,\n        62.5, 62.75,  63.0, 63.25,  63.5, 63.75,  64.0, 64.25,  64.5, 64.75,\n        65.0, 65.25,  65.5, 65.75,  66.0, 66.25,  66.5, 66.75,  67.0, 67.25,\n        67.5, 67.75,  68.0, 68.25,  68.5, 68.75,  69.0, 69.25,  69.5, 69.75,\n        70.0, 70.25,  70.5, 70.75,  71.0, 71.25,  71.5, 71.75,  72.0, 72.25,\n        72.5, 72.75,  73.0, 73.25,  73.5, 73.75,  74.0, 74.25,  74.5, 74.75,\n        75.0, 75.25,  75.5, 75.75,  76.0, 76.25,  76.5, 76.75,  77.0, 77.25,\n        77.5, 77.75,  78.0, 78.25,  78.5, 78.75,  79.0, 79.25,  79.5, 79.75,\n        80.0],\n      dtype='float32', name='lon'))Attributes: (17)title :Sample of Climate Data for Coastal Upwelling Machine Learning Project in Indian Oceansummary :Daily mean of 0.25 x 0.25 degrees gridded data from multiple climate variables that may influence the patterns of coastal upwelling in the focused areacreator_name :Minh Phancreator_email :minhphan@uw.educreator_type :personsource :OSCAR, ERA5 Reanalysis, Copernicus Climate Change Service (C3S), Copernicus Marine Environment Monitoring Service (CMEMS)geospatial_lat_min :5.0geospatial_lat_max :25.0geospatial_lat_units :degrees_northgeospatial_lat_resolution :0.25geospatial_lon_min :60.0geospatial_lon_max :80.0geospatial_lon_units :degrees_eastgeospatial_lon_resolution :0.25time_coverage_start :2000-01-01T00:00:00time_coverage_end :2002-12-31T23:59:59date_created :2023-14-14",
    "crumbs": [
      "Python Tutorials",
      "IO Zarr Database",
      "Combine, clean, and export the data"
    ]
  },
  {
    "objectID": "python-notebooks/04_Combining_Data.html#adding-consistency-across-variables-convert-data-types-rechunking",
    "href": "python-notebooks/04_Combining_Data.html#adding-consistency-across-variables-convert-data-types-rechunking",
    "title": "Combine, clean, and export the data",
    "section": "Adding consistency across variables: convert data types, rechunking",
    "text": "Adding consistency across variables: convert data types, rechunking\nNote that some variables has float64 dtype, while others have float32. We want to add consistency by converting all of them to float32 to save some disk space. The precision from float64 is most likely not going to be lost after the conversion, as our figures do not have enough precision to exceed the limit.\n\nfor var in final_ds.data_vars:\n    if str(final_ds[var].dtype) == 'float64':\n        final_ds[var].values = final_ds[var].astype('float32')\n\nWe also need to rechunk the dataset before we can export the data. Some of the variables have original chunk sizes, which is specified in their encodings, and we need to reset these values so that zarr would not reconvert the encoding when export is happening. This is a known issue.\n\nfor var in final_ds.data_vars:\n    if 'chunks' in list(final_ds[var].encoding.keys()):\n        del final_ds[var].encoding['chunks']\n\nFinally, reset chunk size for exporting to zarr:\n\narray_chunk_size = {'time': 100, 'lat': final_ds.lat.shape[0], 'lon': final_ds.lon.shape[0]}\nfinal_ds = final_ds.chunk(array_chunk_size)",
    "crumbs": [
      "Python Tutorials",
      "IO Zarr Database",
      "Combine, clean, and export the data"
    ]
  },
  {
    "objectID": "python-notebooks/04_Combining_Data.html#export-data",
    "href": "python-notebooks/04_Combining_Data.html#export-data",
    "title": "Combine, clean, and export the data",
    "section": "Export data",
    "text": "Export data\n\nfinal_ds.to_zarr('demonstrated data/final-sample.zarr')\n\n&lt;xarray.backends.zarr.ZarrStore at 0x7fc73003ce40&gt;",
    "crumbs": [
      "Python Tutorials",
      "IO Zarr Database",
      "Combine, clean, and export the data"
    ]
  },
  {
    "objectID": "python-notebooks/02_Zarr_Access_netCDF.html#summary",
    "href": "python-notebooks/02_Zarr_Access_netCDF.html#summary",
    "title": "Zarr Access for NetCDF4 files",
    "section": "Summary",
    "text": "Summary\nZarr is an open source library for storing N-dimensional array data. It supports multidimensional arrays with attributes and dimensions similar to NetCDF4, and it can be read by XArray. Zarr is often used for data held in cloud object storage (like Amazon S3), because it is better optimized for these situations than NetCDF4.\nDO NOT USE zarr-eodis-store is depends on only XArray\nThe zarr-eosdis-store library allows NASA EOSDIS NetCDF4 files to be read more efficiently by transferring only file metadata and data needed for computation in a small number of requests, rather than moving the whole file or making many small requests. It works by making the files directly readable by the Zarr Python library and XArray across a network. To use it, files must have a corresponding metadata file ending in .dmrpp, which increasingly true for cloud-accessible EOSDIS data. https://github.com/nasa/zarr-eosdis-store\nThe zarr-eosdis-store library provides several benefits over downloading EOSDIS data files and accessing them using XArray, NetCDF4, or HDF5 Python libraries:\n\nIt only downloads the chunks of data you actually read, so if you don’t read all variables or the full spatiotemporal extent of a file, you usually won’t spend time downloading those portions of the file\nIt parallelizes and optimizes downloads for the portions of files you do read, so download speeds can be faster in general\nIt automatically interoperates with Earthdata Login if you have a .netrc file set up\nIt is aware of some EOSDIS cloud implementation quirks and provides caching that can save time for repeated requests to individual files\n\nIt can also be faster than using XArray pointing NetCDF4 files with s3:// URLs, depending on the file’s internal structure, and is often more convenient.\nConsider using this library when: 1. The portion of the data file you need to use is much smaller than the full file, e.g. in cases of spatial subsets or reading a single variable from a file containing several 1. s3:// URLs are not readily available 1. Code need to run outside of the AWS cloud or us-west-2 region or in a hybrid cloud / non-cloud manner 1. s3:// access using XArray seems slower than you would expect (possibly due to unoptimized internal file structure) 1. No readily-available, public, cloud-optimized version of the data exists already. The example we show is also available as an AWS Public Dataset: https://registry.opendata.aws/mur/ 1. Adding “.dmrpp” to the end of a data URL returns a file\n\nObjectives\n\nBuild on prior knowledge from CMR and Earthdata Login tutorials\nWork through an example of using the EOSDIS Zarr Store to access data using XArray\nLearn about the Zarr format and library for accessing data in the cloud",
    "crumbs": [
      "Python Tutorials",
      "Using Data",
      "Zarr Access for NetCDF4 files"
    ]
  },
  {
    "objectID": "python-notebooks/02_Zarr_Access_netCDF.html#exercise",
    "href": "python-notebooks/02_Zarr_Access_netCDF.html#exercise",
    "title": "Zarr Access for NetCDF4 files",
    "section": "Exercise",
    "text": "Exercise\nIn this exercise, we will be using the eosdis-zarr-store library to aggregate and analyze a month of sea surface temperature for the Great Lakes region\n\nSet up\n\nImport Required Packages\n\n# Core libraries for this tutorial\n# Available via `pip install zarr zarr-eosdis-store`\nfrom eosdis_store import EosdisStore\nimport xarray as xr\n\n# Other Python libraries\nimport requests\nfrom pqdm.threads import pqdm\nfrom matplotlib import animation, pyplot as plt\nfrom IPython.core.display import display, HTML\n\n# Python standard library imports\nfrom pprint import pprint\n\nAlso set the width / height for plots we show\n\nplt.rcParams['figure.figsize'] = 12, 6\n\n\n\nSet Dataset, Time, and Region of Interest\nLook in PO.DAAC’s cloud archive for Group for High Resolution Sea Surface Temperature (GHRSST) Level 4 Multiscale Ultrahigh Resolution (MUR) data\n\ndata_provider = 'POCLOUD'\nmur_short_name = 'MUR-JPL-L4-GLOB-v4.1'\n\nLooking for data from the month of September over the Great Lakes\n\nstart_time = '2021-09-01T21:00:00Z'\nend_time = '2021-09-30T20:59:59Z'\n\n# Bounding box around the Great Lakes\nlats = slice(41, 49)\nlons = slice(-93, -76)\n\n# Some other possibly interesting bounding boxes:\n\n# Hawaiian Islands\n# lats = slice(18, 22.5)\n# lons = slice(-161, -154)\n\n# Mediterranean Sea\n# lats = slice(29, 45)\n# lons = slice(-7, 37)\n\n\n\n\nFind URLs for the dataset and AOI\nSet up a CMR granules search for our area of interest, as we saw in prior tutorials\n\ncmr_url = 'https://cmr.earthdata.nasa.gov/search/granules.json'\n\nSearch for granules in our area of interest, expecting one granule per day of September\n\nresponse = requests.get(cmr_url, \n                        params={\n                            'provider': data_provider,\n                            'short_name': mur_short_name, \n                            'temporal': f'{start_time},{end_time}',\n                            'bounding_box': f'{lons.start},{lats.start},{lons.stop},{lats.stop}',\n                            'page_size': 2000,\n                            }\n                       )\n\n\ngranules = response.json()['feed']['entry']\n\nfor granule in granules:\n    print(granule['title'])\n\n20210901090000-JPL-L4_GHRSST-SSTfnd-MUR-GLOB-v02.0-fv04.1\n20210902090000-JPL-L4_GHRSST-SSTfnd-MUR-GLOB-v02.0-fv04.1\n20210903090000-JPL-L4_GHRSST-SSTfnd-MUR-GLOB-v02.0-fv04.1\n20210904090000-JPL-L4_GHRSST-SSTfnd-MUR-GLOB-v02.0-fv04.1\n20210905090000-JPL-L4_GHRSST-SSTfnd-MUR-GLOB-v02.0-fv04.1\n20210906090000-JPL-L4_GHRSST-SSTfnd-MUR-GLOB-v02.0-fv04.1\n20210907090000-JPL-L4_GHRSST-SSTfnd-MUR-GLOB-v02.0-fv04.1\n20210908090000-JPL-L4_GHRSST-SSTfnd-MUR-GLOB-v02.0-fv04.1\n20210909090000-JPL-L4_GHRSST-SSTfnd-MUR-GLOB-v02.0-fv04.1\n20210910090000-JPL-L4_GHRSST-SSTfnd-MUR-GLOB-v02.0-fv04.1\n20210911090000-JPL-L4_GHRSST-SSTfnd-MUR-GLOB-v02.0-fv04.1\n20210912090000-JPL-L4_GHRSST-SSTfnd-MUR-GLOB-v02.0-fv04.1\n20210913090000-JPL-L4_GHRSST-SSTfnd-MUR-GLOB-v02.0-fv04.1\n20210914090000-JPL-L4_GHRSST-SSTfnd-MUR-GLOB-v02.0-fv04.1\n20210915090000-JPL-L4_GHRSST-SSTfnd-MUR-GLOB-v02.0-fv04.1\n20210916090000-JPL-L4_GHRSST-SSTfnd-MUR-GLOB-v02.0-fv04.1\n20210917090000-JPL-L4_GHRSST-SSTfnd-MUR-GLOB-v02.0-fv04.1\n20210918090000-JPL-L4_GHRSST-SSTfnd-MUR-GLOB-v02.0-fv04.1\n20210919090000-JPL-L4_GHRSST-SSTfnd-MUR-GLOB-v02.0-fv04.1\n20210920090000-JPL-L4_GHRSST-SSTfnd-MUR-GLOB-v02.0-fv04.1\n20210921090000-JPL-L4_GHRSST-SSTfnd-MUR-GLOB-v02.0-fv04.1\n20210922090000-JPL-L4_GHRSST-SSTfnd-MUR-GLOB-v02.0-fv04.1\n20210923090000-JPL-L4_GHRSST-SSTfnd-MUR-GLOB-v02.0-fv04.1\n20210924090000-JPL-L4_GHRSST-SSTfnd-MUR-GLOB-v02.0-fv04.1\n20210925090000-JPL-L4_GHRSST-SSTfnd-MUR-GLOB-v02.0-fv04.1\n20210926090000-JPL-L4_GHRSST-SSTfnd-MUR-GLOB-v02.0-fv04.1\n20210927090000-JPL-L4_GHRSST-SSTfnd-MUR-GLOB-v02.0-fv04.1\n20210928090000-JPL-L4_GHRSST-SSTfnd-MUR-GLOB-v02.0-fv04.1\n20210929090000-JPL-L4_GHRSST-SSTfnd-MUR-GLOB-v02.0-fv04.1\n20210930090000-JPL-L4_GHRSST-SSTfnd-MUR-GLOB-v02.0-fv04.1\n\n\n\npprint(granules[0])\n\n{'boxes': ['-90 -180 90 180'],\n 'browse_flag': False,\n 'collection_concept_id': 'C1996881146-POCLOUD',\n 'coordinate_system': 'CARTESIAN',\n 'data_center': 'POCLOUD',\n 'dataset_id': 'GHRSST Level 4 MUR Global Foundation Sea Surface Temperature '\n               'Analysis (v4.1)',\n 'day_night_flag': 'UNSPECIFIED',\n 'granule_size': '9.059906005859375E-5',\n 'id': 'G2113241213-POCLOUD',\n 'links': [{'href': 's3://podaac-ops-cumulus-protected/MUR-JPL-L4-GLOB-v4.1/20210901090000-JPL-L4_GHRSST-SSTfnd-MUR-GLOB-v02.0-fv04.1.nc',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/s3#',\n            'title': 'This link provides direct download access via S3 to the '\n                     'granule.'},\n           {'href': 'https://archive.podaac.earthdata.nasa.gov/podaac-ops-cumulus-protected/MUR-JPL-L4-GLOB-v4.1/20210901090000-JPL-L4_GHRSST-SSTfnd-MUR-GLOB-v02.0-fv04.1.nc',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/data#',\n            'title': 'Download '\n                     '20210901090000-JPL-L4_GHRSST-SSTfnd-MUR-GLOB-v02.0-fv04.1.nc'},\n           {'href': 'https://archive.podaac.earthdata.nasa.gov/podaac-ops-cumulus-public/MUR-JPL-L4-GLOB-v4.1/20210901090000-JPL-L4_GHRSST-SSTfnd-MUR-GLOB-v02.0-fv04.1.nc.md5',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/metadata#',\n            'title': 'Download '\n                     '20210901090000-JPL-L4_GHRSST-SSTfnd-MUR-GLOB-v02.0-fv04.1.nc.md5'},\n           {'href': 'https://archive.podaac.earthdata.nasa.gov/s3credentials',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/metadata#',\n            'title': 'api endpoint to retrieve temporary credentials valid for '\n                     'same-region direct s3 access'},\n           {'href': 'https://opendap.earthdata.nasa.gov/collections/C1996881146-POCLOUD/granules/20210901090000-JPL-L4_GHRSST-SSTfnd-MUR-GLOB-v02.0-fv04.1',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/service#',\n            'title': 'OPeNDAP request URL'},\n           {'href': 'https://github.com/nasa/podaac_tools_and_services/tree/master/subset_opendap',\n            'hreflang': 'en-US',\n            'inherited': True,\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/documentation#'},\n           {'href': 'https://ghrsst.jpl.nasa.gov',\n            'hreflang': 'en-US',\n            'inherited': True,\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/documentation#'},\n           {'href': 'https://earthdata.nasa.gov/esds/competitive-programs/measures/mur-sst',\n            'hreflang': 'en-US',\n            'inherited': True,\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/metadata#'},\n           {'href': 'http://journals.ametsoc.org/doi/abs/10.1175/1520-0426%281998%29015%3C0741:BSHWSS%3E2.0.CO;2',\n            'hreflang': 'en-US',\n            'inherited': True,\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/documentation#'},\n           {'href': 'https://podaac-tools.jpl.nasa.gov/drive/files/OceanTemperature/ghrsst/docs/GDS20r5.pdf',\n            'hreflang': 'en-US',\n            'inherited': True,\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/documentation#'},\n           {'href': 'https://github.com/podaac/data-readers',\n            'hreflang': 'en-US',\n            'inherited': True,\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/documentation#'},\n           {'href': 'https://doi.org/10.1016/j.rse.2017.07.029',\n            'hreflang': 'en-US',\n            'inherited': True,\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/documentation#'},\n           {'href': 'https://registry.opendata.aws/mur/#usageexa',\n            'hreflang': 'en-US',\n            'inherited': True,\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/metadata#'},\n           {'href': 'http://www.ghrsst.org',\n            'hreflang': 'en-US',\n            'inherited': True,\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/documentation#'},\n           {'href': 'https://podaac.jpl.nasa.gov/CitingPODAAC',\n            'hreflang': 'en-US',\n            'inherited': True,\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/documentation#'},\n           {'href': 'https://cmr.earthdata.nasa.gov/virtual-directory/collections/C1996881146-POCLOUD ',\n            'hreflang': 'en-US',\n            'inherited': True,\n            'length': '300.0MB',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/data#'},\n           {'href': ' '\n                    'https://search.earthdata.nasa.gov/search/granules?p=C1996881146-POCLOUD ',\n            'hreflang': 'en-US',\n            'inherited': True,\n            'length': '700.0MB',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/data#'},\n           {'href': 'https://podaac.jpl.nasa.gov/MEaSUREs-MUR',\n            'hreflang': 'en-US',\n            'inherited': True,\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/documentation#'},\n           {'href': 'https://github.com/podaac/tutorials/blob/master/notebooks/SWOT-EA-2021/Colocate_satellite_insitu_ocean.ipynb',\n            'hreflang': 'en-US',\n            'inherited': True,\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/documentation#'}],\n 'online_access_flag': True,\n 'original_format': 'UMM_JSON',\n 'time_end': '2021-09-01T21:00:00.000Z',\n 'time_start': '2021-08-31T21:00:00.000Z',\n 'title': '20210901090000-JPL-L4_GHRSST-SSTfnd-MUR-GLOB-v02.0-fv04.1',\n 'updated': '2021-09-10T07:29:40.511Z'}\n\n\n\nurls = []\nfor granule in granules:\n    for link in granule['links']:\n        if link['rel'].endswith('/data#'):\n            urls.append(link['href'])\n            break\npprint(urls)\n\n['https://archive.podaac.earthdata.nasa.gov/podaac-ops-cumulus-protected/MUR-JPL-L4-GLOB-v4.1/20210901090000-JPL-L4_GHRSST-SSTfnd-MUR-GLOB-v02.0-fv04.1.nc',\n 'https://archive.podaac.earthdata.nasa.gov/podaac-ops-cumulus-protected/MUR-JPL-L4-GLOB-v4.1/20210902090000-JPL-L4_GHRSST-SSTfnd-MUR-GLOB-v02.0-fv04.1.nc',\n 'https://archive.podaac.earthdata.nasa.gov/podaac-ops-cumulus-protected/MUR-JPL-L4-GLOB-v4.1/20210903090000-JPL-L4_GHRSST-SSTfnd-MUR-GLOB-v02.0-fv04.1.nc',\n 'https://archive.podaac.earthdata.nasa.gov/podaac-ops-cumulus-protected/MUR-JPL-L4-GLOB-v4.1/20210904090000-JPL-L4_GHRSST-SSTfnd-MUR-GLOB-v02.0-fv04.1.nc',\n 'https://archive.podaac.earthdata.nasa.gov/podaac-ops-cumulus-protected/MUR-JPL-L4-GLOB-v4.1/20210905090000-JPL-L4_GHRSST-SSTfnd-MUR-GLOB-v02.0-fv04.1.nc',\n 'https://archive.podaac.earthdata.nasa.gov/podaac-ops-cumulus-protected/MUR-JPL-L4-GLOB-v4.1/20210906090000-JPL-L4_GHRSST-SSTfnd-MUR-GLOB-v02.0-fv04.1.nc',\n 'https://archive.podaac.earthdata.nasa.gov/podaac-ops-cumulus-protected/MUR-JPL-L4-GLOB-v4.1/20210907090000-JPL-L4_GHRSST-SSTfnd-MUR-GLOB-v02.0-fv04.1.nc',\n 'https://archive.podaac.earthdata.nasa.gov/podaac-ops-cumulus-protected/MUR-JPL-L4-GLOB-v4.1/20210908090000-JPL-L4_GHRSST-SSTfnd-MUR-GLOB-v02.0-fv04.1.nc',\n 'https://archive.podaac.earthdata.nasa.gov/podaac-ops-cumulus-protected/MUR-JPL-L4-GLOB-v4.1/20210909090000-JPL-L4_GHRSST-SSTfnd-MUR-GLOB-v02.0-fv04.1.nc',\n 'https://archive.podaac.earthdata.nasa.gov/podaac-ops-cumulus-protected/MUR-JPL-L4-GLOB-v4.1/20210910090000-JPL-L4_GHRSST-SSTfnd-MUR-GLOB-v02.0-fv04.1.nc',\n 'https://archive.podaac.earthdata.nasa.gov/podaac-ops-cumulus-protected/MUR-JPL-L4-GLOB-v4.1/20210911090000-JPL-L4_GHRSST-SSTfnd-MUR-GLOB-v02.0-fv04.1.nc',\n 'https://archive.podaac.earthdata.nasa.gov/podaac-ops-cumulus-protected/MUR-JPL-L4-GLOB-v4.1/20210912090000-JPL-L4_GHRSST-SSTfnd-MUR-GLOB-v02.0-fv04.1.nc',\n 'https://archive.podaac.earthdata.nasa.gov/podaac-ops-cumulus-protected/MUR-JPL-L4-GLOB-v4.1/20210913090000-JPL-L4_GHRSST-SSTfnd-MUR-GLOB-v02.0-fv04.1.nc',\n 'https://archive.podaac.earthdata.nasa.gov/podaac-ops-cumulus-protected/MUR-JPL-L4-GLOB-v4.1/20210914090000-JPL-L4_GHRSST-SSTfnd-MUR-GLOB-v02.0-fv04.1.nc',\n 'https://archive.podaac.earthdata.nasa.gov/podaac-ops-cumulus-protected/MUR-JPL-L4-GLOB-v4.1/20210915090000-JPL-L4_GHRSST-SSTfnd-MUR-GLOB-v02.0-fv04.1.nc',\n 'https://archive.podaac.earthdata.nasa.gov/podaac-ops-cumulus-protected/MUR-JPL-L4-GLOB-v4.1/20210916090000-JPL-L4_GHRSST-SSTfnd-MUR-GLOB-v02.0-fv04.1.nc',\n 'https://archive.podaac.earthdata.nasa.gov/podaac-ops-cumulus-protected/MUR-JPL-L4-GLOB-v4.1/20210917090000-JPL-L4_GHRSST-SSTfnd-MUR-GLOB-v02.0-fv04.1.nc',\n 'https://archive.podaac.earthdata.nasa.gov/podaac-ops-cumulus-protected/MUR-JPL-L4-GLOB-v4.1/20210918090000-JPL-L4_GHRSST-SSTfnd-MUR-GLOB-v02.0-fv04.1.nc',\n 'https://archive.podaac.earthdata.nasa.gov/podaac-ops-cumulus-protected/MUR-JPL-L4-GLOB-v4.1/20210919090000-JPL-L4_GHRSST-SSTfnd-MUR-GLOB-v02.0-fv04.1.nc',\n 'https://archive.podaac.earthdata.nasa.gov/podaac-ops-cumulus-protected/MUR-JPL-L4-GLOB-v4.1/20210920090000-JPL-L4_GHRSST-SSTfnd-MUR-GLOB-v02.0-fv04.1.nc',\n 'https://archive.podaac.earthdata.nasa.gov/podaac-ops-cumulus-protected/MUR-JPL-L4-GLOB-v4.1/20210921090000-JPL-L4_GHRSST-SSTfnd-MUR-GLOB-v02.0-fv04.1.nc',\n 'https://archive.podaac.earthdata.nasa.gov/podaac-ops-cumulus-protected/MUR-JPL-L4-GLOB-v4.1/20210922090000-JPL-L4_GHRSST-SSTfnd-MUR-GLOB-v02.0-fv04.1.nc',\n 'https://archive.podaac.earthdata.nasa.gov/podaac-ops-cumulus-protected/MUR-JPL-L4-GLOB-v4.1/20210923090000-JPL-L4_GHRSST-SSTfnd-MUR-GLOB-v02.0-fv04.1.nc',\n 'https://archive.podaac.earthdata.nasa.gov/podaac-ops-cumulus-protected/MUR-JPL-L4-GLOB-v4.1/20210924090000-JPL-L4_GHRSST-SSTfnd-MUR-GLOB-v02.0-fv04.1.nc',\n 'https://archive.podaac.earthdata.nasa.gov/podaac-ops-cumulus-protected/MUR-JPL-L4-GLOB-v4.1/20210925090000-JPL-L4_GHRSST-SSTfnd-MUR-GLOB-v02.0-fv04.1.nc',\n 'https://archive.podaac.earthdata.nasa.gov/podaac-ops-cumulus-protected/MUR-JPL-L4-GLOB-v4.1/20210926090000-JPL-L4_GHRSST-SSTfnd-MUR-GLOB-v02.0-fv04.1.nc',\n 'https://archive.podaac.earthdata.nasa.gov/podaac-ops-cumulus-protected/MUR-JPL-L4-GLOB-v4.1/20210927090000-JPL-L4_GHRSST-SSTfnd-MUR-GLOB-v02.0-fv04.1.nc',\n 'https://archive.podaac.earthdata.nasa.gov/podaac-ops-cumulus-protected/MUR-JPL-L4-GLOB-v4.1/20210928090000-JPL-L4_GHRSST-SSTfnd-MUR-GLOB-v02.0-fv04.1.nc',\n 'https://archive.podaac.earthdata.nasa.gov/podaac-ops-cumulus-protected/MUR-JPL-L4-GLOB-v4.1/20210929090000-JPL-L4_GHRSST-SSTfnd-MUR-GLOB-v02.0-fv04.1.nc',\n 'https://archive.podaac.earthdata.nasa.gov/podaac-ops-cumulus-protected/MUR-JPL-L4-GLOB-v4.1/20210930090000-JPL-L4_GHRSST-SSTfnd-MUR-GLOB-v02.0-fv04.1.nc']\n\n\n\n\nOpen and view our AOI without downloading a whole file\n\nCheck to see if we can use an efficient partial-access technique\n\nresponse = requests.head(f'{urls[0]}.dmrpp')\n\nprint('Can we use EosdisZarrStore and XArray to access these files more efficiently?')\nprint('Yes' if response.ok else 'No')\n\nCan we use EosdisZarrStore and XArray to access these files more efficiently?\nYes\n\n\nOpen our first URL using the Zarr library\n\nurl = urls[0]\n\nds = xr.open_zarr(EosdisStore(url), consolidated=False)\n\nThat’s it! No downloads, temporary credentials, or S3 filesystems. Hereafter, we interact with the ds variable as with any XArray dataset. We need not worry about the EosdisStore anymore.\nView the file’s variable structure\n\nds\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt;\nDimensions:           (time: 1, lat: 17999, lon: 36000)\nCoordinates:\n  * lat               (lat) float32 -89.99 -89.98 -89.97 ... 89.97 89.98 89.99\n  * lon               (lon) float32 -180.0 -180.0 -180.0 ... 180.0 180.0 180.0\n  * time              (time) datetime64[ns] 2021-09-01T09:00:00\nData variables:\n    analysed_sst      (time, lat, lon) float32 dask.array&lt;chunksize=(1, 1023, 2047), meta=np.ndarray&gt;\n    analysis_error    (time, lat, lon) float32 dask.array&lt;chunksize=(1, 1023, 2047), meta=np.ndarray&gt;\n    dt_1km_data       (time, lat, lon) timedelta64[ns] dask.array&lt;chunksize=(1, 1447, 2895), meta=np.ndarray&gt;\n    mask              (time, lat, lon) float32 dask.array&lt;chunksize=(1, 1447, 2895), meta=np.ndarray&gt;\n    sea_ice_fraction  (time, lat, lon) float32 dask.array&lt;chunksize=(1, 1447, 2895), meta=np.ndarray&gt;\n    sst_anomaly       (time, lat, lon) float32 dask.array&lt;chunksize=(1, 1023, 2047), meta=np.ndarray&gt;\nAttributes: (12/47)\n    Conventions:                CF-1.7\n    title:                      Daily MUR SST, Final product\n    summary:                    A merged, multi-sensor L4 Foundation SST anal...\n    references:                 http://podaac.jpl.nasa.gov/Multi-scale_Ultra-...\n    institution:                Jet Propulsion Laboratory\n    history:                    created at nominal 4-day latency; replaced nr...\n    ...                         ...\n    project:                    NASA Making Earth Science Data Records for Us...\n    publisher_name:             GHRSST Project Office\n    publisher_url:              http://www.ghrsst.org\n    publisher_email:            ghrsst-po@nceo.ac.uk\n    processing_level:           L4\n    cdm_data_type:              gridxarray.DatasetDimensions:time: 1lat: 17999lon: 36000Coordinates: (3)lat(lat)float32-89.99 -89.98 ... 89.98 89.99long_name :latitudestandard_name :latitudeaxis :Yunits :degrees_northvalid_min :-90.0valid_max :90.0comment :geolocations inherited from the input data without correctionarray([-89.99, -89.98, -89.97, ...,  89.97,  89.98,  89.99], dtype=float32)lon(lon)float32-180.0 -180.0 ... 180.0 180.0long_name :longitudestandard_name :longitudeaxis :Xunits :degrees_eastvalid_min :-180.0valid_max :180.0comment :geolocations inherited from the input data without correctionarray([-179.99, -179.98, -179.97, ...,  179.98,  179.99,  180.  ],\n      dtype=float32)time(time)datetime64[ns]2021-09-01T09:00:00long_name :reference time of sst fieldstandard_name :timeaxis :Tcomment :Nominal time of analyzed fieldsarray(['2021-09-01T09:00:00.000000000'], dtype='datetime64[ns]')Data variables: (6)analysed_sst(time, lat, lon)float32dask.array&lt;chunksize=(1, 1023, 2047), meta=np.ndarray&gt;long_name :analysed sea surface temperaturestandard_name :sea_surface_foundation_temperatureunits :kelvinvalid_min :-32767valid_max :32767comment :\\\"Final\\\" version using Multi-Resolution Variational Analysis (MRVA) method for interpolationsource :MODIS_T-JPL, MODIS_A-JPL, AMSR2-REMSS, AVHRRMTA_G-NAVO, AVHRRMTB_G-NAVO, iQUAM-NOAA/NESDIS, Ice_Conc-OSISAF\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n2.41 GiB\n7.99 MiB\n\n\nShape\n(1, 17999, 36000)\n(1, 1023, 2047)\n\n\nCount\n325 Tasks\n324 Chunks\n\n\nType\nfloat32\nnumpy.ndarray\n\n\n\n\n\n\n\n\nanalysis_error\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(1, 1023, 2047), meta=np.ndarray&gt;\n\n\n\n\nlong_name :\n\nestimated error standard deviation of analysed_sst\n\nunits :\n\nkelvin\n\nvalid_min :\n\n0\n\nvalid_max :\n\n32767\n\ncomment :\n\nuncertainty in \\\"analysed_sst\\\"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n2.41 GiB\n7.99 MiB\n\n\nShape\n(1, 17999, 36000)\n(1, 1023, 2047)\n\n\nCount\n325 Tasks\n324 Chunks\n\n\nType\nfloat32\nnumpy.ndarray\n\n\n\n\n\n\n\n\n\ndt_1km_data\n\n\n(time, lat, lon)\n\n\ntimedelta64[ns]\n\n\ndask.array&lt;chunksize=(1, 1447, 2895), meta=np.ndarray&gt;\n\n\n\n\nlong_name :\n\ntime to most recent 1km data\n\nvalid_min :\n\n-127\n\nvalid_max :\n\n127\n\nsource :\n\nMODIS and VIIRS pixels ingested by MUR\n\ncomment :\n\nThe grid value is hours between the analysis time and the most recent MODIS or VIIRS 1km L2P datum within 0.01 degrees from the grid point. \\\"Fill value\\\" indicates absence of such 1km data at the grid point.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n4.83 GiB\n31.96 MiB\n\n\nShape\n(1, 17999, 36000)\n(1, 1447, 2895)\n\n\nCount\n170 Tasks\n169 Chunks\n\n\nType\ntimedelta64[ns]\nnumpy.ndarray\n\n\n\n\n\n\n\n\n\nmask\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(1, 1447, 2895), meta=np.ndarray&gt;\n\n\n\n\nlong_name :\n\nsea/land field composite mask\n\nvalid_min :\n\n1\n\nvalid_max :\n\n31\n\nflag_masks :\n\n[1, 2, 4, 8, 16]\n\nflag_meanings :\n\nopen_sea land open_lake open_sea_with_ice_in_the_grid open_lake_with_ice_in_the_grid\n\ncomment :\n\nmask can be used to further filter the data.\n\nsource :\n\nGMT \\\"grdlandmask\\\", ice flag from sea_ice_fraction data\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n2.41 GiB\n15.98 MiB\n\n\nShape\n(1, 17999, 36000)\n(1, 1447, 2895)\n\n\nCount\n170 Tasks\n169 Chunks\n\n\nType\nfloat32\nnumpy.ndarray\n\n\n\n\n\n\n\n\n\nsea_ice_fraction\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(1, 1447, 2895), meta=np.ndarray&gt;\n\n\n\n\nlong_name :\n\nsea ice area fraction\n\nstandard_name :\n\nsea_ice_area_fraction\n\nvalid_min :\n\n0\n\nvalid_max :\n\n100\n\nsource :\n\nEUMETSAT OSI-SAF, copyright EUMETSAT\n\ncomment :\n\nice fraction is a dimensionless quantity between 0 and 1; it has been interpolated by a nearest neighbor approach.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n2.41 GiB\n15.98 MiB\n\n\nShape\n(1, 17999, 36000)\n(1, 1447, 2895)\n\n\nCount\n170 Tasks\n169 Chunks\n\n\nType\nfloat32\nnumpy.ndarray\n\n\n\n\n\n\n\n\n\nsst_anomaly\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(1, 1023, 2047), meta=np.ndarray&gt;\n\n\n\n\nlong_name :\n\nSST anomaly from a seasonal SST climatology based on the MUR data over 2003-2014 period\n\nunits :\n\nkelvin\n\nvalid_min :\n\n-32767\n\nvalid_max :\n\n32767\n\ncomment :\n\nanomaly reference to the day-of-year average between 2003 and 2014\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n2.41 GiB\n7.99 MiB\n\n\nShape\n(1, 17999, 36000)\n(1, 1023, 2047)\n\n\nCount\n325 Tasks\n324 Chunks\n\n\nType\nfloat32\nnumpy.ndarray\n\n\n\n\n\n\n\n\nAttributes: (47)Conventions :CF-1.7title :Daily MUR SST, Final productsummary :A merged, multi-sensor L4 Foundation SST analysis product from JPL.references :http://podaac.jpl.nasa.gov/Multi-scale_Ultra-high_Resolution_MUR-SSTinstitution :Jet Propulsion Laboratoryhistory :created at nominal 4-day latency; replaced nrt (1-day latency) version.comment :MUR = \\\"Multi-scale Ultra-high Resolution\\\"license :These data are available free of charge under data policy of JPL PO.DAAC.id :MUR-JPL-L4-GLOB-v04.1naming_authority :org.ghrsstproduct_version :04.1uuid :27665bc0-d5fc-11e1-9b23-0800200c9a66gds_version_id :2.0netcdf_version_id :4.1date_created :20210910T072132Zstart_time :20210901T090000Zstop_time :20210901T090000Ztime_coverage_start :20210831T210000Ztime_coverage_end :20210901T210000Zfile_quality_level :3source :MODIS_T-JPL, MODIS_A-JPL, AMSR2-REMSS, AVHRRMTA_G-NAVO, AVHRRMTB_G-NAVO, iQUAM-NOAA/NESDIS, Ice_Conc-OSISAFplatform :Terra, Aqua, GCOM-W, MetOp-A, MetOp-B, Buoys/Shipssensor :MODIS, AMSR2, AVHRR, in-situMetadata_Conventions :Unidata Observation Dataset v1.0metadata_link :http://podaac.jpl.nasa.gov/ws/metadata/dataset/?format=iso&shortName=MUR-JPL-L4-GLOB-v04.1keywords :Oceans &gt; Ocean Temperature &gt; Sea Surface Temperaturekeywords_vocabulary :NASA Global Change Master Directory (GCMD) Science Keywordsstandard_name_vocabulary :NetCDF Climate and Forecast (CF) Metadata Conventionsouthernmost_latitude :-90.0northernmost_latitude :90.0westernmost_longitude :-180.0easternmost_longitude :180.0spatial_resolution :0.01 degreesgeospatial_lat_units :degrees northgeospatial_lat_resolution :0.009999999776geospatial_lon_units :degrees eastgeospatial_lon_resolution :0.009999999776acknowledgment :Please acknowledge the use of these data with the following statement:  These data were provided by JPL under support by NASA MEaSUREs program.creator_name :JPL MUR SST projectcreator_email :ghrsst@podaac.jpl.nasa.govcreator_url :http://mur.jpl.nasa.govproject :NASA Making Earth Science Data Records for Use in Research Environments (MEaSUREs) Programpublisher_name :GHRSST Project Officepublisher_url :http://www.ghrsst.orgpublisher_email :ghrsst-po@nceo.ac.ukprocessing_level :L4cdm_data_type :grid\n\n\n\nds.analysed_sst\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray 'analysed_sst' (time: 1, lat: 17999, lon: 36000)&gt;\ndask.array&lt;open_dataset-4d5a9a1e1fda090e80524b67b2e413c6analysed_sst, shape=(1, 17999, 36000), dtype=float32, chunksize=(1, 1023, 2047), chunktype=numpy.ndarray&gt;\nCoordinates:\n  * lat      (lat) float32 -89.99 -89.98 -89.97 -89.96 ... 89.97 89.98 89.99\n  * lon      (lon) float32 -180.0 -180.0 -180.0 -180.0 ... 180.0 180.0 180.0\n  * time     (time) datetime64[ns] 2021-09-01T09:00:00\nAttributes:\n    long_name:      analysed sea surface temperature\n    standard_name:  sea_surface_foundation_temperature\n    units:          kelvin\n    valid_min:      -32767\n    valid_max:      32767\n    comment:        \\\"Final\\\" version using Multi-Resolution Variational Anal...\n    source:         MODIS_T-JPL, MODIS_A-JPL, AMSR2-REMSS, AVHRRMTA_G-NAVO, A...xarray.DataArray'analysed_sst'time: 1lat: 17999lon: 36000dask.array&lt;chunksize=(1, 1023, 2047), meta=np.ndarray&gt;\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n2.41 GiB\n7.99 MiB\n\n\nShape\n(1, 17999, 36000)\n(1, 1023, 2047)\n\n\nCount\n325 Tasks\n324 Chunks\n\n\nType\nfloat32\nnumpy.ndarray\n\n\n\n\n\n\n\nCoordinates: (3)lat(lat)float32-89.99 -89.98 ... 89.98 89.99long_name :latitudestandard_name :latitudeaxis :Yunits :degrees_northvalid_min :-90.0valid_max :90.0comment :geolocations inherited from the input data without correctionarray([-89.99, -89.98, -89.97, ...,  89.97,  89.98,  89.99], dtype=float32)lon(lon)float32-180.0 -180.0 ... 180.0 180.0long_name :longitudestandard_name :longitudeaxis :Xunits :degrees_eastvalid_min :-180.0valid_max :180.0comment :geolocations inherited from the input data without correctionarray([-179.99, -179.98, -179.97, ...,  179.98,  179.99,  180.  ],\n      dtype=float32)time(time)datetime64[ns]2021-09-01T09:00:00long_name :reference time of sst fieldstandard_name :timeaxis :Tcomment :Nominal time of analyzed fieldsarray(['2021-09-01T09:00:00.000000000'], dtype='datetime64[ns]')Attributes: (7)long_name :analysed sea surface temperaturestandard_name :sea_surface_foundation_temperatureunits :kelvinvalid_min :-32767valid_max :32767comment :\\\"Final\\\" version using Multi-Resolution Variational Analysis (MRVA) method for interpolationsource :MODIS_T-JPL, MODIS_A-JPL, AMSR2-REMSS, AVHRRMTA_G-NAVO, AVHRRMTB_G-NAVO, iQUAM-NOAA/NESDIS, Ice_Conc-OSISAF\n\n\n\nsst = ds.analysed_sst.sel(lat=lats, lon=lons)\nsst\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray 'analysed_sst' (time: 1, lat: 801, lon: 1701)&gt;\ndask.array&lt;getitem, shape=(1, 801, 1701), dtype=float32, chunksize=(1, 601, 1536), chunktype=numpy.ndarray&gt;\nCoordinates:\n  * lat      (lat) float32 41.0 41.01 41.02 41.03 ... 48.97 48.98 48.99 49.0\n  * lon      (lon) float32 -93.0 -92.99 -92.98 -92.97 ... -76.02 -76.01 -76.0\n  * time     (time) datetime64[ns] 2021-09-01T09:00:00\nAttributes:\n    long_name:      analysed sea surface temperature\n    standard_name:  sea_surface_foundation_temperature\n    units:          kelvin\n    valid_min:      -32767\n    valid_max:      32767\n    comment:        \\\"Final\\\" version using Multi-Resolution Variational Anal...\n    source:         MODIS_T-JPL, MODIS_A-JPL, AMSR2-REMSS, AVHRRMTA_G-NAVO, A...xarray.DataArray'analysed_sst'time: 1lat: 801lon: 1701dask.array&lt;chunksize=(1, 200, 1536), meta=np.ndarray&gt;\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n5.20 MiB\n3.52 MiB\n\n\nShape\n(1, 801, 1701)\n(1, 601, 1536)\n\n\nCount\n329 Tasks\n4 Chunks\n\n\nType\nfloat32\nnumpy.ndarray\n\n\n\n\n\n\n\nCoordinates: (3)lat(lat)float3241.0 41.01 41.02 ... 48.99 49.0long_name :latitudestandard_name :latitudeaxis :Yunits :degrees_northvalid_min :-90.0valid_max :90.0comment :geolocations inherited from the input data without correctionarray([41.  , 41.01, 41.02, ..., 48.98, 48.99, 49.  ], dtype=float32)lon(lon)float32-93.0 -92.99 ... -76.01 -76.0long_name :longitudestandard_name :longitudeaxis :Xunits :degrees_eastvalid_min :-180.0valid_max :180.0comment :geolocations inherited from the input data without correctionarray([-93.  , -92.99, -92.98, ..., -76.02, -76.01, -76.  ], dtype=float32)time(time)datetime64[ns]2021-09-01T09:00:00long_name :reference time of sst fieldstandard_name :timeaxis :Tcomment :Nominal time of analyzed fieldsarray(['2021-09-01T09:00:00.000000000'], dtype='datetime64[ns]')Attributes: (7)long_name :analysed sea surface temperaturestandard_name :sea_surface_foundation_temperatureunits :kelvinvalid_min :-32767valid_max :32767comment :\\\"Final\\\" version using Multi-Resolution Variational Analysis (MRVA) method for interpolationsource :MODIS_T-JPL, MODIS_A-JPL, AMSR2-REMSS, AVHRRMTA_G-NAVO, AVHRRMTB_G-NAVO, iQUAM-NOAA/NESDIS, Ice_Conc-OSISAF\n\n\n\nsst.plot()\n\n\n\n\n\n\n\nAggregate and analyze 30 files\nSet up a function to open all of our URLs as XArrays in parallel\n\ndef open_as_zarr_xarray(url):\n    return xr.open_zarr(EosdisStore(url), consolidated=False)\n\ndatasets = pqdm(urls, open_as_zarr_xarray, n_jobs=30)\n\n\n\n\n\n\n\n\n\n\nCombine the individual file-based datasets into a single xarray dataset with a time axis\n\nds = xr.concat(datasets, 'time')\nds\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt;\nDimensions:           (time: 30, lat: 17999, lon: 36000)\nCoordinates:\n  * lat               (lat) float32 -89.99 -89.98 -89.97 ... 89.97 89.98 89.99\n  * lon               (lon) float32 -180.0 -180.0 -180.0 ... 180.0 180.0 180.0\n  * time              (time) datetime64[ns] 2021-09-01T09:00:00 ... 2021-09-3...\nData variables:\n    analysed_sst      (time, lat, lon) float32 dask.array&lt;chunksize=(1, 1023, 2047), meta=np.ndarray&gt;\n    analysis_error    (time, lat, lon) float32 dask.array&lt;chunksize=(1, 1023, 2047), meta=np.ndarray&gt;\n    dt_1km_data       (time, lat, lon) timedelta64[ns] dask.array&lt;chunksize=(1, 1447, 2895), meta=np.ndarray&gt;\n    mask              (time, lat, lon) float32 dask.array&lt;chunksize=(1, 1447, 2895), meta=np.ndarray&gt;\n    sea_ice_fraction  (time, lat, lon) float32 dask.array&lt;chunksize=(1, 1447, 2895), meta=np.ndarray&gt;\n    sst_anomaly       (time, lat, lon) float32 dask.array&lt;chunksize=(1, 1023, 2047), meta=np.ndarray&gt;\nAttributes: (12/47)\n    Conventions:                CF-1.7\n    title:                      Daily MUR SST, Final product\n    summary:                    A merged, multi-sensor L4 Foundation SST anal...\n    references:                 http://podaac.jpl.nasa.gov/Multi-scale_Ultra-...\n    institution:                Jet Propulsion Laboratory\n    history:                    created at nominal 4-day latency; replaced nr...\n    ...                         ...\n    project:                    NASA Making Earth Science Data Records for Us...\n    publisher_name:             GHRSST Project Office\n    publisher_url:              http://www.ghrsst.org\n    publisher_email:            ghrsst-po@nceo.ac.uk\n    processing_level:           L4\n    cdm_data_type:              gridxarray.DatasetDimensions:time: 30lat: 17999lon: 36000Coordinates: (3)lat(lat)float32-89.99 -89.98 ... 89.98 89.99long_name :latitudestandard_name :latitudeaxis :Yunits :degrees_northvalid_min :-90.0valid_max :90.0comment :geolocations inherited from the input data without correctionarray([-89.99, -89.98, -89.97, ...,  89.97,  89.98,  89.99], dtype=float32)lon(lon)float32-180.0 -180.0 ... 180.0 180.0long_name :longitudestandard_name :longitudeaxis :Xunits :degrees_eastvalid_min :-180.0valid_max :180.0comment :geolocations inherited from the input data without correctionarray([-179.99, -179.98, -179.97, ...,  179.98,  179.99,  180.  ],\n      dtype=float32)time(time)datetime64[ns]2021-09-01T09:00:00 ... 2021-09-...long_name :reference time of sst fieldstandard_name :timeaxis :Tcomment :Nominal time of analyzed fieldsarray(['2021-09-01T09:00:00.000000000', '2021-09-02T09:00:00.000000000',\n       '2021-09-03T09:00:00.000000000', '2021-09-04T09:00:00.000000000',\n       '2021-09-05T09:00:00.000000000', '2021-09-06T09:00:00.000000000',\n       '2021-09-07T09:00:00.000000000', '2021-09-08T09:00:00.000000000',\n       '2021-09-09T09:00:00.000000000', '2021-09-10T09:00:00.000000000',\n       '2021-09-11T09:00:00.000000000', '2021-09-12T09:00:00.000000000',\n       '2021-09-13T09:00:00.000000000', '2021-09-14T09:00:00.000000000',\n       '2021-09-15T09:00:00.000000000', '2021-09-16T09:00:00.000000000',\n       '2021-09-17T09:00:00.000000000', '2021-09-18T09:00:00.000000000',\n       '2021-09-19T09:00:00.000000000', '2021-09-20T09:00:00.000000000',\n       '2021-09-21T09:00:00.000000000', '2021-09-22T09:00:00.000000000',\n       '2021-09-23T09:00:00.000000000', '2021-09-24T09:00:00.000000000',\n       '2021-09-25T09:00:00.000000000', '2021-09-26T09:00:00.000000000',\n       '2021-09-27T09:00:00.000000000', '2021-09-28T09:00:00.000000000',\n       '2021-09-29T09:00:00.000000000', '2021-09-30T09:00:00.000000000'],\n      dtype='datetime64[ns]')Data variables: (6)analysed_sst(time, lat, lon)float32dask.array&lt;chunksize=(1, 1023, 2047), meta=np.ndarray&gt;long_name :analysed sea surface temperaturestandard_name :sea_surface_foundation_temperatureunits :kelvinvalid_min :-32767valid_max :32767comment :\\\"Final\\\" version using Multi-Resolution Variational Analysis (MRVA) method for interpolationsource :MODIS_T-JPL, MODIS_A-JPL, AMSR2-REMSS, AVHRRMTA_G-NAVO, AVHRRMTB_G-NAVO, iQUAM-NOAA/NESDIS, Ice_Conc-OSISAF\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n72.42 GiB\n7.99 MiB\n\n\nShape\n(30, 17999, 36000)\n(1, 1023, 2047)\n\n\nCount\n19470 Tasks\n9720 Chunks\n\n\nType\nfloat32\nnumpy.ndarray\n\n\n\n\n\n\n\n\nanalysis_error\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(1, 1023, 2047), meta=np.ndarray&gt;\n\n\n\n\nlong_name :\n\nestimated error standard deviation of analysed_sst\n\nunits :\n\nkelvin\n\nvalid_min :\n\n0\n\nvalid_max :\n\n32767\n\ncomment :\n\nuncertainty in \\\"analysed_sst\\\"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n72.42 GiB\n7.99 MiB\n\n\nShape\n(30, 17999, 36000)\n(1, 1023, 2047)\n\n\nCount\n19470 Tasks\n9720 Chunks\n\n\nType\nfloat32\nnumpy.ndarray\n\n\n\n\n\n\n\n\n\ndt_1km_data\n\n\n(time, lat, lon)\n\n\ntimedelta64[ns]\n\n\ndask.array&lt;chunksize=(1, 1447, 2895), meta=np.ndarray&gt;\n\n\n\n\nlong_name :\n\ntime to most recent 1km data\n\nvalid_min :\n\n-127\n\nvalid_max :\n\n127\n\nsource :\n\nMODIS and VIIRS pixels ingested by MUR\n\ncomment :\n\nThe grid value is hours between the analysis time and the most recent MODIS or VIIRS 1km L2P datum within 0.01 degrees from the grid point. \\\"Fill value\\\" indicates absence of such 1km data at the grid point.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n144.83 GiB\n31.96 MiB\n\n\nShape\n(30, 17999, 36000)\n(1, 1447, 2895)\n\n\nCount\n10170 Tasks\n5070 Chunks\n\n\nType\ntimedelta64[ns]\nnumpy.ndarray\n\n\n\n\n\n\n\n\n\nmask\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(1, 1447, 2895), meta=np.ndarray&gt;\n\n\n\n\nlong_name :\n\nsea/land field composite mask\n\nvalid_min :\n\n1\n\nvalid_max :\n\n31\n\nflag_masks :\n\n[1, 2, 4, 8, 16]\n\nflag_meanings :\n\nopen_sea land open_lake open_sea_with_ice_in_the_grid open_lake_with_ice_in_the_grid\n\ncomment :\n\nmask can be used to further filter the data.\n\nsource :\n\nGMT \\\"grdlandmask\\\", ice flag from sea_ice_fraction data\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n72.42 GiB\n15.98 MiB\n\n\nShape\n(30, 17999, 36000)\n(1, 1447, 2895)\n\n\nCount\n10170 Tasks\n5070 Chunks\n\n\nType\nfloat32\nnumpy.ndarray\n\n\n\n\n\n\n\n\n\nsea_ice_fraction\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(1, 1447, 2895), meta=np.ndarray&gt;\n\n\n\n\nlong_name :\n\nsea ice area fraction\n\nstandard_name :\n\nsea_ice_area_fraction\n\nvalid_min :\n\n0\n\nvalid_max :\n\n100\n\nsource :\n\nEUMETSAT OSI-SAF, copyright EUMETSAT\n\ncomment :\n\nice fraction is a dimensionless quantity between 0 and 1; it has been interpolated by a nearest neighbor approach.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n72.42 GiB\n15.98 MiB\n\n\nShape\n(30, 17999, 36000)\n(1, 1447, 2895)\n\n\nCount\n10170 Tasks\n5070 Chunks\n\n\nType\nfloat32\nnumpy.ndarray\n\n\n\n\n\n\n\n\n\nsst_anomaly\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(1, 1023, 2047), meta=np.ndarray&gt;\n\n\n\n\nlong_name :\n\nSST anomaly from a seasonal SST climatology based on the MUR data over 2003-2014 period\n\nunits :\n\nkelvin\n\nvalid_min :\n\n-32767\n\nvalid_max :\n\n32767\n\ncomment :\n\nanomaly reference to the day-of-year average between 2003 and 2014\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n72.42 GiB\n7.99 MiB\n\n\nShape\n(30, 17999, 36000)\n(1, 1023, 2047)\n\n\nCount\n19470 Tasks\n9720 Chunks\n\n\nType\nfloat32\nnumpy.ndarray\n\n\n\n\n\n\n\n\nAttributes: (47)Conventions :CF-1.7title :Daily MUR SST, Final productsummary :A merged, multi-sensor L4 Foundation SST analysis product from JPL.references :http://podaac.jpl.nasa.gov/Multi-scale_Ultra-high_Resolution_MUR-SSTinstitution :Jet Propulsion Laboratoryhistory :created at nominal 4-day latency; replaced nrt (1-day latency) version.comment :MUR = \\\"Multi-scale Ultra-high Resolution\\\"license :These data are available free of charge under data policy of JPL PO.DAAC.id :MUR-JPL-L4-GLOB-v04.1naming_authority :org.ghrsstproduct_version :04.1uuid :27665bc0-d5fc-11e1-9b23-0800200c9a66gds_version_id :2.0netcdf_version_id :4.1date_created :20210910T072132Zstart_time :20210901T090000Zstop_time :20210901T090000Ztime_coverage_start :20210831T210000Ztime_coverage_end :20210901T210000Zfile_quality_level :3source :MODIS_T-JPL, MODIS_A-JPL, AMSR2-REMSS, AVHRRMTA_G-NAVO, AVHRRMTB_G-NAVO, iQUAM-NOAA/NESDIS, Ice_Conc-OSISAFplatform :Terra, Aqua, GCOM-W, MetOp-A, MetOp-B, Buoys/Shipssensor :MODIS, AMSR2, AVHRR, in-situMetadata_Conventions :Unidata Observation Dataset v1.0metadata_link :http://podaac.jpl.nasa.gov/ws/metadata/dataset/?format=iso&shortName=MUR-JPL-L4-GLOB-v04.1keywords :Oceans &gt; Ocean Temperature &gt; Sea Surface Temperaturekeywords_vocabulary :NASA Global Change Master Directory (GCMD) Science Keywordsstandard_name_vocabulary :NetCDF Climate and Forecast (CF) Metadata Conventionsouthernmost_latitude :-90.0northernmost_latitude :90.0westernmost_longitude :-180.0easternmost_longitude :180.0spatial_resolution :0.01 degreesgeospatial_lat_units :degrees northgeospatial_lat_resolution :0.009999999776geospatial_lon_units :degrees eastgeospatial_lon_resolution :0.009999999776acknowledgment :Please acknowledge the use of these data with the following statement:  These data were provided by JPL under support by NASA MEaSUREs program.creator_name :JPL MUR SST projectcreator_email :ghrsst@podaac.jpl.nasa.govcreator_url :http://mur.jpl.nasa.govproject :NASA Making Earth Science Data Records for Use in Research Environments (MEaSUREs) Programpublisher_name :GHRSST Project Officepublisher_url :http://www.ghrsst.orgpublisher_email :ghrsst-po@nceo.ac.ukprocessing_level :L4cdm_data_type :grid\n\n\nLook at the Analysed SST variable metadata\n\nall_sst = ds.analysed_sst\nall_sst\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray 'analysed_sst' (time: 30, lat: 17999, lon: 36000)&gt;\ndask.array&lt;concatenate, shape=(30, 17999, 36000), dtype=float32, chunksize=(1, 1023, 2047), chunktype=numpy.ndarray&gt;\nCoordinates:\n  * lat      (lat) float32 -89.99 -89.98 -89.97 -89.96 ... 89.97 89.98 89.99\n  * lon      (lon) float32 -180.0 -180.0 -180.0 -180.0 ... 180.0 180.0 180.0\n  * time     (time) datetime64[ns] 2021-09-01T09:00:00 ... 2021-09-30T09:00:00\nAttributes:\n    long_name:      analysed sea surface temperature\n    standard_name:  sea_surface_foundation_temperature\n    units:          kelvin\n    valid_min:      -32767\n    valid_max:      32767\n    comment:        \\\"Final\\\" version using Multi-Resolution Variational Anal...\n    source:         MODIS_T-JPL, MODIS_A-JPL, AMSR2-REMSS, AVHRRMTA_G-NAVO, A...xarray.DataArray'analysed_sst'time: 30lat: 17999lon: 36000dask.array&lt;chunksize=(1, 1023, 2047), meta=np.ndarray&gt;\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n72.42 GiB\n7.99 MiB\n\n\nShape\n(30, 17999, 36000)\n(1, 1023, 2047)\n\n\nCount\n19470 Tasks\n9720 Chunks\n\n\nType\nfloat32\nnumpy.ndarray\n\n\n\n\n\n\n\nCoordinates: (3)lat(lat)float32-89.99 -89.98 ... 89.98 89.99long_name :latitudestandard_name :latitudeaxis :Yunits :degrees_northvalid_min :-90.0valid_max :90.0comment :geolocations inherited from the input data without correctionarray([-89.99, -89.98, -89.97, ...,  89.97,  89.98,  89.99], dtype=float32)lon(lon)float32-180.0 -180.0 ... 180.0 180.0long_name :longitudestandard_name :longitudeaxis :Xunits :degrees_eastvalid_min :-180.0valid_max :180.0comment :geolocations inherited from the input data without correctionarray([-179.99, -179.98, -179.97, ...,  179.98,  179.99,  180.  ],\n      dtype=float32)time(time)datetime64[ns]2021-09-01T09:00:00 ... 2021-09-...long_name :reference time of sst fieldstandard_name :timeaxis :Tcomment :Nominal time of analyzed fieldsarray(['2021-09-01T09:00:00.000000000', '2021-09-02T09:00:00.000000000',\n       '2021-09-03T09:00:00.000000000', '2021-09-04T09:00:00.000000000',\n       '2021-09-05T09:00:00.000000000', '2021-09-06T09:00:00.000000000',\n       '2021-09-07T09:00:00.000000000', '2021-09-08T09:00:00.000000000',\n       '2021-09-09T09:00:00.000000000', '2021-09-10T09:00:00.000000000',\n       '2021-09-11T09:00:00.000000000', '2021-09-12T09:00:00.000000000',\n       '2021-09-13T09:00:00.000000000', '2021-09-14T09:00:00.000000000',\n       '2021-09-15T09:00:00.000000000', '2021-09-16T09:00:00.000000000',\n       '2021-09-17T09:00:00.000000000', '2021-09-18T09:00:00.000000000',\n       '2021-09-19T09:00:00.000000000', '2021-09-20T09:00:00.000000000',\n       '2021-09-21T09:00:00.000000000', '2021-09-22T09:00:00.000000000',\n       '2021-09-23T09:00:00.000000000', '2021-09-24T09:00:00.000000000',\n       '2021-09-25T09:00:00.000000000', '2021-09-26T09:00:00.000000000',\n       '2021-09-27T09:00:00.000000000', '2021-09-28T09:00:00.000000000',\n       '2021-09-29T09:00:00.000000000', '2021-09-30T09:00:00.000000000'],\n      dtype='datetime64[ns]')Attributes: (7)long_name :analysed sea surface temperaturestandard_name :sea_surface_foundation_temperatureunits :kelvinvalid_min :-32767valid_max :32767comment :\\\"Final\\\" version using Multi-Resolution Variational Analysis (MRVA) method for interpolationsource :MODIS_T-JPL, MODIS_A-JPL, AMSR2-REMSS, AVHRRMTA_G-NAVO, AVHRRMTB_G-NAVO, iQUAM-NOAA/NESDIS, Ice_Conc-OSISAF\n\n\nCreate a dataset / variable that is only our area of interest and view its metadata\n\nsst = ds.analysed_sst.sel(lat=lats, lon=lons)\nsst\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray 'analysed_sst' (time: 30, lat: 801, lon: 1701)&gt;\ndask.array&lt;getitem, shape=(30, 801, 1701), dtype=float32, chunksize=(1, 601, 1536), chunktype=numpy.ndarray&gt;\nCoordinates:\n  * lat      (lat) float32 41.0 41.01 41.02 41.03 ... 48.97 48.98 48.99 49.0\n  * lon      (lon) float32 -93.0 -92.99 -92.98 -92.97 ... -76.02 -76.01 -76.0\n  * time     (time) datetime64[ns] 2021-09-01T09:00:00 ... 2021-09-30T09:00:00\nAttributes:\n    long_name:      analysed sea surface temperature\n    standard_name:  sea_surface_foundation_temperature\n    units:          kelvin\n    valid_min:      -32767\n    valid_max:      32767\n    comment:        \\\"Final\\\" version using Multi-Resolution Variational Anal...\n    source:         MODIS_T-JPL, MODIS_A-JPL, AMSR2-REMSS, AVHRRMTA_G-NAVO, A...xarray.DataArray'analysed_sst'time: 30lat: 801lon: 1701dask.array&lt;chunksize=(1, 200, 1536), meta=np.ndarray&gt;\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n155.93 MiB\n3.52 MiB\n\n\nShape\n(30, 801, 1701)\n(1, 601, 1536)\n\n\nCount\n19590 Tasks\n120 Chunks\n\n\nType\nfloat32\nnumpy.ndarray\n\n\n\n\n\n\n\nCoordinates: (3)lat(lat)float3241.0 41.01 41.02 ... 48.99 49.0long_name :latitudestandard_name :latitudeaxis :Yunits :degrees_northvalid_min :-90.0valid_max :90.0comment :geolocations inherited from the input data without correctionarray([41.  , 41.01, 41.02, ..., 48.98, 48.99, 49.  ], dtype=float32)lon(lon)float32-93.0 -92.99 ... -76.01 -76.0long_name :longitudestandard_name :longitudeaxis :Xunits :degrees_eastvalid_min :-180.0valid_max :180.0comment :geolocations inherited from the input data without correctionarray([-93.  , -92.99, -92.98, ..., -76.02, -76.01, -76.  ], dtype=float32)time(time)datetime64[ns]2021-09-01T09:00:00 ... 2021-09-...long_name :reference time of sst fieldstandard_name :timeaxis :Tcomment :Nominal time of analyzed fieldsarray(['2021-09-01T09:00:00.000000000', '2021-09-02T09:00:00.000000000',\n       '2021-09-03T09:00:00.000000000', '2021-09-04T09:00:00.000000000',\n       '2021-09-05T09:00:00.000000000', '2021-09-06T09:00:00.000000000',\n       '2021-09-07T09:00:00.000000000', '2021-09-08T09:00:00.000000000',\n       '2021-09-09T09:00:00.000000000', '2021-09-10T09:00:00.000000000',\n       '2021-09-11T09:00:00.000000000', '2021-09-12T09:00:00.000000000',\n       '2021-09-13T09:00:00.000000000', '2021-09-14T09:00:00.000000000',\n       '2021-09-15T09:00:00.000000000', '2021-09-16T09:00:00.000000000',\n       '2021-09-17T09:00:00.000000000', '2021-09-18T09:00:00.000000000',\n       '2021-09-19T09:00:00.000000000', '2021-09-20T09:00:00.000000000',\n       '2021-09-21T09:00:00.000000000', '2021-09-22T09:00:00.000000000',\n       '2021-09-23T09:00:00.000000000', '2021-09-24T09:00:00.000000000',\n       '2021-09-25T09:00:00.000000000', '2021-09-26T09:00:00.000000000',\n       '2021-09-27T09:00:00.000000000', '2021-09-28T09:00:00.000000000',\n       '2021-09-29T09:00:00.000000000', '2021-09-30T09:00:00.000000000'],\n      dtype='datetime64[ns]')Attributes: (7)long_name :analysed sea surface temperaturestandard_name :sea_surface_foundation_temperatureunits :kelvinvalid_min :-32767valid_max :32767comment :\\\"Final\\\" version using Multi-Resolution Variational Analysis (MRVA) method for interpolationsource :MODIS_T-JPL, MODIS_A-JPL, AMSR2-REMSS, AVHRRMTA_G-NAVO, AVHRRMTB_G-NAVO, iQUAM-NOAA/NESDIS, Ice_Conc-OSISAF\n\n\nXArray reads data lazily, i.e. only when our code actually needs it. Up to this point, we haven’t read any data values, only metadata. The next line will force XArray to read the portions of the source files containing our area of interest. Behind the scenes, the eosdis-zarr-store library is ensuring data is fetched as efficiently as possible.\nNote: This line isn’t strictly necessary, since XArray will automatically read the data we need the first time our code tries to use it, but calling this will make sure that we can read the data multiple times later on without re-fetching anything from the source files.\nThis line will take several seconds to complete, but since it is retrieving only about 50 MB of data from 22 GB of source files, several seconds constitutes a significant time, bandwidth, and disk space savings.\n\nsst.load();\n\nNow we can start looking at aggregations across the time dimension. In this case, plot the standard deviation of the temperature at each point to get a visual sense of how much temperatures fluctuate over the course of the month.\n\n# We expect a warning here, from finding the standard deviation of arrays that contain all N/A values.\n# numpy produces N/A for these points, though, which is exactly what we want.\nstdev_sst = sst.std('time')\nstdev_sst.name = 'stdev of analysed_sst [Kelvin]'\nstdev_sst.plot();\n\n/srv/conda/envs/notebook/lib/python3.9/site-packages/numpy/lib/nanfunctions.py:1670: RuntimeWarning: Degrees of freedom &lt;= 0 for slice.\n  var = nanvar(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n\n\n\n\n\n\nInteractive animation of a month of data\nThis section isn’t as important to fully understand. It shows us a way to get an interactive animation to see what we have retrieved so far\nDefine an animation function to plot the ith time step. We need to make sure each plot is using the same color scale, set by vmin and vmax so the animation is consistent\n\nsst_min = sst.min()\nsst_max = sst.max()\n\ndef show_time_step(i):\n    plt.clf()\n    res = sst[i].plot.imshow(vmin=sst_min, vmax=sst_max)\n    return (res,)\n\nRender each time slice once and show it as an HTML animation with interactive controls\n\n#anim = animation.FuncAnimation(plt.gcf(), func=show_time_step, frames=len(sst))\n#display(HTML(anim.to_jshtml()))\n#plt.close()\n\n\n\n\nSupplemental: What’s happening here?\nFor EOSDIS data in the cloud, we have begun producing a metadata sidecar file in a format called DMR++ that extracts all of the information about arrays, variables, and dimensions from data files, as well as the byte offsets in the NetCDF4 file where data can be found. This information is sufficient to let the Zarr library read data from our NetCDF4 files, but it’s in the wrong format. zarr-eosdis-store knows how to fetch the sidecar file and transform it into something the Zarr library understands. Passing it when reading Zarr using XArray or the Zarr library lets these libraries interact with EOSDIS data exactly as if they were Zarr stores in a way that’s more optimal for reading data in the cloud. Beyond this, the zarr-eosdis-store library makes some optimizations in the way it reads data to help make up for situations where the NetCDF4 file is not internally arranged well for cloud-based access patterns.",
    "crumbs": [
      "Python Tutorials",
      "Using Data",
      "Zarr Access for NetCDF4 files"
    ]
  },
  {
    "objectID": "python-notebooks/02_Xarray.html#why-do-we-need-xarray",
    "href": "python-notebooks/02_Xarray.html#why-do-we-need-xarray",
    "title": "03. Introduction to xarray",
    "section": "Why do we need xarray?",
    "text": "Why do we need xarray?\nAs Geoscientists, we often work with time series of data with two or more dimensions: a time series of calibrated, orthorectified satellite images; two-dimensional grids of surface air temperature from an atmospheric reanalysis; or three-dimensional (level, x, y) cubes of ocean salinity from an ocean model. These data are often provided in GeoTIFF, NetCDF or HDF format with rich and useful metadata that we want to retain, or even use in our analysis. Common analyses include calculating means, standard deviations and anomalies over time or one or more spatial dimensions (e.g. zonal means). Model output often includes multiple variables that you want to apply similar analyses to.\n\n\n\nA schematic of multi-dimensional data\n\n\nThe schematic above shows a typical data structure for multi-dimensional data. There are two data cubes, one for temperature and one for precipitation. Common coordinate variables, in this case latitude, longitude and time are associated with each variable. Each variable, including coordinate variables, will have a set of attributes: name, units, missing value, etc. The file containing the data may also have attributes: source of the data, model name coordinate reference system if the data are projected. Writing code using low-level packages such as netcdf4 and numpy to read the data, then perform analysis, and write the results to file is time consuming and prone to errors."
  },
  {
    "objectID": "python-notebooks/02_Xarray.html#what-is-xarray",
    "href": "python-notebooks/02_Xarray.html#what-is-xarray",
    "title": "03. Introduction to xarray",
    "section": "What is xarray",
    "text": "What is xarray\nxarray is an open-source project and python package to work with labelled multi-dimensional arrays. It is leverages numpy, pandas, matplotlib and dask to build Dataset and DataArray objects with built-in methods to subset, analyze, interpolate, and plot multi-dimensional data. It makes working with multi-dimensional data cubes efficient and fun. It will change your life for the better. You’ll be more attractive, more interesting, and better equiped to take on lifes challenges."
  },
  {
    "objectID": "python-notebooks/02_Xarray.html#what-you-will-learn-from-this-tutorial",
    "href": "python-notebooks/02_Xarray.html#what-you-will-learn-from-this-tutorial",
    "title": "03. Introduction to xarray",
    "section": "What you will learn from this tutorial",
    "text": "What you will learn from this tutorial\nIn this tutorial you will learn how to:\n\nload a netcdf file into xarray\ninterrogate the Dataset and understand the difference between DataArray and Dataset\nsubset a Dataset\ncalculate annual and monthly mean fields\ncalculate a time series of zonal means\nplot these results\n\nAs always, we’ll start by importing xarray. We’ll follow convention by giving the module the shortname xr\n\nimport xarray as xr\nxr.set_options(keep_attrs=True)\n\nI’m going to use one of xarray’s tutorial datasets. In this case, air temperature from the NCEP reanalysis. I’ll assign the result of the open_dataset to ds. I may change this to access a dataset directly\n\nds = xr.tutorial.open_dataset(\"air_temperature\")\n\nAs we are in an interactive environment, we can just type ds to see what we have.\n\nds\n\nFirst thing to notice is that ds is an xarray.Dataset object. It has dimensions, lat, lon, and time. It also has coordinate variables with the same names as these dimensions. These coordinate variables are 1-dimensional. This is a NetCDF convention. The Dataset contains one data variable, air. This has dimensions (time, lat, lon).\nClicking on the document icon reveals attributes for each variable. Clicking on the disk icon reveals a representation of the data.\nEach of the data and coordinate variables can be accessed and examined using the variable name as a key.\n\nds.air\n\n\nds['air']\n\nThese are xarray.DataArray objects. This is the basic building block for xarray.\nVariables can also be accessed as attributes of ds.\n\nds.time\n\nA major difference between accessing a variable as an attribute versus using a key is that the attribute is read-only but the key method can be used to update the variable. For example, if I want to convert the units of air from Kelvin to degrees Celsius.\n\nds['air'] = ds.air - 273.15\n\nThis approach can also be used to add new variables\n\nds['air_kelvin'] = ds.air + 273.15\n\nIt is helpful to update attributes such as units, this saves time, confusion and mistakes, especially when you save the dataset.\n\nds['air'].attrs['units'] = 'degC'\n\n\nds"
  },
  {
    "objectID": "python-notebooks/02_Xarray.html#subsetting-and-indexing",
    "href": "python-notebooks/02_Xarray.html#subsetting-and-indexing",
    "title": "03. Introduction to xarray",
    "section": "Subsetting and Indexing",
    "text": "Subsetting and Indexing\nSubsetting and indexing methods depend on whether you are working with a Dataset or DataArray. A DataArray can be accessed using positional indexing just like a numpy array. To access the temperature field for the first time step, you do the following.\n\nds['air'][0,:,:]\n\nNote this returns a DataArray with coordinates but not attributes.\nHowever, the real power is being able to access variables using coordinate variables. I can get the same subset using the following. (It’s also more explicit about what is being selected and robust in case I modify the DataArray and expect the same output.)\n\nds['air'].sel(time='2013-01-01').time\n\n\nds.air.sel(time='2013-01-01')\n\nI can also do slices. I’ll extract temperatures for the state of Colorado. The bounding box for the state is [-109 E, -102 E, 37 N, 41 N].\nIn the code below, pay attention to both the order of the coordinates and the range of values. The first value of the lat coordinate variable is 41 N, the second value is 37 N. Unfortunately, xarray expects slices of coordinates to be in the same order as the coordinates. Note lon is 0 to 360 not -180 to 180, and I let python calculate it for me within the slice.\n\nds.air.sel(lat=slice(41.,37.), lon=slice(360-109,360-102))\n\nWhat if we want temperature for a point, for example Denver, CO (39.72510678889283 N, -104.98785545855408 E). xarray can handle this! If we just want data from the nearest grid point, we can use sel and specify the method as “nearest”.\n\ndenver_lat, denver_lon = 39.72510678889283, -104.98785545855408\n\n\nds.air.sel(lat=denver_lat, lon=360+denver_lon, method='nearest')\n\nIf we want to interpolate, we can use interp(). In this case I use linear or bilinear interpolation.\ninterp() can also be used to resample data to a new grid and even reproject data\n\nds.air.interp(lat=denver_lat, lon=360+denver_lon, method='linear')\n\nsel() and interp() can also be used on Dataset objects.\n\nds.sel(lat=slice(41.,37.), lon=slice(360-109,360-102))\n\n\nds.interp(lat=denver_lat, lon=360+denver_lon, method='linear')"
  },
  {
    "objectID": "python-notebooks/02_Xarray.html#analysis",
    "href": "python-notebooks/02_Xarray.html#analysis",
    "title": "03. Introduction to xarray",
    "section": "Analysis",
    "text": "Analysis\nAs a simple example, let’s try to calculate a mean field for the whole time range.\n\nds.mean(dim='time')\n\nWe can also calculate a zonal mean (averaging over longitude)\n\nds.mean(dim='lon')\n\nOther aggregation methods include min(), max(), std(), along with others.\n\nds.std(dim='time')\n\nThe data we have are in 6h timesteps. This can be resampled to daily or monthly. If you are familiar with pandas, xarray uses the same methods.\n\nds.resample(time='M').mean()\n\n\nds_mon = ds.resample(time='M').mean()\nds_mon\n\nThis is a really short time series but as an example, let’s calculate a monthly climatology (at least for 2 months). For this we can use groupby()\n\nds_clim = ds_mon.groupby(ds_mon.time.dt.month).mean()"
  },
  {
    "objectID": "python-notebooks/02_Xarray.html#plot-results",
    "href": "python-notebooks/02_Xarray.html#plot-results",
    "title": "03. Introduction to xarray",
    "section": "Plot results",
    "text": "Plot results\nFinally, let’s plot the results! This will plot the lat/lon axes of the original ds DataArray.\n\nds_clim.air.sel(month=10).plot()"
  },
  {
    "objectID": "python-notebooks/02_Graphing_Map_Data.html#install-and-import-necessary-libraries",
    "href": "python-notebooks/02_Graphing_Map_Data.html#install-and-import-necessary-libraries",
    "title": "Graphing Data from our final product",
    "section": "(Install and) Import necessary libraries",
    "text": "(Install and) Import necessary libraries\n\n! pip install cmocean\n\nRequirement already satisfied: cmocean in /srv/conda/envs/notebook/lib/python3.9/site-packages (3.0.3)\nRequirement already satisfied: matplotlib in /srv/conda/envs/notebook/lib/python3.9/site-packages (from cmocean) (3.7.1)\nRequirement already satisfied: numpy in /srv/conda/envs/notebook/lib/python3.9/site-packages (from cmocean) (1.23.5)\nRequirement already satisfied: packaging in /srv/conda/envs/notebook/lib/python3.9/site-packages (from cmocean) (23.1)\nRequirement already satisfied: contourpy&gt;=1.0.1 in /srv/conda/envs/notebook/lib/python3.9/site-packages (from matplotlib-&gt;cmocean) (1.0.7)\nRequirement already satisfied: cycler&gt;=0.10 in /srv/conda/envs/notebook/lib/python3.9/site-packages (from matplotlib-&gt;cmocean) (0.11.0)\nRequirement already satisfied: fonttools&gt;=4.22.0 in /srv/conda/envs/notebook/lib/python3.9/site-packages (from matplotlib-&gt;cmocean) (4.39.4)\nRequirement already satisfied: kiwisolver&gt;=1.0.1 in /srv/conda/envs/notebook/lib/python3.9/site-packages (from matplotlib-&gt;cmocean) (1.4.4)\nRequirement already satisfied: pillow&gt;=6.2.0 in /srv/conda/envs/notebook/lib/python3.9/site-packages (from matplotlib-&gt;cmocean) (9.5.0)\nRequirement already satisfied: pyparsing&gt;=2.3.1 in /srv/conda/envs/notebook/lib/python3.9/site-packages (from matplotlib-&gt;cmocean) (3.0.9)\nRequirement already satisfied: python-dateutil&gt;=2.7 in /srv/conda/envs/notebook/lib/python3.9/site-packages (from matplotlib-&gt;cmocean) (2.8.2)\nRequirement already satisfied: importlib-resources&gt;=3.2.0 in /srv/conda/envs/notebook/lib/python3.9/site-packages (from matplotlib-&gt;cmocean) (5.12.0)\nRequirement already satisfied: zipp&gt;=3.1.0 in /srv/conda/envs/notebook/lib/python3.9/site-packages (from importlib-resources&gt;=3.2.0-&gt;matplotlib-&gt;cmocean) (3.15.0)\nRequirement already satisfied: six&gt;=1.5 in /srv/conda/envs/notebook/lib/python3.9/site-packages (from python-dateutil&gt;=2.7-&gt;matplotlib-&gt;cmocean) (1.16.0)\n\n\n\n! conda install --channel conda-forge pygmt -y\n\nCollecting package metadata (current_repodata.json): done\nSolving environment: done\n\n\n==&gt; WARNING: A newer version of conda exists. &lt;==\n  current version: 22.11.1\n  latest version: 23.7.4\n\nPlease update conda by running\n\n    $ conda update -n base -c conda-forge conda\n\nOr to minimize the number of packages updated during conda update use\n\n     conda install conda=23.7.4\n\n\n\n# All requested packages already installed.\n\n\n\n\n# Run in terminal\n# conda create --name pygmt --channel conda-forge pygmt\n# then run\n# conda activate pygmt\n\n\nCommandNotFoundError: Your shell has not been properly configured to use 'conda activate'.\nTo initialize your shell, run\n\n    $ conda init &lt;SHELL_NAME&gt;\n\nCurrently supported shells are:\n  - bash\n  - fish\n  - tcsh\n  - xonsh\n  - zsh\n  - powershell\n\nSee 'conda init --help' for more information and options.\n\nIMPORTANT: You may need to close and restart your shell after running 'conda init'.\n\n\n\n\n\nimport xarray as xr\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport holoviews as hv\nimport hvplot.xarray\nimport cmocean\nimport cartopy\n\n\n\n\n\n\n\n\n\n\n\nimport pygmt"
  },
  {
    "objectID": "python-notebooks/02_Graphing_Map_Data.html#read-data",
    "href": "python-notebooks/02_Graphing_Map_Data.html#read-data",
    "title": "Graphing Data from our final product",
    "section": "Read data",
    "text": "Read data\n\n# Change the file path accordingly\nds = xr.open_zarr('/home/jovyan/shared-readwrite/data/INDIAN_OCEAN_025GRID_DAILY.zarr/')"
  },
  {
    "objectID": "python-notebooks/02_Graphing_Map_Data.html#slicing-data",
    "href": "python-notebooks/02_Graphing_Map_Data.html#slicing-data",
    "title": "Graphing Data from our final product",
    "section": "Slicing data",
    "text": "Slicing data\n\nds\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt;\nDimensions:          (time: 8523, lat: 177, lon: 241)\nCoordinates:\n  * lat              (lat) float32 32.0 31.75 31.5 31.25 ... -11.5 -11.75 -12.0\n  * lon              (lon) float32 42.0 42.25 42.5 42.75 ... 101.5 101.8 102.0\n  * time             (time) datetime64[ns] 1997-09-01 1997-09-02 ... 2020-12-31\nData variables: (12/14)\n    CHL              (time, lat, lon) float32 dask.array&lt;chunksize=(29, 177, 241), meta=np.ndarray&gt;\n    CHL_uncertainty  (time, lat, lon) float32 dask.array&lt;chunksize=(29, 177, 241), meta=np.ndarray&gt;\n    adt              (time, lat, lon) float32 dask.array&lt;chunksize=(29, 177, 241), meta=np.ndarray&gt;\n    air_temp         (time, lat, lon) float32 dask.array&lt;chunksize=(29, 177, 241), meta=np.ndarray&gt;\n    direction        (time, lat, lon) float32 dask.array&lt;chunksize=(29, 177, 241), meta=np.ndarray&gt;\n    sla              (time, lat, lon) float32 dask.array&lt;chunksize=(29, 177, 241), meta=np.ndarray&gt;\n    ...               ...\n    u_curr           (time, lat, lon) float32 dask.array&lt;chunksize=(29, 177, 241), meta=np.ndarray&gt;\n    u_wind           (time, lat, lon) float32 dask.array&lt;chunksize=(29, 177, 241), meta=np.ndarray&gt;\n    ug_curr          (time, lat, lon) float32 dask.array&lt;chunksize=(29, 177, 241), meta=np.ndarray&gt;\n    v_curr           (time, lat, lon) float32 dask.array&lt;chunksize=(29, 177, 241), meta=np.ndarray&gt;\n    v_wind           (time, lat, lon) float32 dask.array&lt;chunksize=(29, 177, 241), meta=np.ndarray&gt;\n    vg_curr          (time, lat, lon) float32 dask.array&lt;chunksize=(29, 177, 241), meta=np.ndarray&gt;\nAttributes: (12/17)\n    creator_email:              minhphan@uw.edu\n    creator_name:               Minh Phan\n    creator_type:               person\n    date_created:               2023-07-07\n    geospatial_lat_max:         32.0\n    geospatial_lat_min:         -12.0\n    ...                         ...\n    geospatial_lon_units:       degrees_east\n    source:                     OSCAR, ERA5 Reanalysis, Copernicus Climate Ch...\n    summary:                    Daily mean of 0.25 x 0.25 degrees gridded dat...\n    time_coverage_end:          2020-12-31T23:59:59\n    time_coverage_start:        2000-01-01T00:00:00\n    title:                      Climate Data for Coastal Upwelling Machine Le...xarray.DatasetDimensions:time: 8523lat: 177lon: 241Coordinates: (3)lat(lat)float3232.0 31.75 31.5 ... -11.75 -12.0long_name :latitudestandard_name :latitudeunits :degrees_northarray([ 32.  ,  31.75,  31.5 ,  31.25,  31.  ,  30.75,  30.5 ,  30.25,  30.  ,\n        29.75,  29.5 ,  29.25,  29.  ,  28.75,  28.5 ,  28.25,  28.  ,  27.75,\n        27.5 ,  27.25,  27.  ,  26.75,  26.5 ,  26.25,  26.  ,  25.75,  25.5 ,\n        25.25,  25.  ,  24.75,  24.5 ,  24.25,  24.  ,  23.75,  23.5 ,  23.25,\n        23.  ,  22.75,  22.5 ,  22.25,  22.  ,  21.75,  21.5 ,  21.25,  21.  ,\n        20.75,  20.5 ,  20.25,  20.  ,  19.75,  19.5 ,  19.25,  19.  ,  18.75,\n        18.5 ,  18.25,  18.  ,  17.75,  17.5 ,  17.25,  17.  ,  16.75,  16.5 ,\n        16.25,  16.  ,  15.75,  15.5 ,  15.25,  15.  ,  14.75,  14.5 ,  14.25,\n        14.  ,  13.75,  13.5 ,  13.25,  13.  ,  12.75,  12.5 ,  12.25,  12.  ,\n        11.75,  11.5 ,  11.25,  11.  ,  10.75,  10.5 ,  10.25,  10.  ,   9.75,\n         9.5 ,   9.25,   9.  ,   8.75,   8.5 ,   8.25,   8.  ,   7.75,   7.5 ,\n         7.25,   7.  ,   6.75,   6.5 ,   6.25,   6.  ,   5.75,   5.5 ,   5.25,\n         5.  ,   4.75,   4.5 ,   4.25,   4.  ,   3.75,   3.5 ,   3.25,   3.  ,\n         2.75,   2.5 ,   2.25,   2.  ,   1.75,   1.5 ,   1.25,   1.  ,   0.75,\n         0.5 ,   0.25,   0.  ,  -0.25,  -0.5 ,  -0.75,  -1.  ,  -1.25,  -1.5 ,\n        -1.75,  -2.  ,  -2.25,  -2.5 ,  -2.75,  -3.  ,  -3.25,  -3.5 ,  -3.75,\n        -4.  ,  -4.25,  -4.5 ,  -4.75,  -5.  ,  -5.25,  -5.5 ,  -5.75,  -6.  ,\n        -6.25,  -6.5 ,  -6.75,  -7.  ,  -7.25,  -7.5 ,  -7.75,  -8.  ,  -8.25,\n        -8.5 ,  -8.75,  -9.  ,  -9.25,  -9.5 ,  -9.75, -10.  , -10.25, -10.5 ,\n       -10.75, -11.  , -11.25, -11.5 , -11.75, -12.  ], dtype=float32)lon(lon)float3242.0 42.25 42.5 ... 101.8 102.0long_name :longitudestandard_name :longitudeunits :degrees_eastarray([ 42.  ,  42.25,  42.5 , ..., 101.5 , 101.75, 102.  ], dtype=float32)time(time)datetime64[ns]1997-09-01 ... 2020-12-31axis :Tcomment :Data is averaged over the daylong_name :time centered on the daystandard_name :timetime_bounds :2000-01-01 00:00:00 to 2000-01-01 23:59:59array(['1997-09-01T00:00:00.000000000', '1997-09-02T00:00:00.000000000',\n       '1997-09-03T00:00:00.000000000', ..., '2020-12-29T00:00:00.000000000',\n       '2020-12-30T00:00:00.000000000', '2020-12-31T00:00:00.000000000'],\n      dtype='datetime64[ns]')Data variables: (14)CHL(time, lat, lon)float32dask.array&lt;chunksize=(29, 177, 241), meta=np.ndarray&gt;_ChunkSizes :[1, 256, 256]ancillary_variables :flags CHL_uncertaintycoverage_content_type :modelResultinput_files_reprocessings :Processors versions: MODIS R2022.0NRT/VIIRSN R2022.0NRT/OLCIA 07.02/VIIRSJ1 R2022.0NRT/OLCIB 07.02long_name :Chlorophyll-a concentration - Mean of the binned pixelsstandard_name :mass_concentration_of_chlorophyll_a_in_sea_watertype :surfaceunits :milligram m-3valid_max :1000.0valid_min :0.0\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n1.35 GiB\n16.27 MiB\n\n\nShape\n(8523, 177, 241)\n(100, 177, 241)\n\n\nDask graph\n87 chunks in 3 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\nCHL_uncertainty\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(29, 177, 241), meta=np.ndarray&gt;\n\n\n\n\n_ChunkSizes :\n\n[1, 256, 256]\n\ncoverage_content_type :\n\nqualityInformation\n\nlong_name :\n\nChlorophyll-a concentration - Uncertainty estimation\n\nunits :\n\n%\n\nvalid_max :\n\n32767\n\nvalid_min :\n\n0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n1.35 GiB\n16.27 MiB\n\n\nShape\n(8523, 177, 241)\n(100, 177, 241)\n\n\nDask graph\n87 chunks in 3 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nadt\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(29, 177, 241), meta=np.ndarray&gt;\n\n\n\n\ncomment :\n\nThe absolute dynamic topography is the sea surface height above geoid; the adt is obtained as follows: adt=sla+mdt where mdt is the mean dynamic topography; see the product user manual for details\n\ngrid_mapping :\n\ncrs\n\nlong_name :\n\nAbsolute dynamic topography\n\nstandard_name :\n\nsea_surface_height_above_geoid\n\nunits :\n\nm\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n1.35 GiB\n16.27 MiB\n\n\nShape\n(8523, 177, 241)\n(100, 177, 241)\n\n\nDask graph\n87 chunks in 3 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nair_temp\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(29, 177, 241), meta=np.ndarray&gt;\n\n\n\n\nlong_name :\n\n2 metre temperature\n\nnameCDM :\n\n2_metre_temperature_surface\n\nnameECMWF :\n\n2 metre temperature\n\nproduct_type :\n\nanalysis\n\nshortNameECMWF :\n\n2t\n\nstandard_name :\n\nair_temperature\n\nunits :\n\nK\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n1.35 GiB\n16.27 MiB\n\n\nShape\n(8523, 177, 241)\n(100, 177, 241)\n\n\nDask graph\n87 chunks in 3 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\ndirection\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(29, 177, 241), meta=np.ndarray&gt;\n\n\n\n\nlong_name :\n\n10 metre wind direction\n\nunits :\n\n°C\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n1.35 GiB\n16.27 MiB\n\n\nShape\n(8523, 177, 241)\n(100, 177, 241)\n\n\nDask graph\n87 chunks in 3 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nsla\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(29, 177, 241), meta=np.ndarray&gt;\n\n\n\n\nancillary_variables :\n\nerr_sla\n\ncomment :\n\nThe sea level anomaly is the sea surface height above mean sea surface; it is referenced to the [1993, 2012] period; see the product user manual for details\n\ngrid_mapping :\n\ncrs\n\nlong_name :\n\nSea level anomaly\n\nstandard_name :\n\nsea_surface_height_above_sea_level\n\nunits :\n\nm\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n1.35 GiB\n16.27 MiB\n\n\nShape\n(8523, 177, 241)\n(100, 177, 241)\n\n\nDask graph\n87 chunks in 3 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nspeed\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(29, 177, 241), meta=np.ndarray&gt;\n\n\n\n\nlong_name :\n\n10 metre absolute speed\n\nunits :\n\nm s**-1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n1.35 GiB\n16.27 MiB\n\n\nShape\n(8523, 177, 241)\n(100, 177, 241)\n\n\nDask graph\n87 chunks in 3 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nsst\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(29, 177, 241), meta=np.ndarray&gt;\n\n\n\n\nlong_name :\n\nSea surface temperature\n\nnameCDM :\n\nSea_surface_temperature_surface\n\nnameECMWF :\n\nSea surface temperature\n\nproduct_type :\n\nanalysis\n\nshortNameECMWF :\n\nsst\n\nstandard_name :\n\nsea_surface_temperature\n\nunits :\n\nK\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n1.35 GiB\n16.27 MiB\n\n\nShape\n(8523, 177, 241)\n(100, 177, 241)\n\n\nDask graph\n87 chunks in 3 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nu_curr\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(29, 177, 241), meta=np.ndarray&gt;\n\n\n\n\ncomment :\n\nVelocities are an average over the top 30m of the mixed layer\n\ncoverage_content_type :\n\nmodelResult\n\ndepth :\n\n15m\n\nlong_name :\n\nzonal total surface current\n\nsource :\n\nSSH source: CMEMS SSALTO/DUACS SEALEVEL_GLO_PHY_L4_MY_008_047 DOI: 10.48670/moi-00148 ; WIND source: ECMWF ERA5 10m wind DOI: 10.24381/cds.adbb2d47 ; SST source: CMC 0.2 deg SST V2.0 DOI: 10.5067/GHCMC-4FM02\n\nstandard_name :\n\neastward_sea_water_velocity\n\nunits :\n\nm s-1\n\nvalid_max :\n\n3.0\n\nvalid_min :\n\n-3.0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n1.35 GiB\n16.27 MiB\n\n\nShape\n(8523, 177, 241)\n(100, 177, 241)\n\n\nDask graph\n87 chunks in 3 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nu_wind\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(29, 177, 241), meta=np.ndarray&gt;\n\n\n\n\nlong_name :\n\n10 metre U wind component\n\nnameCDM :\n\n10_metre_U_wind_component_surface\n\nnameECMWF :\n\n10 metre U wind component\n\nproduct_type :\n\nanalysis\n\nshortNameECMWF :\n\n10u\n\nstandard_name :\n\neastward_wind\n\nunits :\n\nm s**-1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n1.35 GiB\n16.27 MiB\n\n\nShape\n(8523, 177, 241)\n(100, 177, 241)\n\n\nDask graph\n87 chunks in 3 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nug_curr\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(29, 177, 241), meta=np.ndarray&gt;\n\n\n\n\ncomment :\n\nGeostrophic velocities calculated from absolute dynamic topography\n\ndepth :\n\n15m\n\nlong_name :\n\nzonal geostrophic surface current\n\nsource :\n\nSSH source: CMEMS SSALTO/DUACS SEALEVEL_GLO_PHY_L4_MY_008_047 DOI: 10.48670/moi-00148\n\nstandard_name :\n\ngeostrophic_eastward_sea_water_velocity\n\nunits :\n\nm s-1\n\nvalid_max :\n\n3.0\n\nvalid_min :\n\n-3.0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n1.35 GiB\n16.27 MiB\n\n\nShape\n(8523, 177, 241)\n(100, 177, 241)\n\n\nDask graph\n87 chunks in 3 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nv_curr\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(29, 177, 241), meta=np.ndarray&gt;\n\n\n\n\ncomment :\n\nVelocities are an average over the top 30m of the mixed layer\n\ncoverage_content_type :\n\nmodelResult\n\ndepth :\n\n15m\n\nlong_name :\n\nmeridional total surface current\n\nsource :\n\nSSH source: CMEMS SSALTO/DUACS SEALEVEL_GLO_PHY_L4_MY_008_047 DOI: 10.48670/moi-00148 ; WIND source: ECMWF ERA5 10m wind DOI: 10.24381/cds.adbb2d47 ; SST source: CMC 0.2 deg SST V2.0 DOI: 10.5067/GHCMC-4FM02\n\nstandard_name :\n\nnorthward_sea_water_velocity\n\nunits :\n\nm s-1\n\nvalid_max :\n\n3.0\n\nvalid_min :\n\n-3.0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n1.35 GiB\n16.27 MiB\n\n\nShape\n(8523, 177, 241)\n(100, 177, 241)\n\n\nDask graph\n87 chunks in 3 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nv_wind\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(29, 177, 241), meta=np.ndarray&gt;\n\n\n\n\nlong_name :\n\n10 metre V wind component\n\nnameCDM :\n\n10_metre_V_wind_component_surface\n\nnameECMWF :\n\n10 metre V wind component\n\nproduct_type :\n\nanalysis\n\nshortNameECMWF :\n\n10v\n\nstandard_name :\n\nnorthward_wind\n\nunits :\n\nm s**-1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n1.35 GiB\n16.27 MiB\n\n\nShape\n(8523, 177, 241)\n(100, 177, 241)\n\n\nDask graph\n87 chunks in 3 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nvg_curr\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(29, 177, 241), meta=np.ndarray&gt;\n\n\n\n\ncomment :\n\nGeostrophic velocities calculated from absolute dynamic topography\n\ndepth :\n\n15m\n\nlong_name :\n\nmeridional geostrophic surface current\n\nsource :\n\nSSH source: CMEMS SSALTO/DUACS SEALEVEL_GLO_PHY_L4_MY_008_047 DOI: 10.48670/moi-00148\n\nstandard_name :\n\ngeostrophic_northward_sea_water_velocity\n\nunits :\n\nm s-1\n\nvalid_max :\n\n3.0\n\nvalid_min :\n\n-3.0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n1.35 GiB\n16.27 MiB\n\n\nShape\n(8523, 177, 241)\n(100, 177, 241)\n\n\nDask graph\n87 chunks in 3 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\nAttributes: (17)creator_email :minhphan@uw.educreator_name :Minh Phancreator_type :persondate_created :2023-07-07geospatial_lat_max :32.0geospatial_lat_min :-12.0geospatial_lat_resolution :0.25geospatial_lat_units :degrees_northgeospatial_lon_max :102.0geospatial_lon_min :42.0geospatial_lon_resolution :0.25geospatial_lon_units :degrees_eastsource :OSCAR, ERA5 Reanalysis, Copernicus Climate Change Service (C3S), Copernicus Marine Environment Monitoring Service (CMEMS)summary :Daily mean of 0.25 x 0.25 degrees gridded data from multiple climate variables that may influence the patterns of coastal upwelling in the focused areatime_coverage_end :2020-12-31T23:59:59time_coverage_start :2000-01-01T00:00:00title :Climate Data for Coastal Upwelling Machine Learning Project in Indian Ocean\n\n\nWe can slice data by the dimensions (latitude, longitude, time) and data variables.\n\n# slice by latitude\n# notice how we specify the range in reverse\nds.sel(lat=slice(0, -12))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt;\nDimensions:          (time: 8523, lat: 49, lon: 241)\nCoordinates:\n  * lat              (lat) float32 0.0 -0.25 -0.5 -0.75 ... -11.5 -11.75 -12.0\n  * lon              (lon) float32 42.0 42.25 42.5 42.75 ... 101.5 101.8 102.0\n  * time             (time) datetime64[ns] 1997-09-01 1997-09-02 ... 2020-12-31\nData variables: (12/14)\n    CHL              (time, lat, lon) float32 dask.array&lt;chunksize=(29, 49, 241), meta=np.ndarray&gt;\n    CHL_uncertainty  (time, lat, lon) float32 dask.array&lt;chunksize=(29, 49, 241), meta=np.ndarray&gt;\n    adt              (time, lat, lon) float32 dask.array&lt;chunksize=(29, 49, 241), meta=np.ndarray&gt;\n    air_temp         (time, lat, lon) float32 dask.array&lt;chunksize=(29, 49, 241), meta=np.ndarray&gt;\n    direction        (time, lat, lon) float32 dask.array&lt;chunksize=(29, 49, 241), meta=np.ndarray&gt;\n    sla              (time, lat, lon) float32 dask.array&lt;chunksize=(29, 49, 241), meta=np.ndarray&gt;\n    ...               ...\n    u_curr           (time, lat, lon) float32 dask.array&lt;chunksize=(29, 49, 241), meta=np.ndarray&gt;\n    u_wind           (time, lat, lon) float32 dask.array&lt;chunksize=(29, 49, 241), meta=np.ndarray&gt;\n    ug_curr          (time, lat, lon) float32 dask.array&lt;chunksize=(29, 49, 241), meta=np.ndarray&gt;\n    v_curr           (time, lat, lon) float32 dask.array&lt;chunksize=(29, 49, 241), meta=np.ndarray&gt;\n    v_wind           (time, lat, lon) float32 dask.array&lt;chunksize=(29, 49, 241), meta=np.ndarray&gt;\n    vg_curr          (time, lat, lon) float32 dask.array&lt;chunksize=(29, 49, 241), meta=np.ndarray&gt;\nAttributes: (12/17)\n    creator_email:              minhphan@uw.edu\n    creator_name:               Minh Phan\n    creator_type:               person\n    date_created:               2023-07-07\n    geospatial_lat_max:         32.0\n    geospatial_lat_min:         -12.0\n    ...                         ...\n    geospatial_lon_units:       degrees_east\n    source:                     OSCAR, ERA5 Reanalysis, Copernicus Climate Ch...\n    summary:                    Daily mean of 0.25 x 0.25 degrees gridded dat...\n    time_coverage_end:          2020-12-31T23:59:59\n    time_coverage_start:        2000-01-01T00:00:00\n    title:                      Climate Data for Coastal Upwelling Machine Le...xarray.DatasetDimensions:time: 8523lat: 49lon: 241Coordinates: (3)lat(lat)float320.0 -0.25 -0.5 ... -11.75 -12.0long_name :latitudestandard_name :latitudeunits :degrees_northarray([  0.  ,  -0.25,  -0.5 ,  -0.75,  -1.  ,  -1.25,  -1.5 ,  -1.75,  -2.  ,\n        -2.25,  -2.5 ,  -2.75,  -3.  ,  -3.25,  -3.5 ,  -3.75,  -4.  ,  -4.25,\n        -4.5 ,  -4.75,  -5.  ,  -5.25,  -5.5 ,  -5.75,  -6.  ,  -6.25,  -6.5 ,\n        -6.75,  -7.  ,  -7.25,  -7.5 ,  -7.75,  -8.  ,  -8.25,  -8.5 ,  -8.75,\n        -9.  ,  -9.25,  -9.5 ,  -9.75, -10.  , -10.25, -10.5 , -10.75, -11.  ,\n       -11.25, -11.5 , -11.75, -12.  ], dtype=float32)lon(lon)float3242.0 42.25 42.5 ... 101.8 102.0long_name :longitudestandard_name :longitudeunits :degrees_eastarray([ 42.  ,  42.25,  42.5 , ..., 101.5 , 101.75, 102.  ], dtype=float32)time(time)datetime64[ns]1997-09-01 ... 2020-12-31axis :Tcomment :Data is averaged over the daylong_name :time centered on the daystandard_name :timetime_bounds :2000-01-01 00:00:00 to 2000-01-01 23:59:59array(['1997-09-01T00:00:00.000000000', '1997-09-02T00:00:00.000000000',\n       '1997-09-03T00:00:00.000000000', ..., '2020-12-29T00:00:00.000000000',\n       '2020-12-30T00:00:00.000000000', '2020-12-31T00:00:00.000000000'],\n      dtype='datetime64[ns]')Data variables: (14)CHL(time, lat, lon)float32dask.array&lt;chunksize=(29, 49, 241), meta=np.ndarray&gt;_ChunkSizes :[1, 256, 256]ancillary_variables :flags CHL_uncertaintycoverage_content_type :modelResultinput_files_reprocessings :Processors versions: MODIS R2022.0NRT/VIIRSN R2022.0NRT/OLCIA 07.02/VIIRSJ1 R2022.0NRT/OLCIB 07.02long_name :Chlorophyll-a concentration - Mean of the binned pixelsstandard_name :mass_concentration_of_chlorophyll_a_in_sea_watertype :surfaceunits :milligram m-3valid_max :1000.0valid_min :0.0\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n383.94 MiB\n4.50 MiB\n\n\nShape\n(8523, 49, 241)\n(100, 49, 241)\n\n\nDask graph\n87 chunks in 4 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\nCHL_uncertainty\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(29, 49, 241), meta=np.ndarray&gt;\n\n\n\n\n_ChunkSizes :\n\n[1, 256, 256]\n\ncoverage_content_type :\n\nqualityInformation\n\nlong_name :\n\nChlorophyll-a concentration - Uncertainty estimation\n\nunits :\n\n%\n\nvalid_max :\n\n32767\n\nvalid_min :\n\n0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n383.94 MiB\n4.50 MiB\n\n\nShape\n(8523, 49, 241)\n(100, 49, 241)\n\n\nDask graph\n87 chunks in 4 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nadt\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(29, 49, 241), meta=np.ndarray&gt;\n\n\n\n\ncomment :\n\nThe absolute dynamic topography is the sea surface height above geoid; the adt is obtained as follows: adt=sla+mdt where mdt is the mean dynamic topography; see the product user manual for details\n\ngrid_mapping :\n\ncrs\n\nlong_name :\n\nAbsolute dynamic topography\n\nstandard_name :\n\nsea_surface_height_above_geoid\n\nunits :\n\nm\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n383.94 MiB\n4.50 MiB\n\n\nShape\n(8523, 49, 241)\n(100, 49, 241)\n\n\nDask graph\n87 chunks in 4 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nair_temp\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(29, 49, 241), meta=np.ndarray&gt;\n\n\n\n\nlong_name :\n\n2 metre temperature\n\nnameCDM :\n\n2_metre_temperature_surface\n\nnameECMWF :\n\n2 metre temperature\n\nproduct_type :\n\nanalysis\n\nshortNameECMWF :\n\n2t\n\nstandard_name :\n\nair_temperature\n\nunits :\n\nK\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n383.94 MiB\n4.50 MiB\n\n\nShape\n(8523, 49, 241)\n(100, 49, 241)\n\n\nDask graph\n87 chunks in 4 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\ndirection\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(29, 49, 241), meta=np.ndarray&gt;\n\n\n\n\nlong_name :\n\n10 metre wind direction\n\nunits :\n\n°C\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n383.94 MiB\n4.50 MiB\n\n\nShape\n(8523, 49, 241)\n(100, 49, 241)\n\n\nDask graph\n87 chunks in 4 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nsla\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(29, 49, 241), meta=np.ndarray&gt;\n\n\n\n\nancillary_variables :\n\nerr_sla\n\ncomment :\n\nThe sea level anomaly is the sea surface height above mean sea surface; it is referenced to the [1993, 2012] period; see the product user manual for details\n\ngrid_mapping :\n\ncrs\n\nlong_name :\n\nSea level anomaly\n\nstandard_name :\n\nsea_surface_height_above_sea_level\n\nunits :\n\nm\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n383.94 MiB\n4.50 MiB\n\n\nShape\n(8523, 49, 241)\n(100, 49, 241)\n\n\nDask graph\n87 chunks in 4 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nspeed\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(29, 49, 241), meta=np.ndarray&gt;\n\n\n\n\nlong_name :\n\n10 metre absolute speed\n\nunits :\n\nm s**-1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n383.94 MiB\n4.50 MiB\n\n\nShape\n(8523, 49, 241)\n(100, 49, 241)\n\n\nDask graph\n87 chunks in 4 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nsst\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(29, 49, 241), meta=np.ndarray&gt;\n\n\n\n\nlong_name :\n\nSea surface temperature\n\nnameCDM :\n\nSea_surface_temperature_surface\n\nnameECMWF :\n\nSea surface temperature\n\nproduct_type :\n\nanalysis\n\nshortNameECMWF :\n\nsst\n\nstandard_name :\n\nsea_surface_temperature\n\nunits :\n\nK\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n383.94 MiB\n4.50 MiB\n\n\nShape\n(8523, 49, 241)\n(100, 49, 241)\n\n\nDask graph\n87 chunks in 4 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nu_curr\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(29, 49, 241), meta=np.ndarray&gt;\n\n\n\n\ncomment :\n\nVelocities are an average over the top 30m of the mixed layer\n\ncoverage_content_type :\n\nmodelResult\n\ndepth :\n\n15m\n\nlong_name :\n\nzonal total surface current\n\nsource :\n\nSSH source: CMEMS SSALTO/DUACS SEALEVEL_GLO_PHY_L4_MY_008_047 DOI: 10.48670/moi-00148 ; WIND source: ECMWF ERA5 10m wind DOI: 10.24381/cds.adbb2d47 ; SST source: CMC 0.2 deg SST V2.0 DOI: 10.5067/GHCMC-4FM02\n\nstandard_name :\n\neastward_sea_water_velocity\n\nunits :\n\nm s-1\n\nvalid_max :\n\n3.0\n\nvalid_min :\n\n-3.0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n383.94 MiB\n4.50 MiB\n\n\nShape\n(8523, 49, 241)\n(100, 49, 241)\n\n\nDask graph\n87 chunks in 4 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nu_wind\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(29, 49, 241), meta=np.ndarray&gt;\n\n\n\n\nlong_name :\n\n10 metre U wind component\n\nnameCDM :\n\n10_metre_U_wind_component_surface\n\nnameECMWF :\n\n10 metre U wind component\n\nproduct_type :\n\nanalysis\n\nshortNameECMWF :\n\n10u\n\nstandard_name :\n\neastward_wind\n\nunits :\n\nm s**-1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n383.94 MiB\n4.50 MiB\n\n\nShape\n(8523, 49, 241)\n(100, 49, 241)\n\n\nDask graph\n87 chunks in 4 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nug_curr\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(29, 49, 241), meta=np.ndarray&gt;\n\n\n\n\ncomment :\n\nGeostrophic velocities calculated from absolute dynamic topography\n\ndepth :\n\n15m\n\nlong_name :\n\nzonal geostrophic surface current\n\nsource :\n\nSSH source: CMEMS SSALTO/DUACS SEALEVEL_GLO_PHY_L4_MY_008_047 DOI: 10.48670/moi-00148\n\nstandard_name :\n\ngeostrophic_eastward_sea_water_velocity\n\nunits :\n\nm s-1\n\nvalid_max :\n\n3.0\n\nvalid_min :\n\n-3.0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n383.94 MiB\n4.50 MiB\n\n\nShape\n(8523, 49, 241)\n(100, 49, 241)\n\n\nDask graph\n87 chunks in 4 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nv_curr\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(29, 49, 241), meta=np.ndarray&gt;\n\n\n\n\ncomment :\n\nVelocities are an average over the top 30m of the mixed layer\n\ncoverage_content_type :\n\nmodelResult\n\ndepth :\n\n15m\n\nlong_name :\n\nmeridional total surface current\n\nsource :\n\nSSH source: CMEMS SSALTO/DUACS SEALEVEL_GLO_PHY_L4_MY_008_047 DOI: 10.48670/moi-00148 ; WIND source: ECMWF ERA5 10m wind DOI: 10.24381/cds.adbb2d47 ; SST source: CMC 0.2 deg SST V2.0 DOI: 10.5067/GHCMC-4FM02\n\nstandard_name :\n\nnorthward_sea_water_velocity\n\nunits :\n\nm s-1\n\nvalid_max :\n\n3.0\n\nvalid_min :\n\n-3.0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n383.94 MiB\n4.50 MiB\n\n\nShape\n(8523, 49, 241)\n(100, 49, 241)\n\n\nDask graph\n87 chunks in 4 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nv_wind\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(29, 49, 241), meta=np.ndarray&gt;\n\n\n\n\nlong_name :\n\n10 metre V wind component\n\nnameCDM :\n\n10_metre_V_wind_component_surface\n\nnameECMWF :\n\n10 metre V wind component\n\nproduct_type :\n\nanalysis\n\nshortNameECMWF :\n\n10v\n\nstandard_name :\n\nnorthward_wind\n\nunits :\n\nm s**-1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n383.94 MiB\n4.50 MiB\n\n\nShape\n(8523, 49, 241)\n(100, 49, 241)\n\n\nDask graph\n87 chunks in 4 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nvg_curr\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(29, 49, 241), meta=np.ndarray&gt;\n\n\n\n\ncomment :\n\nGeostrophic velocities calculated from absolute dynamic topography\n\ndepth :\n\n15m\n\nlong_name :\n\nmeridional geostrophic surface current\n\nsource :\n\nSSH source: CMEMS SSALTO/DUACS SEALEVEL_GLO_PHY_L4_MY_008_047 DOI: 10.48670/moi-00148\n\nstandard_name :\n\ngeostrophic_northward_sea_water_velocity\n\nunits :\n\nm s-1\n\nvalid_max :\n\n3.0\n\nvalid_min :\n\n-3.0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n383.94 MiB\n4.50 MiB\n\n\nShape\n(8523, 49, 241)\n(100, 49, 241)\n\n\nDask graph\n87 chunks in 4 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\nAttributes: (17)creator_email :minhphan@uw.educreator_name :Minh Phancreator_type :persondate_created :2023-07-07geospatial_lat_max :32.0geospatial_lat_min :-12.0geospatial_lat_resolution :0.25geospatial_lat_units :degrees_northgeospatial_lon_max :102.0geospatial_lon_min :42.0geospatial_lon_resolution :0.25geospatial_lon_units :degrees_eastsource :OSCAR, ERA5 Reanalysis, Copernicus Climate Change Service (C3S), Copernicus Marine Environment Monitoring Service (CMEMS)summary :Daily mean of 0.25 x 0.25 degrees gridded data from multiple climate variables that may influence the patterns of coastal upwelling in the focused areatime_coverage_end :2020-12-31T23:59:59time_coverage_start :2000-01-01T00:00:00title :Climate Data for Coastal Upwelling Machine Learning Project in Indian Ocean\n\n\n\n# slice by longitude\nds.sel(lon=slice(42, 45))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt;\nDimensions:          (time: 8523, lat: 177, lon: 13)\nCoordinates:\n  * lat              (lat) float32 32.0 31.75 31.5 31.25 ... -11.5 -11.75 -12.0\n  * lon              (lon) float32 42.0 42.25 42.5 42.75 ... 44.5 44.75 45.0\n  * time             (time) datetime64[ns] 1997-09-01 1997-09-02 ... 2020-12-31\nData variables: (12/14)\n    CHL              (time, lat, lon) float32 dask.array&lt;chunksize=(29, 177, 13), meta=np.ndarray&gt;\n    CHL_uncertainty  (time, lat, lon) float32 dask.array&lt;chunksize=(29, 177, 13), meta=np.ndarray&gt;\n    adt              (time, lat, lon) float32 dask.array&lt;chunksize=(29, 177, 13), meta=np.ndarray&gt;\n    air_temp         (time, lat, lon) float32 dask.array&lt;chunksize=(29, 177, 13), meta=np.ndarray&gt;\n    direction        (time, lat, lon) float32 dask.array&lt;chunksize=(29, 177, 13), meta=np.ndarray&gt;\n    sla              (time, lat, lon) float32 dask.array&lt;chunksize=(29, 177, 13), meta=np.ndarray&gt;\n    ...               ...\n    u_curr           (time, lat, lon) float32 dask.array&lt;chunksize=(29, 177, 13), meta=np.ndarray&gt;\n    u_wind           (time, lat, lon) float32 dask.array&lt;chunksize=(29, 177, 13), meta=np.ndarray&gt;\n    ug_curr          (time, lat, lon) float32 dask.array&lt;chunksize=(29, 177, 13), meta=np.ndarray&gt;\n    v_curr           (time, lat, lon) float32 dask.array&lt;chunksize=(29, 177, 13), meta=np.ndarray&gt;\n    v_wind           (time, lat, lon) float32 dask.array&lt;chunksize=(29, 177, 13), meta=np.ndarray&gt;\n    vg_curr          (time, lat, lon) float32 dask.array&lt;chunksize=(29, 177, 13), meta=np.ndarray&gt;\nAttributes: (12/17)\n    creator_email:              minhphan@uw.edu\n    creator_name:               Minh Phan\n    creator_type:               person\n    date_created:               2023-07-07\n    geospatial_lat_max:         32.0\n    geospatial_lat_min:         -12.0\n    ...                         ...\n    geospatial_lon_units:       degrees_east\n    source:                     OSCAR, ERA5 Reanalysis, Copernicus Climate Ch...\n    summary:                    Daily mean of 0.25 x 0.25 degrees gridded dat...\n    time_coverage_end:          2020-12-31T23:59:59\n    time_coverage_start:        2000-01-01T00:00:00\n    title:                      Climate Data for Coastal Upwelling Machine Le...xarray.DatasetDimensions:time: 8523lat: 177lon: 13Coordinates: (3)lat(lat)float3232.0 31.75 31.5 ... -11.75 -12.0long_name :latitudestandard_name :latitudeunits :degrees_northarray([ 32.  ,  31.75,  31.5 ,  31.25,  31.  ,  30.75,  30.5 ,  30.25,  30.  ,\n        29.75,  29.5 ,  29.25,  29.  ,  28.75,  28.5 ,  28.25,  28.  ,  27.75,\n        27.5 ,  27.25,  27.  ,  26.75,  26.5 ,  26.25,  26.  ,  25.75,  25.5 ,\n        25.25,  25.  ,  24.75,  24.5 ,  24.25,  24.  ,  23.75,  23.5 ,  23.25,\n        23.  ,  22.75,  22.5 ,  22.25,  22.  ,  21.75,  21.5 ,  21.25,  21.  ,\n        20.75,  20.5 ,  20.25,  20.  ,  19.75,  19.5 ,  19.25,  19.  ,  18.75,\n        18.5 ,  18.25,  18.  ,  17.75,  17.5 ,  17.25,  17.  ,  16.75,  16.5 ,\n        16.25,  16.  ,  15.75,  15.5 ,  15.25,  15.  ,  14.75,  14.5 ,  14.25,\n        14.  ,  13.75,  13.5 ,  13.25,  13.  ,  12.75,  12.5 ,  12.25,  12.  ,\n        11.75,  11.5 ,  11.25,  11.  ,  10.75,  10.5 ,  10.25,  10.  ,   9.75,\n         9.5 ,   9.25,   9.  ,   8.75,   8.5 ,   8.25,   8.  ,   7.75,   7.5 ,\n         7.25,   7.  ,   6.75,   6.5 ,   6.25,   6.  ,   5.75,   5.5 ,   5.25,\n         5.  ,   4.75,   4.5 ,   4.25,   4.  ,   3.75,   3.5 ,   3.25,   3.  ,\n         2.75,   2.5 ,   2.25,   2.  ,   1.75,   1.5 ,   1.25,   1.  ,   0.75,\n         0.5 ,   0.25,   0.  ,  -0.25,  -0.5 ,  -0.75,  -1.  ,  -1.25,  -1.5 ,\n        -1.75,  -2.  ,  -2.25,  -2.5 ,  -2.75,  -3.  ,  -3.25,  -3.5 ,  -3.75,\n        -4.  ,  -4.25,  -4.5 ,  -4.75,  -5.  ,  -5.25,  -5.5 ,  -5.75,  -6.  ,\n        -6.25,  -6.5 ,  -6.75,  -7.  ,  -7.25,  -7.5 ,  -7.75,  -8.  ,  -8.25,\n        -8.5 ,  -8.75,  -9.  ,  -9.25,  -9.5 ,  -9.75, -10.  , -10.25, -10.5 ,\n       -10.75, -11.  , -11.25, -11.5 , -11.75, -12.  ], dtype=float32)lon(lon)float3242.0 42.25 42.5 ... 44.5 44.75 45.0long_name :longitudestandard_name :longitudeunits :degrees_eastarray([42.  , 42.25, 42.5 , 42.75, 43.  , 43.25, 43.5 , 43.75, 44.  , 44.25,\n       44.5 , 44.75, 45.  ], dtype=float32)time(time)datetime64[ns]1997-09-01 ... 2020-12-31axis :Tcomment :Data is averaged over the daylong_name :time centered on the daystandard_name :timetime_bounds :2000-01-01 00:00:00 to 2000-01-01 23:59:59array(['1997-09-01T00:00:00.000000000', '1997-09-02T00:00:00.000000000',\n       '1997-09-03T00:00:00.000000000', ..., '2020-12-29T00:00:00.000000000',\n       '2020-12-30T00:00:00.000000000', '2020-12-31T00:00:00.000000000'],\n      dtype='datetime64[ns]')Data variables: (14)CHL(time, lat, lon)float32dask.array&lt;chunksize=(29, 177, 13), meta=np.ndarray&gt;_ChunkSizes :[1, 256, 256]ancillary_variables :flags CHL_uncertaintycoverage_content_type :modelResultinput_files_reprocessings :Processors versions: MODIS R2022.0NRT/VIIRSN R2022.0NRT/OLCIA 07.02/VIIRSJ1 R2022.0NRT/OLCIB 07.02long_name :Chlorophyll-a concentration - Mean of the binned pixelsstandard_name :mass_concentration_of_chlorophyll_a_in_sea_watertype :surfaceunits :milligram m-3valid_max :1000.0valid_min :0.0\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n74.81 MiB\n898.83 kiB\n\n\nShape\n(8523, 177, 13)\n(100, 177, 13)\n\n\nDask graph\n87 chunks in 4 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\nCHL_uncertainty\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(29, 177, 13), meta=np.ndarray&gt;\n\n\n\n\n_ChunkSizes :\n\n[1, 256, 256]\n\ncoverage_content_type :\n\nqualityInformation\n\nlong_name :\n\nChlorophyll-a concentration - Uncertainty estimation\n\nunits :\n\n%\n\nvalid_max :\n\n32767\n\nvalid_min :\n\n0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n74.81 MiB\n898.83 kiB\n\n\nShape\n(8523, 177, 13)\n(100, 177, 13)\n\n\nDask graph\n87 chunks in 4 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nadt\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(29, 177, 13), meta=np.ndarray&gt;\n\n\n\n\ncomment :\n\nThe absolute dynamic topography is the sea surface height above geoid; the adt is obtained as follows: adt=sla+mdt where mdt is the mean dynamic topography; see the product user manual for details\n\ngrid_mapping :\n\ncrs\n\nlong_name :\n\nAbsolute dynamic topography\n\nstandard_name :\n\nsea_surface_height_above_geoid\n\nunits :\n\nm\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n74.81 MiB\n898.83 kiB\n\n\nShape\n(8523, 177, 13)\n(100, 177, 13)\n\n\nDask graph\n87 chunks in 4 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nair_temp\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(29, 177, 13), meta=np.ndarray&gt;\n\n\n\n\nlong_name :\n\n2 metre temperature\n\nnameCDM :\n\n2_metre_temperature_surface\n\nnameECMWF :\n\n2 metre temperature\n\nproduct_type :\n\nanalysis\n\nshortNameECMWF :\n\n2t\n\nstandard_name :\n\nair_temperature\n\nunits :\n\nK\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n74.81 MiB\n898.83 kiB\n\n\nShape\n(8523, 177, 13)\n(100, 177, 13)\n\n\nDask graph\n87 chunks in 4 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\ndirection\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(29, 177, 13), meta=np.ndarray&gt;\n\n\n\n\nlong_name :\n\n10 metre wind direction\n\nunits :\n\n°C\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n74.81 MiB\n898.83 kiB\n\n\nShape\n(8523, 177, 13)\n(100, 177, 13)\n\n\nDask graph\n87 chunks in 4 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nsla\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(29, 177, 13), meta=np.ndarray&gt;\n\n\n\n\nancillary_variables :\n\nerr_sla\n\ncomment :\n\nThe sea level anomaly is the sea surface height above mean sea surface; it is referenced to the [1993, 2012] period; see the product user manual for details\n\ngrid_mapping :\n\ncrs\n\nlong_name :\n\nSea level anomaly\n\nstandard_name :\n\nsea_surface_height_above_sea_level\n\nunits :\n\nm\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n74.81 MiB\n898.83 kiB\n\n\nShape\n(8523, 177, 13)\n(100, 177, 13)\n\n\nDask graph\n87 chunks in 4 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nspeed\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(29, 177, 13), meta=np.ndarray&gt;\n\n\n\n\nlong_name :\n\n10 metre absolute speed\n\nunits :\n\nm s**-1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n74.81 MiB\n898.83 kiB\n\n\nShape\n(8523, 177, 13)\n(100, 177, 13)\n\n\nDask graph\n87 chunks in 4 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nsst\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(29, 177, 13), meta=np.ndarray&gt;\n\n\n\n\nlong_name :\n\nSea surface temperature\n\nnameCDM :\n\nSea_surface_temperature_surface\n\nnameECMWF :\n\nSea surface temperature\n\nproduct_type :\n\nanalysis\n\nshortNameECMWF :\n\nsst\n\nstandard_name :\n\nsea_surface_temperature\n\nunits :\n\nK\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n74.81 MiB\n898.83 kiB\n\n\nShape\n(8523, 177, 13)\n(100, 177, 13)\n\n\nDask graph\n87 chunks in 4 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nu_curr\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(29, 177, 13), meta=np.ndarray&gt;\n\n\n\n\ncomment :\n\nVelocities are an average over the top 30m of the mixed layer\n\ncoverage_content_type :\n\nmodelResult\n\ndepth :\n\n15m\n\nlong_name :\n\nzonal total surface current\n\nsource :\n\nSSH source: CMEMS SSALTO/DUACS SEALEVEL_GLO_PHY_L4_MY_008_047 DOI: 10.48670/moi-00148 ; WIND source: ECMWF ERA5 10m wind DOI: 10.24381/cds.adbb2d47 ; SST source: CMC 0.2 deg SST V2.0 DOI: 10.5067/GHCMC-4FM02\n\nstandard_name :\n\neastward_sea_water_velocity\n\nunits :\n\nm s-1\n\nvalid_max :\n\n3.0\n\nvalid_min :\n\n-3.0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n74.81 MiB\n898.83 kiB\n\n\nShape\n(8523, 177, 13)\n(100, 177, 13)\n\n\nDask graph\n87 chunks in 4 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nu_wind\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(29, 177, 13), meta=np.ndarray&gt;\n\n\n\n\nlong_name :\n\n10 metre U wind component\n\nnameCDM :\n\n10_metre_U_wind_component_surface\n\nnameECMWF :\n\n10 metre U wind component\n\nproduct_type :\n\nanalysis\n\nshortNameECMWF :\n\n10u\n\nstandard_name :\n\neastward_wind\n\nunits :\n\nm s**-1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n74.81 MiB\n898.83 kiB\n\n\nShape\n(8523, 177, 13)\n(100, 177, 13)\n\n\nDask graph\n87 chunks in 4 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nug_curr\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(29, 177, 13), meta=np.ndarray&gt;\n\n\n\n\ncomment :\n\nGeostrophic velocities calculated from absolute dynamic topography\n\ndepth :\n\n15m\n\nlong_name :\n\nzonal geostrophic surface current\n\nsource :\n\nSSH source: CMEMS SSALTO/DUACS SEALEVEL_GLO_PHY_L4_MY_008_047 DOI: 10.48670/moi-00148\n\nstandard_name :\n\ngeostrophic_eastward_sea_water_velocity\n\nunits :\n\nm s-1\n\nvalid_max :\n\n3.0\n\nvalid_min :\n\n-3.0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n74.81 MiB\n898.83 kiB\n\n\nShape\n(8523, 177, 13)\n(100, 177, 13)\n\n\nDask graph\n87 chunks in 4 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nv_curr\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(29, 177, 13), meta=np.ndarray&gt;\n\n\n\n\ncomment :\n\nVelocities are an average over the top 30m of the mixed layer\n\ncoverage_content_type :\n\nmodelResult\n\ndepth :\n\n15m\n\nlong_name :\n\nmeridional total surface current\n\nsource :\n\nSSH source: CMEMS SSALTO/DUACS SEALEVEL_GLO_PHY_L4_MY_008_047 DOI: 10.48670/moi-00148 ; WIND source: ECMWF ERA5 10m wind DOI: 10.24381/cds.adbb2d47 ; SST source: CMC 0.2 deg SST V2.0 DOI: 10.5067/GHCMC-4FM02\n\nstandard_name :\n\nnorthward_sea_water_velocity\n\nunits :\n\nm s-1\n\nvalid_max :\n\n3.0\n\nvalid_min :\n\n-3.0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n74.81 MiB\n898.83 kiB\n\n\nShape\n(8523, 177, 13)\n(100, 177, 13)\n\n\nDask graph\n87 chunks in 4 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nv_wind\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(29, 177, 13), meta=np.ndarray&gt;\n\n\n\n\nlong_name :\n\n10 metre V wind component\n\nnameCDM :\n\n10_metre_V_wind_component_surface\n\nnameECMWF :\n\n10 metre V wind component\n\nproduct_type :\n\nanalysis\n\nshortNameECMWF :\n\n10v\n\nstandard_name :\n\nnorthward_wind\n\nunits :\n\nm s**-1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n74.81 MiB\n898.83 kiB\n\n\nShape\n(8523, 177, 13)\n(100, 177, 13)\n\n\nDask graph\n87 chunks in 4 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nvg_curr\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(29, 177, 13), meta=np.ndarray&gt;\n\n\n\n\ncomment :\n\nGeostrophic velocities calculated from absolute dynamic topography\n\ndepth :\n\n15m\n\nlong_name :\n\nmeridional geostrophic surface current\n\nsource :\n\nSSH source: CMEMS SSALTO/DUACS SEALEVEL_GLO_PHY_L4_MY_008_047 DOI: 10.48670/moi-00148\n\nstandard_name :\n\ngeostrophic_northward_sea_water_velocity\n\nunits :\n\nm s-1\n\nvalid_max :\n\n3.0\n\nvalid_min :\n\n-3.0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n74.81 MiB\n898.83 kiB\n\n\nShape\n(8523, 177, 13)\n(100, 177, 13)\n\n\nDask graph\n87 chunks in 4 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\nAttributes: (17)creator_email :minhphan@uw.educreator_name :Minh Phancreator_type :persondate_created :2023-07-07geospatial_lat_max :32.0geospatial_lat_min :-12.0geospatial_lat_resolution :0.25geospatial_lat_units :degrees_northgeospatial_lon_max :102.0geospatial_lon_min :42.0geospatial_lon_resolution :0.25geospatial_lon_units :degrees_eastsource :OSCAR, ERA5 Reanalysis, Copernicus Climate Change Service (C3S), Copernicus Marine Environment Monitoring Service (CMEMS)summary :Daily mean of 0.25 x 0.25 degrees gridded data from multiple climate variables that may influence the patterns of coastal upwelling in the focused areatime_coverage_end :2020-12-31T23:59:59time_coverage_start :2000-01-01T00:00:00title :Climate Data for Coastal Upwelling Machine Learning Project in Indian Ocean\n\n\n\n# slice by time\nds.sel(time=slice('1998', '1999'))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt;\nDimensions:          (time: 730, lat: 177, lon: 241)\nCoordinates:\n  * lat              (lat) float32 32.0 31.75 31.5 31.25 ... -11.5 -11.75 -12.0\n  * lon              (lon) float32 42.0 42.25 42.5 42.75 ... 101.5 101.8 102.0\n  * time             (time) datetime64[ns] 1998-01-01 1998-01-02 ... 1999-12-31\nData variables: (12/14)\n    CHL              (time, lat, lon) float32 dask.array&lt;chunksize=(7, 177, 241), meta=np.ndarray&gt;\n    CHL_uncertainty  (time, lat, lon) float32 dask.array&lt;chunksize=(7, 177, 241), meta=np.ndarray&gt;\n    adt              (time, lat, lon) float32 dask.array&lt;chunksize=(7, 177, 241), meta=np.ndarray&gt;\n    air_temp         (time, lat, lon) float32 dask.array&lt;chunksize=(7, 177, 241), meta=np.ndarray&gt;\n    direction        (time, lat, lon) float32 dask.array&lt;chunksize=(7, 177, 241), meta=np.ndarray&gt;\n    sla              (time, lat, lon) float32 dask.array&lt;chunksize=(7, 177, 241), meta=np.ndarray&gt;\n    ...               ...\n    u_curr           (time, lat, lon) float32 dask.array&lt;chunksize=(7, 177, 241), meta=np.ndarray&gt;\n    u_wind           (time, lat, lon) float32 dask.array&lt;chunksize=(7, 177, 241), meta=np.ndarray&gt;\n    ug_curr          (time, lat, lon) float32 dask.array&lt;chunksize=(7, 177, 241), meta=np.ndarray&gt;\n    v_curr           (time, lat, lon) float32 dask.array&lt;chunksize=(7, 177, 241), meta=np.ndarray&gt;\n    v_wind           (time, lat, lon) float32 dask.array&lt;chunksize=(7, 177, 241), meta=np.ndarray&gt;\n    vg_curr          (time, lat, lon) float32 dask.array&lt;chunksize=(7, 177, 241), meta=np.ndarray&gt;\nAttributes: (12/17)\n    creator_email:              minhphan@uw.edu\n    creator_name:               Minh Phan\n    creator_type:               person\n    date_created:               2023-07-07\n    geospatial_lat_max:         32.0\n    geospatial_lat_min:         -12.0\n    ...                         ...\n    geospatial_lon_units:       degrees_east\n    source:                     OSCAR, ERA5 Reanalysis, Copernicus Climate Ch...\n    summary:                    Daily mean of 0.25 x 0.25 degrees gridded dat...\n    time_coverage_end:          2020-12-31T23:59:59\n    time_coverage_start:        2000-01-01T00:00:00\n    title:                      Climate Data for Coastal Upwelling Machine Le...xarray.DatasetDimensions:time: 730lat: 177lon: 241Coordinates: (3)lat(lat)float3232.0 31.75 31.5 ... -11.75 -12.0long_name :latitudestandard_name :latitudeunits :degrees_northarray([ 32.  ,  31.75,  31.5 ,  31.25,  31.  ,  30.75,  30.5 ,  30.25,  30.  ,\n        29.75,  29.5 ,  29.25,  29.  ,  28.75,  28.5 ,  28.25,  28.  ,  27.75,\n        27.5 ,  27.25,  27.  ,  26.75,  26.5 ,  26.25,  26.  ,  25.75,  25.5 ,\n        25.25,  25.  ,  24.75,  24.5 ,  24.25,  24.  ,  23.75,  23.5 ,  23.25,\n        23.  ,  22.75,  22.5 ,  22.25,  22.  ,  21.75,  21.5 ,  21.25,  21.  ,\n        20.75,  20.5 ,  20.25,  20.  ,  19.75,  19.5 ,  19.25,  19.  ,  18.75,\n        18.5 ,  18.25,  18.  ,  17.75,  17.5 ,  17.25,  17.  ,  16.75,  16.5 ,\n        16.25,  16.  ,  15.75,  15.5 ,  15.25,  15.  ,  14.75,  14.5 ,  14.25,\n        14.  ,  13.75,  13.5 ,  13.25,  13.  ,  12.75,  12.5 ,  12.25,  12.  ,\n        11.75,  11.5 ,  11.25,  11.  ,  10.75,  10.5 ,  10.25,  10.  ,   9.75,\n         9.5 ,   9.25,   9.  ,   8.75,   8.5 ,   8.25,   8.  ,   7.75,   7.5 ,\n         7.25,   7.  ,   6.75,   6.5 ,   6.25,   6.  ,   5.75,   5.5 ,   5.25,\n         5.  ,   4.75,   4.5 ,   4.25,   4.  ,   3.75,   3.5 ,   3.25,   3.  ,\n         2.75,   2.5 ,   2.25,   2.  ,   1.75,   1.5 ,   1.25,   1.  ,   0.75,\n         0.5 ,   0.25,   0.  ,  -0.25,  -0.5 ,  -0.75,  -1.  ,  -1.25,  -1.5 ,\n        -1.75,  -2.  ,  -2.25,  -2.5 ,  -2.75,  -3.  ,  -3.25,  -3.5 ,  -3.75,\n        -4.  ,  -4.25,  -4.5 ,  -4.75,  -5.  ,  -5.25,  -5.5 ,  -5.75,  -6.  ,\n        -6.25,  -6.5 ,  -6.75,  -7.  ,  -7.25,  -7.5 ,  -7.75,  -8.  ,  -8.25,\n        -8.5 ,  -8.75,  -9.  ,  -9.25,  -9.5 ,  -9.75, -10.  , -10.25, -10.5 ,\n       -10.75, -11.  , -11.25, -11.5 , -11.75, -12.  ], dtype=float32)lon(lon)float3242.0 42.25 42.5 ... 101.8 102.0long_name :longitudestandard_name :longitudeunits :degrees_eastarray([ 42.  ,  42.25,  42.5 , ..., 101.5 , 101.75, 102.  ], dtype=float32)time(time)datetime64[ns]1998-01-01 ... 1999-12-31axis :Tcomment :Data is averaged over the daylong_name :time centered on the daystandard_name :timetime_bounds :2000-01-01 00:00:00 to 2000-01-01 23:59:59array(['1998-01-01T00:00:00.000000000', '1998-01-02T00:00:00.000000000',\n       '1998-01-03T00:00:00.000000000', ..., '1999-12-29T00:00:00.000000000',\n       '1999-12-30T00:00:00.000000000', '1999-12-31T00:00:00.000000000'],\n      dtype='datetime64[ns]')Data variables: (14)CHL(time, lat, lon)float32dask.array&lt;chunksize=(7, 177, 241), meta=np.ndarray&gt;_ChunkSizes :[1, 256, 256]ancillary_variables :flags CHL_uncertaintycoverage_content_type :modelResultinput_files_reprocessings :Processors versions: MODIS R2022.0NRT/VIIRSN R2022.0NRT/OLCIA 07.02/VIIRSJ1 R2022.0NRT/OLCIB 07.02long_name :Chlorophyll-a concentration - Mean of the binned pixelsstandard_name :mass_concentration_of_chlorophyll_a_in_sea_watertype :surfaceunits :milligram m-3valid_max :1000.0valid_min :0.0\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n118.79 MiB\n16.27 MiB\n\n\nShape\n(730, 177, 241)\n(100, 177, 241)\n\n\nDask graph\n9 chunks in 4 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\nCHL_uncertainty\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(7, 177, 241), meta=np.ndarray&gt;\n\n\n\n\n_ChunkSizes :\n\n[1, 256, 256]\n\ncoverage_content_type :\n\nqualityInformation\n\nlong_name :\n\nChlorophyll-a concentration - Uncertainty estimation\n\nunits :\n\n%\n\nvalid_max :\n\n32767\n\nvalid_min :\n\n0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n118.79 MiB\n16.27 MiB\n\n\nShape\n(730, 177, 241)\n(100, 177, 241)\n\n\nDask graph\n9 chunks in 4 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nadt\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(7, 177, 241), meta=np.ndarray&gt;\n\n\n\n\ncomment :\n\nThe absolute dynamic topography is the sea surface height above geoid; the adt is obtained as follows: adt=sla+mdt where mdt is the mean dynamic topography; see the product user manual for details\n\ngrid_mapping :\n\ncrs\n\nlong_name :\n\nAbsolute dynamic topography\n\nstandard_name :\n\nsea_surface_height_above_geoid\n\nunits :\n\nm\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n118.79 MiB\n16.27 MiB\n\n\nShape\n(730, 177, 241)\n(100, 177, 241)\n\n\nDask graph\n9 chunks in 4 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nair_temp\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(7, 177, 241), meta=np.ndarray&gt;\n\n\n\n\nlong_name :\n\n2 metre temperature\n\nnameCDM :\n\n2_metre_temperature_surface\n\nnameECMWF :\n\n2 metre temperature\n\nproduct_type :\n\nanalysis\n\nshortNameECMWF :\n\n2t\n\nstandard_name :\n\nair_temperature\n\nunits :\n\nK\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n118.79 MiB\n16.27 MiB\n\n\nShape\n(730, 177, 241)\n(100, 177, 241)\n\n\nDask graph\n9 chunks in 4 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\ndirection\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(7, 177, 241), meta=np.ndarray&gt;\n\n\n\n\nlong_name :\n\n10 metre wind direction\n\nunits :\n\n°C\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n118.79 MiB\n16.27 MiB\n\n\nShape\n(730, 177, 241)\n(100, 177, 241)\n\n\nDask graph\n9 chunks in 4 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nsla\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(7, 177, 241), meta=np.ndarray&gt;\n\n\n\n\nancillary_variables :\n\nerr_sla\n\ncomment :\n\nThe sea level anomaly is the sea surface height above mean sea surface; it is referenced to the [1993, 2012] period; see the product user manual for details\n\ngrid_mapping :\n\ncrs\n\nlong_name :\n\nSea level anomaly\n\nstandard_name :\n\nsea_surface_height_above_sea_level\n\nunits :\n\nm\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n118.79 MiB\n16.27 MiB\n\n\nShape\n(730, 177, 241)\n(100, 177, 241)\n\n\nDask graph\n9 chunks in 4 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nspeed\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(7, 177, 241), meta=np.ndarray&gt;\n\n\n\n\nlong_name :\n\n10 metre absolute speed\n\nunits :\n\nm s**-1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n118.79 MiB\n16.27 MiB\n\n\nShape\n(730, 177, 241)\n(100, 177, 241)\n\n\nDask graph\n9 chunks in 4 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nsst\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(7, 177, 241), meta=np.ndarray&gt;\n\n\n\n\nlong_name :\n\nSea surface temperature\n\nnameCDM :\n\nSea_surface_temperature_surface\n\nnameECMWF :\n\nSea surface temperature\n\nproduct_type :\n\nanalysis\n\nshortNameECMWF :\n\nsst\n\nstandard_name :\n\nsea_surface_temperature\n\nunits :\n\nK\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n118.79 MiB\n16.27 MiB\n\n\nShape\n(730, 177, 241)\n(100, 177, 241)\n\n\nDask graph\n9 chunks in 4 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nu_curr\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(7, 177, 241), meta=np.ndarray&gt;\n\n\n\n\ncomment :\n\nVelocities are an average over the top 30m of the mixed layer\n\ncoverage_content_type :\n\nmodelResult\n\ndepth :\n\n15m\n\nlong_name :\n\nzonal total surface current\n\nsource :\n\nSSH source: CMEMS SSALTO/DUACS SEALEVEL_GLO_PHY_L4_MY_008_047 DOI: 10.48670/moi-00148 ; WIND source: ECMWF ERA5 10m wind DOI: 10.24381/cds.adbb2d47 ; SST source: CMC 0.2 deg SST V2.0 DOI: 10.5067/GHCMC-4FM02\n\nstandard_name :\n\neastward_sea_water_velocity\n\nunits :\n\nm s-1\n\nvalid_max :\n\n3.0\n\nvalid_min :\n\n-3.0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n118.79 MiB\n16.27 MiB\n\n\nShape\n(730, 177, 241)\n(100, 177, 241)\n\n\nDask graph\n9 chunks in 4 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nu_wind\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(7, 177, 241), meta=np.ndarray&gt;\n\n\n\n\nlong_name :\n\n10 metre U wind component\n\nnameCDM :\n\n10_metre_U_wind_component_surface\n\nnameECMWF :\n\n10 metre U wind component\n\nproduct_type :\n\nanalysis\n\nshortNameECMWF :\n\n10u\n\nstandard_name :\n\neastward_wind\n\nunits :\n\nm s**-1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n118.79 MiB\n16.27 MiB\n\n\nShape\n(730, 177, 241)\n(100, 177, 241)\n\n\nDask graph\n9 chunks in 4 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nug_curr\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(7, 177, 241), meta=np.ndarray&gt;\n\n\n\n\ncomment :\n\nGeostrophic velocities calculated from absolute dynamic topography\n\ndepth :\n\n15m\n\nlong_name :\n\nzonal geostrophic surface current\n\nsource :\n\nSSH source: CMEMS SSALTO/DUACS SEALEVEL_GLO_PHY_L4_MY_008_047 DOI: 10.48670/moi-00148\n\nstandard_name :\n\ngeostrophic_eastward_sea_water_velocity\n\nunits :\n\nm s-1\n\nvalid_max :\n\n3.0\n\nvalid_min :\n\n-3.0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n118.79 MiB\n16.27 MiB\n\n\nShape\n(730, 177, 241)\n(100, 177, 241)\n\n\nDask graph\n9 chunks in 4 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nv_curr\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(7, 177, 241), meta=np.ndarray&gt;\n\n\n\n\ncomment :\n\nVelocities are an average over the top 30m of the mixed layer\n\ncoverage_content_type :\n\nmodelResult\n\ndepth :\n\n15m\n\nlong_name :\n\nmeridional total surface current\n\nsource :\n\nSSH source: CMEMS SSALTO/DUACS SEALEVEL_GLO_PHY_L4_MY_008_047 DOI: 10.48670/moi-00148 ; WIND source: ECMWF ERA5 10m wind DOI: 10.24381/cds.adbb2d47 ; SST source: CMC 0.2 deg SST V2.0 DOI: 10.5067/GHCMC-4FM02\n\nstandard_name :\n\nnorthward_sea_water_velocity\n\nunits :\n\nm s-1\n\nvalid_max :\n\n3.0\n\nvalid_min :\n\n-3.0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n118.79 MiB\n16.27 MiB\n\n\nShape\n(730, 177, 241)\n(100, 177, 241)\n\n\nDask graph\n9 chunks in 4 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nv_wind\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(7, 177, 241), meta=np.ndarray&gt;\n\n\n\n\nlong_name :\n\n10 metre V wind component\n\nnameCDM :\n\n10_metre_V_wind_component_surface\n\nnameECMWF :\n\n10 metre V wind component\n\nproduct_type :\n\nanalysis\n\nshortNameECMWF :\n\n10v\n\nstandard_name :\n\nnorthward_wind\n\nunits :\n\nm s**-1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n118.79 MiB\n16.27 MiB\n\n\nShape\n(730, 177, 241)\n(100, 177, 241)\n\n\nDask graph\n9 chunks in 4 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nvg_curr\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(7, 177, 241), meta=np.ndarray&gt;\n\n\n\n\ncomment :\n\nGeostrophic velocities calculated from absolute dynamic topography\n\ndepth :\n\n15m\n\nlong_name :\n\nmeridional geostrophic surface current\n\nsource :\n\nSSH source: CMEMS SSALTO/DUACS SEALEVEL_GLO_PHY_L4_MY_008_047 DOI: 10.48670/moi-00148\n\nstandard_name :\n\ngeostrophic_northward_sea_water_velocity\n\nunits :\n\nm s-1\n\nvalid_max :\n\n3.0\n\nvalid_min :\n\n-3.0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n118.79 MiB\n16.27 MiB\n\n\nShape\n(730, 177, 241)\n(100, 177, 241)\n\n\nDask graph\n9 chunks in 4 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\nAttributes: (17)creator_email :minhphan@uw.educreator_name :Minh Phancreator_type :persondate_created :2023-07-07geospatial_lat_max :32.0geospatial_lat_min :-12.0geospatial_lat_resolution :0.25geospatial_lat_units :degrees_northgeospatial_lon_max :102.0geospatial_lon_min :42.0geospatial_lon_resolution :0.25geospatial_lon_units :degrees_eastsource :OSCAR, ERA5 Reanalysis, Copernicus Climate Change Service (C3S), Copernicus Marine Environment Monitoring Service (CMEMS)summary :Daily mean of 0.25 x 0.25 degrees gridded data from multiple climate variables that may influence the patterns of coastal upwelling in the focused areatime_coverage_end :2020-12-31T23:59:59time_coverage_start :2000-01-01T00:00:00title :Climate Data for Coastal Upwelling Machine Learning Project in Indian Ocean\n\n\n\n# slice by variable\nds[['u_curr', 'u_wind']]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt;\nDimensions:  (time: 8523, lat: 177, lon: 241)\nCoordinates:\n  * lat      (lat) float32 32.0 31.75 31.5 31.25 ... -11.25 -11.5 -11.75 -12.0\n  * lon      (lon) float32 42.0 42.25 42.5 42.75 ... 101.2 101.5 101.8 102.0\n  * time     (time) datetime64[ns] 1997-09-01 1997-09-02 ... 2020-12-31\nData variables:\n    u_curr   (time, lat, lon) float32 dask.array&lt;chunksize=(29, 177, 241), meta=np.ndarray&gt;\n    u_wind   (time, lat, lon) float32 dask.array&lt;chunksize=(29, 177, 241), meta=np.ndarray&gt;\nAttributes: (12/17)\n    creator_email:              minhphan@uw.edu\n    creator_name:               Minh Phan\n    creator_type:               person\n    date_created:               2023-07-07\n    geospatial_lat_max:         32.0\n    geospatial_lat_min:         -12.0\n    ...                         ...\n    geospatial_lon_units:       degrees_east\n    source:                     OSCAR, ERA5 Reanalysis, Copernicus Climate Ch...\n    summary:                    Daily mean of 0.25 x 0.25 degrees gridded dat...\n    time_coverage_end:          2020-12-31T23:59:59\n    time_coverage_start:        2000-01-01T00:00:00\n    title:                      Climate Data for Coastal Upwelling Machine Le...xarray.DatasetDimensions:time: 8523lat: 177lon: 241Coordinates: (3)lat(lat)float3232.0 31.75 31.5 ... -11.75 -12.0long_name :latitudestandard_name :latitudeunits :degrees_northarray([ 32.  ,  31.75,  31.5 ,  31.25,  31.  ,  30.75,  30.5 ,  30.25,  30.  ,\n        29.75,  29.5 ,  29.25,  29.  ,  28.75,  28.5 ,  28.25,  28.  ,  27.75,\n        27.5 ,  27.25,  27.  ,  26.75,  26.5 ,  26.25,  26.  ,  25.75,  25.5 ,\n        25.25,  25.  ,  24.75,  24.5 ,  24.25,  24.  ,  23.75,  23.5 ,  23.25,\n        23.  ,  22.75,  22.5 ,  22.25,  22.  ,  21.75,  21.5 ,  21.25,  21.  ,\n        20.75,  20.5 ,  20.25,  20.  ,  19.75,  19.5 ,  19.25,  19.  ,  18.75,\n        18.5 ,  18.25,  18.  ,  17.75,  17.5 ,  17.25,  17.  ,  16.75,  16.5 ,\n        16.25,  16.  ,  15.75,  15.5 ,  15.25,  15.  ,  14.75,  14.5 ,  14.25,\n        14.  ,  13.75,  13.5 ,  13.25,  13.  ,  12.75,  12.5 ,  12.25,  12.  ,\n        11.75,  11.5 ,  11.25,  11.  ,  10.75,  10.5 ,  10.25,  10.  ,   9.75,\n         9.5 ,   9.25,   9.  ,   8.75,   8.5 ,   8.25,   8.  ,   7.75,   7.5 ,\n         7.25,   7.  ,   6.75,   6.5 ,   6.25,   6.  ,   5.75,   5.5 ,   5.25,\n         5.  ,   4.75,   4.5 ,   4.25,   4.  ,   3.75,   3.5 ,   3.25,   3.  ,\n         2.75,   2.5 ,   2.25,   2.  ,   1.75,   1.5 ,   1.25,   1.  ,   0.75,\n         0.5 ,   0.25,   0.  ,  -0.25,  -0.5 ,  -0.75,  -1.  ,  -1.25,  -1.5 ,\n        -1.75,  -2.  ,  -2.25,  -2.5 ,  -2.75,  -3.  ,  -3.25,  -3.5 ,  -3.75,\n        -4.  ,  -4.25,  -4.5 ,  -4.75,  -5.  ,  -5.25,  -5.5 ,  -5.75,  -6.  ,\n        -6.25,  -6.5 ,  -6.75,  -7.  ,  -7.25,  -7.5 ,  -7.75,  -8.  ,  -8.25,\n        -8.5 ,  -8.75,  -9.  ,  -9.25,  -9.5 ,  -9.75, -10.  , -10.25, -10.5 ,\n       -10.75, -11.  , -11.25, -11.5 , -11.75, -12.  ], dtype=float32)lon(lon)float3242.0 42.25 42.5 ... 101.8 102.0long_name :longitudestandard_name :longitudeunits :degrees_eastarray([ 42.  ,  42.25,  42.5 , ..., 101.5 , 101.75, 102.  ], dtype=float32)time(time)datetime64[ns]1997-09-01 ... 2020-12-31axis :Tcomment :Data is averaged over the daylong_name :time centered on the daystandard_name :timetime_bounds :2000-01-01 00:00:00 to 2000-01-01 23:59:59array(['1997-09-01T00:00:00.000000000', '1997-09-02T00:00:00.000000000',\n       '1997-09-03T00:00:00.000000000', ..., '2020-12-29T00:00:00.000000000',\n       '2020-12-30T00:00:00.000000000', '2020-12-31T00:00:00.000000000'],\n      dtype='datetime64[ns]')Data variables: (2)u_curr(time, lat, lon)float32dask.array&lt;chunksize=(29, 177, 241), meta=np.ndarray&gt;comment :Velocities are an average over the top 30m of the mixed layercoverage_content_type :modelResultdepth :15mlong_name :zonal total surface currentsource :SSH source: CMEMS SSALTO/DUACS SEALEVEL_GLO_PHY_L4_MY_008_047 DOI: 10.48670/moi-00148 ; WIND source: ECMWF ERA5 10m wind DOI: 10.24381/cds.adbb2d47 ; SST source: CMC 0.2 deg SST V2.0 DOI: 10.5067/GHCMC-4FM02standard_name :eastward_sea_water_velocityunits :m s-1valid_max :3.0valid_min :-3.0\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n1.35 GiB\n16.27 MiB\n\n\nShape\n(8523, 177, 241)\n(100, 177, 241)\n\n\nDask graph\n87 chunks in 3 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\nu_wind\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(29, 177, 241), meta=np.ndarray&gt;\n\n\n\n\nlong_name :\n\n10 metre U wind component\n\nnameCDM :\n\n10_metre_U_wind_component_surface\n\nnameECMWF :\n\n10 metre U wind component\n\nproduct_type :\n\nanalysis\n\nshortNameECMWF :\n\n10u\n\nstandard_name :\n\neastward_wind\n\nunits :\n\nm s**-1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n1.35 GiB\n16.27 MiB\n\n\nShape\n(8523, 177, 241)\n(100, 177, 241)\n\n\nDask graph\n87 chunks in 3 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\nAttributes: (17)creator_email :minhphan@uw.educreator_name :Minh Phancreator_type :persondate_created :2023-07-07geospatial_lat_max :32.0geospatial_lat_min :-12.0geospatial_lat_resolution :0.25geospatial_lat_units :degrees_northgeospatial_lon_max :102.0geospatial_lon_min :42.0geospatial_lon_resolution :0.25geospatial_lon_units :degrees_eastsource :OSCAR, ERA5 Reanalysis, Copernicus Climate Change Service (C3S), Copernicus Marine Environment Monitoring Service (CMEMS)summary :Daily mean of 0.25 x 0.25 degrees gridded data from multiple climate variables that may influence the patterns of coastal upwelling in the focused areatime_coverage_end :2020-12-31T23:59:59time_coverage_start :2000-01-01T00:00:00title :Climate Data for Coastal Upwelling Machine Learning Project in Indian Ocean\n\n\n\n# combine multiple slicing options all at once\nds[['u_curr', 'u_wind']].sel(time=slice('1998', '1999'), \n                             lat=slice(0, -12), \n                             lon=slice(42, 45))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt;\nDimensions:  (time: 730, lat: 49, lon: 13)\nCoordinates:\n  * lat      (lat) float32 0.0 -0.25 -0.5 -0.75 ... -11.25 -11.5 -11.75 -12.0\n  * lon      (lon) float32 42.0 42.25 42.5 42.75 43.0 ... 44.25 44.5 44.75 45.0\n  * time     (time) datetime64[ns] 1998-01-01 1998-01-02 ... 1999-12-31\nData variables:\n    u_curr   (time, lat, lon) float32 dask.array&lt;chunksize=(7, 49, 13), meta=np.ndarray&gt;\n    u_wind   (time, lat, lon) float32 dask.array&lt;chunksize=(7, 49, 13), meta=np.ndarray&gt;\nAttributes: (12/17)\n    creator_email:              minhphan@uw.edu\n    creator_name:               Minh Phan\n    creator_type:               person\n    date_created:               2023-07-07\n    geospatial_lat_max:         32.0\n    geospatial_lat_min:         -12.0\n    ...                         ...\n    geospatial_lon_units:       degrees_east\n    source:                     OSCAR, ERA5 Reanalysis, Copernicus Climate Ch...\n    summary:                    Daily mean of 0.25 x 0.25 degrees gridded dat...\n    time_coverage_end:          2020-12-31T23:59:59\n    time_coverage_start:        2000-01-01T00:00:00\n    title:                      Climate Data for Coastal Upwelling Machine Le...xarray.DatasetDimensions:time: 730lat: 49lon: 13Coordinates: (3)lat(lat)float320.0 -0.25 -0.5 ... -11.75 -12.0long_name :latitudestandard_name :latitudeunits :degrees_northarray([  0.  ,  -0.25,  -0.5 ,  -0.75,  -1.  ,  -1.25,  -1.5 ,  -1.75,  -2.  ,\n        -2.25,  -2.5 ,  -2.75,  -3.  ,  -3.25,  -3.5 ,  -3.75,  -4.  ,  -4.25,\n        -4.5 ,  -4.75,  -5.  ,  -5.25,  -5.5 ,  -5.75,  -6.  ,  -6.25,  -6.5 ,\n        -6.75,  -7.  ,  -7.25,  -7.5 ,  -7.75,  -8.  ,  -8.25,  -8.5 ,  -8.75,\n        -9.  ,  -9.25,  -9.5 ,  -9.75, -10.  , -10.25, -10.5 , -10.75, -11.  ,\n       -11.25, -11.5 , -11.75, -12.  ], dtype=float32)lon(lon)float3242.0 42.25 42.5 ... 44.5 44.75 45.0long_name :longitudestandard_name :longitudeunits :degrees_eastarray([42.  , 42.25, 42.5 , 42.75, 43.  , 43.25, 43.5 , 43.75, 44.  , 44.25,\n       44.5 , 44.75, 45.  ], dtype=float32)time(time)datetime64[ns]1998-01-01 ... 1999-12-31axis :Tcomment :Data is averaged over the daylong_name :time centered on the daystandard_name :timetime_bounds :2000-01-01 00:00:00 to 2000-01-01 23:59:59array(['1998-01-01T00:00:00.000000000', '1998-01-02T00:00:00.000000000',\n       '1998-01-03T00:00:00.000000000', ..., '1999-12-29T00:00:00.000000000',\n       '1999-12-30T00:00:00.000000000', '1999-12-31T00:00:00.000000000'],\n      dtype='datetime64[ns]')Data variables: (2)u_curr(time, lat, lon)float32dask.array&lt;chunksize=(7, 49, 13), meta=np.ndarray&gt;comment :Velocities are an average over the top 30m of the mixed layercoverage_content_type :modelResultdepth :15mlong_name :zonal total surface currentsource :SSH source: CMEMS SSALTO/DUACS SEALEVEL_GLO_PHY_L4_MY_008_047 DOI: 10.48670/moi-00148 ; WIND source: ECMWF ERA5 10m wind DOI: 10.24381/cds.adbb2d47 ; SST source: CMC 0.2 deg SST V2.0 DOI: 10.5067/GHCMC-4FM02standard_name :eastward_sea_water_velocityunits :m s-1valid_max :3.0valid_min :-3.0\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n1.77 MiB\n248.83 kiB\n\n\nShape\n(730, 49, 13)\n(100, 49, 13)\n\n\nDask graph\n9 chunks in 4 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\nu_wind\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(7, 49, 13), meta=np.ndarray&gt;\n\n\n\n\nlong_name :\n\n10 metre U wind component\n\nnameCDM :\n\n10_metre_U_wind_component_surface\n\nnameECMWF :\n\n10 metre U wind component\n\nproduct_type :\n\nanalysis\n\nshortNameECMWF :\n\n10u\n\nstandard_name :\n\neastward_wind\n\nunits :\n\nm s**-1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n1.77 MiB\n248.83 kiB\n\n\nShape\n(730, 49, 13)\n(100, 49, 13)\n\n\nDask graph\n9 chunks in 4 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\nAttributes: (17)creator_email :minhphan@uw.educreator_name :Minh Phancreator_type :persondate_created :2023-07-07geospatial_lat_max :32.0geospatial_lat_min :-12.0geospatial_lat_resolution :0.25geospatial_lat_units :degrees_northgeospatial_lon_max :102.0geospatial_lon_min :42.0geospatial_lon_resolution :0.25geospatial_lon_units :degrees_eastsource :OSCAR, ERA5 Reanalysis, Copernicus Climate Change Service (C3S), Copernicus Marine Environment Monitoring Service (CMEMS)summary :Daily mean of 0.25 x 0.25 degrees gridded data from multiple climate variables that may influence the patterns of coastal upwelling in the focused areatime_coverage_end :2020-12-31T23:59:59time_coverage_start :2000-01-01T00:00:00title :Climate Data for Coastal Upwelling Machine Learning Project in Indian Ocean"
  },
  {
    "objectID": "python-notebooks/02_Graphing_Map_Data.html#basic-data-graphing",
    "href": "python-notebooks/02_Graphing_Map_Data.html#basic-data-graphing",
    "title": "Graphing Data from our final product",
    "section": "Basic data graphing",
    "text": "Basic data graphing\nWe can also graph the data right from slicing, especially heatmaps from 2D arrays, or line charts. This is especially useful when we want to inspect elements on the go.\n\n# make sure that the array you slice for a heatmap visualization is a 2D array\nheatmap_arr = ds['wind_speed'].sel(time='2000-01-02')\nheatmap_arr\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray 'wind_speed' (lat: 177, lon: 241)&gt;\ndask.array&lt;getitem, shape=(177, 241), dtype=float32, chunksize=(177, 241), chunktype=numpy.ndarray&gt;\nCoordinates:\n  * lat      (lat) float32 32.0 31.75 31.5 31.25 ... -11.25 -11.5 -11.75 -12.0\n  * lon      (lon) float32 42.0 42.25 42.5 42.75 ... 101.2 101.5 101.8 102.0\n    time     datetime64[ns] 2000-01-02\nAttributes:\n    long_name:  10 metre absolute speed\n    units:      m s**-1xarray.DataArray'wind_speed'lat: 177lon: 241dask.array&lt;chunksize=(177, 241), meta=np.ndarray&gt;\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n166.63 kiB\n166.63 kiB\n\n\nShape\n(177, 241)\n(177, 241)\n\n\nDask graph\n1 chunks in 3 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\nCoordinates: (3)lat(lat)float3232.0 31.75 31.5 ... -11.75 -12.0long_name :latitudestandard_name :latitudeunits :degrees_northarray([ 32.  ,  31.75,  31.5 ,  31.25,  31.  ,  30.75,  30.5 ,  30.25,  30.  ,\n        29.75,  29.5 ,  29.25,  29.  ,  28.75,  28.5 ,  28.25,  28.  ,  27.75,\n        27.5 ,  27.25,  27.  ,  26.75,  26.5 ,  26.25,  26.  ,  25.75,  25.5 ,\n        25.25,  25.  ,  24.75,  24.5 ,  24.25,  24.  ,  23.75,  23.5 ,  23.25,\n        23.  ,  22.75,  22.5 ,  22.25,  22.  ,  21.75,  21.5 ,  21.25,  21.  ,\n        20.75,  20.5 ,  20.25,  20.  ,  19.75,  19.5 ,  19.25,  19.  ,  18.75,\n        18.5 ,  18.25,  18.  ,  17.75,  17.5 ,  17.25,  17.  ,  16.75,  16.5 ,\n        16.25,  16.  ,  15.75,  15.5 ,  15.25,  15.  ,  14.75,  14.5 ,  14.25,\n        14.  ,  13.75,  13.5 ,  13.25,  13.  ,  12.75,  12.5 ,  12.25,  12.  ,\n        11.75,  11.5 ,  11.25,  11.  ,  10.75,  10.5 ,  10.25,  10.  ,   9.75,\n         9.5 ,   9.25,   9.  ,   8.75,   8.5 ,   8.25,   8.  ,   7.75,   7.5 ,\n         7.25,   7.  ,   6.75,   6.5 ,   6.25,   6.  ,   5.75,   5.5 ,   5.25,\n         5.  ,   4.75,   4.5 ,   4.25,   4.  ,   3.75,   3.5 ,   3.25,   3.  ,\n         2.75,   2.5 ,   2.25,   2.  ,   1.75,   1.5 ,   1.25,   1.  ,   0.75,\n         0.5 ,   0.25,   0.  ,  -0.25,  -0.5 ,  -0.75,  -1.  ,  -1.25,  -1.5 ,\n        -1.75,  -2.  ,  -2.25,  -2.5 ,  -2.75,  -3.  ,  -3.25,  -3.5 ,  -3.75,\n        -4.  ,  -4.25,  -4.5 ,  -4.75,  -5.  ,  -5.25,  -5.5 ,  -5.75,  -6.  ,\n        -6.25,  -6.5 ,  -6.75,  -7.  ,  -7.25,  -7.5 ,  -7.75,  -8.  ,  -8.25,\n        -8.5 ,  -8.75,  -9.  ,  -9.25,  -9.5 ,  -9.75, -10.  , -10.25, -10.5 ,\n       -10.75, -11.  , -11.25, -11.5 , -11.75, -12.  ], dtype=float32)lon(lon)float3242.0 42.25 42.5 ... 101.8 102.0long_name :longitudestandard_name :longitudeunits :degrees_eastarray([ 42.  ,  42.25,  42.5 , ..., 101.5 , 101.75, 102.  ], dtype=float32)time()datetime64[ns]2000-01-02axis :Tcomment :Data is averaged over the daylong_name :time centered on the daystandard_name :timetime_bounds :2000-01-01 00:00:00 to 2000-01-01 23:59:59array('2000-01-02T00:00:00.000000000', dtype='datetime64[ns]')Indexes: (2)latPandasIndexPandasIndex(Index([  32.0,  31.75,   31.5,  31.25,   31.0,  30.75,   30.5,  30.25,   30.0,\n        29.75,\n       ...\n        -9.75,  -10.0, -10.25,  -10.5, -10.75,  -11.0, -11.25,  -11.5, -11.75,\n        -12.0],\n      dtype='float32', name='lat', length=177))lonPandasIndexPandasIndex(Index([  42.0,  42.25,   42.5,  42.75,   43.0,  43.25,   43.5,  43.75,   44.0,\n        44.25,\n       ...\n        99.75,  100.0, 100.25,  100.5, 100.75,  101.0, 101.25,  101.5, 101.75,\n        102.0],\n      dtype='float32', name='lon', length=241))Attributes: (2)long_name :10 metre absolute speedunits :m s**-1\n\n\n\n2D array graphing with matplotlib\nBy default, graphing arrays with Xarray is done using matplotlib.\nFor 2D arrays we have multiple options to choose for our graphs\n\nHeatmaps\n\nheatmap_arr.plot.imshow()\n\n\n\n\n\n\nContour maps\n\n# contour map with no filling\nheatmap_arr.plot.contour()\n\n\n\n\n\n# contour map with color filling\nheatmap_arr.plot.contourf()\n\n\n\n\n\n\nSurface plot\n\nheatmap_arr.plot.surface()\n\n\n\n\n\n\n\n1D array graphing with Matplotlib\n\nLine plots\nWe can also plot line graphs if our data is presented in 1D arrays. For the example below, we average our wind speed over the whole area and plot it over time. Matplotlib automatically infers what graph to create if you don’t explicitly call which one to. This piece of code below is a demonstration:\n\nds['wind_speed'].mean(dim=['lat', 'lon']).plot(figsize=(10, 5))\n\n\n\n\nWe can add in parameters to customize our graphs, as additional arguments are passed to the underlying matplotlib plot() function.\n\nds['air_temp'].mean(dim=['lat', 'lon']).sel(time=slice('2007', '2009')).plot.line('r-o', figsize=(10,5), markersize=1)\n\n\n\n\n\n\nHistogram\n\n# creating a new Axe object if there is no currently\n# available one\nax = plt.gca() \nds['wind_dir'].plot.hist(ax = ax)\nax.set_xlabel('10 metre wind direction (degrees east)')\nax.set_ylabel('frequency')\nax.set_title('Daily average wind direction distribution over covered area (1979-2022)')\n\nText(0.5, 1.0, 'Daily average wind direction distribution over covered area (1979-2022)')\n\n\n\n\n\n\n\n\nGraphing with holoviews\nXarray also support graphing using holoviews if you want interactive visualizations.\n\n# scroll along the axes to stretch the graph\nheatmap_arr.hvplot().options(cmap='bgy', width=600, height=500)"
  },
  {
    "objectID": "python-notebooks/02_Graphing_Map_Data.html#maps",
    "href": "python-notebooks/02_Graphing_Map_Data.html#maps",
    "title": "Graphing Data from our final product",
    "section": "Maps",
    "text": "Maps\nSometimes we want to identify areas in which our data may behave differently. One of the best ways to do this is to create maps.\n\nCoastline mapping\nCoastlines can help us separate land and ocean area. We can plot the coastline for our region of interest without using any data.\n\ngeo_axes = plt.axes(projection=cartopy.crs.PlateCarree())\n# add stock image\n# geo_axes.stock_img()\ngeo_axes.set_extent([42, 102, -12, 32])\ngeo_axes.coastlines()\n\n\n\n\nWe can also apply the coastline onto our data. The example below shows a heatmap with the coastline drawn on top of it\n\ntopo = xr.open_dataset('/home/jovyan/shared-readwrite/data/INDIAN_OCEAN_HQ_TOPO.nc')\n\n\nfig, ax = plt.subplots(subplot_kw=dict(projection=cartopy.crs.PlateCarree()))\ntopo.topo.plot.imshow(ax=ax)\nax.add_feature(cartopy.feature.COASTLINE)\n\n\n\n\n\n\nTopography mapping\nWe also have a topography dataset that spans the same grid as our final dataset. At the moment of writing this moment, this dataset has not been incoporated into the latter one yet, as you have seen earlier in this notebook that we have to import it separately.\nWe can use this dataset to visualize the topography through contour maps.\n\ntopo\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt;\nDimensions:    (latitude: 5281, longitude: 7201)\nCoordinates:\n  * latitude   (latitude) float64 32.0 31.99 31.98 31.98 ... -11.98 -11.99 -12.0\n  * longitude  (longitude) float64 42.0 42.01 42.02 42.03 ... 102.0 102.0 102.0\nData variables:\n    topo       (latitude, longitude) float32 349.0 352.0 ... -3.727e+03xarray.DatasetDimensions:latitude: 5281longitude: 7201Coordinates: (2)latitude(latitude)float6432.0 31.99 31.98 ... -11.99 -12.0_CoordinateAxisType :Latactual_range :[-89.99166667  90.        ]axis :Yioos_category :Locationlong_name :Latitudestandard_name :latitudeunits :degrees_northarray([ 32.      ,  31.991667,  31.983333, ..., -11.983333, -11.991667,\n       -12.      ])longitude(longitude)float6442.0 42.01 42.02 ... 102.0 102.0_CoordinateAxisType :Lonactual_range :[-180.          179.99166667]axis :Xioos_category :Locationlong_name :Longitudestandard_name :longitudeunits :degrees_eastarray([ 42.      ,  42.008333,  42.016667, ..., 101.983333, 101.991667,\n       102.      ])Data variables: (1)topo(latitude, longitude)float32349.0 352.0 ... -3.727e+03colorBarMaximum :8000.0colorBarMinimum :-8000.0colorBarPalette :Topographygrid_mapping :GDAL_Geographicsioos_category :Locationlong_name :Topographystandard_name :altitudeunits :metersarray([[  349.,   352.,   351., ...,  3313.,  2918.,  2786.],\n       [  349.,   351.,   351., ...,  3538.,  3142.,  3249.],\n       [  351.,   353.,   351., ...,  3497.,  3352.,  3515.],\n       ...,\n       [-2651., -2653., -2657., ..., -3197., -3361., -3517.],\n       [-2654., -2657., -2661., ..., -3318., -3472., -3616.],\n       [-2658., -2661., -2666., ..., -3450., -3594., -3727.]], dtype=float32)Indexes: (2)latitudePandasIndexPandasIndex(Index([               32.0,  31.991666666666667,  31.983333333333334,\n                    31.975,   31.96666666666667,  31.958333333333336,\n        31.950000000000003,   31.94166666666667,  31.933333333333337,\n        31.925000000000004,\n       ...\n       -11.924999999999997, -11.933333333333337, -11.941666666666663,\n       -11.950000000000003, -11.958333333333329, -11.966666666666669,\n       -11.974999999999994, -11.983333333333334,  -11.99166666666666,\n                     -12.0],\n      dtype='float64', name='latitude', length=5281))longitudePandasIndexPandasIndex(Index([              42.0, 42.008333333333326,  42.01666666666665,\n       42.025000000000006,  42.03333333333333,  42.04166666666666,\n        42.04999999999998,  42.05833333333334,  42.06666666666666,\n        42.07499999999999,\n       ...\n       101.92500000000001, 101.93333333333334, 101.94166666666666,\n       101.94999999999999, 101.95833333333331, 101.96666666666664,\n       101.97500000000002, 101.98333333333335, 101.99166666666667,\n                    102.0],\n      dtype='float64', name='longitude', length=7201))Attributes: (0)\n\n\n\nfig, (ax1, ax2) = plt.subplots(2,1, figsize=(8, 12))\ntopo_contour = topo.topo.plot.contour(levels=12, ax=ax1, cmap='coolwarm')\ntopo_contourf = topo.topo.plot.contourf(levels=12, ax=ax2, cmap='coolwarm', label='')\n# add label in line\n# ax.clabel(topo_contour, inline=1, fontsize=10)\n\nhue1, color1 = topo_contour.legend_elements()\nhue2, color2 = topo_contourf.legend_elements()\nax1.legend(hue1, color1, loc='center left', bbox_to_anchor=(1, 0.5))\nax1.set_aspect('equal')\nax2.set_aspect('equal')\n# ax2.get_legend().set_visible(False)\n\n/srv/conda/envs/notebook/lib/python3.9/site-packages/xarray/plot/dataarray_plot.py:2141: UserWarning: The following kwargs were not used by contour: 'label'\n  primitive = ax.contourf(x, y, z, **kwargs)\n\n\n\n\n\nExporting the contour lines to graph on other datasets:\n\n# extract lines from contour paths\nlines = []\nfor contour_path in topo_contour.collections:\n    for path in contour_path.get_paths():\n        lines.extend(path.to_polygons())\n\n\nlen(lines)\n\n12327\n\n\nWe want to graph the topography lines on top of this heatmap. Can we do this with matplotlib?\nLet’s take a look of the contour lines generated from matplotlib.pyplot.contour() function: it returned a list of PathCollection objects containing elevent different contour types based on elevation (each with a different color). Each elevation has multiple “lines” as each of the line denotes an elevation mark.\n\ntopo_contour.collections\n\n&lt;a list of 11 PathCollection objects&gt;\n\n\n\nlen(topo_contour.collections[0].get_paths())\n\n0\n\n\n\nlen(topo_contour.collections[2].get_paths())\n\n30\n\n\nIn the example below, we graph the contour lines back onto the original contour maps.\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 7))\ntopo.topo.plot.imshow(ax=ax1)\nfor contour in topo_contour.collections[2:9]:\n    for path in contour.get_paths():\n        for line in path.to_polygons():\n            ax1.scatter(line[:,0], line[:, 1], color='black', s=0.1) # graph some of the contours (as dots)\n\ntopo.topo.plot.imshow(ax=ax2)\nfor line in lines:\n    ax2.scatter(line[:,0], line[:, 1], color='black', s=0.5) # graph all contours (as dots)\n    \nax1.set_aspect('equal')\nax2.set_aspect('equal')\n\n\n\n\n\n# very weird lines\nax = plt.gca()\ntopo.topo.plot.imshow(ax=ax)\nfor line in lines[:1200]: # there is an order of the lines sorted by topography height\n    ax.plot(line[:,0], line[:, 1], color='black', markersize=0.1)\n\n\n\n\n\n\nCreating topography contour maps with pygmt\nBecause of the weird lines caused by the connection of the open-ended paths, we want to experiment with some other packages.\n\n# Load Earth relief grids (topography and bathymetry) in various resolutions.\n# get contour grid\ngrid = pygmt.datasets.load_earth_relief(resolution=\"15m\", region=[42, 102, -12, 32])\nfig = pygmt.Figure()\n# add the heatmap first\nfig.grdimage(topo.topo, region='42/102/-12/32', cmap='topo', projection='M4i') # mercator projection\n# and then the contour map on top of it\nfig.grdcontour(grid=grid,\n              annotation='1000+f6p', # font 6p\n              interval=1000,\n              limit=[-5000, 0],\n              projection='M4i',\n              pen=\"a0.15p\") # 0.15 point\nfig.show()\n\n\n\n\n\n# coastline only\nfig2 = pygmt.Figure()\nfig2.grdimage(topo.topo, region='42/102/-12/32', cmap='topo', projection='M4i')\nfig2.coast(\n    region='42/102/-12/32',\n    projection='M4i',\n    shorelines=True\n)\nfig2.colorbar()\nfig2.show()\n\n\n\n\n\n# wind direction\ngrid = pygmt.datasets.load_earth_relief(resolution=\"15m\", region=[42, 102, -12, 32])\n\nfig3 = pygmt.Figure()\nfig3.grdimage(ds.v_wind.isel(time=0), region='42/102/-12/32', projection='M4i')\nfig3.grdcontour(grid=grid,\n              annotation='1000+f6p', # font 6p\n              interval=1000, # every 1000m elevation\n              limit=[-5000, 0],\n              projection='M4i',\n              pen=\"a0.15p\") # 0.15 point\nfig3.coast(\n    region='42/102/-12/32',\n    projection='M4i',\n    shorelines=\"1p,black,solid\"\n)\nfig3.colorbar()\nfig3.show()\n\n\n\n\n\n# wind direction\nfig3 = pygmt.Figure()\nfig3.grdimage(ds.direction.isel(time=0), region='42/102/-12/32', projection='M4i')\nfig3.coast(\n    region='42/102/-12/32',\n    projection='M4i',\n    shorelines=True\n)\nfig3.colorbar()\nfig3.show()\n\n\nfig, ax = plt.subplots(subplot_kw=dict(projection=cartopy.crs.PlateCarree()))\nds.wind_dir.isel(time=0).plot.imshow(ax=ax)\nax.set_aspect('equal')\nax.add_feature(cartopy.feature.COASTLINE)"
  },
  {
    "objectID": "python-notebooks/01_earthaccess_Downloader.html#authorize-credentials",
    "href": "python-notebooks/01_earthaccess_Downloader.html#authorize-credentials",
    "title": "NASA Earth Data",
    "section": "Authorize credentials",
    "text": "Authorize credentials\nTo download data from NASA’s Earth Data database, it’s recommended that you set up a .netrc credential file so that you don’t have to manually log in every time you run a downloading script. To do this, consult 2021 Cloud Hackathon’s tutorial here. Make sure to register an account with Earth Data first before following the tutorial.\nWhen you finished implementing the .netrc file, continue with the tutorial below",
    "crumbs": [
      "Python Tutorials",
      "Data Access",
      "NASA Earth Data"
    ]
  },
  {
    "objectID": "python-notebooks/01_earthaccess_Downloader.html#import-necessary-libraries",
    "href": "python-notebooks/01_earthaccess_Downloader.html#import-necessary-libraries",
    "title": "NASA Earth Data",
    "section": "Import necessary libraries",
    "text": "Import necessary libraries\n\nimport xarray as xr\nimport earthaccess\nimport numpy as np\nimport pandas as pd\nimport os, glob",
    "crumbs": [
      "Python Tutorials",
      "Data Access",
      "NASA Earth Data"
    ]
  },
  {
    "objectID": "python-notebooks/01_earthaccess_Downloader.html#download-data-to-your-local-machine-using-earthaccess-library",
    "href": "python-notebooks/01_earthaccess_Downloader.html#download-data-to-your-local-machine-using-earthaccess-library",
    "title": "NASA Earth Data",
    "section": "Download data to your local machine using earthaccess library",
    "text": "Download data to your local machine using earthaccess library\nEarthaccess library streamlines your downloading, slicing, and searching for granules easier than ever. For cloud-hosted datasets (which is what this tutorial best works with), we choose to download granules to the local machine instead of streaming them to the working Python scripts as some users may not be physically available in the us-west region for streaming to be effective. Local downloading may result in heavy file sizes, but is consistent, and we am also providing some tweaks to save as much as you can, especially if your research interest area requires a long temporal range and does not cover globally.\n\n# Log in using .netrc file\nauth = earthaccess.login(strategy=\"netrc\")\n\nWe are already authenticated with NASA EDL",
    "crumbs": [
      "Python Tutorials",
      "Data Access",
      "NASA Earth Data"
    ]
  },
  {
    "objectID": "python-notebooks/01_earthaccess_Downloader.html#streaming-granules",
    "href": "python-notebooks/01_earthaccess_Downloader.html#streaming-granules",
    "title": "NASA Earth Data",
    "section": "Streaming granules",
    "text": "Streaming granules\n\n# EarthAccess's approach to collecting granules\nresults = earthaccess.search_data(\n    short_name='MUR-JPL-L4-GLOB-v4.1',\n    cloud_hosted=True,\n    bounding_box = (60, 5, 80, 25),\n    temporal=(\"2003-01-01\", \"2003-02-28\")\n)\n\nGranules found: 59\n\n\n\nfiles = earthaccess.open(results) # s3\n\n Opening 59 granules, approx size: 0.0 GB\n\n\n\n\n\n\n\n\n\n\n\n\nds = xr.open_mfdataset(files)\n\n\nds\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt;\nDimensions:           (time: 59, lat: 17999, lon: 36000)\nCoordinates:\n  * time              (time) datetime64[ns] 2003-01-01T09:00:00 ... 2003-02-2...\n  * lat               (lat) float32 -89.99 -89.98 -89.97 ... 89.97 89.98 89.99\n  * lon               (lon) float32 -180.0 -180.0 -180.0 ... 180.0 180.0 180.0\nData variables:\n    analysed_sst      (time, lat, lon) float32 dask.array&lt;chunksize=(1, 17999, 36000), meta=np.ndarray&gt;\n    analysis_error    (time, lat, lon) float32 dask.array&lt;chunksize=(1, 17999, 36000), meta=np.ndarray&gt;\n    mask              (time, lat, lon) float32 dask.array&lt;chunksize=(1, 17999, 36000), meta=np.ndarray&gt;\n    sea_ice_fraction  (time, lat, lon) float32 dask.array&lt;chunksize=(1, 17999, 36000), meta=np.ndarray&gt;\nAttributes: (12/47)\n    Conventions:                CF-1.5\n    title:                      Daily MUR SST, Final product\n    summary:                    A merged, multi-sensor L4 Foundation SST anal...\n    references:                 http://podaac.jpl.nasa.gov/Multi-scale_Ultra-...\n    institution:                Jet Propulsion Laboratory\n    history:                    created at nominal 4-day latency; replaced nr...\n    ...                         ...\n    project:                    NASA Making Earth Science Data Records for Us...\n    publisher_name:             GHRSST Project Office\n    publisher_url:              http://www.ghrsst.org\n    publisher_email:            ghrsst-po@nceo.ac.uk\n    processing_level:           L4\n    cdm_data_type:              gridxarray.DatasetDimensions:time: 59lat: 17999lon: 36000Coordinates: (3)time(time)datetime64[ns]2003-01-01T09:00:00 ... 2003-02-...long_name :reference time of sst fieldstandard_name :timeaxis :Tcomment :Nominal time of analyzed fieldsarray(['2003-01-01T09:00:00.000000000', '2003-01-02T09:00:00.000000000',\n       '2003-01-03T09:00:00.000000000', '2003-01-04T09:00:00.000000000',\n       '2003-01-05T09:00:00.000000000', '2003-01-06T09:00:00.000000000',\n       '2003-01-07T09:00:00.000000000', '2003-01-08T09:00:00.000000000',\n       '2003-01-09T09:00:00.000000000', '2003-01-10T09:00:00.000000000',\n       '2003-01-11T09:00:00.000000000', '2003-01-12T09:00:00.000000000',\n       '2003-01-13T09:00:00.000000000', '2003-01-14T09:00:00.000000000',\n       '2003-01-15T09:00:00.000000000', '2003-01-16T09:00:00.000000000',\n       '2003-01-17T09:00:00.000000000', '2003-01-18T09:00:00.000000000',\n       '2003-01-19T09:00:00.000000000', '2003-01-20T09:00:00.000000000',\n       '2003-01-21T09:00:00.000000000', '2003-01-22T09:00:00.000000000',\n       '2003-01-23T09:00:00.000000000', '2003-01-24T09:00:00.000000000',\n       '2003-01-25T09:00:00.000000000', '2003-01-26T09:00:00.000000000',\n       '2003-01-27T09:00:00.000000000', '2003-01-28T09:00:00.000000000',\n       '2003-01-29T09:00:00.000000000', '2003-01-30T09:00:00.000000000',\n       '2003-01-31T09:00:00.000000000', '2003-02-01T09:00:00.000000000',\n       '2003-02-02T09:00:00.000000000', '2003-02-03T09:00:00.000000000',\n       '2003-02-04T09:00:00.000000000', '2003-02-05T09:00:00.000000000',\n       '2003-02-06T09:00:00.000000000', '2003-02-07T09:00:00.000000000',\n       '2003-02-08T09:00:00.000000000', '2003-02-09T09:00:00.000000000',\n       '2003-02-10T09:00:00.000000000', '2003-02-11T09:00:00.000000000',\n       '2003-02-12T09:00:00.000000000', '2003-02-13T09:00:00.000000000',\n       '2003-02-14T09:00:00.000000000', '2003-02-15T09:00:00.000000000',\n       '2003-02-16T09:00:00.000000000', '2003-02-17T09:00:00.000000000',\n       '2003-02-18T09:00:00.000000000', '2003-02-19T09:00:00.000000000',\n       '2003-02-20T09:00:00.000000000', '2003-02-21T09:00:00.000000000',\n       '2003-02-22T09:00:00.000000000', '2003-02-23T09:00:00.000000000',\n       '2003-02-24T09:00:00.000000000', '2003-02-25T09:00:00.000000000',\n       '2003-02-26T09:00:00.000000000', '2003-02-27T09:00:00.000000000',\n       '2003-02-28T09:00:00.000000000'], dtype='datetime64[ns]')lat(lat)float32-89.99 -89.98 ... 89.98 89.99long_name :latitudestandard_name :latitudeaxis :Yunits :degrees_northvalid_min :-90.0valid_max :90.0comment :nonearray([-89.99, -89.98, -89.97, ...,  89.97,  89.98,  89.99], dtype=float32)lon(lon)float32-180.0 -180.0 ... 180.0 180.0long_name :longitudestandard_name :longitudeaxis :Xunits :degrees_eastvalid_min :-180.0valid_max :180.0comment :nonearray([-179.99, -179.98, -179.97, ...,  179.98,  179.99,  180.  ],\n      dtype=float32)Data variables: (4)analysed_sst(time, lat, lon)float32dask.array&lt;chunksize=(1, 17999, 36000), meta=np.ndarray&gt;long_name :analysed sea surface temperaturestandard_name :sea_surface_foundation_temperatureunits :kelvinvalid_min :-32767valid_max :32767comment :\"Final\" version using Multi-Resolution Variational Analysis (MRVA) method for interpolationsource :AMSRE-REMSS, AVHRR_Pathfinder-PFV5.2-NODC_day, AVHRR_Pathfinder-PFV5.2-NODC_night, MODIS_A-JPL, MODIS_T-JPL, iQUAM-NOAA/NESDIS, Ice_Conc-OSISAF\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n142.42 GiB\n2.41 GiB\n\n\nShape\n(59, 17999, 36000)\n(1, 17999, 36000)\n\n\nDask graph\n59 chunks in 119 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\nanalysis_error\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(1, 17999, 36000), meta=np.ndarray&gt;\n\n\n\n\nlong_name :\n\nestimated error standard deviation of analysed_sst\n\nunits :\n\nkelvin\n\nvalid_min :\n\n0\n\nvalid_max :\n\n32767\n\ncomment :\n\nnone\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n142.42 GiB\n2.41 GiB\n\n\nShape\n(59, 17999, 36000)\n(1, 17999, 36000)\n\n\nDask graph\n59 chunks in 119 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nmask\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(1, 17999, 36000), meta=np.ndarray&gt;\n\n\n\n\nlong_name :\n\nsea/land field composite mask\n\nvalid_min :\n\n1\n\nvalid_max :\n\n31\n\nflag_masks :\n\n[ 1 2 4 8 16]\n\nflag_values :\n\n[ 1 2 5 9 13]\n\nflag_meanings :\n\n1=open-sea, 2=land, 5=open-lake, 9=open-sea with ice in the grid, 13=open-lake with ice in the grid\n\ncomment :\n\nmask can be used to further filter the data.\n\nsource :\n\nGMT \"grdlandmask\", ice flag from sea_ice_fraction data\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n142.42 GiB\n2.41 GiB\n\n\nShape\n(59, 17999, 36000)\n(1, 17999, 36000)\n\n\nDask graph\n59 chunks in 119 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nsea_ice_fraction\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(1, 17999, 36000), meta=np.ndarray&gt;\n\n\n\n\nlong_name :\n\nsea ice area fraction\n\nstandard_name :\n\nsea ice area fraction\n\nunits :\n\nfraction (between 0 and 1)\n\nvalid_min :\n\n0\n\nvalid_max :\n\n100\n\nsource :\n\nEUMETSAT OSI-SAF, copyright EUMETSAT\n\ncomment :\n\nice data interpolated by a nearest neighbor approach.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n142.42 GiB\n2.41 GiB\n\n\nShape\n(59, 17999, 36000)\n(1, 17999, 36000)\n\n\nDask graph\n59 chunks in 119 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\nIndexes: (3)timePandasIndexPandasIndex(DatetimeIndex(['2003-01-01 09:00:00', '2003-01-02 09:00:00',\n               '2003-01-03 09:00:00', '2003-01-04 09:00:00',\n               '2003-01-05 09:00:00', '2003-01-06 09:00:00',\n               '2003-01-07 09:00:00', '2003-01-08 09:00:00',\n               '2003-01-09 09:00:00', '2003-01-10 09:00:00',\n               '2003-01-11 09:00:00', '2003-01-12 09:00:00',\n               '2003-01-13 09:00:00', '2003-01-14 09:00:00',\n               '2003-01-15 09:00:00', '2003-01-16 09:00:00',\n               '2003-01-17 09:00:00', '2003-01-18 09:00:00',\n               '2003-01-19 09:00:00', '2003-01-20 09:00:00',\n               '2003-01-21 09:00:00', '2003-01-22 09:00:00',\n               '2003-01-23 09:00:00', '2003-01-24 09:00:00',\n               '2003-01-25 09:00:00', '2003-01-26 09:00:00',\n               '2003-01-27 09:00:00', '2003-01-28 09:00:00',\n               '2003-01-29 09:00:00', '2003-01-30 09:00:00',\n               '2003-01-31 09:00:00', '2003-02-01 09:00:00',\n               '2003-02-02 09:00:00', '2003-02-03 09:00:00',\n               '2003-02-04 09:00:00', '2003-02-05 09:00:00',\n               '2003-02-06 09:00:00', '2003-02-07 09:00:00',\n               '2003-02-08 09:00:00', '2003-02-09 09:00:00',\n               '2003-02-10 09:00:00', '2003-02-11 09:00:00',\n               '2003-02-12 09:00:00', '2003-02-13 09:00:00',\n               '2003-02-14 09:00:00', '2003-02-15 09:00:00',\n               '2003-02-16 09:00:00', '2003-02-17 09:00:00',\n               '2003-02-18 09:00:00', '2003-02-19 09:00:00',\n               '2003-02-20 09:00:00', '2003-02-21 09:00:00',\n               '2003-02-22 09:00:00', '2003-02-23 09:00:00',\n               '2003-02-24 09:00:00', '2003-02-25 09:00:00',\n               '2003-02-26 09:00:00', '2003-02-27 09:00:00',\n               '2003-02-28 09:00:00'],\n              dtype='datetime64[ns]', name='time', freq=None))latPandasIndexPandasIndex(Index([-89.98999786376953,  -89.9800033569336, -89.97000122070312,\n       -89.95999908447266, -89.94999694824219, -89.94000244140625,\n       -89.93000030517578, -89.91999816894531, -89.91000366210938,\n        -89.9000015258789,\n       ...\n         89.9000015258789,  89.91000366210938,  89.91999816894531,\n        89.93000030517578,  89.94000244140625,  89.94999694824219,\n        89.95999908447266,  89.97000122070312,   89.9800033569336,\n        89.98999786376953],\n      dtype='float32', name='lat', length=17999))lonPandasIndexPandasIndex(Index([-179.99000549316406, -179.97999572753906, -179.97000122070312,\n        -179.9600067138672,  -179.9499969482422, -179.94000244140625,\n       -179.92999267578125,  -179.9199981689453, -179.91000366210938,\n       -179.89999389648438,\n       ...\n        179.91000366210938,   179.9199981689453,  179.92999267578125,\n        179.94000244140625,   179.9499969482422,   179.9600067138672,\n        179.97000122070312,  179.97999572753906,  179.99000549316406,\n                     180.0],\n      dtype='float32', name='lon', length=36000))Attributes: (47)Conventions :CF-1.5title :Daily MUR SST, Final productsummary :A merged, multi-sensor L4 Foundation SST analysis product from JPL.references :http://podaac.jpl.nasa.gov/Multi-scale_Ultra-high_Resolution_MUR-SSTinstitution :Jet Propulsion Laboratoryhistory :created at nominal 4-day latency; replaced nrt (1-day latency) version.comment :MUR = \"Multi-scale Ultra-high Reolution\"license :These data are available free of charge under data policy of JPL PO.DAAC.id :MUR-JPL-L4-GLOB-v04.1naming_authority :org.ghrsstproduct_version :04.1uuid :27665bc0-d5fc-11e1-9b23-0800200c9a66gds_version_id :2.0netcdf_version_id :4.1date_created :20150818T185930Zstart_time :20030101T090000Zstop_time :20030101T090000Ztime_coverage_start :20021231T210000Ztime_coverage_end :20030101T210000Zfile_quality_level :1source :AMSRE-REMSS, AVHRR_Pathfinder-PFV5.2-NODC_day, AVHRR_Pathfinder-PFV5.2-NODC_night, MODIS_A-JPL, MODIS_T-JPL, iQUAM-NOAA/NESDIS, Ice_Conc-OSISAFplatform :Aqua, DMSP, NOAA-POES, Suomi-NPP, Terrasensor :AMSR-E, AVHRR, MODIS, SSM/I, VIIRS, in-situMetadata_Conventions :Unidata Observation Dataset v1.0metadata_link :http://podaac.jpl.nasa.gov/ws/metadata/dataset/?format=iso&shortName=MUR-JPL-L4-GLOB-v04.1keywords :Oceans &gt; Ocean Temperature &gt; Sea Surface Temperaturekeywords_vocabulary :NASA Global Change Master Directory (GCMD) Science Keywordsstandard_name_vocabulary :NetCDF Climate and Forecast (CF) Metadata Conventionsouthernmost_latitude :-90.0northernmost_latitude :90.0westernmost_longitude :-180.0easternmost_longitude :180.0spatial_resolution :0.01 degreesgeospatial_lat_units :degrees northgeospatial_lat_resolution :0.01 degreesgeospatial_lon_units :degrees eastgeospatial_lon_resolution :0.01 degreesacknowledgment :Please acknowledge the use of these data with the following statement:  These data were provided by JPL under support by NASA MEaSUREs program.creator_name :JPL MUR SST projectcreator_email :ghrsst@podaac.jpl.nasa.govcreator_url :http://mur.jpl.nasa.govproject :NASA Making Earth Science Data Records for Use in Research Environments (MEaSUREs) Programpublisher_name :GHRSST Project Officepublisher_url :http://www.ghrsst.orgpublisher_email :ghrsst-po@nceo.ac.ukprocessing_level :L4cdm_data_type :grid\n\n\n\nds.sel(lat=slice(5, 25), lon=slice(60, 80))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt;\nDimensions:           (time: 59, lat: 2001, lon: 2001)\nCoordinates:\n  * time              (time) datetime64[ns] 2003-01-01T09:00:00 ... 2003-02-2...\n  * lat               (lat) float32 5.0 5.01 5.02 5.03 ... 24.98 24.99 25.0\n  * lon               (lon) float32 60.0 60.01 60.02 60.03 ... 79.98 79.99 80.0\nData variables:\n    analysed_sst      (time, lat, lon) float32 dask.array&lt;chunksize=(1, 2001, 2001), meta=np.ndarray&gt;\n    analysis_error    (time, lat, lon) float32 dask.array&lt;chunksize=(1, 2001, 2001), meta=np.ndarray&gt;\n    mask              (time, lat, lon) float32 dask.array&lt;chunksize=(1, 2001, 2001), meta=np.ndarray&gt;\n    sea_ice_fraction  (time, lat, lon) float32 dask.array&lt;chunksize=(1, 2001, 2001), meta=np.ndarray&gt;\nAttributes: (12/47)\n    Conventions:                CF-1.5\n    title:                      Daily MUR SST, Final product\n    summary:                    A merged, multi-sensor L4 Foundation SST anal...\n    references:                 http://podaac.jpl.nasa.gov/Multi-scale_Ultra-...\n    institution:                Jet Propulsion Laboratory\n    history:                    created at nominal 4-day latency; replaced nr...\n    ...                         ...\n    project:                    NASA Making Earth Science Data Records for Us...\n    publisher_name:             GHRSST Project Office\n    publisher_url:              http://www.ghrsst.org\n    publisher_email:            ghrsst-po@nceo.ac.uk\n    processing_level:           L4\n    cdm_data_type:              gridxarray.DatasetDimensions:time: 59lat: 2001lon: 2001Coordinates: (3)time(time)datetime64[ns]2003-01-01T09:00:00 ... 2003-02-...long_name :reference time of sst fieldstandard_name :timeaxis :Tcomment :Nominal time of analyzed fieldsarray(['2003-01-01T09:00:00.000000000', '2003-01-02T09:00:00.000000000',\n       '2003-01-03T09:00:00.000000000', '2003-01-04T09:00:00.000000000',\n       '2003-01-05T09:00:00.000000000', '2003-01-06T09:00:00.000000000',\n       '2003-01-07T09:00:00.000000000', '2003-01-08T09:00:00.000000000',\n       '2003-01-09T09:00:00.000000000', '2003-01-10T09:00:00.000000000',\n       '2003-01-11T09:00:00.000000000', '2003-01-12T09:00:00.000000000',\n       '2003-01-13T09:00:00.000000000', '2003-01-14T09:00:00.000000000',\n       '2003-01-15T09:00:00.000000000', '2003-01-16T09:00:00.000000000',\n       '2003-01-17T09:00:00.000000000', '2003-01-18T09:00:00.000000000',\n       '2003-01-19T09:00:00.000000000', '2003-01-20T09:00:00.000000000',\n       '2003-01-21T09:00:00.000000000', '2003-01-22T09:00:00.000000000',\n       '2003-01-23T09:00:00.000000000', '2003-01-24T09:00:00.000000000',\n       '2003-01-25T09:00:00.000000000', '2003-01-26T09:00:00.000000000',\n       '2003-01-27T09:00:00.000000000', '2003-01-28T09:00:00.000000000',\n       '2003-01-29T09:00:00.000000000', '2003-01-30T09:00:00.000000000',\n       '2003-01-31T09:00:00.000000000', '2003-02-01T09:00:00.000000000',\n       '2003-02-02T09:00:00.000000000', '2003-02-03T09:00:00.000000000',\n       '2003-02-04T09:00:00.000000000', '2003-02-05T09:00:00.000000000',\n       '2003-02-06T09:00:00.000000000', '2003-02-07T09:00:00.000000000',\n       '2003-02-08T09:00:00.000000000', '2003-02-09T09:00:00.000000000',\n       '2003-02-10T09:00:00.000000000', '2003-02-11T09:00:00.000000000',\n       '2003-02-12T09:00:00.000000000', '2003-02-13T09:00:00.000000000',\n       '2003-02-14T09:00:00.000000000', '2003-02-15T09:00:00.000000000',\n       '2003-02-16T09:00:00.000000000', '2003-02-17T09:00:00.000000000',\n       '2003-02-18T09:00:00.000000000', '2003-02-19T09:00:00.000000000',\n       '2003-02-20T09:00:00.000000000', '2003-02-21T09:00:00.000000000',\n       '2003-02-22T09:00:00.000000000', '2003-02-23T09:00:00.000000000',\n       '2003-02-24T09:00:00.000000000', '2003-02-25T09:00:00.000000000',\n       '2003-02-26T09:00:00.000000000', '2003-02-27T09:00:00.000000000',\n       '2003-02-28T09:00:00.000000000'], dtype='datetime64[ns]')lat(lat)float325.0 5.01 5.02 ... 24.98 24.99 25.0long_name :latitudestandard_name :latitudeaxis :Yunits :degrees_northvalid_min :-90.0valid_max :90.0comment :nonearray([ 5.  ,  5.01,  5.02, ..., 24.98, 24.99, 25.  ], dtype=float32)lon(lon)float3260.0 60.01 60.02 ... 79.99 80.0long_name :longitudestandard_name :longitudeaxis :Xunits :degrees_eastvalid_min :-180.0valid_max :180.0comment :nonearray([60.  , 60.01, 60.02, ..., 79.98, 79.99, 80.  ], dtype=float32)Data variables: (4)analysed_sst(time, lat, lon)float32dask.array&lt;chunksize=(1, 2001, 2001), meta=np.ndarray&gt;long_name :analysed sea surface temperaturestandard_name :sea_surface_foundation_temperatureunits :kelvinvalid_min :-32767valid_max :32767comment :\"Final\" version using Multi-Resolution Variational Analysis (MRVA) method for interpolationsource :AMSRE-REMSS, AVHRR_Pathfinder-PFV5.2-NODC_day, AVHRR_Pathfinder-PFV5.2-NODC_night, MODIS_A-JPL, MODIS_T-JPL, iQUAM-NOAA/NESDIS, Ice_Conc-OSISAF\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n901.17 MiB\n15.27 MiB\n\n\nShape\n(59, 2001, 2001)\n(1, 2001, 2001)\n\n\nDask graph\n59 chunks in 120 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\nanalysis_error\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(1, 2001, 2001), meta=np.ndarray&gt;\n\n\n\n\nlong_name :\n\nestimated error standard deviation of analysed_sst\n\nunits :\n\nkelvin\n\nvalid_min :\n\n0\n\nvalid_max :\n\n32767\n\ncomment :\n\nnone\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n901.17 MiB\n15.27 MiB\n\n\nShape\n(59, 2001, 2001)\n(1, 2001, 2001)\n\n\nDask graph\n59 chunks in 120 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nmask\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(1, 2001, 2001), meta=np.ndarray&gt;\n\n\n\n\nlong_name :\n\nsea/land field composite mask\n\nvalid_min :\n\n1\n\nvalid_max :\n\n31\n\nflag_masks :\n\n[ 1 2 4 8 16]\n\nflag_values :\n\n[ 1 2 5 9 13]\n\nflag_meanings :\n\n1=open-sea, 2=land, 5=open-lake, 9=open-sea with ice in the grid, 13=open-lake with ice in the grid\n\ncomment :\n\nmask can be used to further filter the data.\n\nsource :\n\nGMT \"grdlandmask\", ice flag from sea_ice_fraction data\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n901.17 MiB\n15.27 MiB\n\n\nShape\n(59, 2001, 2001)\n(1, 2001, 2001)\n\n\nDask graph\n59 chunks in 120 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nsea_ice_fraction\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(1, 2001, 2001), meta=np.ndarray&gt;\n\n\n\n\nlong_name :\n\nsea ice area fraction\n\nstandard_name :\n\nsea ice area fraction\n\nunits :\n\nfraction (between 0 and 1)\n\nvalid_min :\n\n0\n\nvalid_max :\n\n100\n\nsource :\n\nEUMETSAT OSI-SAF, copyright EUMETSAT\n\ncomment :\n\nice data interpolated by a nearest neighbor approach.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n901.17 MiB\n15.27 MiB\n\n\nShape\n(59, 2001, 2001)\n(1, 2001, 2001)\n\n\nDask graph\n59 chunks in 120 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\nIndexes: (3)timePandasIndexPandasIndex(DatetimeIndex(['2003-01-01 09:00:00', '2003-01-02 09:00:00',\n               '2003-01-03 09:00:00', '2003-01-04 09:00:00',\n               '2003-01-05 09:00:00', '2003-01-06 09:00:00',\n               '2003-01-07 09:00:00', '2003-01-08 09:00:00',\n               '2003-01-09 09:00:00', '2003-01-10 09:00:00',\n               '2003-01-11 09:00:00', '2003-01-12 09:00:00',\n               '2003-01-13 09:00:00', '2003-01-14 09:00:00',\n               '2003-01-15 09:00:00', '2003-01-16 09:00:00',\n               '2003-01-17 09:00:00', '2003-01-18 09:00:00',\n               '2003-01-19 09:00:00', '2003-01-20 09:00:00',\n               '2003-01-21 09:00:00', '2003-01-22 09:00:00',\n               '2003-01-23 09:00:00', '2003-01-24 09:00:00',\n               '2003-01-25 09:00:00', '2003-01-26 09:00:00',\n               '2003-01-27 09:00:00', '2003-01-28 09:00:00',\n               '2003-01-29 09:00:00', '2003-01-30 09:00:00',\n               '2003-01-31 09:00:00', '2003-02-01 09:00:00',\n               '2003-02-02 09:00:00', '2003-02-03 09:00:00',\n               '2003-02-04 09:00:00', '2003-02-05 09:00:00',\n               '2003-02-06 09:00:00', '2003-02-07 09:00:00',\n               '2003-02-08 09:00:00', '2003-02-09 09:00:00',\n               '2003-02-10 09:00:00', '2003-02-11 09:00:00',\n               '2003-02-12 09:00:00', '2003-02-13 09:00:00',\n               '2003-02-14 09:00:00', '2003-02-15 09:00:00',\n               '2003-02-16 09:00:00', '2003-02-17 09:00:00',\n               '2003-02-18 09:00:00', '2003-02-19 09:00:00',\n               '2003-02-20 09:00:00', '2003-02-21 09:00:00',\n               '2003-02-22 09:00:00', '2003-02-23 09:00:00',\n               '2003-02-24 09:00:00', '2003-02-25 09:00:00',\n               '2003-02-26 09:00:00', '2003-02-27 09:00:00',\n               '2003-02-28 09:00:00'],\n              dtype='datetime64[ns]', name='time', freq=None))latPandasIndexPandasIndex(Index([               5.0,  5.010000228881836,  5.019999980926514,\n         5.03000020980835,  5.039999961853027,  5.050000190734863,\n        5.059999942779541,  5.070000171661377,  5.079999923706055,\n        5.090000152587891,\n       ...\n        24.90999984741211, 24.920000076293945,  24.93000030517578,\n       24.940000534057617, 24.950000762939453, 24.959999084472656,\n       24.969999313354492, 24.979999542236328, 24.989999771118164,\n                     25.0],\n      dtype='float32', name='lat', length=2001))lonPandasIndexPandasIndex(Index([              60.0,   60.0099983215332,  60.02000045776367,\n       60.029998779296875, 60.040000915527344,  60.04999923706055,\n       60.060001373291016,  60.06999969482422,  60.08000183105469,\n        60.09000015258789,\n       ...\n        79.91000366210938,  79.91999816894531,  79.93000030517578,\n        79.94000244140625,  79.94999694824219,  79.95999908447266,\n        79.97000122070312,   79.9800033569336,  79.98999786376953,\n                     80.0],\n      dtype='float32', name='lon', length=2001))Attributes: (47)Conventions :CF-1.5title :Daily MUR SST, Final productsummary :A merged, multi-sensor L4 Foundation SST analysis product from JPL.references :http://podaac.jpl.nasa.gov/Multi-scale_Ultra-high_Resolution_MUR-SSTinstitution :Jet Propulsion Laboratoryhistory :created at nominal 4-day latency; replaced nrt (1-day latency) version.comment :MUR = \"Multi-scale Ultra-high Reolution\"license :These data are available free of charge under data policy of JPL PO.DAAC.id :MUR-JPL-L4-GLOB-v04.1naming_authority :org.ghrsstproduct_version :04.1uuid :27665bc0-d5fc-11e1-9b23-0800200c9a66gds_version_id :2.0netcdf_version_id :4.1date_created :20150818T185930Zstart_time :20030101T090000Zstop_time :20030101T090000Ztime_coverage_start :20021231T210000Ztime_coverage_end :20030101T210000Zfile_quality_level :1source :AMSRE-REMSS, AVHRR_Pathfinder-PFV5.2-NODC_day, AVHRR_Pathfinder-PFV5.2-NODC_night, MODIS_A-JPL, MODIS_T-JPL, iQUAM-NOAA/NESDIS, Ice_Conc-OSISAFplatform :Aqua, DMSP, NOAA-POES, Suomi-NPP, Terrasensor :AMSR-E, AVHRR, MODIS, SSM/I, VIIRS, in-situMetadata_Conventions :Unidata Observation Dataset v1.0metadata_link :http://podaac.jpl.nasa.gov/ws/metadata/dataset/?format=iso&shortName=MUR-JPL-L4-GLOB-v04.1keywords :Oceans &gt; Ocean Temperature &gt; Sea Surface Temperaturekeywords_vocabulary :NASA Global Change Master Directory (GCMD) Science Keywordsstandard_name_vocabulary :NetCDF Climate and Forecast (CF) Metadata Conventionsouthernmost_latitude :-90.0northernmost_latitude :90.0westernmost_longitude :-180.0easternmost_longitude :180.0spatial_resolution :0.01 degreesgeospatial_lat_units :degrees northgeospatial_lat_resolution :0.01 degreesgeospatial_lon_units :degrees eastgeospatial_lon_resolution :0.01 degreesacknowledgment :Please acknowledge the use of these data with the following statement:  These data were provided by JPL under support by NASA MEaSUREs program.creator_name :JPL MUR SST projectcreator_email :ghrsst@podaac.jpl.nasa.govcreator_url :http://mur.jpl.nasa.govproject :NASA Making Earth Science Data Records for Use in Research Environments (MEaSUREs) Programpublisher_name :GHRSST Project Officepublisher_url :http://www.ghrsst.orgpublisher_email :ghrsst-po@nceo.ac.ukprocessing_level :L4cdm_data_type :grid",
    "crumbs": [
      "Python Tutorials",
      "Data Access",
      "NASA Earth Data"
    ]
  },
  {
    "objectID": "python-notebooks/01_earthaccess_Downloader.html#download-granules-for-an-extended-period-of-time",
    "href": "python-notebooks/01_earthaccess_Downloader.html#download-granules-for-an-extended-period-of-time",
    "title": "NASA Earth Data",
    "section": "Download granules for an extended period of time",
    "text": "Download granules for an extended period of time\nSince you cannot slice data spatially, downloading granules is going to take up a lot of disk space if you only need data within a small bounding box. For our task, we wrote a simple function to slice data and wrote it into a new file.\nYou can consult the earthacess library website or their notebooks for code snippets on how to browse and look up collections. For this notebook, we mainly focus on the downloading aspect. First, we need to get the list of granules to download.\nSince earthacess does not support spatial slicing, we developed a method to download, slice, combine, and export data yearly, then finally delete temporary downloaded files to save disk space. Assumed that you already knew the temporal, spatial range of the dataset of your chosen, we first download the data by year into a temporary folder, then slice the data and then export the combined data to another folder.\n\n# Our approach\n# Month end not included\n\ndef download_and_combine_granules(short_name, month_start, month_end, lat1=5, lat2=25, lon1=60, lon2=80):\n    for month in pd.date_range(month_start, month_end, freq='M').strftime('%Y-%m-%d'):      \n        print('Collecting granules')\n        granules = earthaccess.granule_query().short_name(short_name).temporal(f'{month[:7]}-01',month).get(366)\n        \n        MAIN_FOLDER = 'demonstrated data/earth_data'\n        TEMP_FOLDER = 'temp'\n        path_temp_folder = os.path.join(MAIN_FOLDER, TEMP_FOLDER)\n        path_processed_folder = os.path.join(MAIN_FOLDER, short_name)\n        # create folder to store data\n        if not os.path.exists(path_temp_folder):\n            os.makedirs(path_temp_folder)\n        if not os.path.exists(path_processed_folder):\n            os.makedirs(path_processed_folder)\n        files = earthaccess.download(granules, path_temp_folder)\n       \n        \n        ## if dataset coordinates are slice-able, use:\n        print('Slicing...')\n        data = xr.open_mfdataset(f'{path_temp_folder}/*.nc').sel(lat=slice(lat1, lat2), lon=slice(lon1, lon2))\n    \n        \n        # combine files together \n        ## for some collections, coordinate names are 'lat' and 'lon' while their underlying indices are 'latitude' and 'longitude', respectively\n        ## may or may not be applicable for other datasets on the site.\n        ## get bounding box if not sliceable\n        ### lat1_idx, lat2_idx, lon1_idx, lon2_idx = get_bounding_box(os.path.join(path_temp_folder, first_file), lat1, lat2, lon1, lon2)\n        ### data = xr.open_mfdataset(f'{path_temp_folder}/*.nc').isel(latitude=slice(lat1_idx, lat2_idx+1), longitude=slice(lon1_idx, lon2_idx+1))\n        \n        data.to_netcdf(f'{path_processed_folder}/{month}.nc')\n        \n        # delete files in the temporary folder\n        print('Deleting temporary files...')\n        files = glob.glob(f'{path_temp_folder}/*.*')\n        for f in files:\n            os.remove(f)\n\ndef get_bounding_box(file_path, lat1=0, lat2=30, lon1=60, lon2=80):\n    \"\"\"\n    The dataset we experimented did not have indexed coordinates, \n    so we resorted to slicing using index positions\n    \"\"\"\n    ds = xr.open_dataset(file_path)\n    \n    lat_vals = ds.lat.values\n    lon_vals = ds.lon.values\n    \n    lat1_idx = np.abs(lat_vals-lat1).argmin()\n    lat2_idx = np.abs(lat_vals-lat2).argmin()\n    lon1_idx = np.abs(lon_vals-lon1).argmin()\n    lon2_idx = np.abs(lon_vals-lon1).argmin()\n    \n    return lat1_idx, lat2_idx, lon1_idx, lon2_idx\n\n\n# download and combine every month's worth of data\ndownload_and_combine_granules(short_name='MUR25-JPL-L4-GLOB-v04.2',\n                          month_start='2003-01', month_end='2003-03', \n                          lat1=5, lat2=25, lon1=60, lon2=80)\n\nCollecting granules\n Getting 31 granules, approx download size: 0.0 GB\nSlicing...\nDeleting temporary files...\nCollecting granules\n Getting 28 granules, approx download size: 0.0 GB\nSlicing...\nDeleting temporary files...\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nlist_of_missing_dates = [\"2003-01-04\", \"2003-01-18\", \"2003-02-18\"]\n\nfor date in list_of_missing_dates:\n    result = earthaccess.search_data(\n        short_name='MUR-JPL-L4-GLOB-v4.1',\n        cloud_hosted=True,\n        bounding_box = (60, 5, 80, 25),\n        temporal=(date, date)\n    )\n    \n    MAIN_FOLDER = 'demonstrated data/earth_data'\n    TEMP_FOLDER = 'temp'\n    path_temp_folder = os.path.join(MAIN_FOLDER, TEMP_FOLDER)\n    file = earthaccess.download(result, path_temp_folder)\n\nGranules found: 1\n Getting 1 granules, approx download size: 0.0 GB\nError while downloading the file 20030218090000-JPL-L4_GHRSST-SSTfnd-MUR-GLOB-v02.0-fv04.1.nc\nTraceback (most recent call last):\n  File \"/srv/conda/envs/notebook/lib/python3.9/site-packages/earthaccess/store.py\", line 488, in _download_file\n    r.raise_for_status()\n  File \"/srv/conda/envs/notebook/lib/python3.9/site-packages/requests/models.py\", line 960, in raise_for_status\n    raise HTTPError(http_error_msg, response=self)\nrequests.exceptions.HTTPError: 502 Server Error: Bad Gateway for url: https://archive.podaac.earthdata.nasa.gov/podaac-ops-cumulus-protected/MUR-JPL-L4-GLOB-v4.1/20030218090000-JPL-L4_GHRSST-SSTfnd-MUR-GLOB-v02.0-fv04.1.nc",
    "crumbs": [
      "Python Tutorials",
      "Data Access",
      "NASA Earth Data"
    ]
  },
  {
    "objectID": "python-notebooks/01_earthaccess_Downloader.html#combine-files-together",
    "href": "python-notebooks/01_earthaccess_Downloader.html#combine-files-together",
    "title": "NASA Earth Data",
    "section": "Combine files together",
    "text": "Combine files together\nNow that we have all netcdf4 files in one place, all spatially sliced, combining the rest of the data is a piece of cake! Note that some of the data will be overlap in the process of combing data every year earlier, so it’s best practice to remove duplicates (if any)\n\nds\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt;\nDimensions:           (time: 59, lat: 17999, lon: 36000)\nCoordinates:\n  * time              (time) datetime64[ns] 2003-01-01T09:00:00 ... 2003-02-2...\n  * lat               (lat) float32 -89.99 -89.98 -89.97 ... 89.97 89.98 89.99\n  * lon               (lon) float32 -180.0 -180.0 -180.0 ... 180.0 180.0 180.0\nData variables:\n    analysed_sst      (time, lat, lon) float32 dask.array&lt;chunksize=(1, 17999, 36000), meta=np.ndarray&gt;\n    analysis_error    (time, lat, lon) float32 dask.array&lt;chunksize=(1, 17999, 36000), meta=np.ndarray&gt;\n    mask              (time, lat, lon) float32 dask.array&lt;chunksize=(1, 17999, 36000), meta=np.ndarray&gt;\n    sea_ice_fraction  (time, lat, lon) float32 dask.array&lt;chunksize=(1, 17999, 36000), meta=np.ndarray&gt;\nAttributes: (12/47)\n    Conventions:                CF-1.5\n    title:                      Daily MUR SST, Final product\n    summary:                    A merged, multi-sensor L4 Foundation SST anal...\n    references:                 http://podaac.jpl.nasa.gov/Multi-scale_Ultra-...\n    institution:                Jet Propulsion Laboratory\n    history:                    created at nominal 4-day latency; replaced nr...\n    ...                         ...\n    project:                    NASA Making Earth Science Data Records for Us...\n    publisher_name:             GHRSST Project Office\n    publisher_url:              http://www.ghrsst.org\n    publisher_email:            ghrsst-po@nceo.ac.uk\n    processing_level:           L4\n    cdm_data_type:              gridxarray.DatasetDimensions:time: 59lat: 17999lon: 36000Coordinates: (3)time(time)datetime64[ns]2003-01-01T09:00:00 ... 2003-02-...long_name :reference time of sst fieldstandard_name :timeaxis :Tcomment :Nominal time of analyzed fieldsarray(['2003-01-01T09:00:00.000000000', '2003-01-02T09:00:00.000000000',\n       '2003-01-03T09:00:00.000000000', '2003-01-04T09:00:00.000000000',\n       '2003-01-05T09:00:00.000000000', '2003-01-06T09:00:00.000000000',\n       '2003-01-07T09:00:00.000000000', '2003-01-08T09:00:00.000000000',\n       '2003-01-09T09:00:00.000000000', '2003-01-10T09:00:00.000000000',\n       '2003-01-11T09:00:00.000000000', '2003-01-12T09:00:00.000000000',\n       '2003-01-13T09:00:00.000000000', '2003-01-14T09:00:00.000000000',\n       '2003-01-15T09:00:00.000000000', '2003-01-16T09:00:00.000000000',\n       '2003-01-17T09:00:00.000000000', '2003-01-18T09:00:00.000000000',\n       '2003-01-19T09:00:00.000000000', '2003-01-20T09:00:00.000000000',\n       '2003-01-21T09:00:00.000000000', '2003-01-22T09:00:00.000000000',\n       '2003-01-23T09:00:00.000000000', '2003-01-24T09:00:00.000000000',\n       '2003-01-25T09:00:00.000000000', '2003-01-26T09:00:00.000000000',\n       '2003-01-27T09:00:00.000000000', '2003-01-28T09:00:00.000000000',\n       '2003-01-29T09:00:00.000000000', '2003-01-30T09:00:00.000000000',\n       '2003-01-31T09:00:00.000000000', '2003-02-01T09:00:00.000000000',\n       '2003-02-02T09:00:00.000000000', '2003-02-03T09:00:00.000000000',\n       '2003-02-04T09:00:00.000000000', '2003-02-05T09:00:00.000000000',\n       '2003-02-06T09:00:00.000000000', '2003-02-07T09:00:00.000000000',\n       '2003-02-08T09:00:00.000000000', '2003-02-09T09:00:00.000000000',\n       '2003-02-10T09:00:00.000000000', '2003-02-11T09:00:00.000000000',\n       '2003-02-12T09:00:00.000000000', '2003-02-13T09:00:00.000000000',\n       '2003-02-14T09:00:00.000000000', '2003-02-15T09:00:00.000000000',\n       '2003-02-16T09:00:00.000000000', '2003-02-17T09:00:00.000000000',\n       '2003-02-18T09:00:00.000000000', '2003-02-19T09:00:00.000000000',\n       '2003-02-20T09:00:00.000000000', '2003-02-21T09:00:00.000000000',\n       '2003-02-22T09:00:00.000000000', '2003-02-23T09:00:00.000000000',\n       '2003-02-24T09:00:00.000000000', '2003-02-25T09:00:00.000000000',\n       '2003-02-26T09:00:00.000000000', '2003-02-27T09:00:00.000000000',\n       '2003-02-28T09:00:00.000000000'], dtype='datetime64[ns]')lat(lat)float32-89.99 -89.98 ... 89.98 89.99long_name :latitudestandard_name :latitudeaxis :Yunits :degrees_northvalid_min :-90.0valid_max :90.0comment :nonearray([-89.99, -89.98, -89.97, ...,  89.97,  89.98,  89.99], dtype=float32)lon(lon)float32-180.0 -180.0 ... 180.0 180.0long_name :longitudestandard_name :longitudeaxis :Xunits :degrees_eastvalid_min :-180.0valid_max :180.0comment :nonearray([-179.99, -179.98, -179.97, ...,  179.98,  179.99,  180.  ],\n      dtype=float32)Data variables: (4)analysed_sst(time, lat, lon)float32dask.array&lt;chunksize=(1, 17999, 36000), meta=np.ndarray&gt;long_name :analysed sea surface temperaturestandard_name :sea_surface_foundation_temperatureunits :kelvinvalid_min :-32767valid_max :32767comment :\"Final\" version using Multi-Resolution Variational Analysis (MRVA) method for interpolationsource :AMSRE-REMSS, AVHRR_Pathfinder-PFV5.2-NODC_day, AVHRR_Pathfinder-PFV5.2-NODC_night, MODIS_A-JPL, MODIS_T-JPL, iQUAM-NOAA/NESDIS, Ice_Conc-OSISAF\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n142.42 GiB\n2.41 GiB\n\n\nShape\n(59, 17999, 36000)\n(1, 17999, 36000)\n\n\nDask graph\n59 chunks in 119 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\nanalysis_error\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(1, 17999, 36000), meta=np.ndarray&gt;\n\n\n\n\nlong_name :\n\nestimated error standard deviation of analysed_sst\n\nunits :\n\nkelvin\n\nvalid_min :\n\n0\n\nvalid_max :\n\n32767\n\ncomment :\n\nnone\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n142.42 GiB\n2.41 GiB\n\n\nShape\n(59, 17999, 36000)\n(1, 17999, 36000)\n\n\nDask graph\n59 chunks in 119 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nmask\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(1, 17999, 36000), meta=np.ndarray&gt;\n\n\n\n\nlong_name :\n\nsea/land field composite mask\n\nvalid_min :\n\n1\n\nvalid_max :\n\n31\n\nflag_masks :\n\n[ 1 2 4 8 16]\n\nflag_values :\n\n[ 1 2 5 9 13]\n\nflag_meanings :\n\n1=open-sea, 2=land, 5=open-lake, 9=open-sea with ice in the grid, 13=open-lake with ice in the grid\n\ncomment :\n\nmask can be used to further filter the data.\n\nsource :\n\nGMT \"grdlandmask\", ice flag from sea_ice_fraction data\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n142.42 GiB\n2.41 GiB\n\n\nShape\n(59, 17999, 36000)\n(1, 17999, 36000)\n\n\nDask graph\n59 chunks in 119 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nsea_ice_fraction\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(1, 17999, 36000), meta=np.ndarray&gt;\n\n\n\n\nlong_name :\n\nsea ice area fraction\n\nstandard_name :\n\nsea ice area fraction\n\nunits :\n\nfraction (between 0 and 1)\n\nvalid_min :\n\n0\n\nvalid_max :\n\n100\n\nsource :\n\nEUMETSAT OSI-SAF, copyright EUMETSAT\n\ncomment :\n\nice data interpolated by a nearest neighbor approach.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n142.42 GiB\n2.41 GiB\n\n\nShape\n(59, 17999, 36000)\n(1, 17999, 36000)\n\n\nDask graph\n59 chunks in 119 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\nIndexes: (3)timePandasIndexPandasIndex(DatetimeIndex(['2003-01-01 09:00:00', '2003-01-02 09:00:00',\n               '2003-01-03 09:00:00', '2003-01-04 09:00:00',\n               '2003-01-05 09:00:00', '2003-01-06 09:00:00',\n               '2003-01-07 09:00:00', '2003-01-08 09:00:00',\n               '2003-01-09 09:00:00', '2003-01-10 09:00:00',\n               '2003-01-11 09:00:00', '2003-01-12 09:00:00',\n               '2003-01-13 09:00:00', '2003-01-14 09:00:00',\n               '2003-01-15 09:00:00', '2003-01-16 09:00:00',\n               '2003-01-17 09:00:00', '2003-01-18 09:00:00',\n               '2003-01-19 09:00:00', '2003-01-20 09:00:00',\n               '2003-01-21 09:00:00', '2003-01-22 09:00:00',\n               '2003-01-23 09:00:00', '2003-01-24 09:00:00',\n               '2003-01-25 09:00:00', '2003-01-26 09:00:00',\n               '2003-01-27 09:00:00', '2003-01-28 09:00:00',\n               '2003-01-29 09:00:00', '2003-01-30 09:00:00',\n               '2003-01-31 09:00:00', '2003-02-01 09:00:00',\n               '2003-02-02 09:00:00', '2003-02-03 09:00:00',\n               '2003-02-04 09:00:00', '2003-02-05 09:00:00',\n               '2003-02-06 09:00:00', '2003-02-07 09:00:00',\n               '2003-02-08 09:00:00', '2003-02-09 09:00:00',\n               '2003-02-10 09:00:00', '2003-02-11 09:00:00',\n               '2003-02-12 09:00:00', '2003-02-13 09:00:00',\n               '2003-02-14 09:00:00', '2003-02-15 09:00:00',\n               '2003-02-16 09:00:00', '2003-02-17 09:00:00',\n               '2003-02-18 09:00:00', '2003-02-19 09:00:00',\n               '2003-02-20 09:00:00', '2003-02-21 09:00:00',\n               '2003-02-22 09:00:00', '2003-02-23 09:00:00',\n               '2003-02-24 09:00:00', '2003-02-25 09:00:00',\n               '2003-02-26 09:00:00', '2003-02-27 09:00:00',\n               '2003-02-28 09:00:00'],\n              dtype='datetime64[ns]', name='time', freq=None))latPandasIndexPandasIndex(Index([-89.98999786376953,  -89.9800033569336, -89.97000122070312,\n       -89.95999908447266, -89.94999694824219, -89.94000244140625,\n       -89.93000030517578, -89.91999816894531, -89.91000366210938,\n        -89.9000015258789,\n       ...\n         89.9000015258789,  89.91000366210938,  89.91999816894531,\n        89.93000030517578,  89.94000244140625,  89.94999694824219,\n        89.95999908447266,  89.97000122070312,   89.9800033569336,\n        89.98999786376953],\n      dtype='float32', name='lat', length=17999))lonPandasIndexPandasIndex(Index([-179.99000549316406, -179.97999572753906, -179.97000122070312,\n        -179.9600067138672,  -179.9499969482422, -179.94000244140625,\n       -179.92999267578125,  -179.9199981689453, -179.91000366210938,\n       -179.89999389648438,\n       ...\n        179.91000366210938,   179.9199981689453,  179.92999267578125,\n        179.94000244140625,   179.9499969482422,   179.9600067138672,\n        179.97000122070312,  179.97999572753906,  179.99000549316406,\n                     180.0],\n      dtype='float32', name='lon', length=36000))Attributes: (47)Conventions :CF-1.5title :Daily MUR SST, Final productsummary :A merged, multi-sensor L4 Foundation SST analysis product from JPL.references :http://podaac.jpl.nasa.gov/Multi-scale_Ultra-high_Resolution_MUR-SSTinstitution :Jet Propulsion Laboratoryhistory :created at nominal 4-day latency; replaced nrt (1-day latency) version.comment :MUR = \"Multi-scale Ultra-high Reolution\"license :These data are available free of charge under data policy of JPL PO.DAAC.id :MUR-JPL-L4-GLOB-v04.1naming_authority :org.ghrsstproduct_version :04.1uuid :27665bc0-d5fc-11e1-9b23-0800200c9a66gds_version_id :2.0netcdf_version_id :4.1date_created :20150818T185930Zstart_time :20030101T090000Zstop_time :20030101T090000Ztime_coverage_start :20021231T210000Ztime_coverage_end :20030101T210000Zfile_quality_level :1source :AMSRE-REMSS, AVHRR_Pathfinder-PFV5.2-NODC_day, AVHRR_Pathfinder-PFV5.2-NODC_night, MODIS_A-JPL, MODIS_T-JPL, iQUAM-NOAA/NESDIS, Ice_Conc-OSISAFplatform :Aqua, DMSP, NOAA-POES, Suomi-NPP, Terrasensor :AMSR-E, AVHRR, MODIS, SSM/I, VIIRS, in-situMetadata_Conventions :Unidata Observation Dataset v1.0metadata_link :http://podaac.jpl.nasa.gov/ws/metadata/dataset/?format=iso&shortName=MUR-JPL-L4-GLOB-v04.1keywords :Oceans &gt; Ocean Temperature &gt; Sea Surface Temperaturekeywords_vocabulary :NASA Global Change Master Directory (GCMD) Science Keywordsstandard_name_vocabulary :NetCDF Climate and Forecast (CF) Metadata Conventionsouthernmost_latitude :-90.0northernmost_latitude :90.0westernmost_longitude :-180.0easternmost_longitude :180.0spatial_resolution :0.01 degreesgeospatial_lat_units :degrees northgeospatial_lat_resolution :0.01 degreesgeospatial_lon_units :degrees eastgeospatial_lon_resolution :0.01 degreesacknowledgment :Please acknowledge the use of these data with the following statement:  These data were provided by JPL under support by NASA MEaSUREs program.creator_name :JPL MUR SST projectcreator_email :ghrsst@podaac.jpl.nasa.govcreator_url :http://mur.jpl.nasa.govproject :NASA Making Earth Science Data Records for Use in Research Environments (MEaSUREs) Programpublisher_name :GHRSST Project Officepublisher_url :http://www.ghrsst.orgpublisher_email :ghrsst-po@nceo.ac.ukprocessing_level :L4cdm_data_type :grid\n\n\n\ncombined = xr.open_mfdataset('demonstrated data/earth_data/MUR25-JPL-L4-GLOB-v04.2/*.nc')\n\ncombined\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt;\nDimensions:           (time: 56, lat: 80, lon: 80)\nCoordinates:\n  * time              (time) datetime64[ns] 2003-01-01T09:00:00 ... 2003-02-2...\n  * lat               (lat) float32 5.125 5.375 5.625 ... 24.38 24.62 24.88\n  * lon               (lon) float32 60.12 60.38 60.62 ... 79.38 79.62 79.88\nData variables:\n    analysed_sst      (time, lat, lon) float32 dask.array&lt;chunksize=(29, 80, 80), meta=np.ndarray&gt;\n    analysis_error    (time, lat, lon) float32 dask.array&lt;chunksize=(29, 80, 80), meta=np.ndarray&gt;\n    mask              (time, lat, lon) float32 dask.array&lt;chunksize=(29, 80, 80), meta=np.ndarray&gt;\n    sea_ice_fraction  (time, lat, lon) float32 dask.array&lt;chunksize=(29, 80, 80), meta=np.ndarray&gt;\n    sst_anomaly       (time, lat, lon) float32 dask.array&lt;chunksize=(29, 80, 80), meta=np.ndarray&gt;\nAttributes: (12/54)\n    Conventions:                CF-1.7, ACDD-1.3\n    title:                      Daily 0.25-degree MUR SST, Final product\n    summary:                    A low-resolution version of the MUR SST analy...\n    keywords:                   Oceans &gt; Ocean Temperature &gt; Sea Surface Temp...\n    keywords_vocabulary:        NASA Global Change Master Directory (GCMD) Sc...\n    standard_name_vocabulary:   NetCDF Climate and Forecast (CF) Metadata Con...\n    ...                         ...\n    publisher_name:             GHRSST Project Office\n    publisher_url:              https://www.ghrsst.org\n    publisher_email:            gpc@ghrsst.org\n    file_quality_level:         3\n    metadata_link:              http://podaac.jpl.nasa.gov/ws/metadata/datase...\n    acknowledgment:             Please acknowledge the use of these data with...xarray.DatasetDimensions:time: 56lat: 80lon: 80Coordinates: (3)time(time)datetime64[ns]2003-01-01T09:00:00 ... 2003-02-...long_name :reference time of sst fieldstandard_name :timecoverage_content_type :coordinateaxis :Tcomment :Nominal time of analyzed fieldsarray(['2003-01-01T09:00:00.000000000', '2003-01-02T09:00:00.000000000',\n       '2003-01-03T09:00:00.000000000', '2003-01-05T09:00:00.000000000',\n       '2003-01-06T09:00:00.000000000', '2003-01-07T09:00:00.000000000',\n       '2003-01-08T09:00:00.000000000', '2003-01-09T09:00:00.000000000',\n       '2003-01-10T09:00:00.000000000', '2003-01-11T09:00:00.000000000',\n       '2003-01-12T09:00:00.000000000', '2003-01-13T09:00:00.000000000',\n       '2003-01-14T09:00:00.000000000', '2003-01-15T09:00:00.000000000',\n       '2003-01-16T09:00:00.000000000', '2003-01-17T09:00:00.000000000',\n       '2003-01-19T09:00:00.000000000', '2003-01-20T09:00:00.000000000',\n       '2003-01-21T09:00:00.000000000', '2003-01-22T09:00:00.000000000',\n       '2003-01-23T09:00:00.000000000', '2003-01-24T09:00:00.000000000',\n       '2003-01-25T09:00:00.000000000', '2003-01-26T09:00:00.000000000',\n       '2003-01-27T09:00:00.000000000', '2003-01-28T09:00:00.000000000',\n       '2003-01-29T09:00:00.000000000', '2003-01-30T09:00:00.000000000',\n       '2003-01-31T09:00:00.000000000', '2003-02-01T09:00:00.000000000',\n       '2003-02-02T09:00:00.000000000', '2003-02-03T09:00:00.000000000',\n       '2003-02-04T09:00:00.000000000', '2003-02-05T09:00:00.000000000',\n       '2003-02-06T09:00:00.000000000', '2003-02-07T09:00:00.000000000',\n       '2003-02-08T09:00:00.000000000', '2003-02-09T09:00:00.000000000',\n       '2003-02-10T09:00:00.000000000', '2003-02-11T09:00:00.000000000',\n       '2003-02-12T09:00:00.000000000', '2003-02-13T09:00:00.000000000',\n       '2003-02-14T09:00:00.000000000', '2003-02-15T09:00:00.000000000',\n       '2003-02-16T09:00:00.000000000', '2003-02-17T09:00:00.000000000',\n       '2003-02-19T09:00:00.000000000', '2003-02-20T09:00:00.000000000',\n       '2003-02-21T09:00:00.000000000', '2003-02-22T09:00:00.000000000',\n       '2003-02-23T09:00:00.000000000', '2003-02-24T09:00:00.000000000',\n       '2003-02-25T09:00:00.000000000', '2003-02-26T09:00:00.000000000',\n       '2003-02-27T09:00:00.000000000', '2003-02-28T09:00:00.000000000'],\n      dtype='datetime64[ns]')lat(lat)float325.125 5.375 5.625 ... 24.62 24.88long_name :latitudestandard_name :latitudecoverage_content_type :coordinateaxis :Yunits :degrees_northvalid_min :-90.0valid_max :90.0comment :geolocations inherited from the input data without correctionarray([ 5.125,  5.375,  5.625,  5.875,  6.125,  6.375,  6.625,  6.875,  7.125,\n        7.375,  7.625,  7.875,  8.125,  8.375,  8.625,  8.875,  9.125,  9.375,\n        9.625,  9.875, 10.125, 10.375, 10.625, 10.875, 11.125, 11.375, 11.625,\n       11.875, 12.125, 12.375, 12.625, 12.875, 13.125, 13.375, 13.625, 13.875,\n       14.125, 14.375, 14.625, 14.875, 15.125, 15.375, 15.625, 15.875, 16.125,\n       16.375, 16.625, 16.875, 17.125, 17.375, 17.625, 17.875, 18.125, 18.375,\n       18.625, 18.875, 19.125, 19.375, 19.625, 19.875, 20.125, 20.375, 20.625,\n       20.875, 21.125, 21.375, 21.625, 21.875, 22.125, 22.375, 22.625, 22.875,\n       23.125, 23.375, 23.625, 23.875, 24.125, 24.375, 24.625, 24.875],\n      dtype=float32)lon(lon)float3260.12 60.38 60.62 ... 79.62 79.88long_name :longitudestandard_name :longitudecoverage_content_type :coordinateaxis :Xunits :degrees_eastvalid_min :-180.0valid_max :180.0comment :geolocations inherited from the input data without correctionarray([60.125, 60.375, 60.625, 60.875, 61.125, 61.375, 61.625, 61.875, 62.125,\n       62.375, 62.625, 62.875, 63.125, 63.375, 63.625, 63.875, 64.125, 64.375,\n       64.625, 64.875, 65.125, 65.375, 65.625, 65.875, 66.125, 66.375, 66.625,\n       66.875, 67.125, 67.375, 67.625, 67.875, 68.125, 68.375, 68.625, 68.875,\n       69.125, 69.375, 69.625, 69.875, 70.125, 70.375, 70.625, 70.875, 71.125,\n       71.375, 71.625, 71.875, 72.125, 72.375, 72.625, 72.875, 73.125, 73.375,\n       73.625, 73.875, 74.125, 74.375, 74.625, 74.875, 75.125, 75.375, 75.625,\n       75.875, 76.125, 76.375, 76.625, 76.875, 77.125, 77.375, 77.625, 77.875,\n       78.125, 78.375, 78.625, 78.875, 79.125, 79.375, 79.625, 79.875],\n      dtype=float32)Data variables: (5)analysed_sst(time, lat, lon)float32dask.array&lt;chunksize=(29, 80, 80), meta=np.ndarray&gt;long_name :analysed sea surface temperaturestandard_name :sea_surface_foundation_temperaturecoverage_content_type :physicalMeasurementunits :kelvinvalid_min :-32767valid_max :32767comment :\"Final\" version using Multi-Resolution Variational Analysis (MRVA) method for interpolationsource :MODIS_T-JPL, MODIS_A-JPL, AMSRE-REMSS, AVHRR17_G-ACSPO, iQUAM-NOAA/NESDIS, Ice_Conc-OSISAF\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n1.37 MiB\n725.00 kiB\n\n\nShape\n(56, 80, 80)\n(29, 80, 80)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\nanalysis_error\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(29, 80, 80), meta=np.ndarray&gt;\n\n\n\n\nlong_name :\n\nestimated error standard deviation of analysed_sst\n\ncoverage_content_type :\n\nqualityInformation\n\nunits :\n\nkelvin\n\nvalid_min :\n\n0\n\nvalid_max :\n\n32767\n\ncomment :\n\nuncertainty in \"analysed_sst\"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n1.37 MiB\n725.00 kiB\n\n\nShape\n(56, 80, 80)\n(29, 80, 80)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nmask\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(29, 80, 80), meta=np.ndarray&gt;\n\n\n\n\nlong_name :\n\nsea/land field composite mask\n\ncoverage_content_type :\n\nreferenceInformation\n\nvalid_min :\n\n1\n\nvalid_max :\n\n31\n\nflag_masks :\n\n[ 1 2 4 8 16]\n\nflag_meanings :\n\nwater land optional_lake_surface sea_ice optional_river_surface\n\ncomment :\n\nflag interpretation as integer values: 1=water, 2=land, 5=lake, 9=water with ice in the grid, 13=lake with ice in the grid, 17=river\n\nsource :\n\nGMT \"grdlandmask\", ice flag from sea_ice_fraction data\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n1.37 MiB\n725.00 kiB\n\n\nShape\n(56, 80, 80)\n(29, 80, 80)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nsea_ice_fraction\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(29, 80, 80), meta=np.ndarray&gt;\n\n\n\n\nlong_name :\n\nsea ice area fraction\n\nstandard_name :\n\nsea_ice_area_fraction\n\ncoverage_content_type :\n\nauxiliaryInformation\n\nvalid_min :\n\n0\n\nvalid_max :\n\n100\n\nsource :\n\nEUMETSAT OSI-SAF, copyright EUMETSAT\n\ncomment :\n\nice fraction is a dimensionless quantity between 0 and 1; it has been interpolated by a nearest neighbor approach.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n1.37 MiB\n725.00 kiB\n\n\nShape\n(56, 80, 80)\n(29, 80, 80)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nsst_anomaly\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(29, 80, 80), meta=np.ndarray&gt;\n\n\n\n\nlong_name :\n\nSST anomaly from a seasonal SST climatology based on the MUR data over 2003-2014 period\n\ncoverage_content_type :\n\nauxiliaryInformation\n\nunits :\n\nkelvin\n\nvalid_min :\n\n-32767\n\nvalid_max :\n\n32767\n\ncomment :\n\nanomaly reference to the day-of-year average between 2003 and 2014\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n1.37 MiB\n725.00 kiB\n\n\nShape\n(56, 80, 80)\n(29, 80, 80)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\nIndexes: (3)timePandasIndexPandasIndex(DatetimeIndex(['2003-01-01 09:00:00', '2003-01-02 09:00:00',\n               '2003-01-03 09:00:00', '2003-01-05 09:00:00',\n               '2003-01-06 09:00:00', '2003-01-07 09:00:00',\n               '2003-01-08 09:00:00', '2003-01-09 09:00:00',\n               '2003-01-10 09:00:00', '2003-01-11 09:00:00',\n               '2003-01-12 09:00:00', '2003-01-13 09:00:00',\n               '2003-01-14 09:00:00', '2003-01-15 09:00:00',\n               '2003-01-16 09:00:00', '2003-01-17 09:00:00',\n               '2003-01-19 09:00:00', '2003-01-20 09:00:00',\n               '2003-01-21 09:00:00', '2003-01-22 09:00:00',\n               '2003-01-23 09:00:00', '2003-01-24 09:00:00',\n               '2003-01-25 09:00:00', '2003-01-26 09:00:00',\n               '2003-01-27 09:00:00', '2003-01-28 09:00:00',\n               '2003-01-29 09:00:00', '2003-01-30 09:00:00',\n               '2003-01-31 09:00:00', '2003-02-01 09:00:00',\n               '2003-02-02 09:00:00', '2003-02-03 09:00:00',\n               '2003-02-04 09:00:00', '2003-02-05 09:00:00',\n               '2003-02-06 09:00:00', '2003-02-07 09:00:00',\n               '2003-02-08 09:00:00', '2003-02-09 09:00:00',\n               '2003-02-10 09:00:00', '2003-02-11 09:00:00',\n               '2003-02-12 09:00:00', '2003-02-13 09:00:00',\n               '2003-02-14 09:00:00', '2003-02-15 09:00:00',\n               '2003-02-16 09:00:00', '2003-02-17 09:00:00',\n               '2003-02-19 09:00:00', '2003-02-20 09:00:00',\n               '2003-02-21 09:00:00', '2003-02-22 09:00:00',\n               '2003-02-23 09:00:00', '2003-02-24 09:00:00',\n               '2003-02-25 09:00:00', '2003-02-26 09:00:00',\n               '2003-02-27 09:00:00', '2003-02-28 09:00:00'],\n              dtype='datetime64[ns]', name='time', freq=None))latPandasIndexPandasIndex(Index([ 5.125,  5.375,  5.625,  5.875,  6.125,  6.375,  6.625,  6.875,  7.125,\n        7.375,  7.625,  7.875,  8.125,  8.375,  8.625,  8.875,  9.125,  9.375,\n        9.625,  9.875, 10.125, 10.375, 10.625, 10.875, 11.125, 11.375, 11.625,\n       11.875, 12.125, 12.375, 12.625, 12.875, 13.125, 13.375, 13.625, 13.875,\n       14.125, 14.375, 14.625, 14.875, 15.125, 15.375, 15.625, 15.875, 16.125,\n       16.375, 16.625, 16.875, 17.125, 17.375, 17.625, 17.875, 18.125, 18.375,\n       18.625, 18.875, 19.125, 19.375, 19.625, 19.875, 20.125, 20.375, 20.625,\n       20.875, 21.125, 21.375, 21.625, 21.875, 22.125, 22.375, 22.625, 22.875,\n       23.125, 23.375, 23.625, 23.875, 24.125, 24.375, 24.625, 24.875],\n      dtype='float32', name='lat'))lonPandasIndexPandasIndex(Index([60.125, 60.375, 60.625, 60.875, 61.125, 61.375, 61.625, 61.875, 62.125,\n       62.375, 62.625, 62.875, 63.125, 63.375, 63.625, 63.875, 64.125, 64.375,\n       64.625, 64.875, 65.125, 65.375, 65.625, 65.875, 66.125, 66.375, 66.625,\n       66.875, 67.125, 67.375, 67.625, 67.875, 68.125, 68.375, 68.625, 68.875,\n       69.125, 69.375, 69.625, 69.875, 70.125, 70.375, 70.625, 70.875, 71.125,\n       71.375, 71.625, 71.875, 72.125, 72.375, 72.625, 72.875, 73.125, 73.375,\n       73.625, 73.875, 74.125, 74.375, 74.625, 74.875, 75.125, 75.375, 75.625,\n       75.875, 76.125, 76.375, 76.625, 76.875, 77.125, 77.375, 77.625, 77.875,\n       78.125, 78.375, 78.625, 78.875, 79.125, 79.375, 79.625, 79.875],\n      dtype='float32', name='lon'))Attributes: (54)Conventions :CF-1.7, ACDD-1.3title :Daily 0.25-degree MUR SST, Final productsummary :A low-resolution version of the MUR SST analysis, a merged, multi-sensor L4 Foundation SST analysis product from JPL.keywords :Oceans &gt; Ocean Temperature &gt; Sea Surface Temperaturekeywords_vocabulary :NASA Global Change Master Directory (GCMD) Science Keywordsstandard_name_vocabulary :NetCDF Climate and Forecast (CF) Metadata Conventionhistory :created at nominal 4-day latency; replaced nrt (1-day latency) version.source :MODIS_T-JPL, MODIS_A-JPL, AMSRE-REMSS, AVHRR17_G-ACSPO, iQUAM-NOAA/NESDIS, Ice_Conc-OSISAFplatform :Terra, Aqua, NOAA-17, Buoys/Shipsinstrument :MODIS, AMSR-E, AVHRR, in-situsensor :MODIS, AMSR-E, AVHRR, in-situprocessing_level :L4cdm_data_type :gridproduct_version :04.2references :Chin et al. (2017) \"Remote Sensing of Environment\", volulme 200, pages 154-169.  http://dx.doi.org/10.1016/j.rse.2017.07.029creator_name :JPL MUR SST projectcreator_email :ghrsst@podaac.jpl.nasa.govcreator_url :http://mur.jpl.nasa.govcreator_institution :Jet Propulsion Laboratoryinstitution :Jet Propulsion Laboratoryproject :NASA MEaSUREs and COVERAGEprogram :NASA Earth Science Data and Information System (ESDIS)southernmost_latitude :-90.0northernmost_latitude :90.0westernmost_longitude :-180.0easternmost_longitude :180.0geospatial_lat_min :-90.0geospatial_lat_max :90.0geospatial_lon_min :-180.0geospatial_lon_max :180.0geospatial_lat_units :degrees northgeospatial_lat_resolution :0.25geospatial_lon_units :degrees eastgeospatial_lon_resolution :0.25date_created :2019-08-28start_time :20030101T090000Zstop_time :20030101T090000Ztime_coverage_start :20021231T210000Ztime_coverage_end :20030101T210000Ztime_coverage_resolution :P1Dlicense :These data are available free of charge under data policy of JPL PO.DAAC.id :MUR25-JPL-L4-GLOB-v04.2uuid :27665bc0-d5fc-11e1-9b23-0800200c9a66comment :MUR = \"Multi-scale Ultra-high Resolution\"naming_authority :org.ghrsstgds_version_id :2.0netcdf_version_id :04.2spatial_resolution :0.25 degreespublisher_name :GHRSST Project Officepublisher_url :https://www.ghrsst.orgpublisher_email :gpc@ghrsst.orgfile_quality_level :3metadata_link :http://podaac.jpl.nasa.gov/ws/metadata/dataset/?format=iso&shortName=MUR25-JPL-L4-GLOB-v04.2acknowledgment :Please acknowledge the use of these data with the following statement:  These data were provided by JPL under support by NASA MEaSUREs and COVERAGE programs.\n\n\nYou can see that we do not need all variables: let’s take a look into each and see what we need.\nThe data is “chunked” (broke down into smaller, more mangaeable pieces to eliminate duplicated copies of repeating data on storage). Therefore you can’t see the actual data if you clicking on the interactive panel above, as it is lazy-loaded to only show the general information of the whole object.\nSimply load the dataset to see the actual underlying data values\n\n# load sst_anomaly to see underlying data\ncombined.sst_anomaly.compute()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray 'sst_anomaly' (time: 56, lat: 80, lon: 80)&gt;\narray([[[-0.086     , -0.02      , -0.026     , ..., -0.252     ,\n         -0.49400002, -0.499     ],\n        [-0.03      , -0.24300002, -0.163     , ..., -0.344     ,\n         -0.416     , -0.38200003],\n        [-0.41700003, -0.37100002, -0.082     , ...,  0.1       ,\n         -0.066     , -0.1       ],\n        ...,\n        [-0.628     , -1.077     , -1.2720001 , ...,         nan,\n                 nan,         nan],\n        [-0.39000002, -0.564     , -0.80500007, ...,         nan,\n                 nan,         nan],\n        [-0.75600004, -0.675     , -0.53900003, ...,         nan,\n                 nan,         nan]],\n\n       [[-0.046     , -0.06900001, -0.089     , ..., -0.333     ,\n         -0.51000005, -0.60400003],\n        [-0.177     , -0.379     , -0.277     , ..., -0.34300002,\n         -0.418     , -0.43100002],\n        [-0.358     , -0.38700002, -0.16600001, ..., -0.026     ,\n         -0.158     , -0.12900001],\n...\n        [ 0.43300003,  0.48000002,  0.633     , ...,         nan,\n                 nan,         nan],\n        [ 0.439     ,  0.54200006,  0.51900005, ...,         nan,\n                 nan,         nan],\n        [ 0.642     ,  0.637     ,  0.609     , ...,         nan,\n                 nan,         nan]],\n\n       [[ 0.194     ,  0.432     ,  0.31      , ..., -0.081     ,\n          0.162     ,  0.14      ],\n        [ 0.147     ,  0.20400001,  0.164     , ...,  0.035     ,\n          0.32500002,  0.094     ],\n        [ 0.194     ,  0.216     ,  0.11300001, ...,  0.24700001,\n          0.33900002,  0.231     ],\n        ...,\n        [-0.085     ,  0.098     ,  0.23200001, ...,         nan,\n                 nan,         nan],\n        [-0.156     , -0.20700002, -0.014     , ...,         nan,\n                 nan,         nan],\n        [-0.037     , -0.07      , -0.012     , ...,         nan,\n                 nan,         nan]]], dtype=float32)\nCoordinates:\n  * time     (time) datetime64[ns] 2003-01-01T09:00:00 ... 2003-02-28T09:00:00\n  * lat      (lat) float32 5.125 5.375 5.625 5.875 ... 24.12 24.38 24.62 24.88\n  * lon      (lon) float32 60.12 60.38 60.62 60.88 ... 79.12 79.38 79.62 79.88\nAttributes:\n    long_name:              SST anomaly from a seasonal SST climatology based...\n    coverage_content_type:  auxiliaryInformation\n    units:                  kelvin\n    valid_min:              -32767\n    valid_max:              32767\n    comment:                anomaly reference to the day-of-year average betw...xarray.DataArray'sst_anomaly'time: 56lat: 80lon: 80-0.086 -0.02 -0.026 -0.038 0.026 0.099 ... nan nan nan nan nan nanarray([[[-0.086     , -0.02      , -0.026     , ..., -0.252     ,\n         -0.49400002, -0.499     ],\n        [-0.03      , -0.24300002, -0.163     , ..., -0.344     ,\n         -0.416     , -0.38200003],\n        [-0.41700003, -0.37100002, -0.082     , ...,  0.1       ,\n         -0.066     , -0.1       ],\n        ...,\n        [-0.628     , -1.077     , -1.2720001 , ...,         nan,\n                 nan,         nan],\n        [-0.39000002, -0.564     , -0.80500007, ...,         nan,\n                 nan,         nan],\n        [-0.75600004, -0.675     , -0.53900003, ...,         nan,\n                 nan,         nan]],\n\n       [[-0.046     , -0.06900001, -0.089     , ..., -0.333     ,\n         -0.51000005, -0.60400003],\n        [-0.177     , -0.379     , -0.277     , ..., -0.34300002,\n         -0.418     , -0.43100002],\n        [-0.358     , -0.38700002, -0.16600001, ..., -0.026     ,\n         -0.158     , -0.12900001],\n...\n        [ 0.43300003,  0.48000002,  0.633     , ...,         nan,\n                 nan,         nan],\n        [ 0.439     ,  0.54200006,  0.51900005, ...,         nan,\n                 nan,         nan],\n        [ 0.642     ,  0.637     ,  0.609     , ...,         nan,\n                 nan,         nan]],\n\n       [[ 0.194     ,  0.432     ,  0.31      , ..., -0.081     ,\n          0.162     ,  0.14      ],\n        [ 0.147     ,  0.20400001,  0.164     , ...,  0.035     ,\n          0.32500002,  0.094     ],\n        [ 0.194     ,  0.216     ,  0.11300001, ...,  0.24700001,\n          0.33900002,  0.231     ],\n        ...,\n        [-0.085     ,  0.098     ,  0.23200001, ...,         nan,\n                 nan,         nan],\n        [-0.156     , -0.20700002, -0.014     , ...,         nan,\n                 nan,         nan],\n        [-0.037     , -0.07      , -0.012     , ...,         nan,\n                 nan,         nan]]], dtype=float32)Coordinates: (3)time(time)datetime64[ns]2003-01-01T09:00:00 ... 2003-02-...long_name :reference time of sst fieldstandard_name :timecoverage_content_type :coordinateaxis :Tcomment :Nominal time of analyzed fieldsarray(['2003-01-01T09:00:00.000000000', '2003-01-02T09:00:00.000000000',\n       '2003-01-03T09:00:00.000000000', '2003-01-05T09:00:00.000000000',\n       '2003-01-06T09:00:00.000000000', '2003-01-07T09:00:00.000000000',\n       '2003-01-08T09:00:00.000000000', '2003-01-09T09:00:00.000000000',\n       '2003-01-10T09:00:00.000000000', '2003-01-11T09:00:00.000000000',\n       '2003-01-12T09:00:00.000000000', '2003-01-13T09:00:00.000000000',\n       '2003-01-14T09:00:00.000000000', '2003-01-15T09:00:00.000000000',\n       '2003-01-16T09:00:00.000000000', '2003-01-17T09:00:00.000000000',\n       '2003-01-19T09:00:00.000000000', '2003-01-20T09:00:00.000000000',\n       '2003-01-21T09:00:00.000000000', '2003-01-22T09:00:00.000000000',\n       '2003-01-23T09:00:00.000000000', '2003-01-24T09:00:00.000000000',\n       '2003-01-25T09:00:00.000000000', '2003-01-26T09:00:00.000000000',\n       '2003-01-27T09:00:00.000000000', '2003-01-28T09:00:00.000000000',\n       '2003-01-29T09:00:00.000000000', '2003-01-30T09:00:00.000000000',\n       '2003-01-31T09:00:00.000000000', '2003-02-01T09:00:00.000000000',\n       '2003-02-02T09:00:00.000000000', '2003-02-03T09:00:00.000000000',\n       '2003-02-04T09:00:00.000000000', '2003-02-05T09:00:00.000000000',\n       '2003-02-06T09:00:00.000000000', '2003-02-07T09:00:00.000000000',\n       '2003-02-08T09:00:00.000000000', '2003-02-09T09:00:00.000000000',\n       '2003-02-10T09:00:00.000000000', '2003-02-11T09:00:00.000000000',\n       '2003-02-12T09:00:00.000000000', '2003-02-13T09:00:00.000000000',\n       '2003-02-14T09:00:00.000000000', '2003-02-15T09:00:00.000000000',\n       '2003-02-16T09:00:00.000000000', '2003-02-17T09:00:00.000000000',\n       '2003-02-19T09:00:00.000000000', '2003-02-20T09:00:00.000000000',\n       '2003-02-21T09:00:00.000000000', '2003-02-22T09:00:00.000000000',\n       '2003-02-23T09:00:00.000000000', '2003-02-24T09:00:00.000000000',\n       '2003-02-25T09:00:00.000000000', '2003-02-26T09:00:00.000000000',\n       '2003-02-27T09:00:00.000000000', '2003-02-28T09:00:00.000000000'],\n      dtype='datetime64[ns]')lat(lat)float325.125 5.375 5.625 ... 24.62 24.88long_name :latitudestandard_name :latitudecoverage_content_type :coordinateaxis :Yunits :degrees_northvalid_min :-90.0valid_max :90.0comment :geolocations inherited from the input data without correctionarray([ 5.125,  5.375,  5.625,  5.875,  6.125,  6.375,  6.625,  6.875,  7.125,\n        7.375,  7.625,  7.875,  8.125,  8.375,  8.625,  8.875,  9.125,  9.375,\n        9.625,  9.875, 10.125, 10.375, 10.625, 10.875, 11.125, 11.375, 11.625,\n       11.875, 12.125, 12.375, 12.625, 12.875, 13.125, 13.375, 13.625, 13.875,\n       14.125, 14.375, 14.625, 14.875, 15.125, 15.375, 15.625, 15.875, 16.125,\n       16.375, 16.625, 16.875, 17.125, 17.375, 17.625, 17.875, 18.125, 18.375,\n       18.625, 18.875, 19.125, 19.375, 19.625, 19.875, 20.125, 20.375, 20.625,\n       20.875, 21.125, 21.375, 21.625, 21.875, 22.125, 22.375, 22.625, 22.875,\n       23.125, 23.375, 23.625, 23.875, 24.125, 24.375, 24.625, 24.875],\n      dtype=float32)lon(lon)float3260.12 60.38 60.62 ... 79.62 79.88long_name :longitudestandard_name :longitudecoverage_content_type :coordinateaxis :Xunits :degrees_eastvalid_min :-180.0valid_max :180.0comment :geolocations inherited from the input data without correctionarray([60.125, 60.375, 60.625, 60.875, 61.125, 61.375, 61.625, 61.875, 62.125,\n       62.375, 62.625, 62.875, 63.125, 63.375, 63.625, 63.875, 64.125, 64.375,\n       64.625, 64.875, 65.125, 65.375, 65.625, 65.875, 66.125, 66.375, 66.625,\n       66.875, 67.125, 67.375, 67.625, 67.875, 68.125, 68.375, 68.625, 68.875,\n       69.125, 69.375, 69.625, 69.875, 70.125, 70.375, 70.625, 70.875, 71.125,\n       71.375, 71.625, 71.875, 72.125, 72.375, 72.625, 72.875, 73.125, 73.375,\n       73.625, 73.875, 74.125, 74.375, 74.625, 74.875, 75.125, 75.375, 75.625,\n       75.875, 76.125, 76.375, 76.625, 76.875, 77.125, 77.375, 77.625, 77.875,\n       78.125, 78.375, 78.625, 78.875, 79.125, 79.375, 79.625, 79.875],\n      dtype=float32)Indexes: (3)timePandasIndexPandasIndex(DatetimeIndex(['2003-01-01 09:00:00', '2003-01-02 09:00:00',\n               '2003-01-03 09:00:00', '2003-01-05 09:00:00',\n               '2003-01-06 09:00:00', '2003-01-07 09:00:00',\n               '2003-01-08 09:00:00', '2003-01-09 09:00:00',\n               '2003-01-10 09:00:00', '2003-01-11 09:00:00',\n               '2003-01-12 09:00:00', '2003-01-13 09:00:00',\n               '2003-01-14 09:00:00', '2003-01-15 09:00:00',\n               '2003-01-16 09:00:00', '2003-01-17 09:00:00',\n               '2003-01-19 09:00:00', '2003-01-20 09:00:00',\n               '2003-01-21 09:00:00', '2003-01-22 09:00:00',\n               '2003-01-23 09:00:00', '2003-01-24 09:00:00',\n               '2003-01-25 09:00:00', '2003-01-26 09:00:00',\n               '2003-01-27 09:00:00', '2003-01-28 09:00:00',\n               '2003-01-29 09:00:00', '2003-01-30 09:00:00',\n               '2003-01-31 09:00:00', '2003-02-01 09:00:00',\n               '2003-02-02 09:00:00', '2003-02-03 09:00:00',\n               '2003-02-04 09:00:00', '2003-02-05 09:00:00',\n               '2003-02-06 09:00:00', '2003-02-07 09:00:00',\n               '2003-02-08 09:00:00', '2003-02-09 09:00:00',\n               '2003-02-10 09:00:00', '2003-02-11 09:00:00',\n               '2003-02-12 09:00:00', '2003-02-13 09:00:00',\n               '2003-02-14 09:00:00', '2003-02-15 09:00:00',\n               '2003-02-16 09:00:00', '2003-02-17 09:00:00',\n               '2003-02-19 09:00:00', '2003-02-20 09:00:00',\n               '2003-02-21 09:00:00', '2003-02-22 09:00:00',\n               '2003-02-23 09:00:00', '2003-02-24 09:00:00',\n               '2003-02-25 09:00:00', '2003-02-26 09:00:00',\n               '2003-02-27 09:00:00', '2003-02-28 09:00:00'],\n              dtype='datetime64[ns]', name='time', freq=None))latPandasIndexPandasIndex(Index([ 5.125,  5.375,  5.625,  5.875,  6.125,  6.375,  6.625,  6.875,  7.125,\n        7.375,  7.625,  7.875,  8.125,  8.375,  8.625,  8.875,  9.125,  9.375,\n        9.625,  9.875, 10.125, 10.375, 10.625, 10.875, 11.125, 11.375, 11.625,\n       11.875, 12.125, 12.375, 12.625, 12.875, 13.125, 13.375, 13.625, 13.875,\n       14.125, 14.375, 14.625, 14.875, 15.125, 15.375, 15.625, 15.875, 16.125,\n       16.375, 16.625, 16.875, 17.125, 17.375, 17.625, 17.875, 18.125, 18.375,\n       18.625, 18.875, 19.125, 19.375, 19.625, 19.875, 20.125, 20.375, 20.625,\n       20.875, 21.125, 21.375, 21.625, 21.875, 22.125, 22.375, 22.625, 22.875,\n       23.125, 23.375, 23.625, 23.875, 24.125, 24.375, 24.625, 24.875],\n      dtype='float32', name='lat'))lonPandasIndexPandasIndex(Index([60.125, 60.375, 60.625, 60.875, 61.125, 61.375, 61.625, 61.875, 62.125,\n       62.375, 62.625, 62.875, 63.125, 63.375, 63.625, 63.875, 64.125, 64.375,\n       64.625, 64.875, 65.125, 65.375, 65.625, 65.875, 66.125, 66.375, 66.625,\n       66.875, 67.125, 67.375, 67.625, 67.875, 68.125, 68.375, 68.625, 68.875,\n       69.125, 69.375, 69.625, 69.875, 70.125, 70.375, 70.625, 70.875, 71.125,\n       71.375, 71.625, 71.875, 72.125, 72.375, 72.625, 72.875, 73.125, 73.375,\n       73.625, 73.875, 74.125, 74.375, 74.625, 74.875, 75.125, 75.375, 75.625,\n       75.875, 76.125, 76.375, 76.625, 76.875, 77.125, 77.375, 77.625, 77.875,\n       78.125, 78.375, 78.625, 78.875, 79.125, 79.375, 79.625, 79.875],\n      dtype='float32', name='lon'))Attributes: (6)long_name :SST anomaly from a seasonal SST climatology based on the MUR data over 2003-2014 periodcoverage_content_type :auxiliaryInformationunits :kelvinvalid_min :-32767valid_max :32767comment :anomaly reference to the day-of-year average between 2003 and 2014",
    "crumbs": [
      "Python Tutorials",
      "Data Access",
      "NASA Earth Data"
    ]
  },
  {
    "objectID": "python-notebooks/01_MOTU_Downloader.html#store-credentials-in-bash",
    "href": "python-notebooks/01_MOTU_Downloader.html#store-credentials-in-bash",
    "title": "Download sliced data using MOTU-Client in Copernicus",
    "section": "Store credentials in bash",
    "text": "Store credentials in bash\nFor security purposes, you can store your account credentials in your bash profile if you don’t want others to know yours. If you fork this notebook on your local machine and are sure that your credentials cannot be exposed anywhere else, you can skip the hassle of saving them to bash and set them as Python variables, then feed them to the prompt generator.\nMake sure to change your username and password of your Copernicus/MOTU account to the two lines below for bash…\n\n! echo 'export motu_password=YOURPASSWORD' &gt;~/.bash_profile\n\n\n! echo 'export motu_username=YOURUSERNAME' &gt;&gt;~/.bash_profile\n\n\n# Vim will get stuck in this cell if you don't stop the cell. As soon as you see the two lines\n# stop the execution and move on to the next cell.\n! vim ~/.bash_profile\n\n\n! source ~/.bash_profile\n\nIMPORTANT Please restart your server for the changes to take effect\n\n# confirm that it is shown before proceeding\n! echo $motu_username",
    "crumbs": [
      "Python Tutorials",
      "Data Access",
      "Download sliced data using MOTU-Client in Copernicus"
    ]
  },
  {
    "objectID": "python-notebooks/01_MOTU_Downloader.html#motu-download-prompt-breakdown",
    "href": "python-notebooks/01_MOTU_Downloader.html#motu-download-prompt-breakdown",
    "title": "Download sliced data using MOTU-Client in Copernicus",
    "section": "MOTU download prompt breakdown",
    "text": "MOTU download prompt breakdown\nLike ERDDAP URLs, MOTU works by sending requests to the API server. Unlike the former, it works through command line requests instead of URLs requests. Again, to prevent overloadding, we will break down the process to smaller time ranges to download.\nHere’s an example of a MOTU API request\n\npython -m motuclient –motu https://my.cmems-du.eu/motu-web/Motu –service-id GLOBAL_MULTIYEAR_BGC_001_029-TDS –product-id cmems_mod_glo_bgc_my_0.25_P1D-m –longitude-min 60 –longitude-max 80 –latitude-min 5 –latitude-max 25 –date-min “2020-12-31 00:00:00” –date-max “2020-12-31 23:59:59” –depth-min 0.5057600140571594 –depth-max 16.525320053100586 –variable chl –variable po4 –variable no3 –out-dir [OUTPUT_DIRECTORY] –out-name [OUTPUT_FILENAME] –user [USERNAME] –pwd [PASSWORD]\n\nAs you can see, we have to provide all the parameters in order for the request to work, with temporal and lat/lon ranges being required on all datasets and depth range required only for some. We also have to provide username and password (that we have after creating the Copernicus account) , as well as where we are going to store the data (output directory and name).",
    "crumbs": [
      "Python Tutorials",
      "Data Access",
      "Download sliced data using MOTU-Client in Copernicus"
    ]
  },
  {
    "objectID": "python-notebooks/01_MOTU_Downloader.html#import-necessary-libraries",
    "href": "python-notebooks/01_MOTU_Downloader.html#import-necessary-libraries",
    "title": "Download sliced data using MOTU-Client in Copernicus",
    "section": "Import necessary libraries",
    "text": "Import necessary libraries\n\nimport xarray as xr\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport os, glob, subprocess",
    "crumbs": [
      "Python Tutorials",
      "Data Access",
      "Download sliced data using MOTU-Client in Copernicus"
    ]
  },
  {
    "objectID": "python-notebooks/01_MOTU_Downloader.html#download-data",
    "href": "python-notebooks/01_MOTU_Downloader.html#download-data",
    "title": "Download sliced data using MOTU-Client in Copernicus",
    "section": "Download data",
    "text": "Download data\n\ndef prompt_generator(params, temp_folder_path):\n    months = pd.date_range(params['coords']['date-min'], params['coords']['date-max'], freq='M')\n    prompts = []\n    \n    for m in months:\n        prompt = f\"motuclient --motu https://my.cmems-du.eu/motu-web/Motu --service-id {params['SERVICE_ID']}-TDS --product-id {params['PRODUCT_ID']} \" + \\\n        f\"--user {os.environ['motu_username']} --pwd {os.environ['motu_password']} --out-dir {temp_folder_path} --out-name {m.strftime('%Y%m')}.nc \"\n        prompt += f\"--date-min '{m.strftime('%Y-%m')}-01 00:00:00' --date-max '{m.strftime('%Y-%m-%d')} 23:59:00' \"\n        for var in params['variables']:\n            prompt += f'--variable {var} '\n        for coord_name, coord_value in params['coords'].items():\n            # since we already work \n            if coord_name not in ['date-min', 'date-max']:\n                prompt += f'--{coord_name} {coord_value} '\n        \n        prompts.append(prompt)\n        \n    return prompts\n\n\ndef download_motu_api(params):\n    # create new folder to store data\n    MAIN_FOLDER = 'demonstrated data/motu'\n    TEMP_FOLDER = params['PRODUCT_ID']\n    path_temp_folder = os.path.join(MAIN_FOLDER, TEMP_FOLDER)\n    \n    if not os.path.exists(path_temp_folder):\n        os.makedirs(path_temp_folder)\n\n    # generate prompts to feed into the downloader\n    prompts = prompt_generator(params, path_temp_folder)\n    for prompt in prompts:\n        process = subprocess.Popen(prompt.split())\n        process.wait()\n        \n\nNext, we’re going to initialize our parameters to download the data. The template is shown below, and werecommend you to choose a spatial range slightly larger than your actual region of interest due to cutoffs that may happen at the edges.\n\n# modify your parameters as you wish to feed into prompt generator\n# here's mine to download chlorophyll-a data\nvar_chlorophyll = {\n    'SERVICE_ID': 'OCEANCOLOUR_GLO_BGC_L4_MY_009_104',\n    'PRODUCT_ID': 'cmems_obs-oc_glo_bgc-plankton_my_l4-gapfree-multi-4km_P1D',\n    'variables': ['CHL', 'CHL_uncertainty'],\n    'coords': {\n        'longitude-min': 60, \n        'longitude-max': 80,\n        'latitude-min': 5,\n        'latitude-max': 25,\n        'date-min': '2000-01',\n        'date-max': '2002-01' # exclusive of last month, so in this case, we get data until DEC '02\n        # optional coordinate params for some datasets: 'depth-min', 'depth-max'\n        # consult data access page for more information\n    }\n}\n\nvar_sla = {\n    'SERVICE_ID': 'SEALEVEL_GLO_PHY_L4_MY_008_047',\n    'PRODUCT_ID': 'cmems_obs-sl_glo_phy-ssh_my_allsat-l4-duacs-0.25deg_P1D',\n    'variables': ['adt', 'sla'],\n    'coords': {\n        'longitude-min': 60,\n        'longitude-max': 80,\n        'latitude-min': 5,\n        'latitude-max': 25,\n        'date-min': '2000-01',\n        'date-max': '2002-01' # exclusive of last month, so in this case, we get data until DEC '02\n    }\n}\n\n\ndownload_motu_api(var_sla)\n\n2023-07-11 04:31:55.819 [ INFO] Asynchronous mode set\n2023-07-11 04:31:55.819 [ INFO] Authenticating user mphan for service https://my.cmems-du.eu/motu-web/Motu\n2023-07-11 04:32:04.748 [ INFO] Requesting file to download (this can take a while)...\n2023-07-11 04:32:14.896 [ INFO] Authenticating user mphan for service https://my.cmems-du.eu/motu-web/Motu\n2023-07-11 04:32:32.754 [ INFO] The product is ready for download\n2023-07-11 04:32:32.754 [ INFO] Downloading file (this can take a while)...\n2023-07-11 04:32:33.680 [ INFO] File type: application/x-netcdf\n2023-07-11 04:32:33.680 [ INFO] File size: 3.3 MB (3260684 B)\n2023-07-11 04:32:33.680 [ INFO] Downloading file /home/jovyan/shared/minh-notebooks/motu/cmems_obs-sl_glo_phy-ssh_my_allsat-l4-duacs-0.25deg_P1D/200001.nc\n2023-07-11 04:32:34.029 [ INFO] Progress -   3.3 MB (2.0%)\n2023-07-11 04:32:34.204 [ INFO] Progress -   3.3 MB (4.0%)\n2023-07-11 04:32:34.379 [ INFO] Progress -   3.3 MB (6.0%)\n2023-07-11 04:32:34.380 [ INFO] Progress -   3.3 MB (8.0%)\n2023-07-11 04:32:34.380 [ INFO] Progress -   3.3 MB (10.0%)\n2023-07-11 04:32:34.555 [ INFO] Progress -   3.3 MB (12.1%)\n2023-07-11 04:32:34.555 [ INFO] Progress -   3.3 MB (14.1%)\n2023-07-11 04:32:34.556 [ INFO] Progress -   3.3 MB (16.1%)\n2023-07-11 04:32:34.556 [ INFO] Progress -   3.3 MB (18.1%)\n2023-07-11 04:32:34.556 [ INFO] Progress -   3.3 MB (20.1%)\n2023-07-11 04:32:34.556 [ INFO] Progress -   3.3 MB (22.1%)\n2023-07-11 04:32:34.557 [ INFO] Progress -   3.3 MB (24.1%)\n2023-07-11 04:32:34.730 [ INFO] Progress -   3.3 MB (26.1%)\n2023-07-11 04:32:34.731 [ INFO] Progress -   3.3 MB (28.1%)\n2023-07-11 04:32:34.731 [ INFO] Progress -   3.3 MB (30.1%)\n2023-07-11 04:32:34.732 [ INFO] Progress -   3.3 MB (32.2%)\n2023-07-11 04:32:34.732 [ INFO] Progress -   3.3 MB (34.2%)\n2023-07-11 04:32:34.733 [ INFO] Progress -   3.3 MB (36.2%)\n2023-07-11 04:32:34.733 [ INFO] Progress -   3.3 MB (38.2%)\n2023-07-11 04:32:34.733 [ INFO] Progress -   3.3 MB (40.2%)\n2023-07-11 04:32:34.905 [ INFO] Progress -   3.3 MB (42.2%)\n2023-07-11 04:32:34.906 [ INFO] Progress -   3.3 MB (44.2%)\n2023-07-11 04:32:34.906 [ INFO] Progress -   3.3 MB (46.2%)\n2023-07-11 04:32:34.907 [ INFO] Progress -   3.3 MB (48.2%)\n2023-07-11 04:32:34.907 [ INFO] Progress -   3.3 MB (50.2%)\n2023-07-11 04:32:34.907 [ INFO] Progress -   3.3 MB (52.3%)\n2023-07-11 04:32:34.908 [ INFO] Progress -   3.3 MB (54.3%)\n2023-07-11 04:32:34.908 [ INFO] Progress -   3.3 MB (56.3%)\n2023-07-11 04:32:34.908 [ INFO] Progress -   3.3 MB (58.3%)\n2023-07-11 04:32:34.908 [ INFO] Progress -   3.3 MB (60.3%)\n2023-07-11 04:32:34.909 [ INFO] Progress -   3.3 MB (62.3%)\n2023-07-11 04:32:34.909 [ INFO] Progress -   3.3 MB (64.3%)\n2023-07-11 04:32:34.910 [ INFO] Progress -   3.3 MB (66.3%)\n2023-07-11 04:32:34.910 [ INFO] Progress -   3.3 MB (68.3%)\n2023-07-11 04:32:34.910 [ INFO] Progress -   3.3 MB (70.3%)\n2023-07-11 04:32:34.910 [ INFO] Progress -   3.3 MB (72.4%)\n2023-07-11 04:32:34.911 [ INFO] Progress -   3.3 MB (74.4%)\n2023-07-11 04:32:34.911 [ INFO] Progress -   3.3 MB (76.4%)\n2023-07-11 04:32:34.911 [ INFO] Progress -   3.3 MB (78.4%)\n2023-07-11 04:32:34.911 [ INFO] Progress -   3.3 MB (80.4%)\n2023-07-11 04:32:34.912 [ INFO] Progress -   3.3 MB (82.4%)\n2023-07-11 04:32:35.080 [ INFO] Progress -   3.3 MB (84.4%)\n2023-07-11 04:32:35.081 [ INFO] Progress -   3.3 MB (86.4%)\n2023-07-11 04:32:35.082 [ INFO] Progress -   3.3 MB (88.4%)\n2023-07-11 04:32:35.082 [ INFO] Progress -   3.3 MB (90.4%)\n2023-07-11 04:32:35.082 [ INFO] Progress -   3.3 MB (92.5%)\n2023-07-11 04:32:35.082 [ INFO] Progress -   3.3 MB (94.5%)\n2023-07-11 04:32:35.083 [ INFO] Progress -   3.3 MB (96.5%)\n2023-07-11 04:32:35.083 [ INFO] Progress -   3.3 MB (98.5%)\n2023-07-11 04:32:35.084 [ INFO] Progress -   3.3 MB (100.0%)\n2023-07-11 04:32:35.084 [ INFO] Processing  time : 0:00:37.863885\n2023-07-11 04:32:35.084 [ INFO] Downloading time : 0:00:01.403882\n2023-07-11 04:32:35.084 [ INFO] Total time       : 0:00:39.267767\n2023-07-11 04:32:35.084 [ INFO] Download rate    : 1.4 MB/s\n2023-07-11 04:32:35.084 [ INFO] Save into        : motu/cmems_obs-sl_glo_phy-ssh_my_allsat-l4-duacs-0.25deg_P1D/200001.nc\n2023-07-11 04:32:35.115 [ INFO] Done\n2023-07-11 04:32:35.360 [ INFO] Asynchronous mode set\n2023-07-11 04:32:35.360 [ INFO] Authenticating user mphan for service https://my.cmems-du.eu/motu-web/Motu\n2023-07-11 04:32:44.852 [ INFO] Requesting file to download (this can take a while)...\n2023-07-11 04:32:53.844 [ INFO] Authenticating user mphan for service https://my.cmems-du.eu/motu-web/Motu\n2023-07-11 04:33:09.545 [ INFO] The product is ready for download\n2023-07-11 04:33:09.546 [ INFO] Downloading file (this can take a while)...\n2023-07-11 04:33:10.291 [ INFO] File type: application/x-netcdf\n2023-07-11 04:33:10.291 [ INFO] File size: 3.1 MB (3050724 B)\n2023-07-11 04:33:10.291 [ INFO] Downloading file /home/jovyan/shared/minh-notebooks/motu/cmems_obs-sl_glo_phy-ssh_my_allsat-l4-duacs-0.25deg_P1D/200002.nc\n2023-07-11 04:33:10.642 [ INFO] Progress -   3.1 MB (2.1%)\n2023-07-11 04:33:10.817 [ INFO] Progress -   3.1 MB (4.3%)\n2023-07-11 04:33:10.990 [ INFO] Progress -   3.1 MB (6.4%)\n2023-07-11 04:33:10.992 [ INFO] Progress -   3.1 MB (8.6%)\n2023-07-11 04:33:10.992 [ INFO] Progress -   3.1 MB (10.7%)\n2023-07-11 04:33:10.993 [ INFO] Progress -   3.1 MB (12.9%)\n2023-07-11 04:33:11.166 [ INFO] Progress -   3.1 MB (15.0%)\n2023-07-11 04:33:11.167 [ INFO] Progress -   3.1 MB (17.2%)\n2023-07-11 04:33:11.168 [ INFO] Progress -   3.1 MB (19.3%)\n2023-07-11 04:33:11.168 [ INFO] Progress -   3.1 MB (21.5%)\n2023-07-11 04:33:11.168 [ INFO] Progress -   3.1 MB (23.6%)\n2023-07-11 04:33:11.168 [ INFO] Progress -   3.1 MB (25.8%)\n2023-07-11 04:33:11.341 [ INFO] Progress -   3.1 MB (27.9%)\n2023-07-11 04:33:11.341 [ INFO] Progress -   3.1 MB (30.1%)\n2023-07-11 04:33:11.342 [ INFO] Progress -   3.1 MB (32.2%)\n2023-07-11 04:33:11.343 [ INFO] Progress -   3.1 MB (34.4%)\n2023-07-11 04:33:11.343 [ INFO] Progress -   3.1 MB (36.5%)\n2023-07-11 04:33:11.344 [ INFO] Progress -   3.1 MB (38.7%)\n2023-07-11 04:33:11.344 [ INFO] Progress -   3.1 MB (40.8%)\n2023-07-11 04:33:11.344 [ INFO] Progress -   3.1 MB (43.0%)\n2023-07-11 04:33:11.344 [ INFO] Progress -   3.1 MB (45.1%)\n2023-07-11 04:33:11.345 [ INFO] Progress -   3.1 MB (47.3%)\n2023-07-11 04:33:11.345 [ INFO] Progress -   3.1 MB (49.4%)\n2023-07-11 04:33:11.345 [ INFO] Progress -   3.1 MB (51.6%)\n2023-07-11 04:33:11.345 [ INFO] Progress -   3.1 MB (53.7%)\n2023-07-11 04:33:11.516 [ INFO] Progress -   3.1 MB (55.9%)\n2023-07-11 04:33:11.516 [ INFO] Progress -   3.1 MB (58.0%)\n2023-07-11 04:33:11.516 [ INFO] Progress -   3.1 MB (60.1%)\n2023-07-11 04:33:11.517 [ INFO] Progress -   3.1 MB (62.3%)\n2023-07-11 04:33:11.517 [ INFO] Progress -   3.1 MB (64.4%)\n2023-07-11 04:33:11.518 [ INFO] Progress -   3.1 MB (66.6%)\n2023-07-11 04:33:11.518 [ INFO] Progress -   3.1 MB (68.7%)\n2023-07-11 04:33:11.519 [ INFO] Progress -   3.1 MB (70.9%)\n2023-07-11 04:33:11.519 [ INFO] Progress -   3.1 MB (73.0%)\n2023-07-11 04:33:11.519 [ INFO] Progress -   3.1 MB (75.2%)\n2023-07-11 04:33:11.520 [ INFO] Progress -   3.1 MB (77.3%)\n2023-07-11 04:33:11.520 [ INFO] Progress -   3.1 MB (79.5%)\n2023-07-11 04:33:11.520 [ INFO] Progress -   3.1 MB (81.6%)\n2023-07-11 04:33:11.520 [ INFO] Progress -   3.1 MB (83.8%)\n2023-07-11 04:33:11.520 [ INFO] Progress -   3.1 MB (85.9%)\n2023-07-11 04:33:11.521 [ INFO] Progress -   3.1 MB (88.1%)\n2023-07-11 04:33:11.521 [ INFO] Progress -   3.1 MB (90.2%)\n2023-07-11 04:33:11.521 [ INFO] Progress -   3.1 MB (92.4%)\n2023-07-11 04:33:11.521 [ INFO] Progress -   3.1 MB (94.5%)\n2023-07-11 04:33:11.522 [ INFO] Progress -   3.1 MB (96.7%)\n2023-07-11 04:33:11.522 [ INFO] Progress -   3.1 MB (98.8%)\n2023-07-11 04:33:11.522 [ INFO] Progress -   3.1 MB (100.0%)\n2023-07-11 04:33:11.522 [ INFO] Processing  time : 0:00:34.933586\n2023-07-11 04:33:11.522 [ INFO] Downloading time : 0:00:01.231103\n2023-07-11 04:33:11.522 [ INFO] Total time       : 0:00:36.164689\n2023-07-11 04:33:11.522 [ INFO] Download rate    : 1.5 MB/s\n2023-07-11 04:33:11.522 [ INFO] Save into        : motu/cmems_obs-sl_glo_phy-ssh_my_allsat-l4-duacs-0.25deg_P1D/200002.nc\n2023-07-11 04:33:11.556 [ INFO] Done\n2023-07-11 04:33:11.798 [ INFO] Asynchronous mode set\n2023-07-11 04:33:11.798 [ INFO] Authenticating user mphan for service https://my.cmems-du.eu/motu-web/Motu\n2023-07-11 04:33:19.952 [ INFO] Requesting file to download (this can take a while)...\n2023-07-11 04:33:28.668 [ INFO] Authenticating user mphan for service https://my.cmems-du.eu/motu-web/Motu\n2023-07-11 04:33:44.971 [ INFO] The product is ready for download\n2023-07-11 04:33:44.971 [ INFO] Downloading file (this can take a while)...\n2023-07-11 04:33:45.715 [ INFO] File type: application/x-netcdf\n2023-07-11 04:33:45.715 [ INFO] File size: 3.3 MB (3260684 B)\n2023-07-11 04:33:45.715 [ INFO] Downloading file /home/jovyan/shared/minh-notebooks/motu/cmems_obs-sl_glo_phy-ssh_my_allsat-l4-duacs-0.25deg_P1D/200003.nc\n2023-07-11 04:33:46.064 [ INFO] Progress -   3.3 MB (2.0%)\n2023-07-11 04:33:46.239 [ INFO] Progress -   3.3 MB (4.0%)\n2023-07-11 04:33:46.415 [ INFO] Progress -   3.3 MB (6.0%)\n2023-07-11 04:33:46.415 [ INFO] Progress -   3.3 MB (8.0%)\n2023-07-11 04:33:46.415 [ INFO] Progress -   3.3 MB (10.0%)\n2023-07-11 04:33:46.590 [ INFO] Progress -   3.3 MB (12.1%)\n2023-07-11 04:33:46.590 [ INFO] Progress -   3.3 MB (14.1%)\n2023-07-11 04:33:46.591 [ INFO] Progress -   3.3 MB (16.1%)\n2023-07-11 04:33:46.591 [ INFO] Progress -   3.3 MB (18.1%)\n2023-07-11 04:33:46.591 [ INFO] Progress -   3.3 MB (20.1%)\n2023-07-11 04:33:46.591 [ INFO] Progress -   3.3 MB (22.1%)\n2023-07-11 04:33:46.765 [ INFO] Progress -   3.3 MB (24.1%)\n2023-07-11 04:33:46.765 [ INFO] Progress -   3.3 MB (26.1%)\n2023-07-11 04:33:46.765 [ INFO] Progress -   3.3 MB (28.1%)\n2023-07-11 04:33:46.766 [ INFO] Progress -   3.3 MB (30.1%)\n2023-07-11 04:33:46.766 [ INFO] Progress -   3.3 MB (32.2%)\n2023-07-11 04:33:46.767 [ INFO] Progress -   3.3 MB (34.2%)\n2023-07-11 04:33:46.767 [ INFO] Progress -   3.3 MB (36.2%)\n2023-07-11 04:33:46.768 [ INFO] Progress -   3.3 MB (38.2%)\n2023-07-11 04:33:46.768 [ INFO] Progress -   3.3 MB (40.2%)\n2023-07-11 04:33:46.768 [ INFO] Progress -   3.3 MB (42.2%)\n2023-07-11 04:33:46.768 [ INFO] Progress -   3.3 MB (44.2%)\n2023-07-11 04:33:46.769 [ INFO] Progress -   3.3 MB (46.2%)\n2023-07-11 04:33:46.769 [ INFO] Progress -   3.3 MB (48.2%)\n2023-07-11 04:33:46.940 [ INFO] Progress -   3.3 MB (50.2%)\n2023-07-11 04:33:46.940 [ INFO] Progress -   3.3 MB (52.3%)\n2023-07-11 04:33:46.940 [ INFO] Progress -   3.3 MB (54.3%)\n2023-07-11 04:33:46.941 [ INFO] Progress -   3.3 MB (56.3%)\n2023-07-11 04:33:46.941 [ INFO] Progress -   3.3 MB (58.3%)\n2023-07-11 04:33:46.941 [ INFO] Progress -   3.3 MB (60.3%)\n2023-07-11 04:33:46.942 [ INFO] Progress -   3.3 MB (62.3%)\n2023-07-11 04:33:46.942 [ INFO] Progress -   3.3 MB (64.3%)\n2023-07-11 04:33:46.943 [ INFO] Progress -   3.3 MB (66.3%)\n2023-07-11 04:33:46.943 [ INFO] Progress -   3.3 MB (68.3%)\n2023-07-11 04:33:46.943 [ INFO] Progress -   3.3 MB (70.3%)\n2023-07-11 04:33:46.944 [ INFO] Progress -   3.3 MB (72.4%)\n2023-07-11 04:33:46.944 [ INFO] Progress -   3.3 MB (74.4%)\n2023-07-11 04:33:46.944 [ INFO] Progress -   3.3 MB (76.4%)\n2023-07-11 04:33:46.945 [ INFO] Progress -   3.3 MB (78.4%)\n2023-07-11 04:33:46.945 [ INFO] Progress -   3.3 MB (80.4%)\n2023-07-11 04:33:46.945 [ INFO] Progress -   3.3 MB (82.4%)\n2023-07-11 04:33:46.945 [ INFO] Progress -   3.3 MB (84.4%)\n2023-07-11 04:33:46.946 [ INFO] Progress -   3.3 MB (86.4%)\n2023-07-11 04:33:46.946 [ INFO] Progress -   3.3 MB (88.4%)\n2023-07-11 04:33:46.946 [ INFO] Progress -   3.3 MB (90.4%)\n2023-07-11 04:33:46.946 [ INFO] Progress -   3.3 MB (92.5%)\n2023-07-11 04:33:46.947 [ INFO] Progress -   3.3 MB (94.5%)\n2023-07-11 04:33:46.947 [ INFO] Progress -   3.3 MB (96.5%)\n2023-07-11 04:33:46.988 [ INFO] Progress -   3.3 MB (98.5%)\n2023-07-11 04:33:47.115 [ INFO] Progress -   3.3 MB (100.0%)\n2023-07-11 04:33:47.115 [ INFO] Processing  time : 0:00:33.919354\n2023-07-11 04:33:47.115 [ INFO] Downloading time : 0:00:01.400042\n2023-07-11 04:33:47.115 [ INFO] Total time       : 0:00:35.319396\n2023-07-11 04:33:47.115 [ INFO] Download rate    : 1.5 MB/s\n2023-07-11 04:33:47.115 [ INFO] Save into        : motu/cmems_obs-sl_glo_phy-ssh_my_allsat-l4-duacs-0.25deg_P1D/200003.nc\n2023-07-11 04:33:47.158 [ INFO] Done\n2023-07-11 04:33:47.403 [ INFO] Asynchronous mode set\n2023-07-11 04:33:47.403 [ INFO] Authenticating user mphan for service https://my.cmems-du.eu/motu-web/Motu\n2023-07-11 04:33:55.632 [ INFO] Requesting file to download (this can take a while)...\n2023-07-11 04:34:05.423 [ INFO] Authenticating user mphan for service https://my.cmems-du.eu/motu-web/Motu\n2023-07-11 04:34:21.086 [ INFO] The product is ready for download\n2023-07-11 04:34:21.086 [ INFO] Downloading file (this can take a while)...\n2023-07-11 04:34:21.835 [ INFO] File type: application/x-netcdf\n2023-07-11 04:34:21.835 [ INFO] File size: 3.2 MB (3155704 B)\n2023-07-11 04:34:21.835 [ INFO] Downloading file /home/jovyan/shared/minh-notebooks/motu/cmems_obs-sl_glo_phy-ssh_my_allsat-l4-duacs-0.25deg_P1D/200004.nc\n2023-07-11 04:34:22.184 [ INFO] Progress -   3.2 MB (2.1%)\n2023-07-11 04:34:22.359 [ INFO] Progress -   3.2 MB (4.2%)\n2023-07-11 04:34:22.535 [ INFO] Progress -   3.2 MB (6.2%)\n2023-07-11 04:34:22.535 [ INFO] Progress -   3.2 MB (8.3%)\n2023-07-11 04:34:22.535 [ INFO] Progress -   3.2 MB (10.4%)\n2023-07-11 04:34:22.710 [ INFO] Progress -   3.2 MB (12.5%)\n2023-07-11 04:34:22.711 [ INFO] Progress -   3.2 MB (14.5%)\n2023-07-11 04:34:22.711 [ INFO] Progress -   3.2 MB (16.6%)\n2023-07-11 04:34:22.711 [ INFO] Progress -   3.2 MB (18.7%)\n2023-07-11 04:34:22.712 [ INFO] Progress -   3.2 MB (20.8%)\n2023-07-11 04:34:22.712 [ INFO] Progress -   3.2 MB (22.8%)\n2023-07-11 04:34:22.712 [ INFO] Progress -   3.2 MB (24.9%)\n2023-07-11 04:34:22.885 [ INFO] Progress -   3.2 MB (27.0%)\n2023-07-11 04:34:22.886 [ INFO] Progress -   3.2 MB (29.1%)\n2023-07-11 04:34:22.886 [ INFO] Progress -   3.2 MB (31.2%)\n2023-07-11 04:34:22.886 [ INFO] Progress -   3.2 MB (33.2%)\n2023-07-11 04:34:22.887 [ INFO] Progress -   3.2 MB (35.3%)\n2023-07-11 04:34:22.888 [ INFO] Progress -   3.2 MB (37.4%)\n2023-07-11 04:34:22.888 [ INFO] Progress -   3.2 MB (39.5%)\n2023-07-11 04:34:22.888 [ INFO] Progress -   3.2 MB (41.5%)\n2023-07-11 04:34:22.888 [ INFO] Progress -   3.2 MB (43.6%)\n2023-07-11 04:34:22.889 [ INFO] Progress -   3.2 MB (45.7%)\n2023-07-11 04:34:22.889 [ INFO] Progress -   3.2 MB (47.8%)\n2023-07-11 04:34:22.889 [ INFO] Progress -   3.2 MB (49.8%)\n2023-07-11 04:34:23.060 [ INFO] Progress -   3.2 MB (51.9%)\n2023-07-11 04:34:23.061 [ INFO] Progress -   3.2 MB (54.0%)\n2023-07-11 04:34:23.061 [ INFO] Progress -   3.2 MB (56.1%)\n2023-07-11 04:34:23.062 [ INFO] Progress -   3.2 MB (58.1%)\n2023-07-11 04:34:23.062 [ INFO] Progress -   3.2 MB (60.2%)\n2023-07-11 04:34:23.063 [ INFO] Progress -   3.2 MB (62.3%)\n2023-07-11 04:34:23.063 [ INFO] Progress -   3.2 MB (64.4%)\n2023-07-11 04:34:23.063 [ INFO] Progress -   3.2 MB (66.5%)\n2023-07-11 04:34:23.064 [ INFO] Progress -   3.2 MB (68.5%)\n2023-07-11 04:34:23.064 [ INFO] Progress -   3.2 MB (70.6%)\n2023-07-11 04:34:23.064 [ INFO] Progress -   3.2 MB (72.7%)\n2023-07-11 04:34:23.065 [ INFO] Progress -   3.2 MB (74.8%)\n2023-07-11 04:34:23.065 [ INFO] Progress -   3.2 MB (76.8%)\n2023-07-11 04:34:23.065 [ INFO] Progress -   3.2 MB (78.9%)\n2023-07-11 04:34:23.065 [ INFO] Progress -   3.2 MB (81.0%)\n2023-07-11 04:34:23.066 [ INFO] Progress -   3.2 MB (83.1%)\n2023-07-11 04:34:23.066 [ INFO] Progress -   3.2 MB (85.1%)\n2023-07-11 04:34:23.066 [ INFO] Progress -   3.2 MB (87.2%)\n2023-07-11 04:34:23.066 [ INFO] Progress -   3.2 MB (89.3%)\n2023-07-11 04:34:23.067 [ INFO] Progress -   3.2 MB (91.4%)\n2023-07-11 04:34:23.067 [ INFO] Progress -   3.2 MB (93.5%)\n2023-07-11 04:34:23.067 [ INFO] Progress -   3.2 MB (95.5%)\n2023-07-11 04:34:23.067 [ INFO] Progress -   3.2 MB (97.6%)\n2023-07-11 04:34:23.235 [ INFO] Progress -   3.2 MB (99.7%)\n2023-07-11 04:34:23.236 [ INFO] Progress -   3.2 MB (100.0%)\n2023-07-11 04:34:23.236 [ INFO] Processing  time : 0:00:34.434477\n2023-07-11 04:34:23.236 [ INFO] Downloading time : 0:00:01.401031\n2023-07-11 04:34:23.236 [ INFO] Total time       : 0:00:35.835508\n2023-07-11 04:34:23.236 [ INFO] Download rate    : 1.5 MB/s\n2023-07-11 04:34:23.236 [ INFO] Save into        : motu/cmems_obs-sl_glo_phy-ssh_my_allsat-l4-duacs-0.25deg_P1D/200004.nc\n2023-07-11 04:34:23.266 [ INFO] Done\n2023-07-11 04:34:23.512 [ INFO] Asynchronous mode set\n2023-07-11 04:34:23.513 [ INFO] Authenticating user mphan for service https://my.cmems-du.eu/motu-web/Motu\n2023-07-11 04:34:30.732 [ INFO] Requesting file to download (this can take a while)...\n2023-07-11 04:34:39.973 [ INFO] Authenticating user mphan for service https://my.cmems-du.eu/motu-web/Motu\n2023-07-11 04:34:55.062 [ INFO] The product is ready for download\n2023-07-11 04:34:55.062 [ INFO] Downloading file (this can take a while)...\n2023-07-11 04:34:55.808 [ INFO] File type: application/x-netcdf\n2023-07-11 04:34:55.808 [ INFO] File size: 3.3 MB (3260684 B)\n2023-07-11 04:34:55.808 [ INFO] Downloading file /home/jovyan/shared/minh-notebooks/motu/cmems_obs-sl_glo_phy-ssh_my_allsat-l4-duacs-0.25deg_P1D/200005.nc\n2023-07-11 04:34:56.158 [ INFO] Progress -   3.3 MB (2.0%)\n2023-07-11 04:34:56.333 [ INFO] Progress -   3.3 MB (4.0%)\n2023-07-11 04:34:56.508 [ INFO] Progress -   3.3 MB (6.0%)\n2023-07-11 04:34:56.509 [ INFO] Progress -   3.3 MB (8.0%)\n2023-07-11 04:34:56.683 [ INFO] Progress -   3.3 MB (10.0%)\n2023-07-11 04:34:56.684 [ INFO] Progress -   3.3 MB (12.1%)\n2023-07-11 04:34:56.684 [ INFO] Progress -   3.3 MB (14.1%)\n2023-07-11 04:34:56.684 [ INFO] Progress -   3.3 MB (16.1%)\n2023-07-11 04:34:56.685 [ INFO] Progress -   3.3 MB (18.1%)\n2023-07-11 04:34:56.859 [ INFO] Progress -   3.3 MB (20.1%)\n2023-07-11 04:34:56.859 [ INFO] Progress -   3.3 MB (22.1%)\n2023-07-11 04:34:56.860 [ INFO] Progress -   3.3 MB (24.1%)\n2023-07-11 04:34:56.860 [ INFO] Progress -   3.3 MB (26.1%)\n2023-07-11 04:34:56.860 [ INFO] Progress -   3.3 MB (28.1%)\n2023-07-11 04:34:56.860 [ INFO] Progress -   3.3 MB (30.1%)\n2023-07-11 04:34:56.861 [ INFO] Progress -   3.3 MB (32.2%)\n2023-07-11 04:34:56.862 [ INFO] Progress -   3.3 MB (34.2%)\n2023-07-11 04:34:56.862 [ INFO] Progress -   3.3 MB (36.2%)\n2023-07-11 04:34:56.862 [ INFO] Progress -   3.3 MB (38.2%)\n2023-07-11 04:34:57.034 [ INFO] Progress -   3.3 MB (40.2%)\n2023-07-11 04:34:57.034 [ INFO] Progress -   3.3 MB (42.2%)\n2023-07-11 04:34:57.035 [ INFO] Progress -   3.3 MB (44.2%)\n2023-07-11 04:34:57.035 [ INFO] Progress -   3.3 MB (46.2%)\n2023-07-11 04:34:57.035 [ INFO] Progress -   3.3 MB (48.2%)\n2023-07-11 04:34:57.035 [ INFO] Progress -   3.3 MB (50.2%)\n2023-07-11 04:34:57.036 [ INFO] Progress -   3.3 MB (52.3%)\n2023-07-11 04:34:57.036 [ INFO] Progress -   3.3 MB (54.3%)\n2023-07-11 04:34:57.036 [ INFO] Progress -   3.3 MB (56.3%)\n2023-07-11 04:34:57.036 [ INFO] Progress -   3.3 MB (58.3%)\n2023-07-11 04:34:57.037 [ INFO] Progress -   3.3 MB (60.3%)\n2023-07-11 04:34:57.037 [ INFO] Progress -   3.3 MB (62.3%)\n2023-07-11 04:34:57.037 [ INFO] Progress -   3.3 MB (64.3%)\n2023-07-11 04:34:57.038 [ INFO] Progress -   3.3 MB (66.3%)\n2023-07-11 04:34:57.038 [ INFO] Progress -   3.3 MB (68.3%)\n2023-07-11 04:34:57.039 [ INFO] Progress -   3.3 MB (70.3%)\n2023-07-11 04:34:57.039 [ INFO] Progress -   3.3 MB (72.4%)\n2023-07-11 04:34:57.039 [ INFO] Progress -   3.3 MB (74.4%)\n2023-07-11 04:34:57.040 [ INFO] Progress -   3.3 MB (76.4%)\n2023-07-11 04:34:57.040 [ INFO] Progress -   3.3 MB (78.4%)\n2023-07-11 04:34:57.209 [ INFO] Progress -   3.3 MB (80.4%)\n2023-07-11 04:34:57.209 [ INFO] Progress -   3.3 MB (82.4%)\n2023-07-11 04:34:57.210 [ INFO] Progress -   3.3 MB (84.4%)\n2023-07-11 04:34:57.210 [ INFO] Progress -   3.3 MB (86.4%)\n2023-07-11 04:34:57.210 [ INFO] Progress -   3.3 MB (88.4%)\n2023-07-11 04:34:57.210 [ INFO] Progress -   3.3 MB (90.4%)\n2023-07-11 04:34:57.211 [ INFO] Progress -   3.3 MB (92.5%)\n2023-07-11 04:34:57.211 [ INFO] Progress -   3.3 MB (94.5%)\n2023-07-11 04:34:57.211 [ INFO] Progress -   3.3 MB (96.5%)\n2023-07-11 04:34:57.212 [ INFO] Progress -   3.3 MB (98.5%)\n2023-07-11 04:34:57.212 [ INFO] Progress -   3.3 MB (100.0%)\n2023-07-11 04:34:57.213 [ INFO] Processing  time : 0:00:32.298162\n2023-07-11 04:34:57.213 [ INFO] Downloading time : 0:00:01.404053\n2023-07-11 04:34:57.213 [ INFO] Total time       : 0:00:33.702215\n2023-07-11 04:34:57.213 [ INFO] Download rate    : 1.5 MB/s\n2023-07-11 04:34:57.213 [ INFO] Save into        : motu/cmems_obs-sl_glo_phy-ssh_my_allsat-l4-duacs-0.25deg_P1D/200005.nc\n2023-07-11 04:34:57.242 [ INFO] Done\n2023-07-11 04:34:57.482 [ INFO] Asynchronous mode set\n2023-07-11 04:34:57.482 [ INFO] Authenticating user mphan for service https://my.cmems-du.eu/motu-web/Motu\n2023-07-11 04:35:05.474 [ INFO] Requesting file to download (this can take a while)...\n2023-07-11 04:35:13.265 [ INFO] Authenticating user mphan for service https://my.cmems-du.eu/motu-web/Motu\n2023-07-11 04:35:29.833 [ INFO] The product is ready for download\n2023-07-11 04:35:29.833 [ INFO] Downloading file (this can take a while)...\n2023-07-11 04:35:30.583 [ INFO] File type: application/x-netcdf\n2023-07-11 04:35:30.583 [ INFO] File size: 3.2 MB (3155704 B)\n2023-07-11 04:35:30.583 [ INFO] Downloading file /home/jovyan/shared/minh-notebooks/motu/cmems_obs-sl_glo_phy-ssh_my_allsat-l4-duacs-0.25deg_P1D/200006.nc\n2023-07-11 04:35:30.932 [ INFO] Progress -   3.2 MB (2.1%)\n2023-07-11 04:35:31.107 [ INFO] Progress -   3.2 MB (4.2%)\n2023-07-11 04:35:31.282 [ INFO] Progress -   3.2 MB (6.2%)\n2023-07-11 04:35:31.283 [ INFO] Progress -   3.2 MB (8.3%)\n2023-07-11 04:35:31.283 [ INFO] Progress -   3.2 MB (10.4%)\n2023-07-11 04:35:31.458 [ INFO] Progress -   3.2 MB (12.5%)\n2023-07-11 04:35:31.458 [ INFO] Progress -   3.2 MB (14.5%)\n2023-07-11 04:35:31.458 [ INFO] Progress -   3.2 MB (16.6%)\n2023-07-11 04:35:31.459 [ INFO] Progress -   3.2 MB (18.7%)\n2023-07-11 04:35:31.459 [ INFO] Progress -   3.2 MB (20.8%)\n2023-07-11 04:35:31.633 [ INFO] Progress -   3.2 MB (22.8%)\n2023-07-11 04:35:31.633 [ INFO] Progress -   3.2 MB (24.9%)\n2023-07-11 04:35:31.634 [ INFO] Progress -   3.2 MB (27.0%)\n2023-07-11 04:35:31.634 [ INFO] Progress -   3.2 MB (29.1%)\n2023-07-11 04:35:31.634 [ INFO] Progress -   3.2 MB (31.2%)\n2023-07-11 04:35:31.634 [ INFO] Progress -   3.2 MB (33.2%)\n2023-07-11 04:35:31.635 [ INFO] Progress -   3.2 MB (35.3%)\n2023-07-11 04:35:31.636 [ INFO] Progress -   3.2 MB (37.4%)\n2023-07-11 04:35:31.636 [ INFO] Progress -   3.2 MB (39.5%)\n2023-07-11 04:35:31.636 [ INFO] Progress -   3.2 MB (41.5%)\n2023-07-11 04:35:31.636 [ INFO] Progress -   3.2 MB (43.6%)\n2023-07-11 04:35:31.808 [ INFO] Progress -   3.2 MB (45.7%)\n2023-07-11 04:35:31.808 [ INFO] Progress -   3.2 MB (47.8%)\n2023-07-11 04:35:31.808 [ INFO] Progress -   3.2 MB (49.8%)\n2023-07-11 04:35:31.809 [ INFO] Progress -   3.2 MB (51.9%)\n2023-07-11 04:35:31.809 [ INFO] Progress -   3.2 MB (54.0%)\n2023-07-11 04:35:31.809 [ INFO] Progress -   3.2 MB (56.1%)\n2023-07-11 04:35:31.810 [ INFO] Progress -   3.2 MB (58.1%)\n2023-07-11 04:35:31.810 [ INFO] Progress -   3.2 MB (60.2%)\n2023-07-11 04:35:31.810 [ INFO] Progress -   3.2 MB (62.3%)\n2023-07-11 04:35:31.810 [ INFO] Progress -   3.2 MB (64.4%)\n2023-07-11 04:35:31.811 [ INFO] Progress -   3.2 MB (66.5%)\n2023-07-11 04:35:31.811 [ INFO] Progress -   3.2 MB (68.5%)\n2023-07-11 04:35:31.812 [ INFO] Progress -   3.2 MB (70.6%)\n2023-07-11 04:35:31.812 [ INFO] Progress -   3.2 MB (72.7%)\n2023-07-11 04:35:31.812 [ INFO] Progress -   3.2 MB (74.8%)\n2023-07-11 04:35:31.812 [ INFO] Progress -   3.2 MB (76.8%)\n2023-07-11 04:35:31.813 [ INFO] Progress -   3.2 MB (78.9%)\n2023-07-11 04:35:31.813 [ INFO] Progress -   3.2 MB (81.0%)\n2023-07-11 04:35:31.813 [ INFO] Progress -   3.2 MB (83.1%)\n2023-07-11 04:35:31.813 [ INFO] Progress -   3.2 MB (85.1%)\n2023-07-11 04:35:31.814 [ INFO] Progress -   3.2 MB (87.2%)\n2023-07-11 04:35:31.983 [ INFO] Progress -   3.2 MB (89.3%)\n2023-07-11 04:35:31.983 [ INFO] Progress -   3.2 MB (91.4%)\n2023-07-11 04:35:31.983 [ INFO] Progress -   3.2 MB (93.5%)\n2023-07-11 04:35:31.984 [ INFO] Progress -   3.2 MB (95.5%)\n2023-07-11 04:35:31.984 [ INFO] Progress -   3.2 MB (97.6%)\n2023-07-11 04:35:31.984 [ INFO] Progress -   3.2 MB (99.7%)\n2023-07-11 04:35:31.985 [ INFO] Progress -   3.2 MB (100.0%)\n2023-07-11 04:35:31.985 [ INFO] Processing  time : 0:00:33.103400\n2023-07-11 04:35:31.985 [ INFO] Downloading time : 0:00:01.401763\n2023-07-11 04:35:31.985 [ INFO] Total time       : 0:00:34.505163\n2023-07-11 04:35:31.985 [ INFO] Download rate    : 1.5 MB/s\n2023-07-11 04:35:31.985 [ INFO] Save into        : motu/cmems_obs-sl_glo_phy-ssh_my_allsat-l4-duacs-0.25deg_P1D/200006.nc\n2023-07-11 04:35:32.015 [ INFO] Done\n2023-07-11 04:35:32.256 [ INFO] Asynchronous mode set\n2023-07-11 04:35:32.257 [ INFO] Authenticating user mphan for service https://my.cmems-du.eu/motu-web/Motu\n2023-07-11 04:35:40.867 [ INFO] Requesting file to download (this can take a while)...\n2023-07-11 04:35:49.156 [ INFO] Authenticating user mphan for service https://my.cmems-du.eu/motu-web/Motu\n2023-07-11 04:36:05.880 [ INFO] The product is ready for download\n2023-07-11 04:36:05.880 [ INFO] Downloading file (this can take a while)...\n2023-07-11 04:36:06.624 [ INFO] File type: application/x-netcdf\n2023-07-11 04:36:06.625 [ INFO] File size: 3.3 MB (3260684 B)\n2023-07-11 04:36:06.625 [ INFO] Downloading file /home/jovyan/shared/minh-notebooks/motu/cmems_obs-sl_glo_phy-ssh_my_allsat-l4-duacs-0.25deg_P1D/200007.nc\n2023-07-11 04:36:06.974 [ INFO] Progress -   3.3 MB (2.0%)\n2023-07-11 04:36:07.149 [ INFO] Progress -   3.3 MB (4.0%)\n2023-07-11 04:36:07.324 [ INFO] Progress -   3.3 MB (6.0%)\n2023-07-11 04:36:07.324 [ INFO] Progress -   3.3 MB (8.0%)\n2023-07-11 04:36:07.325 [ INFO] Progress -   3.3 MB (10.0%)\n2023-07-11 04:36:07.499 [ INFO] Progress -   3.3 MB (12.1%)\n2023-07-11 04:36:07.500 [ INFO] Progress -   3.3 MB (14.1%)\n2023-07-11 04:36:07.500 [ INFO] Progress -   3.3 MB (16.1%)\n2023-07-11 04:36:07.500 [ INFO] Progress -   3.3 MB (18.1%)\n2023-07-11 04:36:07.500 [ INFO] Progress -   3.3 MB (20.1%)\n2023-07-11 04:36:07.501 [ INFO] Progress -   3.3 MB (22.1%)\n2023-07-11 04:36:07.501 [ INFO] Progress -   3.3 MB (24.1%)\n2023-07-11 04:36:07.674 [ INFO] Progress -   3.3 MB (26.1%)\n2023-07-11 04:36:07.675 [ INFO] Progress -   3.3 MB (28.1%)\n2023-07-11 04:36:07.675 [ INFO] Progress -   3.3 MB (30.1%)\n2023-07-11 04:36:07.675 [ INFO] Progress -   3.3 MB (32.2%)\n2023-07-11 04:36:07.676 [ INFO] Progress -   3.3 MB (34.2%)\n2023-07-11 04:36:07.677 [ INFO] Progress -   3.3 MB (36.2%)\n2023-07-11 04:36:07.677 [ INFO] Progress -   3.3 MB (38.2%)\n2023-07-11 04:36:07.677 [ INFO] Progress -   3.3 MB (40.2%)\n2023-07-11 04:36:07.677 [ INFO] Progress -   3.3 MB (42.2%)\n2023-07-11 04:36:07.678 [ INFO] Progress -   3.3 MB (44.2%)\n2023-07-11 04:36:07.678 [ INFO] Progress -   3.3 MB (46.2%)\n2023-07-11 04:36:07.678 [ INFO] Progress -   3.3 MB (48.2%)\n2023-07-11 04:36:07.849 [ INFO] Progress -   3.3 MB (50.2%)\n2023-07-11 04:36:07.850 [ INFO] Progress -   3.3 MB (52.3%)\n2023-07-11 04:36:07.850 [ INFO] Progress -   3.3 MB (54.3%)\n2023-07-11 04:36:07.850 [ INFO] Progress -   3.3 MB (56.3%)\n2023-07-11 04:36:07.851 [ INFO] Progress -   3.3 MB (58.3%)\n2023-07-11 04:36:07.851 [ INFO] Progress -   3.3 MB (60.3%)\n2023-07-11 04:36:07.851 [ INFO] Progress -   3.3 MB (62.3%)\n2023-07-11 04:36:07.852 [ INFO] Progress -   3.3 MB (64.3%)\n2023-07-11 04:36:07.853 [ INFO] Progress -   3.3 MB (66.3%)\n2023-07-11 04:36:07.853 [ INFO] Progress -   3.3 MB (68.3%)\n2023-07-11 04:36:07.853 [ INFO] Progress -   3.3 MB (70.3%)\n2023-07-11 04:36:07.853 [ INFO] Progress -   3.3 MB (72.4%)\n2023-07-11 04:36:07.854 [ INFO] Progress -   3.3 MB (74.4%)\n2023-07-11 04:36:07.854 [ INFO] Progress -   3.3 MB (76.4%)\n2023-07-11 04:36:07.854 [ INFO] Progress -   3.3 MB (78.4%)\n2023-07-11 04:36:07.854 [ INFO] Progress -   3.3 MB (80.4%)\n2023-07-11 04:36:07.854 [ INFO] Progress -   3.3 MB (82.4%)\n2023-07-11 04:36:07.855 [ INFO] Progress -   3.3 MB (84.4%)\n2023-07-11 04:36:07.855 [ INFO] Progress -   3.3 MB (86.4%)\n2023-07-11 04:36:07.855 [ INFO] Progress -   3.3 MB (88.4%)\n2023-07-11 04:36:07.855 [ INFO] Progress -   3.3 MB (90.4%)\n2023-07-11 04:36:07.856 [ INFO] Progress -   3.3 MB (92.5%)\n2023-07-11 04:36:07.856 [ INFO] Progress -   3.3 MB (94.5%)\n2023-07-11 04:36:07.856 [ INFO] Progress -   3.3 MB (96.5%)\n2023-07-11 04:36:07.857 [ INFO] Progress -   3.3 MB (98.5%)\n2023-07-11 04:36:08.024 [ INFO] Progress -   3.3 MB (100.0%)\n2023-07-11 04:36:08.025 [ INFO] Processing  time : 0:00:34.370452\n2023-07-11 04:36:08.025 [ INFO] Downloading time : 0:00:01.399979\n2023-07-11 04:36:08.025 [ INFO] Total time       : 0:00:35.770431\n2023-07-11 04:36:08.025 [ INFO] Download rate    : 1.5 MB/s\n2023-07-11 04:36:08.025 [ INFO] Save into        : motu/cmems_obs-sl_glo_phy-ssh_my_allsat-l4-duacs-0.25deg_P1D/200007.nc\n2023-07-11 04:36:08.055 [ INFO] Done\n2023-07-11 04:36:08.293 [ INFO] Asynchronous mode set\n2023-07-11 04:36:08.294 [ INFO] Authenticating user mphan for service https://my.cmems-du.eu/motu-web/Motu\n2023-07-11 04:36:16.318 [ INFO] Requesting file to download (this can take a while)...\n2023-07-11 04:36:24.794 [ INFO] Authenticating user mphan for service https://my.cmems-du.eu/motu-web/Motu\n2023-07-11 04:36:40.691 [ INFO] The product is ready for download\n2023-07-11 04:36:40.691 [ INFO] Downloading file (this can take a while)...\n2023-07-11 04:36:41.620 [ INFO] File type: application/x-netcdf\n2023-07-11 04:36:41.620 [ INFO] File size: 3.3 MB (3260684 B)\n2023-07-11 04:36:41.620 [ INFO] Downloading file /home/jovyan/shared/minh-notebooks/motu/cmems_obs-sl_glo_phy-ssh_my_allsat-l4-duacs-0.25deg_P1D/200008.nc\n2023-07-11 04:36:41.794 [ INFO] Progress -   3.3 MB (2.0%)\n2023-07-11 04:36:41.969 [ INFO] Progress -   3.3 MB (4.0%)\n2023-07-11 04:36:42.144 [ INFO] Progress -   3.3 MB (6.0%)\n2023-07-11 04:36:42.145 [ INFO] Progress -   3.3 MB (8.0%)\n2023-07-11 04:36:42.145 [ INFO] Progress -   3.3 MB (10.0%)\n2023-07-11 04:36:42.319 [ INFO] Progress -   3.3 MB (12.1%)\n2023-07-11 04:36:42.320 [ INFO] Progress -   3.3 MB (14.1%)\n2023-07-11 04:36:42.320 [ INFO] Progress -   3.3 MB (16.1%)\n2023-07-11 04:36:42.320 [ INFO] Progress -   3.3 MB (18.1%)\n2023-07-11 04:36:42.321 [ INFO] Progress -   3.3 MB (20.1%)\n2023-07-11 04:36:42.321 [ INFO] Progress -   3.3 MB (22.1%)\n2023-07-11 04:36:42.321 [ INFO] Progress -   3.3 MB (24.1%)\n2023-07-11 04:36:42.494 [ INFO] Progress -   3.3 MB (26.1%)\n2023-07-11 04:36:42.495 [ INFO] Progress -   3.3 MB (28.1%)\n2023-07-11 04:36:42.495 [ INFO] Progress -   3.3 MB (30.1%)\n2023-07-11 04:36:42.495 [ INFO] Progress -   3.3 MB (32.2%)\n2023-07-11 04:36:42.496 [ INFO] Progress -   3.3 MB (34.2%)\n2023-07-11 04:36:42.497 [ INFO] Progress -   3.3 MB (36.2%)\n2023-07-11 04:36:42.497 [ INFO] Progress -   3.3 MB (38.2%)\n2023-07-11 04:36:42.497 [ INFO] Progress -   3.3 MB (40.2%)\n2023-07-11 04:36:42.497 [ INFO] Progress -   3.3 MB (42.2%)\n2023-07-11 04:36:42.498 [ INFO] Progress -   3.3 MB (44.2%)\n2023-07-11 04:36:42.498 [ INFO] Progress -   3.3 MB (46.2%)\n2023-07-11 04:36:42.498 [ INFO] Progress -   3.3 MB (48.2%)\n2023-07-11 04:36:42.669 [ INFO] Progress -   3.3 MB (50.2%)\n2023-07-11 04:36:42.670 [ INFO] Progress -   3.3 MB (52.3%)\n2023-07-11 04:36:42.670 [ INFO] Progress -   3.3 MB (54.3%)\n2023-07-11 04:36:42.670 [ INFO] Progress -   3.3 MB (56.3%)\n2023-07-11 04:36:42.671 [ INFO] Progress -   3.3 MB (58.3%)\n2023-07-11 04:36:42.671 [ INFO] Progress -   3.3 MB (60.3%)\n2023-07-11 04:36:42.671 [ INFO] Progress -   3.3 MB (62.3%)\n2023-07-11 04:36:42.672 [ INFO] Progress -   3.3 MB (64.3%)\n2023-07-11 04:36:42.672 [ INFO] Progress -   3.3 MB (66.3%)\n2023-07-11 04:36:42.673 [ INFO] Progress -   3.3 MB (68.3%)\n2023-07-11 04:36:42.673 [ INFO] Progress -   3.3 MB (70.3%)\n2023-07-11 04:36:42.673 [ INFO] Progress -   3.3 MB (72.4%)\n2023-07-11 04:36:42.673 [ INFO] Progress -   3.3 MB (74.4%)\n2023-07-11 04:36:42.674 [ INFO] Progress -   3.3 MB (76.4%)\n2023-07-11 04:36:42.674 [ INFO] Progress -   3.3 MB (78.4%)\n2023-07-11 04:36:42.674 [ INFO] Progress -   3.3 MB (80.4%)\n2023-07-11 04:36:42.674 [ INFO] Progress -   3.3 MB (82.4%)\n2023-07-11 04:36:42.675 [ INFO] Progress -   3.3 MB (84.4%)\n2023-07-11 04:36:42.675 [ INFO] Progress -   3.3 MB (86.4%)\n2023-07-11 04:36:42.675 [ INFO] Progress -   3.3 MB (88.4%)\n2023-07-11 04:36:42.675 [ INFO] Progress -   3.3 MB (90.4%)\n2023-07-11 04:36:42.676 [ INFO] Progress -   3.3 MB (92.5%)\n2023-07-11 04:36:42.676 [ INFO] Progress -   3.3 MB (94.5%)\n2023-07-11 04:36:42.676 [ INFO] Progress -   3.3 MB (96.5%)\n2023-07-11 04:36:42.677 [ INFO] Progress -   3.3 MB (98.5%)\n2023-07-11 04:36:42.845 [ INFO] Progress -   3.3 MB (100.0%)\n2023-07-11 04:36:42.845 [ INFO] Processing  time : 0:00:33.329086\n2023-07-11 04:36:42.845 [ INFO] Downloading time : 0:00:01.224631\n2023-07-11 04:36:42.845 [ INFO] Total time       : 0:00:34.553717\n2023-07-11 04:36:42.845 [ INFO] Download rate    : 1.5 MB/s\n2023-07-11 04:36:42.845 [ INFO] Save into        : motu/cmems_obs-sl_glo_phy-ssh_my_allsat-l4-duacs-0.25deg_P1D/200008.nc\n2023-07-11 04:36:42.876 [ INFO] Done\n2023-07-11 04:36:43.117 [ INFO] Asynchronous mode set\n2023-07-11 04:36:43.118 [ INFO] Authenticating user mphan for service https://my.cmems-du.eu/motu-web/Motu\n2023-07-11 04:36:50.965 [ INFO] Requesting file to download (this can take a while)...\n2023-07-11 04:36:59.415 [ INFO] Authenticating user mphan for service https://my.cmems-du.eu/motu-web/Motu\n2023-07-11 04:37:13.369 [ INFO] The product is ready for download\n2023-07-11 04:37:13.369 [ INFO] Downloading file (this can take a while)...\n2023-07-11 04:37:14.112 [ INFO] File type: application/x-netcdf\n2023-07-11 04:37:14.112 [ INFO] File size: 3.2 MB (3155704 B)\n2023-07-11 04:37:14.113 [ INFO] Downloading file /home/jovyan/shared/minh-notebooks/motu/cmems_obs-sl_glo_phy-ssh_my_allsat-l4-duacs-0.25deg_P1D/200009.nc\n2023-07-11 04:37:14.462 [ INFO] Progress -   3.2 MB (2.1%)\n2023-07-11 04:37:14.637 [ INFO] Progress -   3.2 MB (4.2%)\n2023-07-11 04:37:14.812 [ INFO] Progress -   3.2 MB (6.2%)\n2023-07-11 04:37:14.812 [ INFO] Progress -   3.2 MB (8.3%)\n2023-07-11 04:37:14.813 [ INFO] Progress -   3.2 MB (10.4%)\n2023-07-11 04:37:14.987 [ INFO] Progress -   3.2 MB (12.5%)\n2023-07-11 04:37:14.987 [ INFO] Progress -   3.2 MB (14.5%)\n2023-07-11 04:37:14.988 [ INFO] Progress -   3.2 MB (16.6%)\n2023-07-11 04:37:14.988 [ INFO] Progress -   3.2 MB (18.7%)\n2023-07-11 04:37:14.988 [ INFO] Progress -   3.2 MB (20.8%)\n2023-07-11 04:37:14.988 [ INFO] Progress -   3.2 MB (22.8%)\n2023-07-11 04:37:15.162 [ INFO] Progress -   3.2 MB (24.9%)\n2023-07-11 04:37:15.163 [ INFO] Progress -   3.2 MB (27.0%)\n2023-07-11 04:37:15.163 [ INFO] Progress -   3.2 MB (29.1%)\n2023-07-11 04:37:15.163 [ INFO] Progress -   3.2 MB (31.2%)\n2023-07-11 04:37:15.164 [ INFO] Progress -   3.2 MB (33.2%)\n2023-07-11 04:37:15.164 [ INFO] Progress -   3.2 MB (35.3%)\n2023-07-11 04:37:15.165 [ INFO] Progress -   3.2 MB (37.4%)\n2023-07-11 04:37:15.165 [ INFO] Progress -   3.2 MB (39.5%)\n2023-07-11 04:37:15.165 [ INFO] Progress -   3.2 MB (41.5%)\n2023-07-11 04:37:15.165 [ INFO] Progress -   3.2 MB (43.6%)\n2023-07-11 04:37:15.166 [ INFO] Progress -   3.2 MB (45.7%)\n2023-07-11 04:37:15.337 [ INFO] Progress -   3.2 MB (47.8%)\n2023-07-11 04:37:15.337 [ INFO] Progress -   3.2 MB (49.8%)\n2023-07-11 04:37:15.338 [ INFO] Progress -   3.2 MB (51.9%)\n2023-07-11 04:37:15.338 [ INFO] Progress -   3.2 MB (54.0%)\n2023-07-11 04:37:15.338 [ INFO] Progress -   3.2 MB (56.1%)\n2023-07-11 04:37:15.338 [ INFO] Progress -   3.2 MB (58.1%)\n2023-07-11 04:37:15.339 [ INFO] Progress -   3.2 MB (60.2%)\n2023-07-11 04:37:15.339 [ INFO] Progress -   3.2 MB (62.3%)\n2023-07-11 04:37:15.339 [ INFO] Progress -   3.2 MB (64.4%)\n2023-07-11 04:37:15.339 [ INFO] Progress -   3.2 MB (66.5%)\n2023-07-11 04:37:15.340 [ INFO] Progress -   3.2 MB (68.5%)\n2023-07-11 04:37:15.340 [ INFO] Progress -   3.2 MB (70.6%)\n2023-07-11 04:37:15.341 [ INFO] Progress -   3.2 MB (72.7%)\n2023-07-11 04:37:15.341 [ INFO] Progress -   3.2 MB (74.8%)\n2023-07-11 04:37:15.341 [ INFO] Progress -   3.2 MB (76.8%)\n2023-07-11 04:37:15.341 [ INFO] Progress -   3.2 MB (78.9%)\n2023-07-11 04:37:15.342 [ INFO] Progress -   3.2 MB (81.0%)\n2023-07-11 04:37:15.342 [ INFO] Progress -   3.2 MB (83.1%)\n2023-07-11 04:37:15.342 [ INFO] Progress -   3.2 MB (85.1%)\n2023-07-11 04:37:15.342 [ INFO] Progress -   3.2 MB (87.2%)\n2023-07-11 04:37:15.342 [ INFO] Progress -   3.2 MB (89.3%)\n2023-07-11 04:37:15.343 [ INFO] Progress -   3.2 MB (91.4%)\n2023-07-11 04:37:15.343 [ INFO] Progress -   3.2 MB (93.5%)\n2023-07-11 04:37:15.512 [ INFO] Progress -   3.2 MB (95.5%)\n2023-07-11 04:37:15.512 [ INFO] Progress -   3.2 MB (97.6%)\n2023-07-11 04:37:15.513 [ INFO] Progress -   3.2 MB (99.7%)\n2023-07-11 04:37:15.513 [ INFO] Progress -   3.2 MB (100.0%)\n2023-07-11 04:37:15.513 [ INFO] Processing  time : 0:00:30.997472\n2023-07-11 04:37:15.513 [ INFO] Downloading time : 0:00:01.400914\n2023-07-11 04:37:15.513 [ INFO] Total time       : 0:00:32.398386\n2023-07-11 04:37:15.513 [ INFO] Download rate    : 1.5 MB/s\n2023-07-11 04:37:15.514 [ INFO] Save into        : motu/cmems_obs-sl_glo_phy-ssh_my_allsat-l4-duacs-0.25deg_P1D/200009.nc\n2023-07-11 04:37:15.543 [ INFO] Done\n2023-07-11 04:37:15.783 [ INFO] Asynchronous mode set\n2023-07-11 04:37:15.783 [ INFO] Authenticating user mphan for service https://my.cmems-du.eu/motu-web/Motu\n2023-07-11 04:37:20.984 [ INFO] Requesting file to download (this can take a while)...\n2023-07-11 04:37:30.393 [ INFO] Authenticating user mphan for service https://my.cmems-du.eu/motu-web/Motu\n2023-07-11 04:37:47.071 [ INFO] The product is ready for download\n2023-07-11 04:37:47.071 [ INFO] Downloading file (this can take a while)...\n2023-07-11 04:37:47.821 [ INFO] File type: application/x-netcdf\n2023-07-11 04:37:47.821 [ INFO] File size: 3.3 MB (3260684 B)\n2023-07-11 04:37:47.821 [ INFO] Downloading file /home/jovyan/shared/minh-notebooks/motu/cmems_obs-sl_glo_phy-ssh_my_allsat-l4-duacs-0.25deg_P1D/200010.nc\n2023-07-11 04:37:48.170 [ INFO] Progress -   3.3 MB (2.0%)\n2023-07-11 04:37:48.346 [ INFO] Progress -   3.3 MB (4.0%)\n2023-07-11 04:37:48.520 [ INFO] Progress -   3.3 MB (6.0%)\n2023-07-11 04:37:48.521 [ INFO] Progress -   3.3 MB (8.0%)\n2023-07-11 04:37:48.521 [ INFO] Progress -   3.3 MB (10.0%)\n2023-07-11 04:37:48.695 [ INFO] Progress -   3.3 MB (12.1%)\n2023-07-11 04:37:48.696 [ INFO] Progress -   3.3 MB (14.1%)\n2023-07-11 04:37:48.696 [ INFO] Progress -   3.3 MB (16.1%)\n2023-07-11 04:37:48.696 [ INFO] Progress -   3.3 MB (18.1%)\n2023-07-11 04:37:48.697 [ INFO] Progress -   3.3 MB (20.1%)\n2023-07-11 04:37:48.697 [ INFO] Progress -   3.3 MB (22.1%)\n2023-07-11 04:37:48.697 [ INFO] Progress -   3.3 MB (24.1%)\n2023-07-11 04:37:48.871 [ INFO] Progress -   3.3 MB (26.1%)\n2023-07-11 04:37:48.871 [ INFO] Progress -   3.3 MB (28.1%)\n2023-07-11 04:37:48.872 [ INFO] Progress -   3.3 MB (30.1%)\n2023-07-11 04:37:48.872 [ INFO] Progress -   3.3 MB (32.2%)\n2023-07-11 04:37:48.873 [ INFO] Progress -   3.3 MB (34.2%)\n2023-07-11 04:37:48.873 [ INFO] Progress -   3.3 MB (36.2%)\n2023-07-11 04:37:48.873 [ INFO] Progress -   3.3 MB (38.2%)\n2023-07-11 04:37:48.874 [ INFO] Progress -   3.3 MB (40.2%)\n2023-07-11 04:37:48.874 [ INFO] Progress -   3.3 MB (42.2%)\n2023-07-11 04:37:48.874 [ INFO] Progress -   3.3 MB (44.2%)\n2023-07-11 04:37:48.874 [ INFO] Progress -   3.3 MB (46.2%)\n2023-07-11 04:37:48.874 [ INFO] Progress -   3.3 MB (48.2%)\n2023-07-11 04:37:49.046 [ INFO] Progress -   3.3 MB (50.2%)\n2023-07-11 04:37:49.046 [ INFO] Progress -   3.3 MB (52.3%)\n2023-07-11 04:37:49.046 [ INFO] Progress -   3.3 MB (54.3%)\n2023-07-11 04:37:49.047 [ INFO] Progress -   3.3 MB (56.3%)\n2023-07-11 04:37:49.047 [ INFO] Progress -   3.3 MB (58.3%)\n2023-07-11 04:37:49.047 [ INFO] Progress -   3.3 MB (60.3%)\n2023-07-11 04:37:49.048 [ INFO] Progress -   3.3 MB (62.3%)\n2023-07-11 04:37:49.048 [ INFO] Progress -   3.3 MB (64.3%)\n2023-07-11 04:37:49.049 [ INFO] Progress -   3.3 MB (66.3%)\n2023-07-11 04:37:49.049 [ INFO] Progress -   3.3 MB (68.3%)\n2023-07-11 04:37:49.049 [ INFO] Progress -   3.3 MB (70.3%)\n2023-07-11 04:37:49.049 [ INFO] Progress -   3.3 MB (72.4%)\n2023-07-11 04:37:49.050 [ INFO] Progress -   3.3 MB (74.4%)\n2023-07-11 04:37:49.050 [ INFO] Progress -   3.3 MB (76.4%)\n2023-07-11 04:37:49.050 [ INFO] Progress -   3.3 MB (78.4%)\n2023-07-11 04:37:49.050 [ INFO] Progress -   3.3 MB (80.4%)\n2023-07-11 04:37:49.051 [ INFO] Progress -   3.3 MB (82.4%)\n2023-07-11 04:37:49.051 [ INFO] Progress -   3.3 MB (84.4%)\n2023-07-11 04:37:49.051 [ INFO] Progress -   3.3 MB (86.4%)\n2023-07-11 04:37:49.051 [ INFO] Progress -   3.3 MB (88.4%)\n2023-07-11 04:37:49.051 [ INFO] Progress -   3.3 MB (90.4%)\n2023-07-11 04:37:49.052 [ INFO] Progress -   3.3 MB (92.5%)\n2023-07-11 04:37:49.052 [ INFO] Progress -   3.3 MB (94.5%)\n2023-07-11 04:37:49.052 [ INFO] Progress -   3.3 MB (96.5%)\n2023-07-11 04:37:49.053 [ INFO] Progress -   3.3 MB (98.5%)\n2023-07-11 04:37:49.053 [ INFO] Progress -   3.3 MB (100.0%)\n2023-07-11 04:37:49.053 [ INFO] Processing  time : 0:00:32.040412\n2023-07-11 04:37:49.053 [ INFO] Downloading time : 0:00:01.232141\n2023-07-11 04:37:49.053 [ INFO] Total time       : 0:00:33.272553\n2023-07-11 04:37:49.053 [ INFO] Download rate    : 1.6 MB/s\n2023-07-11 04:37:49.054 [ INFO] Save into        : motu/cmems_obs-sl_glo_phy-ssh_my_allsat-l4-duacs-0.25deg_P1D/200010.nc\n2023-07-11 04:37:49.084 [ INFO] Done\n2023-07-11 04:37:49.325 [ INFO] Asynchronous mode set\n2023-07-11 04:37:49.325 [ INFO] Authenticating user mphan for service https://my.cmems-du.eu/motu-web/Motu\n2023-07-11 04:37:59.411 [ INFO] Requesting file to download (this can take a while)...\n2023-07-11 04:38:06.403 [ INFO] Authenticating user mphan for service https://my.cmems-du.eu/motu-web/Motu\n2023-07-11 04:38:22.049 [ INFO] The product is ready for download\n2023-07-11 04:38:22.049 [ INFO] Downloading file (this can take a while)...\n2023-07-11 04:38:22.795 [ INFO] File type: application/x-netcdf\n2023-07-11 04:38:22.795 [ INFO] File size: 3.2 MB (3155704 B)\n2023-07-11 04:38:22.795 [ INFO] Downloading file /home/jovyan/shared/minh-notebooks/motu/cmems_obs-sl_glo_phy-ssh_my_allsat-l4-duacs-0.25deg_P1D/200011.nc\n2023-07-11 04:38:23.144 [ INFO] Progress -   3.2 MB (2.1%)\n2023-07-11 04:38:23.319 [ INFO] Progress -   3.2 MB (4.2%)\n2023-07-11 04:38:23.494 [ INFO] Progress -   3.2 MB (6.2%)\n2023-07-11 04:38:23.495 [ INFO] Progress -   3.2 MB (8.3%)\n2023-07-11 04:38:23.495 [ INFO] Progress -   3.2 MB (10.4%)\n2023-07-11 04:38:23.669 [ INFO] Progress -   3.2 MB (12.5%)\n2023-07-11 04:38:23.670 [ INFO] Progress -   3.2 MB (14.5%)\n2023-07-11 04:38:23.670 [ INFO] Progress -   3.2 MB (16.6%)\n2023-07-11 04:38:23.670 [ INFO] Progress -   3.2 MB (18.7%)\n2023-07-11 04:38:23.671 [ INFO] Progress -   3.2 MB (20.8%)\n2023-07-11 04:38:23.671 [ INFO] Progress -   3.2 MB (22.8%)\n2023-07-11 04:38:23.844 [ INFO] Progress -   3.2 MB (24.9%)\n2023-07-11 04:38:23.845 [ INFO] Progress -   3.2 MB (27.0%)\n2023-07-11 04:38:23.845 [ INFO] Progress -   3.2 MB (29.1%)\n2023-07-11 04:38:23.845 [ INFO] Progress -   3.2 MB (31.2%)\n2023-07-11 04:38:23.846 [ INFO] Progress -   3.2 MB (33.2%)\n2023-07-11 04:38:23.847 [ INFO] Progress -   3.2 MB (35.3%)\n2023-07-11 04:38:23.847 [ INFO] Progress -   3.2 MB (37.4%)\n2023-07-11 04:38:23.847 [ INFO] Progress -   3.2 MB (39.5%)\n2023-07-11 04:38:23.847 [ INFO] Progress -   3.2 MB (41.5%)\n2023-07-11 04:38:23.848 [ INFO] Progress -   3.2 MB (43.6%)\n2023-07-11 04:38:23.848 [ INFO] Progress -   3.2 MB (45.7%)\n2023-07-11 04:38:23.848 [ INFO] Progress -   3.2 MB (47.8%)\n2023-07-11 04:38:24.019 [ INFO] Progress -   3.2 MB (49.8%)\n2023-07-11 04:38:24.020 [ INFO] Progress -   3.2 MB (51.9%)\n2023-07-11 04:38:24.020 [ INFO] Progress -   3.2 MB (54.0%)\n2023-07-11 04:38:24.020 [ INFO] Progress -   3.2 MB (56.1%)\n2023-07-11 04:38:24.021 [ INFO] Progress -   3.2 MB (58.1%)\n2023-07-11 04:38:24.021 [ INFO] Progress -   3.2 MB (60.2%)\n2023-07-11 04:38:24.021 [ INFO] Progress -   3.2 MB (62.3%)\n2023-07-11 04:38:24.021 [ INFO] Progress -   3.2 MB (64.4%)\n2023-07-11 04:38:24.022 [ INFO] Progress -   3.2 MB (66.5%)\n2023-07-11 04:38:24.022 [ INFO] Progress -   3.2 MB (68.5%)\n2023-07-11 04:38:24.023 [ INFO] Progress -   3.2 MB (70.6%)\n2023-07-11 04:38:24.023 [ INFO] Progress -   3.2 MB (72.7%)\n2023-07-11 04:38:24.023 [ INFO] Progress -   3.2 MB (74.8%)\n2023-07-11 04:38:24.023 [ INFO] Progress -   3.2 MB (76.8%)\n2023-07-11 04:38:24.023 [ INFO] Progress -   3.2 MB (78.9%)\n2023-07-11 04:38:24.024 [ INFO] Progress -   3.2 MB (81.0%)\n2023-07-11 04:38:24.024 [ INFO] Progress -   3.2 MB (83.1%)\n2023-07-11 04:38:24.024 [ INFO] Progress -   3.2 MB (85.1%)\n2023-07-11 04:38:24.024 [ INFO] Progress -   3.2 MB (87.2%)\n2023-07-11 04:38:24.025 [ INFO] Progress -   3.2 MB (89.3%)\n2023-07-11 04:38:24.025 [ INFO] Progress -   3.2 MB (91.4%)\n2023-07-11 04:38:24.025 [ INFO] Progress -   3.2 MB (93.5%)\n2023-07-11 04:38:24.025 [ INFO] Progress -   3.2 MB (95.5%)\n2023-07-11 04:38:24.025 [ INFO] Progress -   3.2 MB (97.6%)\n2023-07-11 04:38:24.194 [ INFO] Progress -   3.2 MB (99.7%)\n2023-07-11 04:38:24.195 [ INFO] Progress -   3.2 MB (100.0%)\n2023-07-11 04:38:24.195 [ INFO] Processing  time : 0:00:33.470549\n2023-07-11 04:38:24.195 [ INFO] Downloading time : 0:00:01.400394\n2023-07-11 04:38:24.195 [ INFO] Total time       : 0:00:34.870943\n2023-07-11 04:38:24.195 [ INFO] Download rate    : 1.5 MB/s\n2023-07-11 04:38:24.195 [ INFO] Save into        : motu/cmems_obs-sl_glo_phy-ssh_my_allsat-l4-duacs-0.25deg_P1D/200011.nc\n2023-07-11 04:38:24.225 [ INFO] Done\n2023-07-11 04:38:24.465 [ INFO] Asynchronous mode set\n2023-07-11 04:38:24.465 [ INFO] Authenticating user mphan for service https://my.cmems-du.eu/motu-web/Motu\n2023-07-11 04:38:30.868 [ INFO] Requesting file to download (this can take a while)...\n2023-07-11 04:38:40.474 [ INFO] Authenticating user mphan for service https://my.cmems-du.eu/motu-web/Motu\n2023-07-11 04:38:55.592 [ INFO] The product is ready for download\n2023-07-11 04:38:55.592 [ INFO] Downloading file (this can take a while)...\n2023-07-11 04:38:56.336 [ INFO] File type: application/x-netcdf\n2023-07-11 04:38:56.336 [ INFO] File size: 3.3 MB (3260684 B)\n2023-07-11 04:38:56.336 [ INFO] Downloading file /home/jovyan/shared/minh-notebooks/motu/cmems_obs-sl_glo_phy-ssh_my_allsat-l4-duacs-0.25deg_P1D/200012.nc\n2023-07-11 04:38:56.687 [ INFO] Progress -   3.3 MB (2.0%)\n2023-07-11 04:38:56.862 [ INFO] Progress -   3.3 MB (4.0%)\n2023-07-11 04:38:57.035 [ INFO] Progress -   3.3 MB (6.0%)\n2023-07-11 04:38:57.037 [ INFO] Progress -   3.3 MB (8.0%)\n2023-07-11 04:38:57.037 [ INFO] Progress -   3.3 MB (10.0%)\n2023-07-11 04:38:57.037 [ INFO] Progress -   3.3 MB (12.1%)\n2023-07-11 04:38:57.211 [ INFO] Progress -   3.3 MB (14.1%)\n2023-07-11 04:38:57.212 [ INFO] Progress -   3.3 MB (16.1%)\n2023-07-11 04:38:57.212 [ INFO] Progress -   3.3 MB (18.1%)\n2023-07-11 04:38:57.212 [ INFO] Progress -   3.3 MB (20.1%)\n2023-07-11 04:38:57.213 [ INFO] Progress -   3.3 MB (22.1%)\n2023-07-11 04:38:57.213 [ INFO] Progress -   3.3 MB (24.1%)\n2023-07-11 04:38:57.386 [ INFO] Progress -   3.3 MB (26.1%)\n2023-07-11 04:38:57.386 [ INFO] Progress -   3.3 MB (28.1%)\n2023-07-11 04:38:57.386 [ INFO] Progress -   3.3 MB (30.1%)\n2023-07-11 04:38:57.387 [ INFO] Progress -   3.3 MB (32.2%)\n2023-07-11 04:38:57.388 [ INFO] Progress -   3.3 MB (34.2%)\n2023-07-11 04:38:57.388 [ INFO] Progress -   3.3 MB (36.2%)\n2023-07-11 04:38:57.388 [ INFO] Progress -   3.3 MB (38.2%)\n2023-07-11 04:38:57.389 [ INFO] Progress -   3.3 MB (40.2%)\n2023-07-11 04:38:57.389 [ INFO] Progress -   3.3 MB (42.2%)\n2023-07-11 04:38:57.389 [ INFO] Progress -   3.3 MB (44.2%)\n2023-07-11 04:38:57.389 [ INFO] Progress -   3.3 MB (46.2%)\n2023-07-11 04:38:57.390 [ INFO] Progress -   3.3 MB (48.2%)\n2023-07-11 04:38:57.390 [ INFO] Progress -   3.3 MB (50.2%)\n2023-07-11 04:38:57.390 [ INFO] Progress -   3.3 MB (52.3%)\n2023-07-11 04:38:57.561 [ INFO] Progress -   3.3 MB (54.3%)\n2023-07-11 04:38:57.561 [ INFO] Progress -   3.3 MB (56.3%)\n2023-07-11 04:38:57.562 [ INFO] Progress -   3.3 MB (58.3%)\n2023-07-11 04:38:57.562 [ INFO] Progress -   3.3 MB (60.3%)\n2023-07-11 04:38:57.562 [ INFO] Progress -   3.3 MB (62.3%)\n2023-07-11 04:38:57.562 [ INFO] Progress -   3.3 MB (64.3%)\n2023-07-11 04:38:57.563 [ INFO] Progress -   3.3 MB (66.3%)\n2023-07-11 04:38:57.564 [ INFO] Progress -   3.3 MB (68.3%)\n2023-07-11 04:38:57.564 [ INFO] Progress -   3.3 MB (70.3%)\n2023-07-11 04:38:57.564 [ INFO] Progress -   3.3 MB (72.4%)\n2023-07-11 04:38:57.565 [ INFO] Progress -   3.3 MB (74.4%)\n2023-07-11 04:38:57.565 [ INFO] Progress -   3.3 MB (76.4%)\n2023-07-11 04:38:57.565 [ INFO] Progress -   3.3 MB (78.4%)\n2023-07-11 04:38:57.565 [ INFO] Progress -   3.3 MB (80.4%)\n2023-07-11 04:38:57.566 [ INFO] Progress -   3.3 MB (82.4%)\n2023-07-11 04:38:57.566 [ INFO] Progress -   3.3 MB (84.4%)\n2023-07-11 04:38:57.566 [ INFO] Progress -   3.3 MB (86.4%)\n2023-07-11 04:38:57.566 [ INFO] Progress -   3.3 MB (88.4%)\n2023-07-11 04:38:57.567 [ INFO] Progress -   3.3 MB (90.4%)\n2023-07-11 04:38:57.567 [ INFO] Progress -   3.3 MB (92.5%)\n2023-07-11 04:38:57.567 [ INFO] Progress -   3.3 MB (94.5%)\n2023-07-11 04:38:57.567 [ INFO] Progress -   3.3 MB (96.5%)\n2023-07-11 04:38:57.568 [ INFO] Progress -   3.3 MB (98.5%)\n2023-07-11 04:38:57.569 [ INFO] Progress -   3.3 MB (100.0%)\n2023-07-11 04:38:57.569 [ INFO] Processing  time : 0:00:31.874707\n2023-07-11 04:38:57.569 [ INFO] Downloading time : 0:00:01.232594\n2023-07-11 04:38:57.569 [ INFO] Total time       : 0:00:33.107301\n2023-07-11 04:38:57.569 [ INFO] Download rate    : 1.6 MB/s\n2023-07-11 04:38:57.569 [ INFO] Save into        : motu/cmems_obs-sl_glo_phy-ssh_my_allsat-l4-duacs-0.25deg_P1D/200012.nc\n2023-07-11 04:38:57.599 [ INFO] Done\n2023-07-11 04:38:57.840 [ INFO] Asynchronous mode set\n2023-07-11 04:38:57.840 [ INFO] Authenticating user mphan for service https://my.cmems-du.eu/motu-web/Motu\n2023-07-11 04:39:04.934 [ INFO] Requesting file to download (this can take a while)...\n2023-07-11 04:39:11.874 [ INFO] Authenticating user mphan for service https://my.cmems-du.eu/motu-web/Motu\n2023-07-11 04:39:27.152 [ INFO] The product is ready for download\n2023-07-11 04:39:27.153 [ INFO] Downloading file (this can take a while)...\n2023-07-11 04:39:27.897 [ INFO] File type: application/x-netcdf\n2023-07-11 04:39:27.897 [ INFO] File size: 3.3 MB (3260684 B)\n2023-07-11 04:39:27.897 [ INFO] Downloading file /home/jovyan/shared/minh-notebooks/motu/cmems_obs-sl_glo_phy-ssh_my_allsat-l4-duacs-0.25deg_P1D/200101.nc\n2023-07-11 04:39:28.247 [ INFO] Progress -   3.3 MB (2.0%)\n2023-07-11 04:39:28.422 [ INFO] Progress -   3.3 MB (4.0%)\n2023-07-11 04:39:28.597 [ INFO] Progress -   3.3 MB (6.0%)\n2023-07-11 04:39:28.597 [ INFO] Progress -   3.3 MB (8.0%)\n2023-07-11 04:39:28.772 [ INFO] Progress -   3.3 MB (10.0%)\n2023-07-11 04:39:28.772 [ INFO] Progress -   3.3 MB (12.1%)\n2023-07-11 04:39:28.772 [ INFO] Progress -   3.3 MB (14.1%)\n2023-07-11 04:39:28.773 [ INFO] Progress -   3.3 MB (16.1%)\n2023-07-11 04:39:28.773 [ INFO] Progress -   3.3 MB (18.1%)\n2023-07-11 04:39:28.947 [ INFO] Progress -   3.3 MB (20.1%)\n2023-07-11 04:39:28.947 [ INFO] Progress -   3.3 MB (22.1%)\n2023-07-11 04:39:28.947 [ INFO] Progress -   3.3 MB (24.1%)\n2023-07-11 04:39:28.948 [ INFO] Progress -   3.3 MB (26.1%)\n2023-07-11 04:39:28.948 [ INFO] Progress -   3.3 MB (28.1%)\n2023-07-11 04:39:28.948 [ INFO] Progress -   3.3 MB (30.1%)\n2023-07-11 04:39:29.122 [ INFO] Progress -   3.3 MB (32.2%)\n2023-07-11 04:39:29.123 [ INFO] Progress -   3.3 MB (34.2%)\n2023-07-11 04:39:29.123 [ INFO] Progress -   3.3 MB (36.2%)\n2023-07-11 04:39:29.124 [ INFO] Progress -   3.3 MB (38.2%)\n2023-07-11 04:39:29.124 [ INFO] Progress -   3.3 MB (40.2%)\n2023-07-11 04:39:29.124 [ INFO] Progress -   3.3 MB (42.2%)\n2023-07-11 04:39:29.124 [ INFO] Progress -   3.3 MB (44.2%)\n2023-07-11 04:39:29.125 [ INFO] Progress -   3.3 MB (46.2%)\n2023-07-11 04:39:29.125 [ INFO] Progress -   3.3 MB (48.2%)\n2023-07-11 04:39:29.125 [ INFO] Progress -   3.3 MB (50.2%)\n2023-07-11 04:39:29.125 [ INFO] Progress -   3.3 MB (52.3%)\n2023-07-11 04:39:29.125 [ INFO] Progress -   3.3 MB (54.3%)\n2023-07-11 04:39:29.126 [ INFO] Progress -   3.3 MB (56.3%)\n2023-07-11 04:39:29.126 [ INFO] Progress -   3.3 MB (58.3%)\n2023-07-11 04:39:29.297 [ INFO] Progress -   3.3 MB (60.3%)\n2023-07-11 04:39:29.297 [ INFO] Progress -   3.3 MB (62.3%)\n2023-07-11 04:39:29.297 [ INFO] Progress -   3.3 MB (64.3%)\n2023-07-11 04:39:29.298 [ INFO] Progress -   3.3 MB (66.3%)\n2023-07-11 04:39:29.299 [ INFO] Progress -   3.3 MB (68.3%)\n2023-07-11 04:39:29.299 [ INFO] Progress -   3.3 MB (70.3%)\n2023-07-11 04:39:29.299 [ INFO] Progress -   3.3 MB (72.4%)\n2023-07-11 04:39:29.300 [ INFO] Progress -   3.3 MB (74.4%)\n2023-07-11 04:39:29.300 [ INFO] Progress -   3.3 MB (76.4%)\n2023-07-11 04:39:29.300 [ INFO] Progress -   3.3 MB (78.4%)\n2023-07-11 04:39:29.300 [ INFO] Progress -   3.3 MB (80.4%)\n2023-07-11 04:39:29.301 [ INFO] Progress -   3.3 MB (82.4%)\n2023-07-11 04:39:29.301 [ INFO] Progress -   3.3 MB (84.4%)\n2023-07-11 04:39:29.301 [ INFO] Progress -   3.3 MB (86.4%)\n2023-07-11 04:39:29.301 [ INFO] Progress -   3.3 MB (88.4%)\n2023-07-11 04:39:29.301 [ INFO] Progress -   3.3 MB (90.4%)\n2023-07-11 04:39:29.302 [ INFO] Progress -   3.3 MB (92.5%)\n2023-07-11 04:39:29.302 [ INFO] Progress -   3.3 MB (94.5%)\n2023-07-11 04:39:29.302 [ INFO] Progress -   3.3 MB (96.5%)\n2023-07-11 04:39:29.303 [ INFO] Progress -   3.3 MB (98.5%)\n2023-07-11 04:39:29.303 [ INFO] Progress -   3.3 MB (100.0%)\n2023-07-11 04:39:29.303 [ INFO] Processing  time : 0:00:30.059472\n2023-07-11 04:39:29.303 [ INFO] Downloading time : 0:00:01.406073\n2023-07-11 04:39:29.303 [ INFO] Total time       : 0:00:31.465545\n2023-07-11 04:39:29.303 [ INFO] Download rate    : 1.5 MB/s\n2023-07-11 04:39:29.304 [ INFO] Save into        : motu/cmems_obs-sl_glo_phy-ssh_my_allsat-l4-duacs-0.25deg_P1D/200101.nc\n2023-07-11 04:39:29.334 [ INFO] Done\n2023-07-11 04:39:29.573 [ INFO] Asynchronous mode set\n2023-07-11 04:39:29.574 [ INFO] Authenticating user mphan for service https://my.cmems-du.eu/motu-web/Motu\n2023-07-11 04:39:36.995 [ INFO] Requesting file to download (this can take a while)...\n2023-07-11 04:39:45.826 [ INFO] Authenticating user mphan for service https://my.cmems-du.eu/motu-web/Motu\n2023-07-11 04:40:00.771 [ INFO] The product is ready for download\n2023-07-11 04:40:00.771 [ INFO] Downloading file (this can take a while)...\n2023-07-11 04:40:01.520 [ INFO] File type: application/x-netcdf\n2023-07-11 04:40:01.520 [ INFO] File size: 2.9 MB (2945744 B)\n2023-07-11 04:40:01.521 [ INFO] Downloading file /home/jovyan/shared/minh-notebooks/motu/cmems_obs-sl_glo_phy-ssh_my_allsat-l4-duacs-0.25deg_P1D/200102.nc\n2023-07-11 04:40:01.870 [ INFO] Progress -   2.9 MB (2.2%)\n2023-07-11 04:40:02.045 [ INFO] Progress -   2.9 MB (4.4%)\n2023-07-11 04:40:02.220 [ INFO] Progress -   2.9 MB (6.7%)\n2023-07-11 04:40:02.220 [ INFO] Progress -   2.9 MB (8.9%)\n2023-07-11 04:40:02.221 [ INFO] Progress -   2.9 MB (11.1%)\n2023-07-11 04:40:02.221 [ INFO] Progress -   2.9 MB (13.3%)\n2023-07-11 04:40:02.395 [ INFO] Progress -   2.9 MB (15.6%)\n2023-07-11 04:40:02.396 [ INFO] Progress -   2.9 MB (17.8%)\n2023-07-11 04:40:02.396 [ INFO] Progress -   2.9 MB (20.0%)\n2023-07-11 04:40:02.396 [ INFO] Progress -   2.9 MB (22.2%)\n2023-07-11 04:40:02.397 [ INFO] Progress -   2.9 MB (24.5%)\n2023-07-11 04:40:02.397 [ INFO] Progress -   2.9 MB (26.7%)\n2023-07-11 04:40:02.570 [ INFO] Progress -   2.9 MB (28.9%)\n2023-07-11 04:40:02.571 [ INFO] Progress -   2.9 MB (31.1%)\n2023-07-11 04:40:02.571 [ INFO] Progress -   2.9 MB (33.4%)\n2023-07-11 04:40:02.571 [ INFO] Progress -   2.9 MB (35.6%)\n2023-07-11 04:40:02.572 [ INFO] Progress -   2.9 MB (37.8%)\n2023-07-11 04:40:02.573 [ INFO] Progress -   2.9 MB (40.0%)\n2023-07-11 04:40:02.573 [ INFO] Progress -   2.9 MB (42.3%)\n2023-07-11 04:40:02.573 [ INFO] Progress -   2.9 MB (44.5%)\n2023-07-11 04:40:02.573 [ INFO] Progress -   2.9 MB (46.7%)\n2023-07-11 04:40:02.573 [ INFO] Progress -   2.9 MB (48.9%)\n2023-07-11 04:40:02.574 [ INFO] Progress -   2.9 MB (51.2%)\n2023-07-11 04:40:02.574 [ INFO] Progress -   2.9 MB (53.4%)\n2023-07-11 04:40:02.745 [ INFO] Progress -   2.9 MB (55.6%)\n2023-07-11 04:40:02.746 [ INFO] Progress -   2.9 MB (57.8%)\n2023-07-11 04:40:02.746 [ INFO] Progress -   2.9 MB (60.1%)\n2023-07-11 04:40:02.747 [ INFO] Progress -   2.9 MB (62.3%)\n2023-07-11 04:40:02.747 [ INFO] Progress -   2.9 MB (64.5%)\n2023-07-11 04:40:02.747 [ INFO] Progress -   2.9 MB (66.7%)\n2023-07-11 04:40:02.747 [ INFO] Progress -   2.9 MB (69.0%)\n2023-07-11 04:40:02.748 [ INFO] Progress -   2.9 MB (71.2%)\n2023-07-11 04:40:02.748 [ INFO] Progress -   2.9 MB (73.4%)\n2023-07-11 04:40:02.749 [ INFO] Progress -   2.9 MB (75.6%)\n2023-07-11 04:40:02.749 [ INFO] Progress -   2.9 MB (77.9%)\n2023-07-11 04:40:02.749 [ INFO] Progress -   2.9 MB (80.1%)\n2023-07-11 04:40:02.749 [ INFO] Progress -   2.9 MB (82.3%)\n2023-07-11 04:40:02.750 [ INFO] Progress -   2.9 MB (84.5%)\n2023-07-11 04:40:02.750 [ INFO] Progress -   2.9 MB (86.8%)\n2023-07-11 04:40:02.750 [ INFO] Progress -   2.9 MB (89.0%)\n2023-07-11 04:40:02.750 [ INFO] Progress -   2.9 MB (91.2%)\n2023-07-11 04:40:02.751 [ INFO] Progress -   2.9 MB (93.4%)\n2023-07-11 04:40:02.751 [ INFO] Progress -   2.9 MB (95.7%)\n2023-07-11 04:40:02.751 [ INFO] Progress -   2.9 MB (97.9%)\n2023-07-11 04:40:02.752 [ INFO] Progress -   2.9 MB (100.0%)\n2023-07-11 04:40:02.752 [ INFO] Processing  time : 0:00:31.949474\n2023-07-11 04:40:02.752 [ INFO] Downloading time : 0:00:01.231045\n2023-07-11 04:40:02.752 [ INFO] Total time       : 0:00:33.180519\n2023-07-11 04:40:02.752 [ INFO] Download rate    : 1.5 MB/s\n2023-07-11 04:40:02.752 [ INFO] Save into        : motu/cmems_obs-sl_glo_phy-ssh_my_allsat-l4-duacs-0.25deg_P1D/200102.nc\n2023-07-11 04:40:02.784 [ INFO] Done\n2023-07-11 04:40:03.024 [ INFO] Asynchronous mode set\n2023-07-11 04:40:03.024 [ INFO] Authenticating user mphan for service https://my.cmems-du.eu/motu-web/Motu\n2023-07-11 04:40:12.056 [ INFO] Requesting file to download (this can take a while)...\n2023-07-11 04:40:18.657 [ INFO] Authenticating user mphan for service https://my.cmems-du.eu/motu-web/Motu\n2023-07-11 04:40:33.365 [ INFO] The product is ready for download\n2023-07-11 04:40:33.365 [ INFO] Downloading file (this can take a while)...\n2023-07-11 04:40:34.109 [ INFO] File type: application/x-netcdf\n2023-07-11 04:40:34.109 [ INFO] File size: 3.3 MB (3260684 B)\n2023-07-11 04:40:34.109 [ INFO] Downloading file /home/jovyan/shared/minh-notebooks/motu/cmems_obs-sl_glo_phy-ssh_my_allsat-l4-duacs-0.25deg_P1D/200103.nc\n2023-07-11 04:40:34.460 [ INFO] Progress -   3.3 MB (2.0%)\n2023-07-11 04:40:34.635 [ INFO] Progress -   3.3 MB (4.0%)\n2023-07-11 04:40:34.808 [ INFO] Progress -   3.3 MB (6.0%)\n2023-07-11 04:40:34.810 [ INFO] Progress -   3.3 MB (8.0%)\n2023-07-11 04:40:34.983 [ INFO] Progress -   3.3 MB (10.0%)\n2023-07-11 04:40:34.984 [ INFO] Progress -   3.3 MB (12.1%)\n2023-07-11 04:40:34.984 [ INFO] Progress -   3.3 MB (14.1%)\n2023-07-11 04:40:34.985 [ INFO] Progress -   3.3 MB (16.1%)\n2023-07-11 04:40:34.985 [ INFO] Progress -   3.3 MB (18.1%)\n2023-07-11 04:40:34.986 [ INFO] Progress -   3.3 MB (20.1%)\n2023-07-11 04:40:35.159 [ INFO] Progress -   3.3 MB (22.1%)\n2023-07-11 04:40:35.159 [ INFO] Progress -   3.3 MB (24.1%)\n2023-07-11 04:40:35.159 [ INFO] Progress -   3.3 MB (26.1%)\n2023-07-11 04:40:35.160 [ INFO] Progress -   3.3 MB (28.1%)\n2023-07-11 04:40:35.160 [ INFO] Progress -   3.3 MB (30.1%)\n2023-07-11 04:40:35.160 [ INFO] Progress -   3.3 MB (32.2%)\n2023-07-11 04:40:35.161 [ INFO] Progress -   3.3 MB (34.2%)\n2023-07-11 04:40:35.161 [ INFO] Progress -   3.3 MB (36.2%)\n2023-07-11 04:40:35.162 [ INFO] Progress -   3.3 MB (38.2%)\n2023-07-11 04:40:35.162 [ INFO] Progress -   3.3 MB (40.2%)\n2023-07-11 04:40:35.162 [ INFO] Progress -   3.3 MB (42.2%)\n2023-07-11 04:40:35.334 [ INFO] Progress -   3.3 MB (44.2%)\n2023-07-11 04:40:35.334 [ INFO] Progress -   3.3 MB (46.2%)\n2023-07-11 04:40:35.334 [ INFO] Progress -   3.3 MB (48.2%)\n2023-07-11 04:40:35.335 [ INFO] Progress -   3.3 MB (50.2%)\n2023-07-11 04:40:35.335 [ INFO] Progress -   3.3 MB (52.3%)\n2023-07-11 04:40:35.335 [ INFO] Progress -   3.3 MB (54.3%)\n2023-07-11 04:40:35.335 [ INFO] Progress -   3.3 MB (56.3%)\n2023-07-11 04:40:35.336 [ INFO] Progress -   3.3 MB (58.3%)\n2023-07-11 04:40:35.336 [ INFO] Progress -   3.3 MB (60.3%)\n2023-07-11 04:40:35.336 [ INFO] Progress -   3.3 MB (62.3%)\n2023-07-11 04:40:35.337 [ INFO] Progress -   3.3 MB (64.3%)\n2023-07-11 04:40:35.337 [ INFO] Progress -   3.3 MB (66.3%)\n2023-07-11 04:40:35.338 [ INFO] Progress -   3.3 MB (68.3%)\n2023-07-11 04:40:35.338 [ INFO] Progress -   3.3 MB (70.3%)\n2023-07-11 04:40:35.338 [ INFO] Progress -   3.3 MB (72.4%)\n2023-07-11 04:40:35.338 [ INFO] Progress -   3.3 MB (74.4%)\n2023-07-11 04:40:35.339 [ INFO] Progress -   3.3 MB (76.4%)\n2023-07-11 04:40:35.339 [ INFO] Progress -   3.3 MB (78.4%)\n2023-07-11 04:40:35.339 [ INFO] Progress -   3.3 MB (80.4%)\n2023-07-11 04:40:35.339 [ INFO] Progress -   3.3 MB (82.4%)\n2023-07-11 04:40:35.340 [ INFO] Progress -   3.3 MB (84.4%)\n2023-07-11 04:40:35.340 [ INFO] Progress -   3.3 MB (86.4%)\n2023-07-11 04:40:35.509 [ INFO] Progress -   3.3 MB (88.4%)\n2023-07-11 04:40:35.509 [ INFO] Progress -   3.3 MB (90.4%)\n2023-07-11 04:40:35.509 [ INFO] Progress -   3.3 MB (92.5%)\n2023-07-11 04:40:35.510 [ INFO] Progress -   3.3 MB (94.5%)\n2023-07-11 04:40:35.510 [ INFO] Progress -   3.3 MB (96.5%)\n2023-07-11 04:40:35.511 [ INFO] Progress -   3.3 MB (98.5%)\n2023-07-11 04:40:35.511 [ INFO] Progress -   3.3 MB (100.0%)\n2023-07-11 04:40:35.511 [ INFO] Processing  time : 0:00:31.085699\n2023-07-11 04:40:35.511 [ INFO] Downloading time : 0:00:01.401936\n2023-07-11 04:40:35.511 [ INFO] Total time       : 0:00:32.487635\n2023-07-11 04:40:35.511 [ INFO] Download rate    : 1.5 MB/s\n2023-07-11 04:40:35.511 [ INFO] Save into        : motu/cmems_obs-sl_glo_phy-ssh_my_allsat-l4-duacs-0.25deg_P1D/200103.nc\n2023-07-11 04:40:35.541 [ INFO] Done\n2023-07-11 04:40:35.783 [ INFO] Asynchronous mode set\n2023-07-11 04:40:35.783 [ INFO] Authenticating user mphan for service https://my.cmems-du.eu/motu-web/Motu\n2023-07-11 04:40:42.416 [ INFO] Requesting file to download (this can take a while)...\n2023-07-11 04:40:51.415 [ INFO] Authenticating user mphan for service https://my.cmems-du.eu/motu-web/Motu\n2023-07-11 04:41:07.906 [ INFO] The product is ready for download\n2023-07-11 04:41:07.906 [ INFO] Downloading file (this can take a while)...\n2023-07-11 04:41:08.658 [ INFO] File type: application/x-netcdf\n2023-07-11 04:41:08.658 [ INFO] File size: 3.2 MB (3155704 B)\n2023-07-11 04:41:08.658 [ INFO] Downloading file /home/jovyan/shared/minh-notebooks/motu/cmems_obs-sl_glo_phy-ssh_my_allsat-l4-duacs-0.25deg_P1D/200104.nc\n2023-07-11 04:41:09.007 [ INFO] Progress -   3.2 MB (2.1%)\n2023-07-11 04:41:09.182 [ INFO] Progress -   3.2 MB (4.2%)\n2023-07-11 04:41:09.183 [ INFO] Progress -   3.2 MB (6.2%)\n2023-07-11 04:41:09.357 [ INFO] Progress -   3.2 MB (8.3%)\n2023-07-11 04:41:09.358 [ INFO] Progress -   3.2 MB (10.4%)\n2023-07-11 04:41:09.358 [ INFO] Progress -   3.2 MB (12.5%)\n2023-07-11 04:41:09.532 [ INFO] Progress -   3.2 MB (14.5%)\n2023-07-11 04:41:09.532 [ INFO] Progress -   3.2 MB (16.6%)\n2023-07-11 04:41:09.533 [ INFO] Progress -   3.2 MB (18.7%)\n2023-07-11 04:41:09.533 [ INFO] Progress -   3.2 MB (20.8%)\n2023-07-11 04:41:09.533 [ INFO] Progress -   3.2 MB (22.8%)\n2023-07-11 04:41:09.534 [ INFO] Progress -   3.2 MB (24.9%)\n2023-07-11 04:41:09.534 [ INFO] Progress -   3.2 MB (27.0%)\n2023-07-11 04:41:09.707 [ INFO] Progress -   3.2 MB (29.1%)\n2023-07-11 04:41:09.708 [ INFO] Progress -   3.2 MB (31.2%)\n2023-07-11 04:41:09.708 [ INFO] Progress -   3.2 MB (33.2%)\n2023-07-11 04:41:09.709 [ INFO] Progress -   3.2 MB (35.3%)\n2023-07-11 04:41:09.709 [ INFO] Progress -   3.2 MB (37.4%)\n2023-07-11 04:41:09.710 [ INFO] Progress -   3.2 MB (39.5%)\n2023-07-11 04:41:09.710 [ INFO] Progress -   3.2 MB (41.5%)\n2023-07-11 04:41:09.710 [ INFO] Progress -   3.2 MB (43.6%)\n2023-07-11 04:41:09.710 [ INFO] Progress -   3.2 MB (45.7%)\n2023-07-11 04:41:09.710 [ INFO] Progress -   3.2 MB (47.8%)\n2023-07-11 04:41:09.711 [ INFO] Progress -   3.2 MB (49.8%)\n2023-07-11 04:41:09.711 [ INFO] Progress -   3.2 MB (51.9%)\n2023-07-11 04:41:09.711 [ INFO] Progress -   3.2 MB (54.0%)\n2023-07-11 04:41:09.711 [ INFO] Progress -   3.2 MB (56.1%)\n2023-07-11 04:41:09.882 [ INFO] Progress -   3.2 MB (58.1%)\n2023-07-11 04:41:09.883 [ INFO] Progress -   3.2 MB (60.2%)\n2023-07-11 04:41:09.883 [ INFO] Progress -   3.2 MB (62.3%)\n2023-07-11 04:41:09.883 [ INFO] Progress -   3.2 MB (64.4%)\n2023-07-11 04:41:09.884 [ INFO] Progress -   3.2 MB (66.5%)\n2023-07-11 04:41:09.884 [ INFO] Progress -   3.2 MB (68.5%)\n2023-07-11 04:41:09.885 [ INFO] Progress -   3.2 MB (70.6%)\n2023-07-11 04:41:09.885 [ INFO] Progress -   3.2 MB (72.7%)\n2023-07-11 04:41:09.885 [ INFO] Progress -   3.2 MB (74.8%)\n2023-07-11 04:41:09.885 [ INFO] Progress -   3.2 MB (76.8%)\n2023-07-11 04:41:09.886 [ INFO] Progress -   3.2 MB (78.9%)\n2023-07-11 04:41:09.886 [ INFO] Progress -   3.2 MB (81.0%)\n2023-07-11 04:41:09.886 [ INFO] Progress -   3.2 MB (83.1%)\n2023-07-11 04:41:09.886 [ INFO] Progress -   3.2 MB (85.1%)\n2023-07-11 04:41:09.887 [ INFO] Progress -   3.2 MB (87.2%)\n2023-07-11 04:41:09.887 [ INFO] Progress -   3.2 MB (89.3%)\n2023-07-11 04:41:09.887 [ INFO] Progress -   3.2 MB (91.4%)\n2023-07-11 04:41:09.887 [ INFO] Progress -   3.2 MB (93.5%)\n2023-07-11 04:41:09.888 [ INFO] Progress -   3.2 MB (95.5%)\n2023-07-11 04:41:09.888 [ INFO] Progress -   3.2 MB (97.6%)\n2023-07-11 04:41:09.888 [ INFO] Progress -   3.2 MB (99.7%)\n2023-07-11 04:41:09.889 [ INFO] Progress -   3.2 MB (100.0%)\n2023-07-11 04:41:09.889 [ INFO] Processing  time : 0:00:32.877246\n2023-07-11 04:41:09.889 [ INFO] Downloading time : 0:00:01.230603\n2023-07-11 04:41:09.889 [ INFO] Total time       : 0:00:34.107849\n2023-07-11 04:41:09.889 [ INFO] Download rate    : 1.6 MB/s\n2023-07-11 04:41:09.889 [ INFO] Save into        : motu/cmems_obs-sl_glo_phy-ssh_my_allsat-l4-duacs-0.25deg_P1D/200104.nc\n2023-07-11 04:41:09.919 [ INFO] Done\n2023-07-11 04:41:10.168 [ INFO] Asynchronous mode set\n2023-07-11 04:41:10.169 [ INFO] Authenticating user mphan for service https://my.cmems-du.eu/motu-web/Motu\n2023-07-11 04:41:17.159 [ INFO] Requesting file to download (this can take a while)...\n2023-07-11 04:41:23.689 [ INFO] Authenticating user mphan for service https://my.cmems-du.eu/motu-web/Motu\n2023-07-11 04:41:40.330 [ INFO] The product is ready for download\n2023-07-11 04:41:40.330 [ INFO] Downloading file (this can take a while)...\n2023-07-11 04:41:41.240 [ INFO] File type: application/x-netcdf\n2023-07-11 04:41:41.240 [ INFO] File size: 3.3 MB (3260684 B)\n2023-07-11 04:41:41.240 [ INFO] Downloading file /home/jovyan/shared/minh-notebooks/motu/cmems_obs-sl_glo_phy-ssh_my_allsat-l4-duacs-0.25deg_P1D/200105.nc\n2023-07-11 04:41:41.413 [ INFO] Progress -   3.3 MB (2.0%)\n2023-07-11 04:41:41.586 [ INFO] Progress -   3.3 MB (4.0%)\n2023-07-11 04:41:41.756 [ INFO] Progress -   3.3 MB (6.0%)\n2023-07-11 04:41:41.758 [ INFO] Progress -   3.3 MB (8.0%)\n2023-07-11 04:41:41.758 [ INFO] Progress -   3.3 MB (10.0%)\n2023-07-11 04:41:41.929 [ INFO] Progress -   3.3 MB (12.1%)\n2023-07-11 04:41:41.929 [ INFO] Progress -   3.3 MB (14.1%)\n2023-07-11 04:41:41.931 [ INFO] Progress -   3.3 MB (16.1%)\n2023-07-11 04:41:41.931 [ INFO] Progress -   3.3 MB (18.1%)\n2023-07-11 04:41:41.931 [ INFO] Progress -   3.3 MB (20.1%)\n2023-07-11 04:41:41.931 [ INFO] Progress -   3.3 MB (22.1%)\n2023-07-11 04:41:41.932 [ INFO] Progress -   3.3 MB (24.1%)\n2023-07-11 04:41:42.101 [ INFO] Progress -   3.3 MB (26.1%)\n2023-07-11 04:41:42.102 [ INFO] Progress -   3.3 MB (28.1%)\n2023-07-11 04:41:42.103 [ INFO] Progress -   3.3 MB (30.1%)\n2023-07-11 04:41:42.103 [ INFO] Progress -   3.3 MB (32.2%)\n2023-07-11 04:41:42.104 [ INFO] Progress -   3.3 MB (34.2%)\n2023-07-11 04:41:42.104 [ INFO] Progress -   3.3 MB (36.2%)\n2023-07-11 04:41:42.105 [ INFO] Progress -   3.3 MB (38.2%)\n2023-07-11 04:41:42.105 [ INFO] Progress -   3.3 MB (40.2%)\n2023-07-11 04:41:42.105 [ INFO] Progress -   3.3 MB (42.2%)\n2023-07-11 04:41:42.105 [ INFO] Progress -   3.3 MB (44.2%)\n2023-07-11 04:41:42.106 [ INFO] Progress -   3.3 MB (46.2%)\n2023-07-11 04:41:42.106 [ INFO] Progress -   3.3 MB (48.2%)\n2023-07-11 04:41:42.274 [ INFO] Progress -   3.3 MB (50.2%)\n2023-07-11 04:41:42.274 [ INFO] Progress -   3.3 MB (52.3%)\n2023-07-11 04:41:42.274 [ INFO] Progress -   3.3 MB (54.3%)\n2023-07-11 04:41:42.275 [ INFO] Progress -   3.3 MB (56.3%)\n2023-07-11 04:41:42.275 [ INFO] Progress -   3.3 MB (58.3%)\n2023-07-11 04:41:42.275 [ INFO] Progress -   3.3 MB (60.3%)\n2023-07-11 04:41:42.276 [ INFO] Progress -   3.3 MB (62.3%)\n2023-07-11 04:41:42.277 [ INFO] Progress -   3.3 MB (64.3%)\n2023-07-11 04:41:42.277 [ INFO] Progress -   3.3 MB (66.3%)\n2023-07-11 04:41:42.278 [ INFO] Progress -   3.3 MB (68.3%)\n2023-07-11 04:41:42.278 [ INFO] Progress -   3.3 MB (70.3%)\n2023-07-11 04:41:42.278 [ INFO] Progress -   3.3 MB (72.4%)\n2023-07-11 04:41:42.278 [ INFO] Progress -   3.3 MB (74.4%)\n2023-07-11 04:41:42.279 [ INFO] Progress -   3.3 MB (76.4%)\n2023-07-11 04:41:42.279 [ INFO] Progress -   3.3 MB (78.4%)\n2023-07-11 04:41:42.279 [ INFO] Progress -   3.3 MB (80.4%)\n2023-07-11 04:41:42.279 [ INFO] Progress -   3.3 MB (82.4%)\n2023-07-11 04:41:42.279 [ INFO] Progress -   3.3 MB (84.4%)\n2023-07-11 04:41:42.280 [ INFO] Progress -   3.3 MB (86.4%)\n2023-07-11 04:41:42.280 [ INFO] Progress -   3.3 MB (88.4%)\n2023-07-11 04:41:42.280 [ INFO] Progress -   3.3 MB (90.4%)\n2023-07-11 04:41:42.280 [ INFO] Progress -   3.3 MB (92.5%)\n2023-07-11 04:41:42.281 [ INFO] Progress -   3.3 MB (94.5%)\n2023-07-11 04:41:42.281 [ INFO] Progress -   3.3 MB (96.5%)\n2023-07-11 04:41:42.282 [ INFO] Progress -   3.3 MB (98.5%)\n2023-07-11 04:41:42.282 [ INFO] Progress -   3.3 MB (100.0%)\n2023-07-11 04:41:42.282 [ INFO] Processing  time : 0:00:31.073788\n2023-07-11 04:41:42.282 [ INFO] Downloading time : 0:00:01.041936\n2023-07-11 04:41:42.282 [ INFO] Total time       : 0:00:32.115724\n2023-07-11 04:41:42.282 [ INFO] Download rate    : 1.7 MB/s\n2023-07-11 04:41:42.282 [ INFO] Save into        : motu/cmems_obs-sl_glo_phy-ssh_my_allsat-l4-duacs-0.25deg_P1D/200105.nc\n2023-07-11 04:41:42.313 [ INFO] Done\n2023-07-11 04:41:42.557 [ INFO] Asynchronous mode set\n2023-07-11 04:41:42.557 [ INFO] Authenticating user mphan for service https://my.cmems-du.eu/motu-web/Motu\n2023-07-11 04:41:51.498 [ INFO] Requesting file to download (this can take a while)...\n2023-07-11 04:41:58.720 [ INFO] Authenticating user mphan for service https://my.cmems-du.eu/motu-web/Motu\n2023-07-11 04:42:13.967 [ INFO] The product is ready for download\n2023-07-11 04:42:13.967 [ INFO] Downloading file (this can take a while)...\n2023-07-11 04:42:14.703 [ INFO] File type: application/x-netcdf\n2023-07-11 04:42:14.703 [ INFO] File size: 3.2 MB (3155704 B)\n2023-07-11 04:42:14.704 [ INFO] Downloading file /home/jovyan/shared/minh-notebooks/motu/cmems_obs-sl_glo_phy-ssh_my_allsat-l4-duacs-0.25deg_P1D/200106.nc\n2023-07-11 04:42:15.048 [ INFO] Progress -   3.2 MB (2.1%)\n2023-07-11 04:42:15.220 [ INFO] Progress -   3.2 MB (4.2%)\n2023-07-11 04:42:15.221 [ INFO] Progress -   3.2 MB (6.2%)\n2023-07-11 04:42:15.392 [ INFO] Progress -   3.2 MB (8.3%)\n2023-07-11 04:42:15.393 [ INFO] Progress -   3.2 MB (10.4%)\n2023-07-11 04:42:15.394 [ INFO] Progress -   3.2 MB (12.5%)\n2023-07-11 04:42:15.564 [ INFO] Progress -   3.2 MB (14.5%)\n2023-07-11 04:42:15.565 [ INFO] Progress -   3.2 MB (16.6%)\n2023-07-11 04:42:15.565 [ INFO] Progress -   3.2 MB (18.7%)\n2023-07-11 04:42:15.565 [ INFO] Progress -   3.2 MB (20.8%)\n2023-07-11 04:42:15.566 [ INFO] Progress -   3.2 MB (22.8%)\n2023-07-11 04:42:15.566 [ INFO] Progress -   3.2 MB (24.9%)\n2023-07-11 04:42:15.566 [ INFO] Progress -   3.2 MB (27.0%)\n2023-07-11 04:42:15.736 [ INFO] Progress -   3.2 MB (29.1%)\n2023-07-11 04:42:15.737 [ INFO] Progress -   3.2 MB (31.2%)\n2023-07-11 04:42:15.737 [ INFO] Progress -   3.2 MB (33.2%)\n2023-07-11 04:42:15.739 [ INFO] Progress -   3.2 MB (35.3%)\n2023-07-11 04:42:15.739 [ INFO] Progress -   3.2 MB (37.4%)\n2023-07-11 04:42:15.739 [ INFO] Progress -   3.2 MB (39.5%)\n2023-07-11 04:42:15.739 [ INFO] Progress -   3.2 MB (41.5%)\n2023-07-11 04:42:15.740 [ INFO] Progress -   3.2 MB (43.6%)\n2023-07-11 04:42:15.740 [ INFO] Progress -   3.2 MB (45.7%)\n2023-07-11 04:42:15.740 [ INFO] Progress -   3.2 MB (47.8%)\n2023-07-11 04:42:15.740 [ INFO] Progress -   3.2 MB (49.8%)\n2023-07-11 04:42:15.741 [ INFO] Progress -   3.2 MB (51.9%)\n2023-07-11 04:42:15.741 [ INFO] Progress -   3.2 MB (54.0%)\n2023-07-11 04:42:15.741 [ INFO] Progress -   3.2 MB (56.1%)\n2023-07-11 04:42:15.909 [ INFO] Progress -   3.2 MB (58.1%)\n2023-07-11 04:42:15.909 [ INFO] Progress -   3.2 MB (60.2%)\n2023-07-11 04:42:15.910 [ INFO] Progress -   3.2 MB (62.3%)\n2023-07-11 04:42:15.910 [ INFO] Progress -   3.2 MB (64.4%)\n2023-07-11 04:42:15.910 [ INFO] Progress -   3.2 MB (66.5%)\n2023-07-11 04:42:15.911 [ INFO] Progress -   3.2 MB (68.5%)\n2023-07-11 04:42:15.911 [ INFO] Progress -   3.2 MB (70.6%)\n2023-07-11 04:42:15.911 [ INFO] Progress -   3.2 MB (72.7%)\n2023-07-11 04:42:15.912 [ INFO] Progress -   3.2 MB (74.8%)\n2023-07-11 04:42:15.912 [ INFO] Progress -   3.2 MB (76.8%)\n2023-07-11 04:42:15.912 [ INFO] Progress -   3.2 MB (78.9%)\n2023-07-11 04:42:15.912 [ INFO] Progress -   3.2 MB (81.0%)\n2023-07-11 04:42:15.913 [ INFO] Progress -   3.2 MB (83.1%)\n2023-07-11 04:42:15.913 [ INFO] Progress -   3.2 MB (85.1%)\n2023-07-11 04:42:15.913 [ INFO] Progress -   3.2 MB (87.2%)\n2023-07-11 04:42:15.913 [ INFO] Progress -   3.2 MB (89.3%)\n2023-07-11 04:42:15.914 [ INFO] Progress -   3.2 MB (91.4%)\n2023-07-11 04:42:15.914 [ INFO] Progress -   3.2 MB (93.5%)\n2023-07-11 04:42:15.914 [ INFO] Progress -   3.2 MB (95.5%)\n2023-07-11 04:42:15.914 [ INFO] Progress -   3.2 MB (97.6%)\n2023-07-11 04:42:15.914 [ INFO] Progress -   3.2 MB (99.7%)\n2023-07-11 04:42:15.915 [ INFO] Progress -   3.2 MB (100.0%)\n2023-07-11 04:42:15.915 [ INFO] Processing  time : 0:00:32.148979\n2023-07-11 04:42:15.915 [ INFO] Downloading time : 0:00:01.211730\n2023-07-11 04:42:15.915 [ INFO] Total time       : 0:00:33.360709\n2023-07-11 04:42:15.915 [ INFO] Download rate    : 1.6 MB/s\n2023-07-11 04:42:15.915 [ INFO] Save into        : motu/cmems_obs-sl_glo_phy-ssh_my_allsat-l4-duacs-0.25deg_P1D/200106.nc\n2023-07-11 04:42:15.945 [ INFO] Done\n2023-07-11 04:42:16.186 [ INFO] Asynchronous mode set\n2023-07-11 04:42:16.186 [ INFO] Authenticating user mphan for service https://my.cmems-du.eu/motu-web/Motu\n2023-07-11 04:42:22.965 [ INFO] Requesting file to download (this can take a while)...\n2023-07-11 04:42:29.614 [ INFO] Authenticating user mphan for service https://my.cmems-du.eu/motu-web/Motu\n2023-07-11 04:42:44.253 [ INFO] The product is ready for download\n2023-07-11 04:42:44.254 [ INFO] Downloading file (this can take a while)...\n2023-07-11 04:42:45.157 [ INFO] File type: application/x-netcdf\n2023-07-11 04:42:45.157 [ INFO] File size: 3.3 MB (3260684 B)\n2023-07-11 04:42:45.157 [ INFO] Downloading file /home/jovyan/shared/minh-notebooks/motu/cmems_obs-sl_glo_phy-ssh_my_allsat-l4-duacs-0.25deg_P1D/200107.nc\n2023-07-11 04:42:45.330 [ INFO] Progress -   3.3 MB (2.0%)\n2023-07-11 04:42:45.503 [ INFO] Progress -   3.3 MB (4.0%)\n2023-07-11 04:42:45.673 [ INFO] Progress -   3.3 MB (6.0%)\n2023-07-11 04:42:45.675 [ INFO] Progress -   3.3 MB (8.0%)\n2023-07-11 04:42:45.675 [ INFO] Progress -   3.3 MB (10.0%)\n2023-07-11 04:42:45.846 [ INFO] Progress -   3.3 MB (12.1%)\n2023-07-11 04:42:45.846 [ INFO] Progress -   3.3 MB (14.1%)\n2023-07-11 04:42:45.848 [ INFO] Progress -   3.3 MB (16.1%)\n2023-07-11 04:42:45.848 [ INFO] Progress -   3.3 MB (18.1%)\n2023-07-11 04:42:45.848 [ INFO] Progress -   3.3 MB (20.1%)\n2023-07-11 04:42:45.848 [ INFO] Progress -   3.3 MB (22.1%)\n2023-07-11 04:42:45.849 [ INFO] Progress -   3.3 MB (24.1%)\n2023-07-11 04:42:46.018 [ INFO] Progress -   3.3 MB (26.1%)\n2023-07-11 04:42:46.019 [ INFO] Progress -   3.3 MB (28.1%)\n2023-07-11 04:42:46.020 [ INFO] Progress -   3.3 MB (30.1%)\n2023-07-11 04:42:46.020 [ INFO] Progress -   3.3 MB (32.2%)\n2023-07-11 04:42:46.021 [ INFO] Progress -   3.3 MB (34.2%)\n2023-07-11 04:42:46.021 [ INFO] Progress -   3.3 MB (36.2%)\n2023-07-11 04:42:46.022 [ INFO] Progress -   3.3 MB (38.2%)\n2023-07-11 04:42:46.022 [ INFO] Progress -   3.3 MB (40.2%)\n2023-07-11 04:42:46.022 [ INFO] Progress -   3.3 MB (42.2%)\n2023-07-11 04:42:46.022 [ INFO] Progress -   3.3 MB (44.2%)\n2023-07-11 04:42:46.023 [ INFO] Progress -   3.3 MB (46.2%)\n2023-07-11 04:42:46.023 [ INFO] Progress -   3.3 MB (48.2%)\n2023-07-11 04:42:46.190 [ INFO] Progress -   3.3 MB (50.2%)\n2023-07-11 04:42:46.191 [ INFO] Progress -   3.3 MB (52.3%)\n2023-07-11 04:42:46.191 [ INFO] Progress -   3.3 MB (54.3%)\n2023-07-11 04:42:46.191 [ INFO] Progress -   3.3 MB (56.3%)\n2023-07-11 04:42:46.192 [ INFO] Progress -   3.3 MB (58.3%)\n2023-07-11 04:42:46.193 [ INFO] Progress -   3.3 MB (60.3%)\n2023-07-11 04:42:46.193 [ INFO] Progress -   3.3 MB (62.3%)\n2023-07-11 04:42:46.193 [ INFO] Progress -   3.3 MB (64.3%)\n2023-07-11 04:42:46.194 [ INFO] Progress -   3.3 MB (66.3%)\n2023-07-11 04:42:46.194 [ INFO] Progress -   3.3 MB (68.3%)\n2023-07-11 04:42:46.195 [ INFO] Progress -   3.3 MB (70.3%)\n2023-07-11 04:42:46.195 [ INFO] Progress -   3.3 MB (72.4%)\n2023-07-11 04:42:46.195 [ INFO] Progress -   3.3 MB (74.4%)\n2023-07-11 04:42:46.195 [ INFO] Progress -   3.3 MB (76.4%)\n2023-07-11 04:42:46.196 [ INFO] Progress -   3.3 MB (78.4%)\n2023-07-11 04:42:46.196 [ INFO] Progress -   3.3 MB (80.4%)\n2023-07-11 04:42:46.196 [ INFO] Progress -   3.3 MB (82.4%)\n2023-07-11 04:42:46.196 [ INFO] Progress -   3.3 MB (84.4%)\n2023-07-11 04:42:46.197 [ INFO] Progress -   3.3 MB (86.4%)\n2023-07-11 04:42:46.197 [ INFO] Progress -   3.3 MB (88.4%)\n2023-07-11 04:42:46.197 [ INFO] Progress -   3.3 MB (90.4%)\n2023-07-11 04:42:46.197 [ INFO] Progress -   3.3 MB (92.5%)\n2023-07-11 04:42:46.198 [ INFO] Progress -   3.3 MB (94.5%)\n2023-07-11 04:42:46.198 [ INFO] Progress -   3.3 MB (96.5%)\n2023-07-11 04:42:46.199 [ INFO] Progress -   3.3 MB (98.5%)\n2023-07-11 04:42:46.199 [ INFO] Progress -   3.3 MB (100.0%)\n2023-07-11 04:42:46.199 [ INFO] Processing  time : 0:00:28.973583\n2023-07-11 04:42:46.199 [ INFO] Downloading time : 0:00:01.042066\n2023-07-11 04:42:46.199 [ INFO] Total time       : 0:00:30.015649\n2023-07-11 04:42:46.199 [ INFO] Download rate    : 1.7 MB/s\n2023-07-11 04:42:46.199 [ INFO] Save into        : motu/cmems_obs-sl_glo_phy-ssh_my_allsat-l4-duacs-0.25deg_P1D/200107.nc\n2023-07-11 04:42:46.232 [ INFO] Done\n2023-07-11 04:42:46.476 [ INFO] Asynchronous mode set\n2023-07-11 04:42:46.476 [ INFO] Authenticating user mphan for service https://my.cmems-du.eu/motu-web/Motu\n2023-07-11 04:42:52.332 [ INFO] Requesting file to download (this can take a while)...\n2023-07-11 04:42:59.632 [ INFO] Authenticating user mphan for service https://my.cmems-du.eu/motu-web/Motu\n2023-07-11 04:43:13.124 [ INFO] The product is ready for download\n2023-07-11 04:43:13.124 [ INFO] Downloading file (this can take a while)...\n2023-07-11 04:43:13.856 [ INFO] File type: application/x-netcdf\n2023-07-11 04:43:13.857 [ INFO] File size: 3.3 MB (3260684 B)\n2023-07-11 04:43:13.857 [ INFO] Downloading file /home/jovyan/shared/minh-notebooks/motu/cmems_obs-sl_glo_phy-ssh_my_allsat-l4-duacs-0.25deg_P1D/200108.nc\n2023-07-11 04:43:14.200 [ INFO] Progress -   3.3 MB (2.0%)\n2023-07-11 04:43:14.373 [ INFO] Progress -   3.3 MB (4.0%)\n2023-07-11 04:43:14.545 [ INFO] Progress -   3.3 MB (6.0%)\n2023-07-11 04:43:14.546 [ INFO] Progress -   3.3 MB (8.0%)\n2023-07-11 04:43:14.718 [ INFO] Progress -   3.3 MB (10.0%)\n2023-07-11 04:43:14.719 [ INFO] Progress -   3.3 MB (12.1%)\n2023-07-11 04:43:14.719 [ INFO] Progress -   3.3 MB (14.1%)\n2023-07-11 04:43:14.719 [ INFO] Progress -   3.3 MB (16.1%)\n2023-07-11 04:43:14.720 [ INFO] Progress -   3.3 MB (18.1%)\n2023-07-11 04:43:14.890 [ INFO] Progress -   3.3 MB (20.1%)\n2023-07-11 04:43:14.891 [ INFO] Progress -   3.3 MB (22.1%)\n2023-07-11 04:43:14.891 [ INFO] Progress -   3.3 MB (24.1%)\n2023-07-11 04:43:14.892 [ INFO] Progress -   3.3 MB (26.1%)\n2023-07-11 04:43:14.893 [ INFO] Progress -   3.3 MB (28.1%)\n2023-07-11 04:43:14.893 [ INFO] Progress -   3.3 MB (30.1%)\n2023-07-11 04:43:14.893 [ INFO] Progress -   3.3 MB (32.2%)\n2023-07-11 04:43:14.895 [ INFO] Progress -   3.3 MB (34.2%)\n2023-07-11 04:43:14.895 [ INFO] Progress -   3.3 MB (36.2%)\n2023-07-11 04:43:15.062 [ INFO] Progress -   3.3 MB (38.2%)\n2023-07-11 04:43:15.063 [ INFO] Progress -   3.3 MB (40.2%)\n2023-07-11 04:43:15.064 [ INFO] Progress -   3.3 MB (42.2%)\n2023-07-11 04:43:15.064 [ INFO] Progress -   3.3 MB (44.2%)\n2023-07-11 04:43:15.064 [ INFO] Progress -   3.3 MB (46.2%)\n2023-07-11 04:43:15.064 [ INFO] Progress -   3.3 MB (48.2%)\n2023-07-11 04:43:15.065 [ INFO] Progress -   3.3 MB (50.2%)\n2023-07-11 04:43:15.065 [ INFO] Progress -   3.3 MB (52.3%)\n2023-07-11 04:43:15.066 [ INFO] Progress -   3.3 MB (54.3%)\n2023-07-11 04:43:15.066 [ INFO] Progress -   3.3 MB (56.3%)\n2023-07-11 04:43:15.067 [ INFO] Progress -   3.3 MB (58.3%)\n2023-07-11 04:43:15.067 [ INFO] Progress -   3.3 MB (60.3%)\n2023-07-11 04:43:15.067 [ INFO] Progress -   3.3 MB (62.3%)\n2023-07-11 04:43:15.068 [ INFO] Progress -   3.3 MB (64.3%)\n2023-07-11 04:43:15.068 [ INFO] Progress -   3.3 MB (66.3%)\n2023-07-11 04:43:15.069 [ INFO] Progress -   3.3 MB (68.3%)\n2023-07-11 04:43:15.069 [ INFO] Progress -   3.3 MB (70.3%)\n2023-07-11 04:43:15.069 [ INFO] Progress -   3.3 MB (72.4%)\n2023-07-11 04:43:15.069 [ INFO] Progress -   3.3 MB (74.4%)\n2023-07-11 04:43:15.070 [ INFO] Progress -   3.3 MB (76.4%)\n2023-07-11 04:43:15.235 [ INFO] Progress -   3.3 MB (78.4%)\n2023-07-11 04:43:15.236 [ INFO] Progress -   3.3 MB (80.4%)\n2023-07-11 04:43:15.236 [ INFO] Progress -   3.3 MB (82.4%)\n2023-07-11 04:43:15.236 [ INFO] Progress -   3.3 MB (84.4%)\n2023-07-11 04:43:15.236 [ INFO] Progress -   3.3 MB (86.4%)\n2023-07-11 04:43:15.237 [ INFO] Progress -   3.3 MB (88.4%)\n2023-07-11 04:43:15.237 [ INFO] Progress -   3.3 MB (90.4%)\n2023-07-11 04:43:15.237 [ INFO] Progress -   3.3 MB (92.5%)\n2023-07-11 04:43:15.237 [ INFO] Progress -   3.3 MB (94.5%)\n2023-07-11 04:43:15.238 [ INFO] Progress -   3.3 MB (96.5%)\n2023-07-11 04:43:15.239 [ INFO] Progress -   3.3 MB (98.5%)\n2023-07-11 04:43:15.239 [ INFO] Progress -   3.3 MB (100.0%)\n2023-07-11 04:43:15.239 [ INFO] Processing  time : 0:00:27.384750\n2023-07-11 04:43:15.239 [ INFO] Downloading time : 0:00:01.382641\n2023-07-11 04:43:15.239 [ INFO] Total time       : 0:00:28.767391\n2023-07-11 04:43:15.239 [ INFO] Download rate    : 1.5 MB/s\n2023-07-11 04:43:15.239 [ INFO] Save into        : motu/cmems_obs-sl_glo_phy-ssh_my_allsat-l4-duacs-0.25deg_P1D/200108.nc\n2023-07-11 04:43:15.269 [ INFO] Done\n2023-07-11 04:43:15.513 [ INFO] Asynchronous mode set\n2023-07-11 04:43:15.513 [ INFO] Authenticating user mphan for service https://my.cmems-du.eu/motu-web/Motu\n2023-07-11 04:43:22.274 [ INFO] Requesting file to download (this can take a while)...\n2023-07-11 04:43:29.643 [ INFO] Authenticating user mphan for service https://my.cmems-du.eu/motu-web/Motu\n2023-07-11 04:43:47.109 [ INFO] The product is ready for download\n2023-07-11 04:43:47.109 [ INFO] Downloading file (this can take a while)...\n2023-07-11 04:43:47.846 [ INFO] File type: application/x-netcdf\n2023-07-11 04:43:47.846 [ INFO] File size: 3.2 MB (3155704 B)\n2023-07-11 04:43:47.847 [ INFO] Downloading file /home/jovyan/shared/minh-notebooks/motu/cmems_obs-sl_glo_phy-ssh_my_allsat-l4-duacs-0.25deg_P1D/200109.nc\n2023-07-11 04:43:48.190 [ INFO] Progress -   3.2 MB (2.1%)\n2023-07-11 04:43:48.363 [ INFO] Progress -   3.2 MB (4.2%)\n2023-07-11 04:43:48.535 [ INFO] Progress -   3.2 MB (6.2%)\n2023-07-11 04:43:48.536 [ INFO] Progress -   3.2 MB (8.3%)\n2023-07-11 04:43:48.708 [ INFO] Progress -   3.2 MB (10.4%)\n2023-07-11 04:43:48.708 [ INFO] Progress -   3.2 MB (12.5%)\n2023-07-11 04:43:48.708 [ INFO] Progress -   3.2 MB (14.5%)\n2023-07-11 04:43:48.709 [ INFO] Progress -   3.2 MB (16.6%)\n2023-07-11 04:43:48.709 [ INFO] Progress -   3.2 MB (18.7%)\n2023-07-11 04:43:48.880 [ INFO] Progress -   3.2 MB (20.8%)\n2023-07-11 04:43:48.880 [ INFO] Progress -   3.2 MB (22.8%)\n2023-07-11 04:43:48.880 [ INFO] Progress -   3.2 MB (24.9%)\n2023-07-11 04:43:48.881 [ INFO] Progress -   3.2 MB (27.0%)\n2023-07-11 04:43:48.881 [ INFO] Progress -   3.2 MB (29.1%)\n2023-07-11 04:43:48.881 [ INFO] Progress -   3.2 MB (31.2%)\n2023-07-11 04:43:48.881 [ INFO] Progress -   3.2 MB (33.2%)\n2023-07-11 04:43:48.882 [ INFO] Progress -   3.2 MB (35.3%)\n2023-07-11 04:43:48.883 [ INFO] Progress -   3.2 MB (37.4%)\n2023-07-11 04:43:49.052 [ INFO] Progress -   3.2 MB (39.5%)\n2023-07-11 04:43:49.052 [ INFO] Progress -   3.2 MB (41.5%)\n2023-07-11 04:43:49.053 [ INFO] Progress -   3.2 MB (43.6%)\n2023-07-11 04:43:49.053 [ INFO] Progress -   3.2 MB (45.7%)\n2023-07-11 04:43:49.053 [ INFO] Progress -   3.2 MB (47.8%)\n2023-07-11 04:43:49.053 [ INFO] Progress -   3.2 MB (49.8%)\n2023-07-11 04:43:49.054 [ INFO] Progress -   3.2 MB (51.9%)\n2023-07-11 04:43:49.054 [ INFO] Progress -   3.2 MB (54.0%)\n2023-07-11 04:43:49.054 [ INFO] Progress -   3.2 MB (56.1%)\n2023-07-11 04:43:49.055 [ INFO] Progress -   3.2 MB (58.1%)\n2023-07-11 04:43:49.055 [ INFO] Progress -   3.2 MB (60.2%)\n2023-07-11 04:43:49.055 [ INFO] Progress -   3.2 MB (62.3%)\n2023-07-11 04:43:49.055 [ INFO] Progress -   3.2 MB (64.4%)\n2023-07-11 04:43:49.056 [ INFO] Progress -   3.2 MB (66.5%)\n2023-07-11 04:43:49.056 [ INFO] Progress -   3.2 MB (68.5%)\n2023-07-11 04:43:49.057 [ INFO] Progress -   3.2 MB (70.6%)\n2023-07-11 04:43:49.057 [ INFO] Progress -   3.2 MB (72.7%)\n2023-07-11 04:43:49.057 [ INFO] Progress -   3.2 MB (74.8%)\n2023-07-11 04:43:49.057 [ INFO] Progress -   3.2 MB (76.8%)\n2023-07-11 04:43:49.225 [ INFO] Progress -   3.2 MB (78.9%)\n2023-07-11 04:43:49.225 [ INFO] Progress -   3.2 MB (81.0%)\n2023-07-11 04:43:49.225 [ INFO] Progress -   3.2 MB (83.1%)\n2023-07-11 04:43:49.226 [ INFO] Progress -   3.2 MB (85.1%)\n2023-07-11 04:43:49.226 [ INFO] Progress -   3.2 MB (87.2%)\n2023-07-11 04:43:49.226 [ INFO] Progress -   3.2 MB (89.3%)\n2023-07-11 04:43:49.226 [ INFO] Progress -   3.2 MB (91.4%)\n2023-07-11 04:43:49.227 [ INFO] Progress -   3.2 MB (93.5%)\n2023-07-11 04:43:49.227 [ INFO] Progress -   3.2 MB (95.5%)\n2023-07-11 04:43:49.227 [ INFO] Progress -   3.2 MB (97.6%)\n2023-07-11 04:43:49.227 [ INFO] Progress -   3.2 MB (99.7%)\n2023-07-11 04:43:49.228 [ INFO] Progress -   3.2 MB (100.0%)\n2023-07-11 04:43:49.228 [ INFO] Processing  time : 0:00:32.336252\n2023-07-11 04:43:49.228 [ INFO] Downloading time : 0:00:01.381585\n2023-07-11 04:43:49.228 [ INFO] Total time       : 0:00:33.717837\n2023-07-11 04:43:49.228 [ INFO] Download rate    : 1.5 MB/s\n2023-07-11 04:43:49.228 [ INFO] Save into        : motu/cmems_obs-sl_glo_phy-ssh_my_allsat-l4-duacs-0.25deg_P1D/200109.nc\n2023-07-11 04:43:49.283 [ INFO] Done\n2023-07-11 04:43:49.521 [ INFO] Asynchronous mode set\n2023-07-11 04:43:49.521 [ INFO] Authenticating user mphan for service https://my.cmems-du.eu/motu-web/Motu\n2023-07-11 04:43:58.915 [ INFO] Requesting file to download (this can take a while)...\n2023-07-11 04:44:08.361 [ INFO] Authenticating user mphan for service https://my.cmems-du.eu/motu-web/Motu\n2023-07-11 04:44:23.488 [ INFO] The product is ready for download\n2023-07-11 04:44:23.488 [ INFO] Downloading file (this can take a while)...\n2023-07-11 04:44:24.223 [ INFO] File type: application/x-netcdf\n2023-07-11 04:44:24.223 [ INFO] File size: 3.3 MB (3260684 B)\n2023-07-11 04:44:24.223 [ INFO] Downloading file /home/jovyan/shared/minh-notebooks/motu/cmems_obs-sl_glo_phy-ssh_my_allsat-l4-duacs-0.25deg_P1D/200110.nc\n2023-07-11 04:44:24.568 [ INFO] Progress -   3.3 MB (2.0%)\n2023-07-11 04:44:24.740 [ INFO] Progress -   3.3 MB (4.0%)\n2023-07-11 04:44:24.741 [ INFO] Progress -   3.3 MB (6.0%)\n2023-07-11 04:44:24.913 [ INFO] Progress -   3.3 MB (8.0%)\n2023-07-11 04:44:24.913 [ INFO] Progress -   3.3 MB (10.0%)\n2023-07-11 04:44:24.914 [ INFO] Progress -   3.3 MB (12.1%)\n2023-07-11 04:44:25.084 [ INFO] Progress -   3.3 MB (14.1%)\n2023-07-11 04:44:25.085 [ INFO] Progress -   3.3 MB (16.1%)\n2023-07-11 04:44:25.086 [ INFO] Progress -   3.3 MB (18.1%)\n2023-07-11 04:44:25.086 [ INFO] Progress -   3.3 MB (20.1%)\n2023-07-11 04:44:25.086 [ INFO] Progress -   3.3 MB (22.1%)\n2023-07-11 04:44:25.087 [ INFO] Progress -   3.3 MB (24.1%)\n2023-07-11 04:44:25.087 [ INFO] Progress -   3.3 MB (26.1%)\n2023-07-11 04:44:25.257 [ INFO] Progress -   3.3 MB (28.1%)\n2023-07-11 04:44:25.257 [ INFO] Progress -   3.3 MB (30.1%)\n2023-07-11 04:44:25.258 [ INFO] Progress -   3.3 MB (32.2%)\n2023-07-11 04:44:25.259 [ INFO] Progress -   3.3 MB (34.2%)\n2023-07-11 04:44:25.259 [ INFO] Progress -   3.3 MB (36.2%)\n2023-07-11 04:44:25.260 [ INFO] Progress -   3.3 MB (38.2%)\n2023-07-11 04:44:25.260 [ INFO] Progress -   3.3 MB (40.2%)\n2023-07-11 04:44:25.260 [ INFO] Progress -   3.3 MB (42.2%)\n2023-07-11 04:44:25.260 [ INFO] Progress -   3.3 MB (44.2%)\n2023-07-11 04:44:25.261 [ INFO] Progress -   3.3 MB (46.2%)\n2023-07-11 04:44:25.261 [ INFO] Progress -   3.3 MB (48.2%)\n2023-07-11 04:44:25.261 [ INFO] Progress -   3.3 MB (50.2%)\n2023-07-11 04:44:25.261 [ INFO] Progress -   3.3 MB (52.3%)\n2023-07-11 04:44:25.429 [ INFO] Progress -   3.3 MB (54.3%)\n2023-07-11 04:44:25.429 [ INFO] Progress -   3.3 MB (56.3%)\n2023-07-11 04:44:25.430 [ INFO] Progress -   3.3 MB (58.3%)\n2023-07-11 04:44:25.430 [ INFO] Progress -   3.3 MB (60.3%)\n2023-07-11 04:44:25.430 [ INFO] Progress -   3.3 MB (62.3%)\n2023-07-11 04:44:25.431 [ INFO] Progress -   3.3 MB (64.3%)\n2023-07-11 04:44:25.432 [ INFO] Progress -   3.3 MB (66.3%)\n2023-07-11 04:44:25.432 [ INFO] Progress -   3.3 MB (68.3%)\n2023-07-11 04:44:25.432 [ INFO] Progress -   3.3 MB (70.3%)\n2023-07-11 04:44:25.432 [ INFO] Progress -   3.3 MB (72.4%)\n2023-07-11 04:44:25.433 [ INFO] Progress -   3.3 MB (74.4%)\n2023-07-11 04:44:25.433 [ INFO] Progress -   3.3 MB (76.4%)\n2023-07-11 04:44:25.433 [ INFO] Progress -   3.3 MB (78.4%)\n2023-07-11 04:44:25.433 [ INFO] Progress -   3.3 MB (80.4%)\n2023-07-11 04:44:25.434 [ INFO] Progress -   3.3 MB (82.4%)\n2023-07-11 04:44:25.434 [ INFO] Progress -   3.3 MB (84.4%)\n2023-07-11 04:44:25.434 [ INFO] Progress -   3.3 MB (86.4%)\n2023-07-11 04:44:25.435 [ INFO] Progress -   3.3 MB (88.4%)\n2023-07-11 04:44:25.435 [ INFO] Progress -   3.3 MB (90.4%)\n2023-07-11 04:44:25.435 [ INFO] Progress -   3.3 MB (92.5%)\n2023-07-11 04:44:25.435 [ INFO] Progress -   3.3 MB (94.5%)\n2023-07-11 04:44:25.436 [ INFO] Progress -   3.3 MB (96.5%)\n2023-07-11 04:44:25.436 [ INFO] Progress -   3.3 MB (98.5%)\n2023-07-11 04:44:25.437 [ INFO] Progress -   3.3 MB (100.0%)\n2023-07-11 04:44:25.437 [ INFO] Processing  time : 0:00:34.702582\n2023-07-11 04:44:25.437 [ INFO] Downloading time : 0:00:01.213662\n2023-07-11 04:44:25.437 [ INFO] Total time       : 0:00:35.916244\n2023-07-11 04:44:25.437 [ INFO] Download rate    : 1.7 MB/s\n2023-07-11 04:44:25.437 [ INFO] Save into        : motu/cmems_obs-sl_glo_phy-ssh_my_allsat-l4-duacs-0.25deg_P1D/200110.nc\n2023-07-11 04:44:25.467 [ INFO] Done\n2023-07-11 04:44:25.708 [ INFO] Asynchronous mode set\n2023-07-11 04:44:25.709 [ INFO] Authenticating user mphan for service https://my.cmems-du.eu/motu-web/Motu\n2023-07-11 04:44:33.978 [ INFO] Requesting file to download (this can take a while)...\n2023-07-11 04:44:41.900 [ INFO] Authenticating user mphan for service https://my.cmems-du.eu/motu-web/Motu\n2023-07-11 04:44:56.540 [ INFO] The product is ready for download\n2023-07-11 04:44:56.540 [ INFO] Downloading file (this can take a while)...\n2023-07-11 04:44:57.296 [ INFO] File type: application/x-netcdf\n2023-07-11 04:44:57.297 [ INFO] File size: 3.2 MB (3155704 B)\n2023-07-11 04:44:57.297 [ INFO] Downloading file /home/jovyan/shared/minh-notebooks/motu/cmems_obs-sl_glo_phy-ssh_my_allsat-l4-duacs-0.25deg_P1D/200111.nc\n2023-07-11 04:44:57.640 [ INFO] Progress -   3.2 MB (2.1%)\n2023-07-11 04:44:57.813 [ INFO] Progress -   3.2 MB (4.2%)\n2023-07-11 04:44:57.985 [ INFO] Progress -   3.2 MB (6.2%)\n2023-07-11 04:44:57.986 [ INFO] Progress -   3.2 MB (8.3%)\n2023-07-11 04:44:58.157 [ INFO] Progress -   3.2 MB (10.4%)\n2023-07-11 04:44:58.158 [ INFO] Progress -   3.2 MB (12.5%)\n2023-07-11 04:44:58.158 [ INFO] Progress -   3.2 MB (14.5%)\n2023-07-11 04:44:58.158 [ INFO] Progress -   3.2 MB (16.6%)\n2023-07-11 04:44:58.159 [ INFO] Progress -   3.2 MB (18.7%)\n2023-07-11 04:44:58.330 [ INFO] Progress -   3.2 MB (20.8%)\n2023-07-11 04:44:58.331 [ INFO] Progress -   3.2 MB (22.8%)\n2023-07-11 04:44:58.331 [ INFO] Progress -   3.2 MB (24.9%)\n2023-07-11 04:44:58.331 [ INFO] Progress -   3.2 MB (27.0%)\n2023-07-11 04:44:58.332 [ INFO] Progress -   3.2 MB (29.1%)\n2023-07-11 04:44:58.332 [ INFO] Progress -   3.2 MB (31.2%)\n2023-07-11 04:44:58.332 [ INFO] Progress -   3.2 MB (33.2%)\n2023-07-11 04:44:58.333 [ INFO] Progress -   3.2 MB (35.3%)\n2023-07-11 04:44:58.333 [ INFO] Progress -   3.2 MB (37.4%)\n2023-07-11 04:44:58.333 [ INFO] Progress -   3.2 MB (39.5%)\n2023-07-11 04:44:58.502 [ INFO] Progress -   3.2 MB (41.5%)\n2023-07-11 04:44:58.503 [ INFO] Progress -   3.2 MB (43.6%)\n2023-07-11 04:44:58.503 [ INFO] Progress -   3.2 MB (45.7%)\n2023-07-11 04:44:58.503 [ INFO] Progress -   3.2 MB (47.8%)\n2023-07-11 04:44:58.504 [ INFO] Progress -   3.2 MB (49.8%)\n2023-07-11 04:44:58.504 [ INFO] Progress -   3.2 MB (51.9%)\n2023-07-11 04:44:58.504 [ INFO] Progress -   3.2 MB (54.0%)\n2023-07-11 04:44:58.505 [ INFO] Progress -   3.2 MB (56.1%)\n2023-07-11 04:44:58.505 [ INFO] Progress -   3.2 MB (58.1%)\n2023-07-11 04:44:58.505 [ INFO] Progress -   3.2 MB (60.2%)\n2023-07-11 04:44:58.505 [ INFO] Progress -   3.2 MB (62.3%)\n2023-07-11 04:44:58.506 [ INFO] Progress -   3.2 MB (64.4%)\n2023-07-11 04:44:58.506 [ INFO] Progress -   3.2 MB (66.5%)\n2023-07-11 04:44:58.507 [ INFO] Progress -   3.2 MB (68.5%)\n2023-07-11 04:44:58.507 [ INFO] Progress -   3.2 MB (70.6%)\n2023-07-11 04:44:58.507 [ INFO] Progress -   3.2 MB (72.7%)\n2023-07-11 04:44:58.507 [ INFO] Progress -   3.2 MB (74.8%)\n2023-07-11 04:44:58.508 [ INFO] Progress -   3.2 MB (76.8%)\n2023-07-11 04:44:58.508 [ INFO] Progress -   3.2 MB (78.9%)\n2023-07-11 04:44:58.508 [ INFO] Progress -   3.2 MB (81.0%)\n2023-07-11 04:44:58.675 [ INFO] Progress -   3.2 MB (83.1%)\n2023-07-11 04:44:58.675 [ INFO] Progress -   3.2 MB (85.1%)\n2023-07-11 04:44:58.676 [ INFO] Progress -   3.2 MB (87.2%)\n2023-07-11 04:44:58.676 [ INFO] Progress -   3.2 MB (89.3%)\n2023-07-11 04:44:58.676 [ INFO] Progress -   3.2 MB (91.4%)\n2023-07-11 04:44:58.676 [ INFO] Progress -   3.2 MB (93.5%)\n2023-07-11 04:44:58.677 [ INFO] Progress -   3.2 MB (95.5%)\n2023-07-11 04:44:58.677 [ INFO] Progress -   3.2 MB (97.6%)\n2023-07-11 04:44:58.677 [ INFO] Progress -   3.2 MB (99.7%)\n2023-07-11 04:44:58.678 [ INFO] Progress -   3.2 MB (100.0%)\n2023-07-11 04:44:58.678 [ INFO] Processing  time : 0:00:31.590883\n2023-07-11 04:44:58.678 [ INFO] Downloading time : 0:00:01.381213\n2023-07-11 04:44:58.678 [ INFO] Total time       : 0:00:32.972096\n2023-07-11 04:44:58.678 [ INFO] Download rate    : 1.5 MB/s\n2023-07-11 04:44:58.678 [ INFO] Save into        : motu/cmems_obs-sl_glo_phy-ssh_my_allsat-l4-duacs-0.25deg_P1D/200111.nc\n2023-07-11 04:44:58.710 [ INFO] Done\n2023-07-11 04:44:58.961 [ INFO] Asynchronous mode set\n2023-07-11 04:44:58.961 [ INFO] Authenticating user mphan for service https://my.cmems-du.eu/motu-web/Motu\n2023-07-11 04:45:04.292 [ INFO] Requesting file to download (this can take a while)...\n2023-07-11 04:45:11.513 [ INFO] Authenticating user mphan for service https://my.cmems-du.eu/motu-web/Motu\n2023-07-11 04:45:26.798 [ INFO] The product is ready for download\n2023-07-11 04:45:26.798 [ INFO] Downloading file (this can take a while)...\n2023-07-11 04:45:27.551 [ INFO] File type: application/x-netcdf\n2023-07-11 04:45:27.551 [ INFO] File size: 3.3 MB (3260684 B)\n2023-07-11 04:45:27.551 [ INFO] Downloading file /home/jovyan/shared/minh-notebooks/motu/cmems_obs-sl_glo_phy-ssh_my_allsat-l4-duacs-0.25deg_P1D/200112.nc\n2023-07-11 04:45:27.895 [ INFO] Progress -   3.3 MB (2.0%)\n2023-07-11 04:45:28.067 [ INFO] Progress -   3.3 MB (4.0%)\n2023-07-11 04:45:28.069 [ INFO] Progress -   3.3 MB (6.0%)\n2023-07-11 04:45:28.240 [ INFO] Progress -   3.3 MB (8.0%)\n2023-07-11 04:45:28.240 [ INFO] Progress -   3.3 MB (10.0%)\n2023-07-11 04:45:28.241 [ INFO] Progress -   3.3 MB (12.1%)\n2023-07-11 04:45:28.412 [ INFO] Progress -   3.3 MB (14.1%)\n2023-07-11 04:45:28.413 [ INFO] Progress -   3.3 MB (16.1%)\n2023-07-11 04:45:28.413 [ INFO] Progress -   3.3 MB (18.1%)\n2023-07-11 04:45:28.413 [ INFO] Progress -   3.3 MB (20.1%)\n2023-07-11 04:45:28.414 [ INFO] Progress -   3.3 MB (22.1%)\n2023-07-11 04:45:28.414 [ INFO] Progress -   3.3 MB (24.1%)\n2023-07-11 04:45:28.414 [ INFO] Progress -   3.3 MB (26.1%)\n2023-07-11 04:45:28.585 [ INFO] Progress -   3.3 MB (28.1%)\n2023-07-11 04:45:28.585 [ INFO] Progress -   3.3 MB (30.1%)\n2023-07-11 04:45:28.586 [ INFO] Progress -   3.3 MB (32.2%)\n2023-07-11 04:45:28.587 [ INFO] Progress -   3.3 MB (34.2%)\n2023-07-11 04:45:28.587 [ INFO] Progress -   3.3 MB (36.2%)\n2023-07-11 04:45:28.587 [ INFO] Progress -   3.3 MB (38.2%)\n2023-07-11 04:45:28.587 [ INFO] Progress -   3.3 MB (40.2%)\n2023-07-11 04:45:28.588 [ INFO] Progress -   3.3 MB (42.2%)\n2023-07-11 04:45:28.588 [ INFO] Progress -   3.3 MB (44.2%)\n2023-07-11 04:45:28.588 [ INFO] Progress -   3.3 MB (46.2%)\n2023-07-11 04:45:28.588 [ INFO] Progress -   3.3 MB (48.2%)\n2023-07-11 04:45:28.588 [ INFO] Progress -   3.3 MB (50.2%)\n2023-07-11 04:45:28.589 [ INFO] Progress -   3.3 MB (52.3%)\n2023-07-11 04:45:28.589 [ INFO] Progress -   3.3 MB (54.3%)\n2023-07-11 04:45:28.757 [ INFO] Progress -   3.3 MB (56.3%)\n2023-07-11 04:45:28.758 [ INFO] Progress -   3.3 MB (58.3%)\n2023-07-11 04:45:28.758 [ INFO] Progress -   3.3 MB (60.3%)\n2023-07-11 04:45:28.758 [ INFO] Progress -   3.3 MB (62.3%)\n2023-07-11 04:45:28.758 [ INFO] Progress -   3.3 MB (64.3%)\n2023-07-11 04:45:28.759 [ INFO] Progress -   3.3 MB (66.3%)\n2023-07-11 04:45:28.759 [ INFO] Progress -   3.3 MB (68.3%)\n2023-07-11 04:45:28.760 [ INFO] Progress -   3.3 MB (70.3%)\n2023-07-11 04:45:28.760 [ INFO] Progress -   3.3 MB (72.4%)\n2023-07-11 04:45:28.760 [ INFO] Progress -   3.3 MB (74.4%)\n2023-07-11 04:45:28.760 [ INFO] Progress -   3.3 MB (76.4%)\n2023-07-11 04:45:28.761 [ INFO] Progress -   3.3 MB (78.4%)\n2023-07-11 04:45:28.761 [ INFO] Progress -   3.3 MB (80.4%)\n2023-07-11 04:45:28.761 [ INFO] Progress -   3.3 MB (82.4%)\n2023-07-11 04:45:28.761 [ INFO] Progress -   3.3 MB (84.4%)\n2023-07-11 04:45:28.762 [ INFO] Progress -   3.3 MB (86.4%)\n2023-07-11 04:45:28.762 [ INFO] Progress -   3.3 MB (88.4%)\n2023-07-11 04:45:28.762 [ INFO] Progress -   3.3 MB (90.4%)\n2023-07-11 04:45:28.762 [ INFO] Progress -   3.3 MB (92.5%)\n2023-07-11 04:45:28.762 [ INFO] Progress -   3.3 MB (94.5%)\n2023-07-11 04:45:28.763 [ INFO] Progress -   3.3 MB (96.5%)\n2023-07-11 04:45:28.763 [ INFO] Progress -   3.3 MB (98.5%)\n2023-07-11 04:45:28.764 [ INFO] Progress -   3.3 MB (100.0%)\n2023-07-11 04:45:28.764 [ INFO] Processing  time : 0:00:28.592514\n2023-07-11 04:45:28.764 [ INFO] Downloading time : 0:00:01.212567\n2023-07-11 04:45:28.764 [ INFO] Total time       : 0:00:29.805081\n2023-07-11 04:45:28.764 [ INFO] Download rate    : 1.7 MB/s\n2023-07-11 04:45:28.764 [ INFO] Save into        : motu/cmems_obs-sl_glo_phy-ssh_my_allsat-l4-duacs-0.25deg_P1D/200112.nc\n2023-07-11 04:45:28.818 [ INFO] Done\n\n\nDISCLAIMER: Downloading time may be extremely slow due to processing time from Motu for each request. We can increase temporal range for each one (6 months or 1 year, for example) but it may exceed the size limit MOTU allows. Feel free to modify the prompt generation as it fits to balance speed and size.\n\n\n\nimage.png",
    "crumbs": [
      "Python Tutorials",
      "Data Access",
      "Download sliced data using MOTU-Client in Copernicus"
    ]
  },
  {
    "objectID": "python-notebooks/01_MOTU_Downloader.html#combine-data",
    "href": "python-notebooks/01_MOTU_Downloader.html#combine-data",
    "title": "Download sliced data using MOTU-Client in Copernicus",
    "section": "Combine data",
    "text": "Combine data\n\nds = xr.open_mfdataset('demonstrated data/motu/cmems_obs-sl_glo_phy-ssh_my_allsat-l4-duacs-0.25deg_P1D//*.nc')\n\n\nds\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt;\nDimensions:    (time: 731, latitude: 81, longitude: 81)\nCoordinates:\n  * latitude   (latitude) float32 4.875 5.125 5.375 5.625 ... 24.38 24.62 24.88\n  * time       (time) datetime64[ns] 2000-01-01 2000-01-02 ... 2001-12-31\n  * longitude  (longitude) float32 59.88 60.12 60.38 60.62 ... 79.38 79.62 79.88\nData variables:\n    adt        (time, latitude, longitude) float64 dask.array&lt;chunksize=(31, 81, 81), meta=np.ndarray&gt;\n    crs        (time) int32 -2147483647 -2147483647 ... -2147483647 -2147483647\n    sla        (time, latitude, longitude) float64 dask.array&lt;chunksize=(31, 81, 81), meta=np.ndarray&gt;\nAttributes: (12/45)\n    Conventions:                                    CF-1.6\n    FROM_ORIGINAL_FILE__Metadata_Conventions:       Unidata Dataset Discovery...\n    cdm_data_type:                                  Grid\n    comment:                                        Sea Surface Height measur...\n    contact:                                        servicedesk.cmems@mercato...\n    creator_email:                                  servicedesk.cmems@mercato...\n    ...                                             ...\n    time_coverage_duration:                         P1D\n    time_coverage_end:                              2022-08-04T12:00:00Z\n    time_coverage_resolution:                       P1D\n    time_coverage_start:                            2022-08-03T12:00:00Z\n    title:                                          DT merged all satellites ...\n    _CoordSysBuilder:                               ucar.nc2.dataset.conv.CF1...xarray.DatasetDimensions:time: 731latitude: 81longitude: 81Coordinates: (3)latitude(latitude)float324.875 5.125 5.375 ... 24.62 24.88axis :Ybounds :lat_bndslong_name :Latitudestandard_name :latitudeunits :degrees_northvalid_max :24.875valid_min :4.875_ChunkSizes :50_CoordinateAxisType :Latarray([ 4.875,  5.125,  5.375,  5.625,  5.875,  6.125,  6.375,  6.625,  6.875,\n        7.125,  7.375,  7.625,  7.875,  8.125,  8.375,  8.625,  8.875,  9.125,\n        9.375,  9.625,  9.875, 10.125, 10.375, 10.625, 10.875, 11.125, 11.375,\n       11.625, 11.875, 12.125, 12.375, 12.625, 12.875, 13.125, 13.375, 13.625,\n       13.875, 14.125, 14.375, 14.625, 14.875, 15.125, 15.375, 15.625, 15.875,\n       16.125, 16.375, 16.625, 16.875, 17.125, 17.375, 17.625, 17.875, 18.125,\n       18.375, 18.625, 18.875, 19.125, 19.375, 19.625, 19.875, 20.125, 20.375,\n       20.625, 20.875, 21.125, 21.375, 21.625, 21.875, 22.125, 22.375, 22.625,\n       22.875, 23.125, 23.375, 23.625, 23.875, 24.125, 24.375, 24.625, 24.875],\n      dtype=float32)time(time)datetime64[ns]2000-01-01 ... 2001-12-31axis :Tlong_name :Timestandard_name :time_ChunkSizes :1_CoordinateAxisType :Timevalid_min :18262.0valid_max :18292.0array(['2000-01-01T00:00:00.000000000', '2000-01-02T00:00:00.000000000',\n       '2000-01-03T00:00:00.000000000', ..., '2001-12-29T00:00:00.000000000',\n       '2001-12-30T00:00:00.000000000', '2001-12-31T00:00:00.000000000'],\n      dtype='datetime64[ns]')longitude(longitude)float3259.88 60.12 60.38 ... 79.62 79.88axis :Xbounds :lon_bndslong_name :Longitudestandard_name :longitudeunits :degrees_eastvalid_max :79.875valid_min :59.875_ChunkSizes :50_CoordinateAxisType :Lonarray([59.875, 60.125, 60.375, 60.625, 60.875, 61.125, 61.375, 61.625, 61.875,\n       62.125, 62.375, 62.625, 62.875, 63.125, 63.375, 63.625, 63.875, 64.125,\n       64.375, 64.625, 64.875, 65.125, 65.375, 65.625, 65.875, 66.125, 66.375,\n       66.625, 66.875, 67.125, 67.375, 67.625, 67.875, 68.125, 68.375, 68.625,\n       68.875, 69.125, 69.375, 69.625, 69.875, 70.125, 70.375, 70.625, 70.875,\n       71.125, 71.375, 71.625, 71.875, 72.125, 72.375, 72.625, 72.875, 73.125,\n       73.375, 73.625, 73.875, 74.125, 74.375, 74.625, 74.875, 75.125, 75.375,\n       75.625, 75.875, 76.125, 76.375, 76.625, 76.875, 77.125, 77.375, 77.625,\n       77.875, 78.125, 78.375, 78.625, 78.875, 79.125, 79.375, 79.625, 79.875],\n      dtype=float32)Data variables: (3)adt(time, latitude, longitude)float64dask.array&lt;chunksize=(31, 81, 81), meta=np.ndarray&gt;comment :The absolute dynamic topography is the sea surface height above geoid; the adt is obtained as follows: adt=sla+mdt where mdt is the mean dynamic topography; see the product user manual for detailsgrid_mapping :crslong_name :Absolute dynamic topographystandard_name :sea_surface_height_above_geoidunits :m_ChunkSizes :[ 1 50 50]\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n36.59 MiB\n1.55 MiB\n\n\nShape\n(731, 81, 81)\n(31, 81, 81)\n\n\nDask graph\n24 chunks in 49 graph layers\n\n\nData type\nfloat64 numpy.ndarray\n\n\n\n\n\n\n\n\ncrs\n\n\n(time)\n\n\nint32\n\n\n-2147483647 ... -2147483647\n\n\n\n\ncomment :\n\nThis is a container variable that describes the grid_mapping used by the data in this file. This variable does not contain any data; only information about the geographic coordinate system.\n\ninverse_flattening :\n\n298.257\n\ngrid_mapping_name :\n\nlatitude_longitude\n\nsemi_major_axis :\n\n6378136.3\n\n_CoordinateTransformType :\n\nProjection\n\n_CoordinateAxisTypes :\n\nGeoX GeoY\n\n\n\n\narray([-2147483647, -2147483647, -2147483647, -2147483647, -2147483647,\n       -2147483647, -2147483647, -2147483647, -2147483647, -2147483647,\n       -2147483647, -2147483647, -2147483647, -2147483647, -2147483647,\n       -2147483647, -2147483647, -2147483647, -2147483647, -2147483647,\n       -2147483647, -2147483647, -2147483647, -2147483647, -2147483647,\n       -2147483647, -2147483647, -2147483647, -2147483647, -2147483647,\n       -2147483647, -2147483647, -2147483647, -2147483647, -2147483647,\n       -2147483647, -2147483647, -2147483647, -2147483647, -2147483647,\n       -2147483647, -2147483647, -2147483647, -2147483647, -2147483647,\n       -2147483647, -2147483647, -2147483647, -2147483647, -2147483647,\n       -2147483647, -2147483647, -2147483647, -2147483647, -2147483647,\n       -2147483647, -2147483647, -2147483647, -2147483647, -2147483647,\n       -2147483647, -2147483647, -2147483647, -2147483647, -2147483647,\n       -2147483647, -2147483647, -2147483647, -2147483647, -2147483647,\n       -2147483647, -2147483647, -2147483647, -2147483647, -2147483647,\n       -2147483647, -2147483647, -2147483647, -2147483647, -2147483647,\n       -2147483647, -2147483647, -2147483647, -2147483647, -2147483647,\n       -2147483647, -2147483647, -2147483647, -2147483647, -2147483647,\n       -2147483647, -2147483647, -2147483647, -2147483647, -2147483647,\n       -2147483647, -2147483647, -2147483647, -2147483647, -2147483647,\n...\n       -2147483647, -2147483647, -2147483647, -2147483647, -2147483647,\n       -2147483647, -2147483647, -2147483647, -2147483647, -2147483647,\n       -2147483647, -2147483647, -2147483647, -2147483647, -2147483647,\n       -2147483647, -2147483647, -2147483647, -2147483647, -2147483647,\n       -2147483647, -2147483647, -2147483647, -2147483647, -2147483647,\n       -2147483647, -2147483647, -2147483647, -2147483647, -2147483647,\n       -2147483647, -2147483647, -2147483647, -2147483647, -2147483647,\n       -2147483647, -2147483647, -2147483647, -2147483647, -2147483647,\n       -2147483647, -2147483647, -2147483647, -2147483647, -2147483647,\n       -2147483647, -2147483647, -2147483647, -2147483647, -2147483647,\n       -2147483647, -2147483647, -2147483647, -2147483647, -2147483647,\n       -2147483647, -2147483647, -2147483647, -2147483647, -2147483647,\n       -2147483647, -2147483647, -2147483647, -2147483647, -2147483647,\n       -2147483647, -2147483647, -2147483647, -2147483647, -2147483647,\n       -2147483647, -2147483647, -2147483647, -2147483647, -2147483647,\n       -2147483647, -2147483647, -2147483647, -2147483647, -2147483647,\n       -2147483647, -2147483647, -2147483647, -2147483647, -2147483647,\n       -2147483647, -2147483647, -2147483647, -2147483647, -2147483647,\n       -2147483647, -2147483647, -2147483647, -2147483647, -2147483647,\n       -2147483647], dtype=int32)\n\n\nsla\n\n\n(time, latitude, longitude)\n\n\nfloat64\n\n\ndask.array&lt;chunksize=(31, 81, 81), meta=np.ndarray&gt;\n\n\n\n\nancillary_variables :\n\nerr_sla\n\ncomment :\n\nThe sea level anomaly is the sea surface height above mean sea surface; it is referenced to the [1993, 2012] period; see the product user manual for details\n\ngrid_mapping :\n\ncrs\n\nlong_name :\n\nSea level anomaly\n\nstandard_name :\n\nsea_surface_height_above_sea_level\n\nunits :\n\nm\n\n_ChunkSizes :\n\n[ 1 50 50]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n36.59 MiB\n1.55 MiB\n\n\nShape\n(731, 81, 81)\n(31, 81, 81)\n\n\nDask graph\n24 chunks in 49 graph layers\n\n\nData type\nfloat64 numpy.ndarray\n\n\n\n\n\n\n\n\nAttributes: (45)Conventions :CF-1.6FROM_ORIGINAL_FILE__Metadata_Conventions :Unidata Dataset Discovery v1.0cdm_data_type :Gridcomment :Sea Surface Height measured by Altimetry and derived variablescontact :servicedesk.cmems@mercator-ocean.eucreator_email :servicedesk.cmems@mercator-ocean.eucreator_name :CMEMS - Sea Level Thematic Assembly Centercreator_url :http://marine.copernicus.eudate_created :2023-03-01T17:56:26Zdate_issued :2023-03-01T17:56:26Zdate_modified :2023-03-01T17:56:26ZFROM_ORIGINAL_FILE__geospatial_lat_max :89.875FROM_ORIGINAL_FILE__geospatial_lat_min :-89.875FROM_ORIGINAL_FILE__geospatial_lat_resolution :0.25FROM_ORIGINAL_FILE__geospatial_lat_units :degrees_northFROM_ORIGINAL_FILE__geospatial_lon_max :179.875FROM_ORIGINAL_FILE__geospatial_lon_min :-179.875FROM_ORIGINAL_FILE__geospatial_lon_resolution :0.25FROM_ORIGINAL_FILE__geospatial_lon_units :degrees_eastgeospatial_vertical_max :0.0geospatial_vertical_min :0.0geospatial_vertical_positive :downgeospatial_vertical_resolution :pointgeospatial_vertical_units :mhistory :2023-03-01 17:56:27Z: Creationinstitution :CLS, CNESkeywords :Oceans &gt; Ocean Topography &gt; Sea Surface Heightkeywords_vocabulary :NetCDF COARDS Climate and Forecast Standard Nameslicense :http://marine.copernicus.eu/web/27-service-commitments-and-licence.phpFROM_ORIGINAL_FILE__platform :Altika Drifting Phase, Cryosat-2 New Orbit, Haiyang-2B, Jason-3 Interleaved, Sentinel-3A, Sentinel-3B, Sentinel-6Aprocessing_level :L4FROM_ORIGINAL_FILE__product_version :vDec2021project :COPERNICUS MARINE ENVIRONMENT MONITORING SERVICE (CMEMS)references :http://marine.copernicus.euFROM_ORIGINAL_FILE__software_version :7.1_DUACS_DT2021_baselinesource :Altimetry measurementsssalto_duacs_comment :The reference mission used for the altimeter inter-calibration processing is Topex/Poseidon between 1993-01-01 and 2002-04-23, Jason-1 between 2002-04-24 and 2008-10-18, OSTM/Jason-2 between 2008-10-19 and 2016-06-25, Jason-3 between 2016-06-25 and 2022-02-09, and Sentinel-6A from 2022-02-10.standard_name_vocabulary :NetCDF Climate and Forecast (CF) Metadata Convention Standard Name Table v37summary :SSALTO/DUACS Delayed-Time Level-4 sea surface height and derived variables measured by multi-satellite altimetry observations over Global Ocean.time_coverage_duration :P1Dtime_coverage_end :2022-08-04T12:00:00Ztime_coverage_resolution :P1Dtime_coverage_start :2022-08-03T12:00:00Ztitle :DT merged all satellites Global Ocean Gridded SSALTO/DUACS Sea Surface Height L4 product and derived variables_CoordSysBuilder :ucar.nc2.dataset.conv.CF1Convention",
    "crumbs": [
      "Python Tutorials",
      "Data Access",
      "Download sliced data using MOTU-Client in Copernicus"
    ]
  },
  {
    "objectID": "python-notebooks/01_ERA5_S3_Downloader.html#variables",
    "href": "python-notebooks/01_ERA5_S3_Downloader.html#variables",
    "title": "Download Copernicus ERA5 Data with S3 without logging in",
    "section": "Variables",
    "text": "Variables\nThe table below lists the 18 ERA5 variables that are available on S3. All variables are surface or single level parameters sourced from the HRES sub-daily forecast stream.\n\n\n\nVariable Name\n\n\nFile Name\n\n\nVariable type (fc/an)\n\n\n\n\n10 metre U wind component\n\n\neastward_wind_at_10_metres.nc\n\n\nan\n\n\n\n\n10 metre V wind component\n\n\nnorthward_wind_at_10_metres.nc\n\n\nan\n\n\n\n\n100 metre U wind component\n\n\neastward_wind_at_100_metres.nc\n\n\nan\n\n\n\n\n100 metre V wind component\n\n\nnorthward_wind_at_100_metres.nc\n\n\nan\n\n\n\n\n2 metre dew point temperature\n\n\ndew_point_temperature_at_2_metres.nc\n\n\nan\n\n\n\n\n2 metre temperature\n\n\nair_temperature_at_2_metres.nc\n\n\nan\n\n\n\n\n2 metres maximum temperature since previous post-processing\n\n\nair_temperature_at_2_metres_1hour_Maximum.nc\n\n\nfc\n\n\n\n\n2 metres minimum temperature since previous post-processing\n\n\nair_temperature_at_2_metres_1hour_Minimum.nc\n\n\nfc\n\n\n\n\nMean sea level pressure\n\n\nair_pressure_at_mean_sea_level.nc\n\n\nan\n\n\n\n\nSea surface temperature\n\n\nsea_surface_temperature.nc\n\n\nan\n\n\n\n\nMean wave period\n\n\nsea_surface_wave_mean_period.nc\n\n\n\n\n\n\nMean direction of waves\n\n\nsea_surface_wave_from_direction.nc\n\n\n\n\n\n\nSignificant height of combined wind waves and swell\n\n\nsignificant_height_of_wind_and_swell_waves.nc\n\n\n\n\n\n\nSnow density\n\n\nsnow_density.nc\n\n\nan\n\n\n\n\nSnow depth\n\n\nlwe_thickness_of_surface_snow_amount.nc\n\n\nan\n\n\n\n\nSurface pressure\n\n\nsurface_air_pressure.nc\n\n\nan\n\n\n\n\nSurface solar radiation downwards\n\n\nintegral_wrt_time_of_surface_direct_downwelling_shortwave_flux_in_air_1hour_Accumulation.nc\n\n\nfc\n\n\n\n\nTotal precipitation\n\n\nprecipitation_amount_1hour_Accumulation.nc\n\n\nfc\n\n\n\nFor my dataset, we collect air temperature (at 2m), sea surface temperature, and u and v wind components so that we can compute speed and direction later.",
    "crumbs": [
      "Python Tutorials",
      "Data Access",
      "Download Copernicus ERA5 Data with S3 without logging in"
    ]
  },
  {
    "objectID": "python-notebooks/01_ERA5_S3_Downloader.html#import-necessary-libraries",
    "href": "python-notebooks/01_ERA5_S3_Downloader.html#import-necessary-libraries",
    "title": "Download Copernicus ERA5 Data with S3 without logging in",
    "section": "Import necessary libraries",
    "text": "Import necessary libraries\n\nimport boto3\nimport botocore\nimport datetime\nimport matplotlib.pyplot as plt\nimport os\nimport xarray as xr\nimport numpy as np\nimport pandas as pd\nimport sys",
    "crumbs": [
      "Python Tutorials",
      "Data Access",
      "Download Copernicus ERA5 Data with S3 without logging in"
    ]
  },
  {
    "objectID": "python-notebooks/01_ERA5_S3_Downloader.html#download-data",
    "href": "python-notebooks/01_ERA5_S3_Downloader.html#download-data",
    "title": "Download Copernicus ERA5 Data with S3 without logging in",
    "section": "Download data",
    "text": "Download data\n\nera5_bucket = 'era5-pds'\nclient = boto3.client('s3', config=botocore.client.Config(signature_version=botocore.UNSIGNED))\n\n\ndef download_era5_s3(var_era5, month_start, month_end, lat1=5, lat2=25, lon1=60, lon2=80):\n    \"\"\"\n    var_era5: variable name\n    month_start: formatted as YYYY-MM \n    month_end: formatted as YYYY-MM (right-exclusive)\n    \"\"\"\n    s3_data_ptrn = '{year}/{month}/data/{var}.nc'\n    \n    path_temp_folder = 'demonstrated data/era5/temp'\n    path_var_folder = f'demonstrated data/era5/{var_era5}'\n    if not os.path.exists(path_temp_folder):\n        os.makedirs(path_temp_folder)\n    if not os.path.exists(path_var_folder):\n        os.makedirs(path_var_folder)\n    \n    data_file_ptrn = os.path.join(path_temp_folder,'{year}{month}_{var}.nc')\n    sliced_data_file_ptrn = os.path.join(path_var_folder, '{year}{month}_{var}.nc')\n    months = pd.date_range(month_start, month_end, freq='M')\n    for month in months:\n        s3_data_key = s3_data_ptrn.format(year=month.year, month=\"{:02d}\".format(month.month), var=var_era5)\n        data_file = data_file_ptrn.format(year=month.year, month=\"{:02d}\".format(month.month), var=var_era5)\n        if not os.path.isfile(data_file): # check if file already exists\n            print(\"Downloading %s from S3...\" % s3_data_key)\n            client.download_file(era5_bucket, s3_data_key, data_file)\n\n        export_file = sliced_data_file_ptrn.format(year = month.year, month = \"{:02d}\".format(month.month), var= var_era5)\n        xr.open_dataset(data_file).sel(lat=slice(lat2, lat1), lon=slice(lon1, lon2)).to_netcdf(export_file)\n        os.remove(data_file)\n\n\n# download data for 4 variables we need \n# consult available names in the table above in the file name column (remove .nc)\n\n# month_end is not included in dataset (right-exclusive)\ndownload_era5_s3(var_era5='eastward_wind_at_10_metres', month_start='2003-01', month_end='2003-03')\ndownload_era5_s3(var_era5='northward_wind_at_10_metres', month_start='2003-01', month_end='2003-03')\n\nDownloading 2003/01/data/eastward_wind_at_10_metres.nc from S3...\nDownloading 2003/02/data/eastward_wind_at_10_metres.nc from S3...\nDownloading 2003/01/data/northward_wind_at_10_metres.nc from S3...\nDownloading 2003/02/data/northward_wind_at_10_metres.nc from S3...",
    "crumbs": [
      "Python Tutorials",
      "Data Access",
      "Download Copernicus ERA5 Data with S3 without logging in"
    ]
  },
  {
    "objectID": "projects/upwelling_ml/intro.html#project-leads",
    "href": "projects/upwelling_ml/intro.html#project-leads",
    "title": "Using image machine learning to study upwelling in the Indian Ocean",
    "section": "Project Leads",
    "text": "Project Leads\n\nJiarui Yu (University of Washington), GitHub\nMinh Phan (University of Washington), GitHub\nEli Holmes (NOAA Fisheries/University of Washington), GitHub"
  },
  {
    "objectID": "projects/upwelling_ml/intro.html#background",
    "href": "projects/upwelling_ml/intro.html#background",
    "title": "Using image machine learning to study upwelling in the Indian Ocean",
    "section": "Background",
    "text": "Background\nStudy of coastal upwelling dynamics using image machine-learning algorithms applied to remote-sensing data PI: Eli Holmes, Northwest Fisheries Science Center, NOAA Fisheries, Seattle WA\nCoastal upwelling is a key driver of productive fisheries. It is largely driven by coastal winds that produce wind stress perpendicular to the coast. This wind stress pulls surface water away from the coast and pulls deep, cold, nutrient rich water to the surface. Strong and consistent coastal upwelling happens in specific regions around the world and supports strong fisheries in these regions. An interesting and curious aspect of upwelling is that it is influenced by ocean-atmosphere teleconnections—correlations in ocean climate across long-distances. In plain language, it means that a pattern that arises in one part of the world’s oceans will be seen later on the other side of the world in a very different part of the world. One of these teleconnections is between temperature anomalies in the North Atlantic Ocean and coastal upwelling in the Arabian Sea.\n\nIn this project, we are exploring the use of machine learning and image machine learning to study relationships between coastal upwelling and the physical oceanography of the Arabian Sea. The goal is to explore whether these approaches provide a promising way to develop predictive models for upwelling intensity. For the project, we are using Python libraries for implementing neural networks with image inputs, including TensorFlow, PyTorch, and Keras."
  },
  {
    "objectID": "projects/hackathon-projects.html",
    "href": "projects/hackathon-projects.html",
    "title": "Hackathon Projects",
    "section": "",
    "text": "We will fill in the projects here at the end of week 1.",
    "crumbs": [
      "Projects",
      "Hackathon Projects"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "NOAA HackDays 2024",
    "section": "",
    "text": "Welcome to the NOAA Fisheries HackDays focused on geospatial analysis using ocean ‘big data’.\nGo to set-up for the basic orientation and how to get on the JupyterHub.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#links",
    "href": "index.html#links",
    "title": "NOAA HackDays 2024",
    "section": "Links",
    "text": "Links\n\nGitHub repo: https://github.com/nmfs-opensci/NOAAHackDays-2024\nJupyterHub: https://dhub.opensci.live/ Make sure to choose an R image.\nDiscussions: https://github.com/nmfs-opensci/NOAAHackDays-2024/discussions",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "content/shell.html#summary",
    "href": "content/shell.html#summary",
    "title": "Terminal or Shell",
    "section": "Summary",
    "text": "Summary\nIn this tutorial, we will provide a brief introduction to\n\nWhat is the terminal/shell\nHow to get to the terminal in Jupyter Lab and RStudio\nNavigating around folders in the terminal\nClosing the terminal window"
  },
  {
    "objectID": "content/shell.html#what-is-the-terminal-or-shell",
    "href": "content/shell.html#what-is-the-terminal-or-shell",
    "title": "Terminal or Shell",
    "section": "What is the terminal or shell?",
    "text": "What is the terminal or shell?\nA way to interact with your computer from text commands instead of a graphical user interface (GUI). There are a few different types of shells and there are slight differences in syntax. However, we will be using very basic commands so the syntax difference won’t affect us.\nWhen in a terminal window you can type echo $0 to find out what shell type you are in."
  },
  {
    "objectID": "content/shell.html#introduction-terminalshell",
    "href": "content/shell.html#introduction-terminalshell",
    "title": "Terminal or Shell",
    "section": "Introduction :: Terminal/Shell",
    "text": "Introduction :: Terminal/Shell\n\nRStudio\nLog into the JupyterHub. If you do not see this\n\nThen go to File &gt; New Launcher\nClick on the RStudio box to open RStudio.\n\n\nJupyter Lab\nLog into the JupyterHub. If you do not see this\n\nThen go to File &gt; New Launcher\nClick on the “Terminal” box to open a new terminal window.\n\n\nShell or Terminal Basics\n\nWhat is Terminal or Shell?\nNavigating Files and Directories\nWorking with Files and Directories\nOptional: Detailed self-paced lesson on running scripts from the shell: Shell Lesson from Software Carpentry\n\nYou will need only basic navigation skills for this course: cd, ls and cat\n\npwd where am I\ncd nameofdir move into a directory\ncd .. move up a directory\nls list the files in the current directory\nls -a list the files including hidden files\nls -l list the files with more info\ncat filename print out the contents of a file\n\n\n\nLet’s try\nls\nls -a\ncd shared\nls\ncd shell-tutorial\ncat lesson1.sh\ncd ..\ncd ..\n\n\nClose the terminal\nJust click on the X in the terminal tab"
  },
  {
    "objectID": "content/shell.html#introduction-file-navigation",
    "href": "content/shell.html#introduction-file-navigation",
    "title": "Terminal or Shell",
    "section": "Introduction :: File Navigation",
    "text": "Introduction :: File Navigation\nIn the far left, you will see a line of icons. The top one is a folder and allows us to move around our file system.\n\nClick on shared. Now you can see the files in the shared directory.\nClick on shell-tutorial. Then click on lesson1.sh. The file opens. You won’t be able to save changes here because you don’t have write permission on this drive.\nClick on the folder icon that looks like this. Click on the actual folder image. \nNow it should look like this folder /\nThis shows me doing this\n\nCreate a new folder.\n\nNext to the blue rectange with a +, is a grey folder with a +. Click that to create a new folder, called lesson-scripts.\nThen click on lesson-scripts to enter the folder\n\n\nCreate a new file\n\nCreate with File &gt; New &gt; Text file\nThe file will open and you can edit it.\nSave with File &gt; Save Text\nDelete the file by right-clicking on it and clicking “Delete”"
  },
  {
    "objectID": "content/shell.html#introduction-version-control-git",
    "href": "content/shell.html#introduction-version-control-git",
    "title": "Terminal or Shell",
    "section": "Introduction :: Version Control (Git)",
    "text": "Introduction :: Version Control (Git)\n\nWhat is version control, git, github, and how to set it up?\nVersion control is managing and tracking changes to your documents (program source code, images, websites, data files, etc.). git is a popular tool used for version control of software code. github.com is popular platform that provides remote server hosting for git repositories. A repository is a collection of various files that you are tracking for changes and versions. Currently GitHub is the most popular platform for file sharing code and code packages.\nThis section is a step-by-step guide to set up git on our JupyterHub. We will also configure git to use your github.com account for managing your repositories hosted on github.com. There are 5 main steps.\n\n\nStep 1: Create a GitHub account\nTo complete the setup, you will need an account on github.com. If you don’t have an account, please visit github.com, create an account (free) and come back to this guide for setting up git.\n\n\nStep 2: Clone a repository\nWe have created a demo repository for you to clone:\nhttps://github.com/nmfs-opensci/Git-Lesson\n\nStart your JupyterHub\nClick on the Git icon\n\n\n\nClick “Clone a Repository”\nWhere is says “Enter the URI of the remote Git repository”, paste in the URL https://github.com/nmfs-opensci/Git-Lesson\nThe folder appears and you can enter the folder and edit and create files.\n\n\nYour task: Create a file with your name and save to the Git-Lesson folder"
  },
  {
    "objectID": "content/shell.html#step-3",
    "href": "content/shell.html#step-3",
    "title": "Terminal or Shell",
    "section": "Step 3:",
    "text": "Step 3:\nConfigure git with your name and email address.\n``` bash\ngit config --global user.name \"Makhan Virdi\"\ngit config --global user.email \"Makhan.Virdi@gmail.com\"\n```\n\n**Note:** This name and email could be different from your github.com credentials. Remember `git` is a program that keeps track of your changes locally (on 2i2c JupyterHub or your own computer) and github.com is a platform to host your repositories. However, since your changes are tracked by `git`, the email/name used in git configuration will show up next to your contributions on github.com when you `push` your repository to github.com (`git push` is discussed in a later step).\n\nConfigure git to store your github credentials to avoid having to enter your github username and token each time you push changes to your repository(in Step 5, we will describe how to use github token instead of a password)\ngit config --global credential.helper store\nCopy link for the demo repository from your github account. Click the green “Code” button and copy the link as shown.\n\nClone the repository using git clone command in the terminal\nTo clone a repository from github, copy the link for the repository (previous step) and use git clone:\ngit clone https://github.com/YOUR-GITHUB-USERNAME/check_github_setup\nNote: Replace YOUR-GITHUB-USERNAME here with your github.com username. For example, it is virdi for my github.com account as seen in this image.\n\nUse ls (list files) to verify the existence of the repository that you just cloned\n\nChange directory to the cloned repository using cd check_github_setup and check the current directory using pwd command (present working directory)\n\nCheck status of your git repository to confirm git set up using git status\n\nYou are all set with using git on your 2i2c JupyterHub! But the collaborative power of git through github needs some additional setup.\nIn the next step, we will create a new file in this repository, track changes to this file, and link it with your github.com account.\n\n\nStep 4. Creating new file and tracking changes\n\nIn the left panel on your 2i2c JupyterHub, click on the “directory” icon and then double click on “check_github_setup” directory.\n\n\nOnce you are in the check_github_setup directory, create a new file using the text editor in your 2i2c JupyterHub (File &gt;&gt; New &gt;&gt; Text File).\n\nName the file lastname.txt. For example, virdi.txt for me (use your last name). Add some content to this file (for example, I added this to my virdi.txt file: my last name is virdi).\n\nNow you should have a new file (lastname.txt) in the git repository directory check_github_setup\nCheck if git can see that you have added a new file using git status. Git reports that you have a new file that is not tracked by git yet, and suggests adding that file to the git tracking system.\n\nAs seen in this image, git suggests adding that file so it can be tracked for changes. You can add file to git for tracking changes using git add. Then, you can commit changes to this file’s content using git commit as shown in the image.\ngit add virdi.txt\ngit status\ngit commit -m \"adding a new file\"\ngit status\n\nAs seen in the image above, git is suggesting to push the change that you just committed to the remote server at github.com (so that your collaborators can also see what changes you made).\nNote: DO NOT execute push yet. Before we push to github.com, let’s configure git further and store our github.com credentials to avoid entering the credentials every time we invoke git push. For doing so, we need to create a token on github.com to be used in place of your github.com password.\n\n\n\nStep 5. Create access token on github.com\n\nGo to your github account and create a new “personal access token”: https://github.com/settings/tokens/new\n\nEnter a description in “Note” field as seen above, select “repo” checkbox, and scroll to the bottom and click the green button “Generate Token”. Once generated, copy the token (or save it in a text file for reference).\nIMPORTANT: You will see this token only once, so be sure to copy this. If you do not copy your token at this stage, you will need to generate a new token.\n\nTo push (transfer) your changes to github, use git push in terminal. It requires you to enter your github credentials. You will be prompted to enter your github username and “password”. When prompted for your “password”, DO NOT use your github password, use the github token that was copied in the previous step.\ngit push\n\nNote: When you paste your token in the terminal window, windows users will press Ctrl+V and mac os users will press Cmd+V. If it does not work, try generating another token and use the copy icon next to the token to copy the token. Then, paste using your computer’s keyboard shortcut for paste.\nNow your password is stored in ~/.git-credentials and you will not be prompted again unless the Github token expires. You can check the presence of this git-credentials file using Terminal. Here the ~ character represents your home directory (/home/jovyan/).\nls -la ~\nThe output looks like this:\ndrwxr-xr-x 13 jovyan jovyan 6144 Oct 22 17:35 .\ndrwxr-xr-x  1 root   root   4096 Oct  4 16:21 ..\n-rw-------  1 jovyan jovyan 1754 Oct 29 18:30 .bash_history\ndrwxr-xr-x  4 jovyan jovyan 6144 Oct 29 16:38 .config\n-rw-------  1 jovyan jovyan   66 Oct 22 17:35 .git-credentials\n-rw-r--r--  1 jovyan jovyan   84 Oct 22 17:14 .gitconfig\ndrwxr-xr-x 10 jovyan jovyan 6144 Oct 21 16:19 2021-Cloud-Hackathon\nYou can also verify your git configuration\n(notebook) jovyan@jupyter-virdi:~$ git config -l\nThe output should have credential.helper = store:\nuser.email        = Makhan.Virdi@gmail.com\nuser.name         = Makhan Virdi\ncredential.helper = store\n\nNow we are all set to collaborate with github on the JupyterHub during the Cloud Hackathon!\n\n\nSummary: Git Commands\n\nCommonly used git commands (modified from source)\n\n\nGit Command\nDescription\n\n\n\n\ngit status\nShows the current state of the repository: the current working branch, files in the staging area, etc.\n\n\ngit add\nAdds a new, previously untracked file to version control and marks already tracked files to be committed with the next commit\n\n\ngit commit\nSaves the current state of the repository and creates an entry in the log\n\n\ngit log\nShows the history for the repository\n\n\ngit diff\nShows content differences between commits, branches, individual files and more\n\n\ngit clone\nCopies a repository to your local environment, including all the history\n\n\ngit pull\nGets the latest changes of a previously cloned repository\n\n\ngit push\nPushes your local changes to the remote repository, sharing them with others\n\n\n\n\n\nGit: More Details\nLesson: For a more detailed self-paced lesson on git, visit Git Lesson from Software Carpentry\nCheatsheet: Frequently used git commands\nDangit, Git!?!: If you are stuck after a git mishap, there are ready-made solutions to common problems at Dangit, Git!?!\n\n\nCloning our repository using the git Jupyter lab extension.\nIf we’re already familiar with git commands and feel more confortable using a GUI our Jupyterhub deployment comes with a git extension. This plugin allows us to operate with git using a simple user interface.\nFor example we can clone our repository using the extension.\n\n\n\ngit extension"
  },
  {
    "objectID": "content/jupyter-notebooks.html#summary",
    "href": "content/jupyter-notebooks.html#summary",
    "title": "Jupyter Notebooks - Python",
    "section": "Summary",
    "text": "Summary\nIn this tutorial, we will provide a brief introduction to:\n\nJupyter Notebooks in the JupyterHub\nProgramming in Python (using Jupyter Notebook)\nGeospatial Fundamentals (optional, self-study)\nCommand line (terminal/shell)\nVersion Control (code management using git)"
  },
  {
    "objectID": "content/jupyter-notebooks.html#introduction-programming-in-python",
    "href": "content/jupyter-notebooks.html#introduction-programming-in-python",
    "title": "Jupyter Notebooks - Python",
    "section": "Introduction :: Programming in Python",
    "text": "Introduction :: Programming in Python\nSwitch to Jupyter Notebook for an introduction to programming in Python\n\nVariables (and mathematical operations)\nData Structures (list, tuple, dict)\nFlow Control using loops (for, while)\nConditionals (if, else, elif)\nFunctions\nErrors and Exceptions (understanding and handling errors)\nUsing modules (libraries, packages)\n\npandas: high-performance, easy-to-use data structures and data analysis tools\nrioxarray: based on the rasterio package for working with rasters and xarray\n\n\n\nPython Learning Resources\nSelf-paced lesson on Programming with Python from Software Carpentry"
  },
  {
    "objectID": "content/jupyter-notebooks.html#introduction-geospatial-fundamentals-optional",
    "href": "content/jupyter-notebooks.html#introduction-geospatial-fundamentals-optional",
    "title": "Jupyter Notebooks - Python",
    "section": "Introduction :: Geospatial Fundamentals (Optional)",
    "text": "Introduction :: Geospatial Fundamentals (Optional)\nDetailed self-paced lesson on Fundamentals of Geospatial Raster and Vector Data with Python from Data Carpentry"
  },
  {
    "objectID": "content/jupyter-notebooks.html#jupyter-notebooks-in-earth-sciences",
    "href": "content/jupyter-notebooks.html#jupyter-notebooks-in-earth-sciences",
    "title": "Jupyter Notebooks - Python",
    "section": "Jupyter Notebooks in Earth Sciences",
    "text": "Jupyter Notebooks in Earth Sciences\nThere are many Jupyter Notebooks that you can copy and run in our JupyterHub. Note our JupyterHub is set up for geospatial research and connection to NASA’s data in the cloud. It is also set up for parallel processing with Dask.\n\nNASA Cloud Hackweek 2022\nOceanHackWeek\nICESat-2 Hackweeks"
  },
  {
    "objectID": "content/jupyter-notebooks.html#pythonconda-environments",
    "href": "content/jupyter-notebooks.html#pythonconda-environments",
    "title": "Jupyter Notebooks - Python",
    "section": "Python/Conda environments",
    "text": "Python/Conda environments\nPython users can create conda environments if they need to install modules. You create a environments.yml file like this:\nname: nsidc\nchannels:\n  - conda-forge\ndependencies:\n  - ipykernel\n  - awscli~=1.21.4\n  - requests\n  - pip\nAnd then in a terminal do this to activate the environment.\nconda env create -f environment.yml --name myenv\nconda activate myenv\nconda list"
  },
  {
    "objectID": "content/git.html#summary",
    "href": "content/git.html#summary",
    "title": "Git - Jupyter Lab",
    "section": "Summary",
    "text": "Summary\nIn this tutorial, we will provide a brief introduction to:\n\nCommand line (terminal/shell)\nNavigating around folders in Jupyter Lab\nVersion Control (code management using git)\nSetting up Git in Jupyter Lab\nThe Git GUI in Jupyter Lab\nBasic Git commands"
  },
  {
    "objectID": "content/git.html#introduction-jupyter-lab",
    "href": "content/git.html#introduction-jupyter-lab",
    "title": "Git - Jupyter Lab",
    "section": "Introduction :: Jupyter Lab",
    "text": "Introduction :: Jupyter Lab\nWhen you start the JupyterHub, you will be in Jupyter Lab. From there you can click on the RStudio box and open RStudio. However for this tutorial, we will stay in Juptyer Lab."
  },
  {
    "objectID": "content/git.html#introduction-terminalshell",
    "href": "content/git.html#introduction-terminalshell",
    "title": "Git - Jupyter Lab",
    "section": "Introduction :: Terminal/Shell",
    "text": "Introduction :: Terminal/Shell\nLog into the JupyterHub. If you do not see this\n\nThen go to File &gt; New Launcher\nClick on the “Terminal” box to open a new terminal window.\n\nShell or Terminal Basics\n\nWhat is Terminal or Shell?\nNavigating Files and Directories\nWorking with Files and Directories\nOptional: Detailed self-paced lesson on running scripts from the shell: Shell Lesson from Software Carpentry\n\nYou will need only basic navigation skills for this course: cd, ls and cat\n\npwd where am I\ncd nameofdir move into a directory\ncd .. move up a directory\nls list the files in the current directory\nls -a list the files including hidden files\nls -l list the files with more info\ncat filename print out the contents of a file\n\n\n\nLet’s try\nls\nls -a\ncd shared\nls\ncd shell-tutorial\ncat lesson1.sh\ncd ..\ncd ..\n\n\nClose the terminal\nJust click on the X in the terminal tab"
  },
  {
    "objectID": "content/git.html#introduction-file-navigation",
    "href": "content/git.html#introduction-file-navigation",
    "title": "Git - Jupyter Lab",
    "section": "Introduction :: File Navigation",
    "text": "Introduction :: File Navigation\nIn the far left, you will see a line of icons. The top one is a folder and allows us to move around our file system.\n\nClick on shared. Now you can see the files in the shared directory.\nClick on shell-tutorial. Then click on lesson1.sh. The file opens. You won’t be able to save changes here because you don’t have write permission on this drive.\nClick on the folder icon that looks like this. Click on the actual folder image. \nNow it should look like this folder /\nThis shows me doing this\n\nCreate a new folder.\n\nNext to the blue rectange with a +, is a grey folder with a +. Click that to create a new folder, called lesson-scripts.\nThen click on lesson-scripts to enter the folder\n\n\nCreate a new file\n\nCreate with File &gt; New &gt; Text file\nThe file will open and you can edit it.\nSave with File &gt; Save Text\nDelete the file by right-clicking on it and clicking “Delete”"
  },
  {
    "objectID": "content/git.html#introduction-version-control-git",
    "href": "content/git.html#introduction-version-control-git",
    "title": "Git - Jupyter Lab",
    "section": "Introduction :: Version Control (Git)",
    "text": "Introduction :: Version Control (Git)\n\nWhat is version control, git, github, and how to set it up?\nVersion control is managing and tracking changes to your documents (program source code, images, websites, data files, etc.). git is a popular tool used for version control of software code. github.com is popular platform that provides remote server hosting for git repositories. A repository is a collection of various files that you are tracking for changes and versions. Currently GitHub is the most popular platform for file sharing code and code packages.\nThis section is a step-by-step guide to set up git on our JupyterHub. We will also configure git to use your github.com account for managing your repositories hosted on github.com. There are 5 main steps.\n\n\nStep 1: Create a GitHub account\nTo complete the setup, you will need an account on github.com. If you don’t have an account, please visit github.com, create an account (free) and come back to this guide for setting up git.\n\n\nStep 2: Clone a repository\nWe have created a demo repository for you to clone:\nhttps://github.com/nmfs-opensci/Git-Lesson\n\nStart your JupyterHub\nClick on the Git icon\n\n\n\nClick “Clone a Repository”\nWhere is says “Enter the URI of the remote Git repository”, paste in the URL https://github.com/nmfs-opensci/Git-Lesson\nThe folder appears and you can enter the folder and edit and create files.\n\n\nYour task: Create a file with your name and save to the Git-Lesson folder"
  },
  {
    "objectID": "content/git.html#step-3",
    "href": "content/git.html#step-3",
    "title": "Git - Jupyter Lab",
    "section": "Step 3:",
    "text": "Step 3:\nConfigure git with your name and email address.\n``` bash\ngit config --global user.name \"Makhan Virdi\"\ngit config --global user.email \"Makhan.Virdi@gmail.com\"\n```\n\n**Note:** This name and email could be different from your github.com credentials. Remember `git` is a program that keeps track of your changes locally (on 2i2c JupyterHub or your own computer) and github.com is a platform to host your repositories. However, since your changes are tracked by `git`, the email/name used in git configuration will show up next to your contributions on github.com when you `push` your repository to github.com (`git push` is discussed in a later step).\n\nConfigure git to store your github credentials to avoid having to enter your github username and token each time you push changes to your repository(in Step 5, we will describe how to use github token instead of a password)\ngit config --global credential.helper store\nCopy link for the demo repository from your github account. Click the green “Code” button and copy the link as shown.\n\nClone the repository using git clone command in the terminal\nTo clone a repository from github, copy the link for the repository (previous step) and use git clone:\ngit clone https://github.com/YOUR-GITHUB-USERNAME/check_github_setup\nNote: Replace YOUR-GITHUB-USERNAME here with your github.com username. For example, it is virdi for my github.com account as seen in this image.\n\nUse ls (list files) to verify the existence of the repository that you just cloned\n\nChange directory to the cloned repository using cd check_github_setup and check the current directory using pwd command (present working directory)\n\nCheck status of your git repository to confirm git set up using git status\n\nYou are all set with using git on your 2i2c JupyterHub! But the collaborative power of git through github needs some additional setup.\nIn the next step, we will create a new file in this repository, track changes to this file, and link it with your github.com account.\n\n\nStep 4. Creating new file and tracking changes\n\nIn the left panel on your 2i2c JupyterHub, click on the “directory” icon and then double click on “check_github_setup” directory.\n\n\nOnce you are in the check_github_setup directory, create a new file using the text editor in your 2i2c JupyterHub (File &gt;&gt; New &gt;&gt; Text File).\n\nName the file lastname.txt. For example, virdi.txt for me (use your last name). Add some content to this file (for example, I added this to my virdi.txt file: my last name is virdi).\n\nNow you should have a new file (lastname.txt) in the git repository directory check_github_setup\nCheck if git can see that you have added a new file using git status. Git reports that you have a new file that is not tracked by git yet, and suggests adding that file to the git tracking system.\n\nAs seen in this image, git suggests adding that file so it can be tracked for changes. You can add file to git for tracking changes using git add. Then, you can commit changes to this file’s content using git commit as shown in the image.\ngit add virdi.txt\ngit status\ngit commit -m \"adding a new file\"\ngit status\n\nAs seen in the image above, git is suggesting to push the change that you just committed to the remote server at github.com (so that your collaborators can also see what changes you made).\nNote: DO NOT execute push yet. Before we push to github.com, let’s configure git further and store our github.com credentials to avoid entering the credentials every time we invoke git push. For doing so, we need to create a token on github.com to be used in place of your github.com password.\n\n\n\nStep 5. Create access token on github.com\n\nGo to your github account and create a new “personal access token”: https://github.com/settings/tokens/new\n\nEnter a description in “Note” field as seen above, select “repo” checkbox, and scroll to the bottom and click the green button “Generate Token”. Once generated, copy the token (or save it in a text file for reference).\nIMPORTANT: You will see this token only once, so be sure to copy this. If you do not copy your token at this stage, you will need to generate a new token.\n\nTo push (transfer) your changes to github, use git push in terminal. It requires you to enter your github credentials. You will be prompted to enter your github username and “password”. When prompted for your “password”, DO NOT use your github password, use the github token that was copied in the previous step.\ngit push\n\nNote: When you paste your token in the terminal window, windows users will press Ctrl+V and mac os users will press Cmd+V. If it does not work, try generating another token and use the copy icon next to the token to copy the token. Then, paste using your computer’s keyboard shortcut for paste.\nNow your password is stored in ~/.git-credentials and you will not be prompted again unless the Github token expires. You can check the presence of this git-credentials file using Terminal. Here the ~ character represents your home directory (/home/jovyan/).\nls -la ~\nThe output looks like this:\ndrwxr-xr-x 13 jovyan jovyan 6144 Oct 22 17:35 .\ndrwxr-xr-x  1 root   root   4096 Oct  4 16:21 ..\n-rw-------  1 jovyan jovyan 1754 Oct 29 18:30 .bash_history\ndrwxr-xr-x  4 jovyan jovyan 6144 Oct 29 16:38 .config\n-rw-------  1 jovyan jovyan   66 Oct 22 17:35 .git-credentials\n-rw-r--r--  1 jovyan jovyan   84 Oct 22 17:14 .gitconfig\ndrwxr-xr-x 10 jovyan jovyan 6144 Oct 21 16:19 2021-Cloud-Hackathon\nYou can also verify your git configuration\n(notebook) jovyan@jupyter-virdi:~$ git config -l\nThe output should have credential.helper = store:\nuser.email        = Makhan.Virdi@gmail.com\nuser.name         = Makhan Virdi\ncredential.helper = store\n\nNow we are all set to collaborate with github on the JupyterHub during the Cloud Hackathon!\n\n\nSummary: Git Commands\n\nCommonly used git commands (modified from source)\n\n\nGit Command\nDescription\n\n\n\n\ngit status\nShows the current state of the repository: the current working branch, files in the staging area, etc.\n\n\ngit add\nAdds a new, previously untracked file to version control and marks already tracked files to be committed with the next commit\n\n\ngit commit\nSaves the current state of the repository and creates an entry in the log\n\n\ngit log\nShows the history for the repository\n\n\ngit diff\nShows content differences between commits, branches, individual files and more\n\n\ngit clone\nCopies a repository to your local environment, including all the history\n\n\ngit pull\nGets the latest changes of a previously cloned repository\n\n\ngit push\nPushes your local changes to the remote repository, sharing them with others\n\n\n\n\n\nGit: More Details\nLesson: For a more detailed self-paced lesson on git, visit Git Lesson from Software Carpentry\nCheatsheet: Frequently used git commands\nDangit, Git!?!: If you are stuck after a git mishap, there are ready-made solutions to common problems at Dangit, Git!?!\n\n\nCloning our repository using the git Jupyter lab extension.\nIf we’re already familiar with git commands and feel more confortable using a GUI our Jupyterhub deployment comes with a git extension. This plugin allows us to operate with git using a simple user interface.\nFor example we can clone our repository using the extension.\n\n\n\ngit extension"
  },
  {
    "objectID": "content/data.html",
    "href": "content/data.html",
    "title": "Data",
    "section": "",
    "text": "Indian Ocean and Bay of Bengal Data\nStudents will learn how to access remote-sensing data in the cloud (meaning on-line), but we have also prepared an “analysis ready” data set for students. This will be available on a shared drive and we will introduce the students to the techniques for accessing large datasets without loading them into memory.\n\n\nBounding box\n\nlatitude: -12deg to 32deg\nlongitude: 42deg to 102deg\ngrid: 0.25 deg\ncenters: 0, 0.25, 0.5., 0.75\n\n\n\nDatasets: ERA5\n\nu wind\nv wind\nwind speed\nwind direction\nair temperature @ 2m\nsea surface temperature\n\n\n\nCopernicus\n\nsea level anomaly\nchlorophyll concentration\n\n\n\nSRTM30_PLUS\n\nBathymetry\n\nReferences\n\nhttps://topex.ucsd.edu/WWW_html/srtm30_plus.html\nhttps://coastwatch.pfeg.noaa.gov/erddap/griddap/usgsCeSrtm30v6.html\n\n\n\n1-km MUR SST\n\nsea surface temperature"
  },
  {
    "objectID": "content/XX-further-resources.html#notebooks-and-hackweeks-with-tutorials",
    "href": "content/XX-further-resources.html#notebooks-and-hackweeks-with-tutorials",
    "title": "Additional resources",
    "section": "Notebooks and hackweeks with tutorials",
    "text": "Notebooks and hackweeks with tutorials\n\nPangeo at Ocean Sciences 2020\nNASA Openscapes EarthData Cloud Book\nNASA Openscapes 2021 Hackweek\nOceanHackWeek\nICESat-2 2023 eScience Institute, University of Washington website\nPO.DAAC Github Repository\nNASA MAAP Notebooks Click the GitHub link in top right to get to the Jupyter Notebooks\nUW eScience Geoscience Hackweek"
  },
  {
    "objectID": "content/XX-further-resources.html#how-to",
    "href": "content/XX-further-resources.html#how-to",
    "title": "Additional resources",
    "section": "How to",
    "text": "How to\n\nNASA Earthdata: How to Cloud\nUSGS Eyes on Earth Podcast: Satellites and Cloud Computing - with Aaron Friesz (LP DAAC!)\nPO.DAAC Cloud Data Page\nPO.DAAC Earthdata Webinar (Aug 2021): Surfing Ocean Data in the Cloud - The Beginner’s Guide to PO.DAAC in the NASA Earthdata Cloud\nNASA Earthdata Cloud Primer helpful tutorials for how to set up your own EC2 cloud instance in AWS, attach storage, move files back and forth, and more.\nSetting up Jupyter Notebooks in a user EC2 instance in AWS - blog post for setting up jupyter notebooks in an EC2 instance in AWS. (Builds on the Cloud Primer tutorials, which are missing that next step)\nRunning the NASA Cloud Workshop notebooks with mybinder.org - by Eli Holmes, 2021 Cloud Hackathon Participant who then set up working in Binder"
  },
  {
    "objectID": "content/03-git-rstudio.html#what-is-git-and-github",
    "href": "content/03-git-rstudio.html#what-is-git-and-github",
    "title": "Basic Git/GitHub Skills Using RStudio",
    "section": "What is Git and GitHub?",
    "text": "What is Git and GitHub?\nGit A program to track your file changes and create a history of those changes. Creates a ‘container’ for a set of files called a repository.\nGitHub A website to host these repositories and allow you to sync local copies (on your computer) to the website. Lots of functionality built on top of this.",
    "crumbs": [
      "Tutorials",
      "Git and GitHub Basics",
      "Git in RStudio"
    ]
  },
  {
    "objectID": "content/03-git-rstudio.html#some-basic-git-jargon",
    "href": "content/03-git-rstudio.html#some-basic-git-jargon",
    "title": "Basic Git/GitHub Skills Using RStudio",
    "section": "Some basic Git jargon",
    "text": "Some basic Git jargon\n\nRepo Repository. It is your code and the record of your changes. This record and also the status of your repo is a hidden folder called .git . You have a local repo and a remote repo. The remote repo is on GitHub (for in our case) is called origin. The local repo is on the JupyterHub.\nStage Tell Git which changes you want to commit (write to the repo history).\nCommit Write a note about what change the staged files and “commit” that note to the repository record. You are also tagging this state of the repo and you could go back to this state if you wanted.\nPush Push local changes (commits) up to the remote repository on GitHub (origin).\nPull Pull changes on GitHub into the local repository on the JupyterHub.\nGit GUIs A graphical interface for Git (which is command line). Today I will use jupyterlab-git which we have installed on JupyterHub.\nShell A terminal window where we can issue git commands.",
    "crumbs": [
      "Tutorials",
      "Git and GitHub Basics",
      "Git in RStudio"
    ]
  },
  {
    "objectID": "content/03-git-rstudio.html#overview",
    "href": "content/03-git-rstudio.html#overview",
    "title": "Basic Git/GitHub Skills Using RStudio",
    "section": "Overview",
    "text": "Overview\nToday I will cover the four basic Git/GitHub skills. The goal for today is to first get you comfortable with the basic skills and terminology. We will use what is called a “trunk-based workflow”.\n\nSimple Trunk-based Workflow:\n\nMake local (on your computer) changes to code.\nRecord what those changes were about and commit to the code change record (history).\nPush those changes to your remote repository (aka origin)\n\nWe’ll do this",
    "crumbs": [
      "Tutorials",
      "Git and GitHub Basics",
      "Git in RStudio"
    ]
  },
  {
    "objectID": "content/03-git-rstudio.html#setting-up-git",
    "href": "content/03-git-rstudio.html#setting-up-git",
    "title": "Basic Git/GitHub Skills Using RStudio",
    "section": "Setting up Git",
    "text": "Setting up Git\nYou should have gotten this done on Tuesday but if not here are the instructions\nBefore we can work with Git in the JupyterHub, we need to do some set up.\n\nTell Git who you are and to store your credentials (GitHub login info)\n\nShow me\nPaste this into a terminal window:\ngit config --global user.email \"&lt;your email&gt;\"\ngit config --global user.name \"&lt;your name&gt;\"\ngit config --global pull.rebase false\ngit config --global credential.helper store\n\nGet a Personal Access Token from GitHub\n\nCopy the token! You will need it in the next step.\nShow me Note, one change to this video is that you need to specify that you want a classic token.\n\nTrigger Git to ask for your password (that personal access token)\n\nYou can do this by cloning a private repo. In the Terminal, issue this command\ngit clone https://github.com/nmfs-opensci/github_setup_check\nIt will ask for your GitHub username and password. At the password part, paste in the Personal Access Token.",
    "crumbs": [
      "Tutorials",
      "Git and GitHub Basics",
      "Git in RStudio"
    ]
  },
  {
    "objectID": "content/03-git-rstudio.html#git-tab",
    "href": "content/03-git-rstudio.html#git-tab",
    "title": "Basic Git/GitHub Skills Using RStudio",
    "section": "Git tab",
    "text": "Git tab\nWhen the instructions say to use or open or click the Git tab,",
    "crumbs": [
      "Tutorials",
      "Git and GitHub Basics",
      "Git in RStudio"
    ]
  },
  {
    "objectID": "content/03-git-rstudio.html#the-key-skills",
    "href": "content/03-git-rstudio.html#the-key-skills",
    "title": "Basic Git/GitHub Skills Using RStudio",
    "section": "The Key Skills",
    "text": "The Key Skills\n\nSkill 1: Create a blank repo on GitHub\nSkill 2: Clone your GitHub repo to RStudio\nSkill 3: Make some changes and commit those local changes\nSkill 4: Push the changes to GitHub\nSkill 1b: Copy someone else’s GitHub repository",
    "crumbs": [
      "Tutorials",
      "Git and GitHub Basics",
      "Git in RStudio"
    ]
  },
  {
    "objectID": "content/03-git-rstudio.html#lets-see-it-done",
    "href": "content/03-git-rstudio.html#lets-see-it-done",
    "title": "Basic Git/GitHub Skills Using RStudio",
    "section": "Let’s see it done!",
    "text": "Let’s see it done!\n\nSkill 1: Create a blank repo on GitHub\n\nClick the + in the upper left from YOUR GitHub page.\nGive your repo the name Test and make sure it is public.\nClick new and check checkbox to add the Readme file and .gitignore\nCopy the URL of your new repo. It’s in the browser where you normally see a URL.\n\nShow me\n\n\nSkill 2: Clone your repo to the RStudio\nIn RStudio we do this by making a new project.\n\nCopy the URL of your repo. https://www.github.com/yourname/Test\nFile &gt; New Project &gt; Version Control &gt; Git\nPast in the URL of your repo from Step 1\nCheck that it is being created in your Home directory which will be denoted ~ in the JupyterHub.\nClick Create.\n\nShow me\n\n\nSkill 3: Make some changes and commit your changes\nThis writes a note about what changes you have made. It also marks a ‘point’ in time that you can go back to if you need to.\n\nMake some changes to the README.md file in the Test repo.\nClick the Git tab, and stage the change(s) by checking the checkboxes next to the files listed.\nClick the Commit button.\nAdd a commit comment, click commit.\n\nShow me\n\n\nSkill 4: Push changes to GitHub / Pull changes from GitHub\nTo push changes you committed in Skill #3\n\nFrom Git tab, click on the Green up arrow that says Push.\nTo pull changes on GitHub that are not on your local computer:\nMake some changes directly on GitHub (not in RStudio)\nFrom Git tab, click on the down arrow that says Pull.\n\nShow me\n\n\nPair-activity 1\nIn RStudio,\n\nMake a copy of README.md\nRename it to .md\nAdd some text.\nStage and commit the added file.\nPush to GitHub.\n\nTry before watching.\nShow me in RStudio – Show me in the shell – Show me in jupyter-git\n\n\nPair-activity 2\nAll of this activity is in RStudio.\n\nClone this repo https://github.com/nmfs-opensci/git-basics to RStudio and create a new project\nNavigate to the files in your new project, create a filed called to &lt;yourname&gt;.md. Use your actual name so the filename is different from everyone elses.\nStage and then commit that new file.\nPush to GitHub.\nMake some more changes and push to GitHub.\nPull in your partner’s (and everyone elses) changes\n\nShow me in RStudio – Show me in JupyterLab\n\n\nPair-activity 3\nYou can copy your own or other people’s repos1.\n\nIn a browser, go to the GitHub repository https://github.com/RWorkflow-Workshops/Week5\nCopy its URL.\nNavigate to your GitHub page: click your icon in the upper right and then ‘your repositories’\nClick the + in top right and click import repository. Paste in the URL and give your repo a name.\nUse Skill #1 to clone your new repo to RStudio and create a new project",
    "crumbs": [
      "Tutorials",
      "Git and GitHub Basics",
      "Git in RStudio"
    ]
  },
  {
    "objectID": "content/02-rstudio.html#open-rstudio-in-the-jupyterhub",
    "href": "content/02-rstudio.html#open-rstudio-in-the-jupyterhub",
    "title": "RStudio - R",
    "section": "Open RStudio in the JupyterHub",
    "text": "Open RStudio in the JupyterHub\n\nLogin the JupyterHub\nClick on the RStudio button when the Launcher appears \nLook for the browser tab with the RStudio icon",
    "crumbs": [
      "Tutorials",
      "R and RStudio Basics",
      "RStudio"
    ]
  },
  {
    "objectID": "content/02-rstudio.html#basic-navigation",
    "href": "content/02-rstudio.html#basic-navigation",
    "title": "RStudio - R",
    "section": "Basic Navigation",
    "text": "Basic Navigation\n\n\n\nRStudio Panels",
    "crumbs": [
      "Tutorials",
      "R and RStudio Basics",
      "RStudio"
    ]
  },
  {
    "objectID": "content/02-rstudio.html#create-an-rstudio-project",
    "href": "content/02-rstudio.html#create-an-rstudio-project",
    "title": "RStudio - R",
    "section": "Create an RStudio project",
    "text": "Create an RStudio project\n\nOpen RStudio\nIn the file panel, click on the Home icon to make sure you are in your home directory\nFrom the file panel, click “New Project” to create a new project\nIn the pop up, select New Directory and then New Project\nName it sandbox\nClick on the dropdown in the upper right corner to select your sandbox project\nClick on Tools &gt; Project Options &gt; General and change the first 2 options about saving and restoring the workspace to “No”",
    "crumbs": [
      "Tutorials",
      "R and RStudio Basics",
      "RStudio"
    ]
  },
  {
    "objectID": "content/02-rstudio.html#installing-packages",
    "href": "content/02-rstudio.html#installing-packages",
    "title": "RStudio - R",
    "section": "Installing packages",
    "text": "Installing packages\nIn the bottom right panel, select the Packages tab, click install and then start typing the name of the package. Then click Install.\nThe JupyterHub comes with many packages already installed so you shouldn’t have to install many packages.\nWhen you want to use a package, you first need to load it with\nlibrary(hello)\nYou will see this in the tutorials. You might also see something like\nhello::thefunction()\nThis is using thefunction() from the hello package.\n\n\n\n\n\n\nNote\n\n\n\nPython users. In R, you will always call a function like funtion(object) and never like object.function(). The exception is something called ‘piping’ in R, which I have never seen in Python. In this case you pass objects left to right. Like object %&gt;% function(). Piping is very common in modern R but you won’t see it much in R from 10 years ago.",
    "crumbs": [
      "Tutorials",
      "R and RStudio Basics",
      "RStudio"
    ]
  },
  {
    "objectID": "content/02-rstudio.html#uploading-and-downloading-files",
    "href": "content/02-rstudio.html#uploading-and-downloading-files",
    "title": "RStudio - R",
    "section": "Uploading and downloading files",
    "text": "Uploading and downloading files\nNote, Upload and download is only for the JupyterHub not on RStudio on your computer.\n\nUploading is easy.\nLook for the Upload button in the Files tab of the bottom right panel.\n\n\nDownload is less intuitive.\n\nClick the checkbox next to the file you want to download. One only.\nClick the “cog” icon in the Files tab of the bottom right panel. Then click Export.",
    "crumbs": [
      "Tutorials",
      "R and RStudio Basics",
      "RStudio"
    ]
  },
  {
    "objectID": "content/02-rstudio.html#creating-files",
    "href": "content/02-rstudio.html#creating-files",
    "title": "RStudio - R",
    "section": "Creating files",
    "text": "Creating files\nWhen you start your server, you will have access to your own virtual drive space. No other users will be able to see or access your files. You can upload files to your virtual drive space and save files here. You can create folders to organize your files. You personal directory is home/rstudio. Everyone has the same home directory but your files are separate and cannot be seen by others.\nPython users: If you open a Python image instead of the R image, your home is home/jovyan.\nThere are a number of different ways to create new files. Let’s practice making new files in RStudio.\n\nR Script\n\nOpen RStudio\nIn the upper right, make sure you are in your sandbox project.\nFrom the file panel, click on “New Blank File” and create a new R script.\nPaste\n\nprint(\"Hello World\")\n1+1\nin the script. 7. Click the Source button (upper left of your new script file) to run this code. 8. Try putting your cursor on one line and running that line of code by clicking “Run” 9. Try selecting lines of code and running that by clicking “Run”\n\n\ncsv file\n\nFrom the file panel, click on “New Blank File” and create a Text File.\nThe file will open in the top left corner. Paste in the following:\n\nname, place, value\nA, 1, 2\nB, 10, 20\nC, 100, 200\n\nClick the save icon (above your new file) to save your csv file\n\n\n\nA Rmarkdown document\nNow let’s create some more complicated files using the RStudio template feature.\n\nFrom the upper left, click File -&gt; New File -&gt; RMarkdown\nClick “Ok” at the bottom.\nWhen the file opens, click Knit (icon at top of file).\nIt will ask for a name. Give it one and save.\nYou file will render into html.\n\nReference sheet for writing in RMarkdown or go to Help &gt; Markdown Quick Reference\n\n\nA Rmarkdown presentation\n\nFrom the upper left, click File -&gt; New File -&gt; RMarkdown\nClick “Presentation” on left of the popup and click “Ok” at the bottom.\nWhen the file opens, click Knit (icon at top of file).\nIt will ask for a name. Give it one and save.\nYou file will render into html.\n\n\n\n(advanced) An interactive application\n\nFrom the upper left, click File -&gt; New File -&gt; Shiny Web App\nIn the popup, give the app a name and make sure the app is saved to my-files\nWhen the file opens, Run App (icon at top of file).\n\n\n\nAnd many more\nPlay around with creating other types of documents using templates. Especially if you already use RStudio.",
    "crumbs": [
      "Tutorials",
      "R and RStudio Basics",
      "RStudio"
    ]
  },
  {
    "objectID": "content/02-rstudio.html#more-tips",
    "href": "content/02-rstudio.html#more-tips",
    "title": "RStudio - R",
    "section": "More tips",
    "text": "More tips\nLearn some tips and tricks from these\n\nhttps://colorado.posit.co/rsc/the-unknown/into-the-unknown.html\nhttps://www.dataquest.io/blog/rstudio-tips-tricks-shortcuts/",
    "crumbs": [
      "Tutorials",
      "R and RStudio Basics",
      "RStudio"
    ]
  },
  {
    "objectID": "content/02-rstudio.html#plotting-a-netcdf-file",
    "href": "content/02-rstudio.html#plotting-a-netcdf-file",
    "title": "RStudio - R",
    "section": "Plotting a netCDF file",
    "text": "Plotting a netCDF file\n\nhttps://pjbartlein.github.io/REarthSysSci/netCDF.html\nhttps://r-spatial.github.io/sf/articles/sf1.html\n\nwebpage:\nhttps://coastwatch.pfeg.noaa.gov/erddap/griddap/ncdcOisst21Agg.graph?sst%5B(2023-08-27T12:00:00Z)%5D%5B(0.0)%5D%5B(-7.8):(44.8)%5D%5B(39.7):(92.3)%5D&.draw=surface&.vars=longitude%7Clatitude%7Csst&.colorBar=%7C%7C%7C%7C%7C&.bgColor=0xffccccff\nurl from the dropdown on that page\nurl &lt;- https://coastwatch.pfeg.noaa.gov/erddap/griddap/ncdcOisst21Agg.nc?sst%5B(2023-08-27T12:00:00Z)%5D%5B(0.0)%5D%5B(-7.875):(44.875)%5D%5B(39.625):(92.375)%5D&.draw=surface&.vars=longitude%7Clatitude%7Csst&.colorBar=%7C%7C%7C%7C%7C&.bgColor=0xffccccff\n\nOpen an R script\n\nAdd this code.\n\nlibrary(ggplot2) # package for plotting\nlibrary(sf)\nlibrary(stars)\nlibrary(dplyr)\n\nurl &lt;- \"https://coastwatch.pfeg.noaa.gov/erddap/griddap/ncdcOisst21Agg.nc?sst%5B(2023-08-27T12:00:00Z)%5D%5B(0.0)%5D%5B(-7.875):(44.875)%5D%5B(39.625):(92.375)%5D&.draw=surface&.vars=longitude%7Clatitude%7Csst&.colorBar=%7C%7C%7C%7C%7C&.bgColor=0xffccccff\"\n\nfil &lt;- \"sst.nc\"\nif(!exists(fil)){\n  download.file(url=url, destfile=fil)\n}\n\nstars_object &lt;- raster::raster(fil) %&gt;% st_as_stars()\nggplot() + geom_stars(data = stars_object)",
    "crumbs": [
      "Tutorials",
      "R and RStudio Basics",
      "RStudio"
    ]
  },
  {
    "objectID": "content/02-github-2.html#import-a-github-repository",
    "href": "content/02-github-2.html#import-a-github-repository",
    "title": "GitHub - Lecture 2",
    "section": "Import a GitHub repository",
    "text": "Import a GitHub repository\n\nIn your browser, navigate to https://github.com/nmfs-opensci/Quarto-Website\nCopy the URL\nClick on the “+” in the top right (on GitHub) and select “import repository”\nFollow instructions to import to your GitHub account\nNow you have your OWN copy of Quarto-Website\nCopy the URL for your OWN copy. It will look like this\n\nhttps://github.com/yourgithubusername/Quarto-Website",
    "crumbs": [
      "Tutorials",
      "Git and GitHub Basics",
      "GitHub 2"
    ]
  },
  {
    "objectID": "content/02-github-2.html#clone-a-github-repository-into-rstudio",
    "href": "content/02-github-2.html#clone-a-github-repository-into-rstudio",
    "title": "GitHub - Lecture 2",
    "section": "Clone a GitHub repository into RStudio",
    "text": "Clone a GitHub repository into RStudio\n\nLogin to the JupyterHub. You might have to wait awhile for the page to load.\nClick on the RStudio button when the Launcher appears \nClick on the tab that opens with the R in a circle. You should see this \nClick File &gt; New Project &gt; Version Control &gt; Git\nPaste the URL into the box\nClick “Create Project”\nThe project is created and the Git tab appears in the upper right panel.\n\nCongratulations! You pulled changes from GitHub into the JupyterHub!",
    "crumbs": [
      "Tutorials",
      "Git and GitHub Basics",
      "GitHub 2"
    ]
  },
  {
    "objectID": "content/02-github-2.html#lets-explore-the-files",
    "href": "content/02-github-2.html#lets-explore-the-files",
    "title": "GitHub - Lecture 2",
    "section": "Let’s explore the files",
    "text": "Let’s explore the files\n\nThis happens to be a website. Let’s build it using the Build tab in the upper right panel.\nClick “Render Website”. The website should build and appear.\n\n\nIf it doesn’t open automatically, go to docs &gt; index.html, right click and open that in a browser.\n\n\nOpen the about.qmd file. Let’s edit that! You can use the “Source” or “Visual” editor.\nWhen you have made some edits, click “Render” to see how it looks.\nYou can also click Build &gt; Render Website to re-create the whole website.",
    "crumbs": [
      "Tutorials",
      "Git and GitHub Basics",
      "GitHub 2"
    ]
  },
  {
    "objectID": "content/02-github-2.html#set-up-authentication",
    "href": "content/02-github-2.html#set-up-authentication",
    "title": "GitHub - Lecture 2",
    "section": "Set up authentication",
    "text": "Set up authentication\nYou need to tell GitHub who you are so you can push your local changes up to GitHub. There are a few ways to do this. I am going to show you a way that works on any computer, including a virtual computer like the JupyterHub.\n\nStep 1: Generate a Personal Access Token\nWe are going to generate a classic token.\n\nGo to https://github.com/settings/tokens\nClick Generate new token &gt; Generate new token (classic)\nWhen the pop-up shows up, fill in a description, click the “repo” checkbox, and then scroll to bottom to click “Generate”.\nSAVE the token. You need it for the next step.\n\n\n\nStep 2: Tell Git who your are\n\nReturn to RStudio\nClick the terminal tab in the bottom right panel\nPaste these 3 lines of code into the terminal\n\ngit config --global user.email \"&lt;your email&gt;\"\ngit config --global user.name \"&lt;your name&gt;\"\ngit config --global pull.rebase false\ngit config --global credential.helper store\n\n\nStep 3: Push changes up to GitHub\n\nClick the Git tab (upper right panel)\nClick the checkboxes next to what changes you want to push\nClick “Commit”\nFill out a comment and click “Commit”\nClose the pop-ups\nClick Push\n\nFull instructions with ways to do this from R",
    "crumbs": [
      "Tutorials",
      "Git and GitHub Basics",
      "GitHub 2"
    ]
  },
  {
    "objectID": "content/02-github-2.html#optional-make-your-website-live",
    "href": "content/02-github-2.html#optional-make-your-website-live",
    "title": "GitHub - Lecture 2",
    "section": "Optional: Make your website live!",
    "text": "Optional: Make your website live!\nFor those who get through all the steps and want to play around more.\n\nGo to your Quarto Website repository on Github\nGo to Settings &gt; Pages (in left navbar)\nYou need to select the branch and the folder where the webpage files are. In this case, they are in docs \nThe URL where your website will appear is shown.\n\nHave you noticed that people have personal websites at username.github.io? How did they do that? If you create a repo named username.github.io in GitHub, it will automatically appear with that URL. Look for a repo with that name in someone’s GitHub account if you see that they use that as their personal website.",
    "crumbs": [
      "Tutorials",
      "Git and GitHub Basics",
      "GitHub 2"
    ]
  },
  {
    "objectID": "content/01-welcome.html",
    "href": "content/01-welcome.html",
    "title": "Welcome",
    "section": "",
    "text": "Link to slides",
    "crumbs": [
      "Tutorials",
      "Welcome",
      "Welcome"
    ]
  },
  {
    "objectID": "content/01-intro-to-cloud-geoscience.html#open-infrastructure-for-geoscience",
    "href": "content/01-intro-to-cloud-geoscience.html#open-infrastructure-for-geoscience",
    "title": "Geoscience in the Cloud",
    "section": "Open infrastructure for geoscience",
    "text": "Open infrastructure for geoscience\nLet’s start by watch a short video by James Colliander, founder of 2i2c which supports many community JupyterHubs to increase open access to cloud computing.",
    "crumbs": [
      "Tutorials",
      "Welcome",
      "Geoscience cloud tools"
    ]
  },
  {
    "objectID": "content/01-intro-to-cloud-geoscience.html#why-are-we-using-a-cloud-environment",
    "href": "content/01-intro-to-cloud-geoscience.html#why-are-we-using-a-cloud-environment",
    "title": "Geoscience in the Cloud",
    "section": "Why are we using a cloud environment?",
    "text": "Why are we using a cloud environment?\n“Anyone working with large-scale Earth System data today faces the same general problems:\n\nThe data we want to work with are huge (typical analyses involve several TB at least)\nThe data we need are produced and distributed by many different organizations (NASA, NOAA, ESGF, Copernicus, etc.)\nWe want to apply a wide range of different analysis methodologies to the data, from simple statistics to signal processing to machine learning.\n\nThe community is waking up to the idea that we can’t simply expect scientists to download all this data to their personal computers for processing.”\nRyan Abernathey, Pangeo Project.\n\n\n\nDownload-based workflow. From Abernathey, Ryan (2020): Data Access Modes in Science\n\n\n\nPangeo Link to tutorial on Pangeo ecosystem",
    "crumbs": [
      "Tutorials",
      "Welcome",
      "Geoscience cloud tools"
    ]
  },
  {
    "objectID": "content/01-intro-to-cloud-geoscience.html#cloud-optimized-geospatial-data",
    "href": "content/01-intro-to-cloud-geoscience.html#cloud-optimized-geospatial-data",
    "title": "Geoscience in the Cloud",
    "section": "Cloud optimized geospatial data",
    "text": "Cloud optimized geospatial data\n\nMethods for Generating and Testing Cloud-Optimized Geospatial Formats",
    "crumbs": [
      "Tutorials",
      "Welcome",
      "Geoscience cloud tools"
    ]
  },
  {
    "objectID": "content/01-intro-to-cloud-geoscience.html#references",
    "href": "content/01-intro-to-cloud-geoscience.html#references",
    "title": "Geoscience in the Cloud",
    "section": "References",
    "text": "References\n\nClosed Platforms vs. Open Architectures for Cloud-Native Earth System Analytics\nWhy Jupyter is data scientists’ computational notebook of choice\nNASA MAAP Project",
    "crumbs": [
      "Tutorials",
      "Welcome",
      "Geoscience cloud tools"
    ]
  },
  {
    "objectID": "coc.html",
    "href": "coc.html",
    "title": "Code of Conduct",
    "section": "",
    "text": "We are dedicated to providing a harassment-free learning experience for everyone regardless of gender, gender identity and expression, sexual orientation, disability, physical appearance, body size, race, age or religion. We do not tolerate harassment of participants in any form. Sexual language and imagery is not appropriate either in-person or virtual form, including the Discussion boards and Slack workspace. Participants (including event volunteers and organizers) violating these rules may be sanctioned or expelled from the event at the discretion of the organizers.",
    "crumbs": [
      "Welcome",
      "Code of Conduct"
    ]
  },
  {
    "objectID": "coc.html#definition-of-harassment",
    "href": "coc.html#definition-of-harassment",
    "title": "Code of Conduct",
    "section": "Definition of Harassment",
    "text": "Definition of Harassment\nHarassment includes, but is not limited to:\n\nVerbal comments that reinforce social structures of domination related to gender, gender identity and expression, sexual orientation, disability, physical appearance, body size, race, age, religion.\nSexual images in public spaces\nDeliberate intimidation, stalking, or following\nHarassing photography or recording\nSustained disruption of talks or other events\nInappropriate physical contact\nUnwelcome sexual attention\nAdvocating for, or encouraging, any of the above behavior",
    "crumbs": [
      "Welcome",
      "Code of Conduct"
    ]
  },
  {
    "objectID": "coc.html#expectations",
    "href": "coc.html#expectations",
    "title": "Code of Conduct",
    "section": "Expectations",
    "text": "Expectations\nParticipants asked to stop any harassing behavior are expected to comply immediately. If a participant engages in harassing behavior, the organizers retain the right to take any actions to keep the event a welcoming environment for all participants. This includes warning the offender or expulsion from the event.\nThe organizers may take action to redress anything designed to, or with the clear impact of, disrupting the event or making the environment hostile for any participants. We expect participants to follow these rules at all the event venues and event-related social activities.",
    "crumbs": [
      "Welcome",
      "Code of Conduct"
    ]
  },
  {
    "objectID": "coc.html#reporting-a-violation",
    "href": "coc.html#reporting-a-violation",
    "title": "Code of Conduct",
    "section": "Reporting a violation",
    "text": "Reporting a violation\nHarassment and other code of conduct violations reduce the value of the event for everyone. If someone makes you or anyone else feel unsafe or unwelcome, please report it as soon as possible.\nIf you feel comfortable contacting someone associated with our event, you may speak with one of the event organizers in person or contact an organizer on a private Slack channel.",
    "crumbs": [
      "Welcome",
      "Code of Conduct"
    ]
  },
  {
    "objectID": "cloud-paradigm.html",
    "href": "cloud-paradigm.html",
    "title": "Cloud Paradigm",
    "section": "",
    "text": "Slides that introduce NASA Earthdata Cloud & the Cloud Paradigm."
  },
  {
    "objectID": "content/01-github.html#what-is-github",
    "href": "content/01-github.html#what-is-github",
    "title": "GitHub - Lecture 1",
    "section": "What is GitHub?",
    "text": "What is GitHub?\nGitHub is a hosting service for Git repositories, enabling us to share code across teams in a web environment.",
    "crumbs": [
      "Tutorials",
      "Git and GitHub Basics",
      "GitHub 1"
    ]
  },
  {
    "objectID": "content/01-github.html#why-do-i-need-a-github-account",
    "href": "content/01-github.html#why-do-i-need-a-github-account",
    "title": "GitHub - Lecture 1",
    "section": "Why do I need a GitHub account?",
    "text": "Why do I need a GitHub account?\n\nYour GitHub accounts will give you access to the hackweek cloud computing resources\nAll hackweek tutorials will be shared on GitHub\nAll project teams will use GitHub to collaborate and work together on their code\nGitHub is widely used to share code across the geosciences. If you don’t know how to use GitHub, you are missing out on a wealth of community shared knowledge.",
    "crumbs": [
      "Tutorials",
      "Git and GitHub Basics",
      "GitHub 1"
    ]
  },
  {
    "objectID": "content/01-github.html#creating-a-github-account",
    "href": "content/01-github.html#creating-a-github-account",
    "title": "GitHub - Lecture 1",
    "section": "Creating a GitHub account",
    "text": "Creating a GitHub account\nGo to GitHub. Next, enter your email address and click on the green ‘Sign up for GitHub’ button. You will need to answer a few required questions in the following dialogs.",
    "crumbs": [
      "Tutorials",
      "Git and GitHub Basics",
      "GitHub 1"
    ]
  },
  {
    "objectID": "content/01-github.html#following-your-github-activity",
    "href": "content/01-github.html#following-your-github-activity",
    "title": "GitHub - Lecture 1",
    "section": "Following your GitHub activity",
    "text": "Following your GitHub activity\nManaging GitHub notifications is important and you probably don’t want all notifications going to email. Only have things you really don’t want to miss going to notifications.\n\nNotification dashboard: https://github.com/notifications\nChanging your notifications settings\nNews dashboard https://github.com/dashboard",
    "crumbs": [
      "Tutorials",
      "Git and GitHub Basics",
      "GitHub 1"
    ]
  },
  {
    "objectID": "content/01-github.html#repositories",
    "href": "content/01-github.html#repositories",
    "title": "GitHub - Lecture 1",
    "section": "Repositories",
    "text": "Repositories\nEach repository is a container for a specific subset of material. GitHub is widely used to share code and tutorials! Let’s explore the content for\nMachine Learning in Environment Science, an recent course by Ben Best at University of California Santa Barbara. GitHub repo\n\nActivity\n\nLook at files\nDownload a file\nLook at files at an earlier time\nLook at what changes have been made",
    "crumbs": [
      "Tutorials",
      "Git and GitHub Basics",
      "GitHub 1"
    ]
  },
  {
    "objectID": "content/01-github.html#create-a-repository",
    "href": "content/01-github.html#create-a-repository",
    "title": "GitHub - Lecture 1",
    "section": "Create a repository",
    "text": "Create a repository\n\n\n\n\n\n\nCore skill\n\n\n\nCreate a new repository in GitHub by\n\nNavigating to your user profile page (click on your image in top right)\nClick the repositories tab in your profile\nClick the green New button and add a README\nAdd files\n\n\n\n\nActivity\nCreate a profile page for your GitHub account. Already have one? Be a helper for this exercise!\n\nCreate a repository and give it your username as the name. For example, mine would be named eeholmes\nAdd a file by clicking “Add File” and name the file “README.md”. This will become a profile page for your GitHub account.\nAdd some text to the README. You can use this as a template or use one of the many examples of profile READMEs that you can copy here: https://github.com/abhisheknaiidu/awesome-github-profile-readme\n\nExample profile README\n### Your Name \n\n- :school: I'm a student at **XYZ** in this city.\n- 🔭 I’m currently working on  **XYZ**\n- :game: When I am not working, I like to **XYZ**\n- At the Hackweek, ask me about &lt;some areas of expertise&gt;\n\n- 🌐 Visit my [website](https://eeholmes.github.io//) for complete background and contact.",
    "crumbs": [
      "Tutorials",
      "Git and GitHub Basics",
      "GitHub 1"
    ]
  },
  {
    "objectID": "content/01-intro-to-jupyterhub.html#log-into-the-jupyterhub",
    "href": "content/01-intro-to-jupyterhub.html#log-into-the-jupyterhub",
    "title": "Intro to JupyterHubs",
    "section": "Log into the JupyterHub",
    "text": "Log into the JupyterHub\nGo to https://dhub.opensci.live/. Click “Login to continue”. You will be asked to log in with your GitHub Account, if you are not logged in already.\n\nImage type: Python or R\nNext you select your image type. We have two different R images and two Python images. The images have geospatial libraries pre-loaded.\n\n\nVirtual Machine size\nYou’ll see something similar to this that allows you to choose a large virtual machine if your project needs it. For the tutorials, you will only need the Small Virtual Machine. Please only choose the large machines if you run out of RAM as the larger machines cost us more.\n\n\n\nMachine Profiles\n\n\n\n\nStart up\nAfter we select our server type and click on start, JupyterHub will allocate our instance using Amazon Web Services (AWS). This may take several minutes.\n\n\n\nJupyterhub Spawning",
    "crumbs": [
      "Tutorials",
      "Welcome",
      "Jupyter hubs"
    ]
  },
  {
    "objectID": "content/01-intro-to-jupyterhub.html#open-rstudio",
    "href": "content/01-intro-to-jupyterhub.html#open-rstudio",
    "title": "Intro to JupyterHubs",
    "section": "Open RStudio",
    "text": "Open RStudio\nWhen you are in the Jupyter Lab tab (note the Jupyter Logo), you will see a Launcher page. If you don’t see this, go to File &gt; New Launcher.\n\n\n\nJupyterhub Launcher\n\n\n\nOpen RStudio by clicking on the “RStudio” box in the Launcher tab:\n\n We will be mainly using RStudio in this course.",
    "crumbs": [
      "Tutorials",
      "Welcome",
      "Jupyter hubs"
    ]
  },
  {
    "objectID": "content/01-intro-to-jupyterhub.html#end-your-session",
    "href": "content/01-intro-to-jupyterhub.html#end-your-session",
    "title": "Intro to JupyterHubs",
    "section": "End your session",
    "text": "End your session\nWhen you are finished working for the day it is important to log out of the JupyterHub. When you keep a session active it uses up AWS resources (costs money) and keeps a series of virtual machines deployed.\n\n\n\n\n\n\nCaution\n\n\n\nYou log out from the Jupyter Lab tab not the RStudio tab.\n\n\nFrom the Jupyter Lab tab, do one of two things to stop the server:\n\nLog out File -&gt; Log Out and click “Log Out”!\nor File -&gt; Hub Control Panel -&gt; Stop My Server\n\n\n\n\n\n\n\nTip\n\n\n\nCan’t find the Jupyter Lab tab? Go to https://dhub.opensci.live/hub/home",
    "crumbs": [
      "Tutorials",
      "Welcome",
      "Jupyter hubs"
    ]
  },
  {
    "objectID": "content/01-intro-to-jupyterhub.html#restart-your-server",
    "href": "content/01-intro-to-jupyterhub.html#restart-your-server",
    "title": "Intro to JupyterHubs",
    "section": "Restart your server",
    "text": "Restart your server\nSometimes the server will crash/stop. This can happen if too many people use a lot of memory all at once. If that happens, go to the Jupyter Lab tab and then File -&gt; Hub Control Panel -&gt; Stop My Server and then Start My Server. You shouldn’t lose your work unless you were uploading a file.",
    "crumbs": [
      "Tutorials",
      "Welcome",
      "Jupyter hubs"
    ]
  },
  {
    "objectID": "content/01-intro-to-jupyterhub.html#your-files",
    "href": "content/01-intro-to-jupyterhub.html#your-files",
    "title": "Intro to JupyterHubs",
    "section": "Your files",
    "text": "Your files\nWhen you start your server, you will have access to your own virtual drive space. No other users will be able to see or access your files. You can upload files to your virtual drive space and save files here. You can create folders to organize your files. You personal directory is home/jovyan. Everyone has the same home directory but your files are separate and cannot be seen by others.\nThere are a number of different ways to create new files. We will practice this in the RStudio lecture.\n\nWill I lose all of my work?\nLogging out will NOT cause any of your work to be lost or deleted. It simply shuts down some resources. It would be equivalent to turning off your desktop computer at the end of the day.",
    "crumbs": [
      "Tutorials",
      "Welcome",
      "Jupyter hubs"
    ]
  },
  {
    "objectID": "content/01-intro-to-jupyterhub.html#shared-files",
    "href": "content/01-intro-to-jupyterhub.html#shared-files",
    "title": "Intro to JupyterHubs",
    "section": "Shared files",
    "text": "Shared files\n\n\n\nShared folder\n\n\nIn the file panel, you will see a folder called shared. These are read-only shared files. There are two main folders:\n\ndata The folder with data files\n2023-Hackbook The full Git repository with this website\n\nYou will also see shared-public. This is a read-write folder for you to put files for everyone to see and use. You can create a team folder here for shared data and files. Note, everyone can see and change these so be careful to communicate with your team so multiple people don’t work on the same file at the same time. You can also create folders for each team member and agree not to change other team members files.\nYou will also see other folders of Python content for the Python users. We will have a separate introduction to Jupyter Notebooks.",
    "crumbs": [
      "Tutorials",
      "Welcome",
      "Jupyter hubs"
    ]
  },
  {
    "objectID": "content/01-intro-to-jupyterhub.html#python-users",
    "href": "content/01-intro-to-jupyterhub.html#python-users",
    "title": "Intro to JupyterHubs",
    "section": "**Python users",
    "text": "**Python users\nYou can open a Jupyter Notebook by clicking on the “Python 3” box. In the Launcher tab:\n\n\n\nJupyterhub Launcher\n\n\nJupyter notebooks are a very common way to share Python code and tutorials. Because we are focusing on R in this course, we will not use Jupyter notebooks much but we have many Juptyer notebooks that you can experiment with in the python-tutorials folder. Get an overview of Jupyter Lab: Intro to Jupyter Lab Learn about the geosciences tools in Python\nNote: Python users can run Python in RStudio and R users can run R in Jupyter Notebooks.",
    "crumbs": [
      "Tutorials",
      "Welcome",
      "Jupyter hubs"
    ]
  },
  {
    "objectID": "content/01-intro-to-jupyterhub.html#faq",
    "href": "content/01-intro-to-jupyterhub.html#faq",
    "title": "Intro to JupyterHubs",
    "section": "FAQ",
    "text": "FAQ\nWhy do we have the same home directory as /home/jovyan? /home/jovyan is the default home directory for ‘jupyter’ based images/dockers. It is the historic home directory for Jupyter deployments.\nCan other users see the files in my /home/jovyan folder? No, other users can not see your credentials.\n\nAcknowledgements\nSome sections of this document have been taken from hackweeks organized by the University of Washington eScience Institute and Openscapes.",
    "crumbs": [
      "Tutorials",
      "Welcome",
      "Jupyter hubs"
    ]
  },
  {
    "objectID": "content/02-git-jupyter.html#summary",
    "href": "content/02-git-jupyter.html#summary",
    "title": "Git - Jupyter Lab",
    "section": "Summary",
    "text": "Summary\nIn this tutorial, we will provide a brief introduction to:\n\nCommand line (terminal/shell)\nNavigating around folders in Jupyter Lab\nVersion Control (code management using git)\nSetting up Git in Jupyter Lab\nThe Git GUI in Jupyter Lab\nBasic Git commands"
  },
  {
    "objectID": "content/02-git-jupyter.html#introduction-jupyter-lab",
    "href": "content/02-git-jupyter.html#introduction-jupyter-lab",
    "title": "Git - Jupyter Lab",
    "section": "Introduction :: Jupyter Lab",
    "text": "Introduction :: Jupyter Lab\nWhen you start the JupyterHub, you will be in Jupyter Lab. From there you can click on the RStudio box and open RStudio. However for this tutorial, we will stay in Juptyer Lab."
  },
  {
    "objectID": "content/02-git-jupyter.html#introduction-terminalshell",
    "href": "content/02-git-jupyter.html#introduction-terminalshell",
    "title": "Git - Jupyter Lab",
    "section": "Introduction :: Terminal/Shell",
    "text": "Introduction :: Terminal/Shell\nLog into the JupyterHub. If you do not see this\n\nThen go to File &gt; New Launcher\nClick on the “Terminal” box to open a new terminal window.\n\nShell or Terminal Basics\n\nWhat is Terminal or Shell?\nNavigating Files and Directories\nWorking with Files and Directories\nOptional: Detailed self-paced lesson on running scripts from the shell: Shell Lesson from Software Carpentry\n\nYou will need only basic navigation skills for this course: cd, ls and cat\n\npwd where am I\ncd nameofdir move into a directory\ncd .. move up a directory\nls list the files in the current directory\nls -a list the files including hidden files\nls -l list the files with more info\ncat filename print out the contents of a file\n\n\n\nLet’s try\nls\nls -a\ncd shared\nls\ncd shell-tutorial\ncat lesson1.sh\ncd ..\ncd ..\n\n\nClose the terminal\nJust click on the X in the terminal tab"
  },
  {
    "objectID": "content/02-git-jupyter.html#introduction-file-navigation",
    "href": "content/02-git-jupyter.html#introduction-file-navigation",
    "title": "Git - Jupyter Lab",
    "section": "Introduction :: File Navigation",
    "text": "Introduction :: File Navigation\nIn the far left, you will see a line of icons. The top one is a folder and allows us to move around our file system.\n\nClick on shared. Now you can see the files in the shared directory.\nClick on shell-tutorial. Then click on lesson1.sh. The file opens. You won’t be able to save changes here because you don’t have write permission on this drive.\nClick on the folder icon that looks like this. Click on the actual folder image. \nNow it should look like this folder /\nThis shows me doing this\n\nCreate a new folder.\n\nNext to the blue rectange with a +, is a grey folder with a +. Click that to create a new folder, called lesson-scripts.\nThen click on lesson-scripts to enter the folder\n\n\nCreate a new file\n\nCreate with File &gt; New &gt; Text file\nThe file will open and you can edit it.\nSave with File &gt; Save Text\nDelete the file by right-clicking on it and clicking “Delete”"
  },
  {
    "objectID": "content/02-git-jupyter.html#introduction-version-control-git",
    "href": "content/02-git-jupyter.html#introduction-version-control-git",
    "title": "Git - Jupyter Lab",
    "section": "Introduction :: Version Control (Git)",
    "text": "Introduction :: Version Control (Git)\n\nWhat is version control, git, github, and how to set it up?\nVersion control is managing and tracking changes to your documents (program source code, images, websites, data files, etc.). git is a popular tool used for version control of software code. github.com is popular platform that provides remote server hosting for git repositories. A repository is a collection of various files that you are tracking for changes and versions. Currently GitHub is the most popular platform for file sharing code and code packages.\nThis section is a step-by-step guide to set up git on our JupyterHub. We will also configure git to use your github.com account for managing your repositories hosted on github.com. There are 5 main steps.\n\n\nStep 1: Create a GitHub account\nTo complete the setup, you will need an account on github.com. If you don’t have an account, please visit github.com, create an account (free) and come back to this guide for setting up git.\n\n\nStep 2: Clone a repository\nWe have created a demo repository for you to clone:\nhttps://github.com/nmfs-opensci/Git-Lesson\n\nStart your JupyterHub\nClick on the Git icon\n\n\n\nClick “Clone a Repository”\nWhere it says “Enter the URI of the remote Git repository”, paste in the URL https://github.com/nmfs-opensci/Git-Lesson\nThe folder appears and you can enter the folder and edit and create files.\n\n\nYour task: Create a file with your name and save to the Git-Lesson folder"
  },
  {
    "objectID": "content/02-git-jupyter.html#step-3",
    "href": "content/02-git-jupyter.html#step-3",
    "title": "Git - Jupyter Lab",
    "section": "Step 3:",
    "text": "Step 3:\nConfigure git with your name and email address.\n``` bash\ngit config --global user.name \"Makhan Virdi\"\ngit config --global user.email \"Makhan.Virdi@gmail.com\"\n```\n\n**Note:** This name and email could be different from your github.com credentials. Remember `git` is a program that keeps track of your changes locally (on the JupyterHub or your own computer) and github.com is a platform to host your repositories. However, since your changes are tracked by `git`, the email/name used in git configuration will show up next to your contributions on github.com when you `push` your repository to github.com (`git push` is discussed in a later step).\n\nConfigure git to store your github credentials to avoid having to enter your github username and token each time you push changes to your repository(in Step 5, we will describe how to use github token instead of a password)\ngit config --global credential.helper store\nCopy link for the demo repository from your github account. Click the green “Code” button and copy the link as shown.\n\nClone the repository using git clone command in the terminal\nTo clone a repository from github, copy the link for the repository (previous step) and use git clone:\ngit clone https://github.com/YOUR-GITHUB-USERNAME/check_github_setup\nNote: Replace YOUR-GITHUB-USERNAME here with your github.com username. For example, it is virdi for my github.com account as seen in this image.\n\nUse ls (list files) to verify the existence of the repository that you just cloned\n\nChange directory to the cloned repository using cd check_github_setup and check the current directory using pwd command (present working directory)\n\nCheck status of your git repository to confirm git set up using git status\n\nYou are all set with using git on your 2i2c JupyterHub! But the collaborative power of git through github needs some additional setup.\nIn the next step, we will create a new file in this repository, track changes to this file, and link it with your github.com account.\n\n\nStep 4. Creating new file and tracking changes\n\nIn the left panel on your 2i2c JupyterHub, click on the “directory” icon and then double click on “check_github_setup” directory.\n\n\nOnce you are in the check_github_setup directory, create a new file using the text editor in your 2i2c JupyterHub (File &gt;&gt; New &gt;&gt; Text File).\n\nName the file lastname.txt. For example, virdi.txt for me (use your last name). Add some content to this file (for example, I added this to my virdi.txt file: my last name is virdi).\n\nNow you should have a new file (lastname.txt) in the git repository directory check_github_setup\nCheck if git can see that you have added a new file using git status. Git reports that you have a new file that is not tracked by git yet, and suggests adding that file to the git tracking system.\n\nAs seen in this image, git suggests adding that file so it can be tracked for changes. You can add file to git for tracking changes using git add. Then, you can commit changes to this file’s content using git commit as shown in the image.\ngit add virdi.txt\ngit status\ngit commit -m \"adding a new file\"\ngit status\n\nAs seen in the image above, git is suggesting to push the change that you just committed to the remote server at github.com (so that your collaborators can also see what changes you made).\nNote: DO NOT execute push yet. Before we push to github.com, let’s configure git further and store our github.com credentials to avoid entering the credentials every time we invoke git push. For doing so, we need to create a token on github.com to be used in place of your github.com password.\n\n\n\nStep 5. Create access token on github.com\n\nGo to your github account and create a new “personal access token”: https://github.com/settings/tokens/new\n\nEnter a description in “Note” field as seen above, select “repo” checkbox, and scroll to the bottom and click the green button “Generate Token”. Once generated, copy the token (or save it in a text file for reference).\nIMPORTANT: You will see this token only once, so be sure to copy this. If you do not copy your token at this stage, you will need to generate a new token.\n\nTo push (transfer) your changes to github, use git push in terminal. It requires you to enter your github credentials. You will be prompted to enter your github username and “password”. When prompted for your “password”, DO NOT use your github password, use the github token that was copied in the previous step.\ngit push\n\nNote: When you paste your token in the terminal window, windows users will press Ctrl+V and mac os users will press Cmd+V. If it does not work, try generating another token and use the copy icon next to the token to copy the token. Then, paste using your computer’s keyboard shortcut for paste.\nNow your password is stored in ~/.git-credentials and you will not be prompted again unless the Github token expires. You can check the presence of this git-credentials file using Terminal. Here the ~ character represents your home directory (/home/jovyan/).\nls -la ~\nThe output looks like this:\ndrwxr-xr-x 13 jovyan jovyan 6144 Oct 22 17:35 .\ndrwxr-xr-x  1 root   root   4096 Oct  4 16:21 ..\n-rw-------  1 jovyan jovyan 1754 Oct 29 18:30 .bash_history\ndrwxr-xr-x  4 jovyan jovyan 6144 Oct 29 16:38 .config\n-rw-------  1 jovyan jovyan   66 Oct 22 17:35 .git-credentials\n-rw-r--r--  1 jovyan jovyan   84 Oct 22 17:14 .gitconfig\ndrwxr-xr-x 10 jovyan jovyan 6144 Oct 21 16:19 2021-Cloud-Hackathon\nYou can also verify your git configuration\n(notebook) jovyan@jupyter-virdi:~$ git config -l\nThe output should have credential.helper = store:\nuser.email        = Makhan.Virdi@gmail.com\nuser.name         = Makhan Virdi\ncredential.helper = store\n\nNow we are all set to collaborate with github on the JupyterHub during the Cloud Hackathon!\n\n\nSummary: Git Commands\n\nCommonly used git commands (modified from source)\n\n\nGit Command\nDescription\n\n\n\n\ngit status\nShows the current state of the repository: the current working branch, files in the staging area, etc.\n\n\ngit add\nAdds a new, previously untracked file to version control and marks already tracked files to be committed with the next commit\n\n\ngit commit\nSaves the current state of the repository and creates an entry in the log\n\n\ngit log\nShows the history for the repository\n\n\ngit diff\nShows content differences between commits, branches, individual files and more\n\n\ngit clone\nCopies a repository to your local environment, including all the history\n\n\ngit pull\nGets the latest changes of a previously cloned repository\n\n\ngit push\nPushes your local changes to the remote repository, sharing them with others\n\n\n\n\n\nGit: More Details\nLesson: For a more detailed self-paced lesson on git, visit Git Lesson from Software Carpentry\nCheatsheet: Frequently used git commands\nDangit, Git!?!: If you are stuck after a git mishap, there are ready-made solutions to common problems at Dangit, Git!?!\n\n\nCloning our repository using the git Jupyter lab extension.\nIf we’re already familiar with git commands and feel more confortable using a GUI our Jupyterhub deployment comes with a git extension. This plugin allows us to operate with git using a simple user interface.\nFor example we can clone our repository using the extension.\n\n\n\ngit extension"
  },
  {
    "objectID": "content/02-quarto.html",
    "href": "content/02-quarto.html",
    "title": "Intro to Quarto",
    "section": "",
    "text": "Quarto was announced at the 2022 RStudio Conference in this great keynote talk: Mine Çetinkaya-Rundel & Julia Stewart Lowndes | Hello Quarto. I am going to start by showing you 2 clips from this keynote.",
    "crumbs": [
      "Tutorials",
      "R and RStudio Basics",
      "Quarto"
    ]
  },
  {
    "objectID": "content/02-quarto.html#what-does-quarto-and-rmarkdown-do",
    "href": "content/02-quarto.html#what-does-quarto-and-rmarkdown-do",
    "title": "Intro to Quarto",
    "section": "What does Quarto (and RMarkdown) do?",
    "text": "What does Quarto (and RMarkdown) do?\n\nQuarto is bundled and comes pre-installed with RStudio v2022.07.1 and beyond!",
    "crumbs": [
      "Tutorials",
      "R and RStudio Basics",
      "Quarto"
    ]
  },
  {
    "objectID": "content/02-quarto.html#lets-learn-by-creating",
    "href": "content/02-quarto.html#lets-learn-by-creating",
    "title": "Intro to Quarto",
    "section": "Let’s learn by creating!",
    "text": "Let’s learn by creating!\n\nOpen up the JupyterHub and navigate to RStudio.\nPlan B. You can open up RStudio on your laptop.\n\n\nCreate a basic Quarto document\nFile &gt; New File &gt; Quarto document\n\n\nCreate a basic Quarto presentation\nFile &gt; New File &gt; Quarto presentation\n\n\nCreate a basic Quarto book\nFile &gt; New Project &gt; Quarto book\n\n\nMake a bit more fancy Quarto book\nActivity for advanced R/GitHub users\n\nGo here https://github.com/nmfs-opensci/NOAA-quarto-book\nClick the “Use this template” button and make copy in your GitHub account and give it a different name. Make sure to click the checkbox to copy all branches. Change owner to your account. \nCopy YOUR repo URL. It’ll look like https://github.com/yourusername/my-quarto-book\nFile &gt; New Project &gt; Version Control and paste in your URL\nClick Create and then you can click the Build tab and render the book.\nMake some changes and push back to GitHub.\nTurn on GitHub Pages via Settings &gt; Pages and selecting the gh-pages branch. \nYour book will automatically be published to the web.",
    "crumbs": [
      "Tutorials",
      "R and RStudio Basics",
      "Quarto"
    ]
  },
  {
    "objectID": "content/02-quarto.html#intro-to-quarto-workshops",
    "href": "content/02-quarto.html#intro-to-quarto-workshops",
    "title": "Intro to Quarto",
    "section": "Intro to Quarto Workshops",
    "text": "Intro to Quarto Workshops\nThomas Mocke (Posit) has great videos on Quarto. I am using material from a 2-hour online workshop.\n\nContent\nVideo\nGitHub repo\n\nHe also gave a 2-day workshop if you want to learn Quarto indepth.\n\n2 day workshop\n\nMore workshop that you can watch and join on Posit YouTube",
    "crumbs": [
      "Tutorials",
      "R and RStudio Basics",
      "Quarto"
    ]
  },
  {
    "objectID": "content/03-ai.html",
    "href": "content/03-ai.html",
    "title": "AI Assisted Programming",
    "section": "",
    "text": "2023 has seen an dramatic shift toward a new workflow for programming and learning to program: AI-Assisted Programming. Using AI as your personal coding buddy and coach is the new way to work. But you need to learn how to interact with your AI helper and ask it questions.\nWe are going to watch a presentation given by Myranda Shirk, Senior Data Scientist, Vanderbilt University, Knoxville, US. This presentation was given at OceanHackWeek 2023 at the University of Washington in August 2023. Myranda will give you an overview of the current AI platforms and then demo how to interact with them. During this hackweek, you get a lot of practice with AI assisted programming and AI assisted brainstorming.\nOne important thing to note is that the chatGPT tool that Myranda used (code interpreter) is now called Advanced Data Analytics.\nVideo: https://youtu.be/JCHa-yGgtr4?si=WV956qZkSEVvy-CQ\nLecture Notes: https://oceanhackweek.org/ohw23/tutorials/02-Wed/ai_assisted_programming_ohw.html",
    "crumbs": [
      "Tutorials",
      "R and RStudio Basics",
      "Programming with AI"
    ]
  },
  {
    "objectID": "content/04-intro-sdm.html#big-picture",
    "href": "content/04-intro-sdm.html#big-picture",
    "title": "Intro to SDM",
    "section": "Big Picture",
    "text": "Big Picture\nIn the broader context, our goal is to uncover where species are likely to be found, not just for their protection but also to strategically plan conservation efforts and to help manage fisheries. We aim to discover where these species are located and anticipate how their habitats might shift in the future.\nBut handling all this data manually is virtually impossible. It’s like trying to piece together a complex puzzle with millions of tiny pieces.\nSpecies Distribution Modeling (SDM) combined with big data analytics comes into play. SDM helps us make sense of this vast environmental data. We can extract meaningful patterns and relationships between the environment and species distribution."
  },
  {
    "objectID": "content/04-intro-sdm.html#nasa-arset",
    "href": "content/04-intro-sdm.html#nasa-arset",
    "title": "Intro to SDM",
    "section": "NASA ARSET",
    "text": "NASA ARSET\nI am going to be showing some slides from NASA ARSET’ Training Course on Species Distribution Modeling (here. This is a 4.5 hour webinar focusing on an R package called Wallace, which I won’t be talking about as it is more focused on land SDMs. But the first lecture gives a good general overview of SDMs.\nMcCullum, A.; Torres-Pérez, J.; Bengtsson, Z.; Johnson, E.; Paz Velez, A.; Blair, M. (2021). Species Distribution Modeling with Remote Sensing. NASA Applied Remote Sensing Training Program (ARSET). https://appliedsciences.nasa.gov/join-mission/training/english/arset-species-distribution-modeling-remote-sensing\nNASA ARSET is the NASA Applied Remote Sensing Training (ARSET) Program and they have virtual and on-line trainings for using remote-sensing data. Their Fundamentals of Remote Sensing Course is a good overview."
  },
  {
    "objectID": "content/04-intro-sdm.html#slides",
    "href": "content/04-intro-sdm.html#slides",
    "title": "Intro to SDM",
    "section": "Slides",
    "text": "Slides"
  },
  {
    "objectID": "content/XX-r-and-python-resources.html#programming-in-r",
    "href": "content/XX-r-and-python-resources.html#programming-in-r",
    "title": "R and Python Resources",
    "section": "Programming in R",
    "text": "Programming in R\nThe following give a basic intro to R and RStudio.\n\nW3Schools Basic R\nBasics of R and installing RStudio\nYouTube: R for Beginners\n\n\nBeyond the basics of R\n\nSoftware Carpentry Novice R\nSoftware Carpentry Intro to R\nFor a longer free course, CodeAcademy has a good Intro to R. Note this is 20 hours. You do NOT need to do this whole course before the hackweek!\nIf you know the basics of R but have never done modeling, then you can start with this 2.5 hour course that covers basic linear regression with R and plotting with ggplot2:\nOne of the best free R courses is Coursera’s Learn R. You can see the lectures for free. Make sure to click AUDIT when it asks you to sign up for a 7-day trial. You do not need to sign up for anything to go through the material."
  },
  {
    "objectID": "content/XX-r-and-python-resources.html#programming-in-python",
    "href": "content/XX-r-and-python-resources.html#programming-in-python",
    "title": "R and Python Resources",
    "section": "Programming in Python",
    "text": "Programming in Python\nSwitch to Jupyter Notebook for an introduction to programming in Python\n\nVariables (and mathematical operations)\nData Structures (list, tuple, dict)\nFlow Control using loops (for, while)\nConditionals (if, else, elif)\nFunctions\nErrors and Exceptions (understanding and handling errors)\nUsing modules (libraries, packages)\n\npandas: high-performance, easy-to-use data structures and data analysis tools\nrioxarray: based on the rasterio package for working with rasters and xarray\n\n\n\nBeyond the basics in Python\n\nSelf-paced lesson on Programming with Python from Software Carpentry\nDetailed self-paced lesson on Fundamentals of Geospatial Raster and Vector Data with Python from Data Carpentry"
  },
  {
    "objectID": "content/earthdata.html#overview",
    "href": "content/earthdata.html#overview",
    "title": "Earthdata Login",
    "section": "Overview",
    "text": "Overview"
  },
  {
    "objectID": "content/earthdata.html#why-do-i-need-an-earthdata-login",
    "href": "content/earthdata.html#why-do-i-need-an-earthdata-login",
    "title": "Earthdata Login",
    "section": "Why do I need an Earthdata login?",
    "text": "Why do I need an Earthdata login?\nWe will be teaching you ways to programmatically access NASA remote-sensing data from within your scripts. You will need to enter your Earthdata username and password in order for this to work."
  },
  {
    "objectID": "content/earthdata.html#getting-an-earthdata-login",
    "href": "content/earthdata.html#getting-an-earthdata-login",
    "title": "Earthdata Login",
    "section": "Getting an Earthdata login",
    "text": "Getting an Earthdata login\nIf you do not already have an Earthdata login, then navigate to the Earthdata Login page, a username and password, and then record this somewhere for use during the tutorials:"
  },
  {
    "objectID": "content/earthdata.html#configure-programmatic-access-to-nasa-servers",
    "href": "content/earthdata.html#configure-programmatic-access-to-nasa-servers",
    "title": "Earthdata Login",
    "section": "Configure programmatic access to NASA servers",
    "text": "Configure programmatic access to NASA servers\nIf you use web interfaces to retrieve nasa data such as Earthdata Search you are prompted to login. We will be using software to retrieve data from NASA Servers during the hackweek, so you must store your credentials on the JupyterHub. Run the following commands on the JupyterHub in a terminal replacing your Earthdata login username and password:\necho \"machine urs.earthdata.nasa.gov login EARTHDATA_LOGIN password EARTHDATA_PASSWORD\" &gt; ~/.netrc\nchmod 0600 .netrc"
  },
  {
    "objectID": "content/index.html",
    "href": "content/index.html",
    "title": "Week 1 Tutorials",
    "section": "",
    "text": "During week 1, participants will gain experience with the platforms used in collaborative science: GitHub and RMarkdown."
  },
  {
    "objectID": "content/index.html#prerequisites",
    "href": "content/index.html#prerequisites",
    "title": "Week 1 Tutorials",
    "section": "Prerequisites",
    "text": "Prerequisites\nPlease follow the set up prerequisites"
  },
  {
    "objectID": "content/index.html#content",
    "href": "content/index.html#content",
    "title": "Week 1 Tutorials",
    "section": "Content",
    "text": "Content\n\nThe R language and RStudio\nIntro to RStudio\nIntroduction to Git and GitHub"
  },
  {
    "objectID": "content/notebooks.html#summary",
    "href": "content/notebooks.html#summary",
    "title": "RMarkdown, R, Git",
    "section": "Summary",
    "text": "Summary\nIn this session, we will provide a brief introduction to:\n\nCommand line (terminal/shell)\nVersion Control (code management using git)\nProgramming in Python (using Jupyter Notebook)\nGeospatial Fundamentals (optional, self-study)\n\nYou will need a working knowledge of git and terminal for this hackathon. We will provide an overview of these topics and also share resources for self-paced learning."
  },
  {
    "objectID": "content/notebooks.html#introduction-command-line-terminalshell",
    "href": "content/notebooks.html#introduction-command-line-terminalshell",
    "title": "RMarkdown, R, Git",
    "section": "Introduction :: Command Line (Terminal/Shell)",
    "text": "Introduction :: Command Line (Terminal/Shell)\n\nShell Basics\n\nWhat is Terminal or Shell?\nNavigating Files and Directories\nWorking with Files and Directories\n\n\n\nShell: More Details\nDetailed self-paced lesson on shell: Shell Lesson from Software Carpentry"
  },
  {
    "objectID": "content/notebooks.html#introduction-version-control-git-and-github",
    "href": "content/notebooks.html#introduction-version-control-git-and-github",
    "title": "RMarkdown, R, Git",
    "section": "Introduction :: Version Control (Git and Github)",
    "text": "Introduction :: Version Control (Git and Github)\n\nWhat is version control, git, github, and how to set it up?\nVersion control is managing and tracking changes to your documents (program source code, images, websites, data files, etc.). git is a popular tool used for version control of software code. github.com is popular platform that provides remote server hosting for git repositories. A repository is a collection of various files that you are tracking for changes and versions (think of it as a directory with files that are being tracked for changes, using git for taking snapshots of versions as you are developing).\nThis section is a step-by-step guide to set up git on your 2i2c JupyterHub instance (referred to as 2i2c JupyterHub in these instruction). We will also configure git to use your github.com account for managing your repositories hosted on github.com. There are 5 main steps with substeps, includes instruction for addressing github’s new approach for token authentication.\n\n\nStep 1: Create a github account\nTo complete the setup, you will need an account on github.com. If you don’t have an account, please visit github.com, create an account (free) and come back to this guide for setting up git.\n\n\nStep 2: Fork a repository\nA fork is a copy of a repository from another github account (for example NASA-Openscapes account) to your github account (for example, my account virdi) that then you have permission to edit. To help you finish this setup correctly, we have created a demo repository on Openscapes github account named check_github_setup. You can fork this repository into your github account following these steps:\n\nLog in to your github.com account\nGo to the demo repository at NASA-Openscapes github\n\n\n\nDemo repository on NASA-Openscapes github\n\n\nClick on the fork icon in the top right corner, as shown in the image below and click your user name if prompted to do so\n\n\n\n\nStep 3: Clone the repository that you just forked\nNow you have a fork of the demo repository in your github account that we can clone it in your 2i2c instance. In the code below, commands beginning with git is a git command for version control and synching; commands that don’t start with git are bash/linux/command line commands.\n\nStart your 2i2c JupyterHub and open a terminal\nFile &gt;&gt; New &gt;&gt; Terminal\nMake sure you are in your home directory by usingpwd command and verifying the output as below\n/home/jovyan\n\nConfigure git with your name and email address.\ngit config --global user.name \"Makhan Virdi\"\ngit config --global user.email \"Makhan.Virdi@gmail.com\"\nNote: This name and email could be different from your github.com credentials. Remember git is a program that keeps track of your changes locally (on 2i2c JupyterHub or your own computer) and github.com is a platform to host your repositories. However, since your changes are tracked by git, the email/name used in git configuration will show up next to your contributions on github.com when you push your repository to github.com (git push is discussed in a later step).\nConfigure git to store your github credentials to avoid having to enter your github username and token each time you push changes to your repository(in Step 5, we will describe how to use github token instead of a password)\ngit config --global credential.helper store\nCopy link for the demo repository from your github account. Click the green “Code” button and copy the link as shown.\n\nClone the repository using git clone command in the terminal\nTo clone a repository from github, copy the link for the repository (previous step) and use git clone:\ngit clone https://github.com/YOUR-GITHUB-USERNAME/check_github_setup\nNote: Replace YOUR-GITHUB-USERNAME here with your github.com username. For example, it is virdi for my github.com account as seen in this image.\n\nUse ls (list files) to verify the existence of the repository that you just cloned\n\nChange directory to the cloned repository using cd check_github_setup and check the current directory using pwd command (present working directory)\n\nCheck status of your git repository to confirm git set up using git status\n\nYou are all set with using git on your 2i2c JupyterHub! But the collaborative power of git through github needs some additional setup.\nIn the next step, we will create a new file in this repository, track changes to this file, and link it with your github.com account.\n\n\n\nStep 4. Creating new file and tracking changes\n\nIn the left panel on your 2i2c JupyterHub, click on the “directory” icon and then double click on “check_github_setup” directory.\n\n\nOnce you are in the check_github_setup directory, create a new file using the text editor in your 2i2c JupyterHub (File &gt;&gt; New &gt;&gt; Text File).\n\nName the file lastname.txt. For example, virdi.txt for me (use your last name). Add some content to this file (for example, I added this to my virdi.txt file: my last name is virdi).\n\nNow you should have a new file (lastname.txt) in the git repository directory check_github_setup\nCheck if git can see that you have added a new file using git status. Git reports that you have a new file that is not tracked by git yet, and suggests adding that file to the git tracking system.\n\nAs seen in this image, git suggests adding that file so it can be tracked for changes. You can add file to git for tracking changes using git add. Then, you can commit changes to this file’s content using git commit as shown in the image.\ngit add virdi.txt\ngit status\ngit commit -m \"adding a new file\"\ngit status\n\nAs seen in the image above, git is suggesting to push the change that you just committed to the remote server at github.com (so that your collaborators can also see what changes you made).\nNote: DO NOT execute push yet. Before we push to github.com, let’s configure git further and store our github.com credentials to avoid entering the credentials every time we invoke git push. For doing so, we need to create a token on github.com to be used in place of your github.com password.\n\n\n\nStep 5. Create access token on github.com\n\nGo to your github account and create a new “personal access token”: https://github.com/settings/tokens/new\n\nEnter a description in “Note” field as seen above, select “repo” checkbox, and scroll to the bottom and click the green button “Generate Token”. Once generated, copy the token (or save it in a text file for reference).\nIMPORTANT: You will see this token only once, so be sure to copy this. If you do not copy your token at this stage, you will need to generate a new token.\n\nTo push (transfer) your changes to github, use git push in terminal. It requires you to enter your github credentials. You will be prompted to enter your github username and “password”. When prompted for your “password”, DO NOT use your github password, use the github token that was copied in the previous step.\ngit push\n\nNote: When you paste your token in the terminal window, windows users will press Ctrl+V and mac os users will press Cmd+V. If it does not work, try generating another token and use the copy icon next to the token to copy the token. Then, paste using your computer’s keyboard shortcut for paste.\nNow your password is stored in ~/.git-credentials and you will not be prompted again unless the Github token expires. You can check the presence of this git-credentials file using Terminal. Here the ~ character represents your home directory (/home/jovyan/).\nls -la ~\nThe output looks like this:\ndrwxr-xr-x 13 jovyan jovyan 6144 Oct 22 17:35 .\ndrwxr-xr-x  1 root   root   4096 Oct  4 16:21 ..\n-rw-------  1 jovyan jovyan 1754 Oct 29 18:30 .bash_history\ndrwxr-xr-x  4 jovyan jovyan 6144 Oct 29 16:38 .config\n-rw-------  1 jovyan jovyan   66 Oct 22 17:35 .git-credentials\n-rw-r--r--  1 jovyan jovyan   84 Oct 22 17:14 .gitconfig\ndrwxr-xr-x 10 jovyan jovyan 6144 Oct 21 16:19 2021-Cloud-Hackathon\nYou can also verify your git configuration\n(notebook) jovyan@jupyter-virdi:~$ git config -l\nThe output should have credential.helper = store:\nuser.email        = Makhan.Virdi@gmail.com\nuser.name         = Makhan Virdi\ncredential.helper = store\n\nNow we are all set to collaborate with github on the JupyterHub during the Cloud Hackathon!\n\n\nSummary: Git Commands\n\nCommonly used git commands (modified from source)\n\n\nGit Command\nDescription\n\n\n\n\ngit status\nShows the current state of the repository: the current working branch, files in the staging area, etc.\n\n\ngit add\nAdds a new, previously untracked file to version control and marks already tracked files to be committed with the next commit\n\n\ngit commit\nSaves the current state of the repository and creates an entry in the log\n\n\ngit log\nShows the history for the repository\n\n\ngit diff\nShows content differences between commits, branches, individual files and more\n\n\ngit clone\nCopies a repository to your local environment, including all the history\n\n\ngit pull\nGets the latest changes of a previously cloned repository\n\n\ngit push\nPushes your local changes to the remote repository, sharing them with others\n\n\n\n\n\nGit: More Details\nLesson: For a more detailed self-paced lesson on git, visit Git Lesson from Software Carpentry\nCheatsheet: Frequently used git commands\nDangit, Git!?!: If you are stuck after a git mishap, there are ready-made solutions to common problems at Dangit, Git!?!\n\n\nCloning our repository using the git Jupyter lab extension.\nIf we’re already familiar with git commands and feel more confortable using a GUI our Jupyterhub deployment comes with a git extension. This plugin allows us to operate with git using a simple user interface.\nFor example we can clone our repository using the extension.\n\n\n\ngit extension"
  },
  {
    "objectID": "content/notebooks.html#introduction-programming-in-python",
    "href": "content/notebooks.html#introduction-programming-in-python",
    "title": "RMarkdown, R, Git",
    "section": "Introduction :: Programming in Python",
    "text": "Introduction :: Programming in Python"
  },
  {
    "objectID": "content/notebooks.html#introduction-programming-in-python-1",
    "href": "content/notebooks.html#introduction-programming-in-python-1",
    "title": "RMarkdown, R, Git",
    "section": "Introduction :: Programming in Python",
    "text": "Introduction :: Programming in Python\nSwitch to Jupyter Notebook for an introduction to programming in Python\n\nVariables (and mathematical operations)\nData Structures (list, tuple, dict)\nFlow Control using loops (for, while)\nConditionals (if, else, elif)\nFunctions\nErrors and Exceptions (understanding and handling errors)\nUsing modules (libraries, packages)\n\npandas: high-performance, easy-to-use data structures and data analysis tools\nrioxarray: based on the rasterio package for working with rasters and xarray\n\n\n\nPython Learning Resources\nSelf-paced lesson on Programming with Python from Software Carpentry"
  },
  {
    "objectID": "content/notebooks.html#introduction-geospatial-fundamentals-optional",
    "href": "content/notebooks.html#introduction-geospatial-fundamentals-optional",
    "title": "RMarkdown, R, Git",
    "section": "Introduction :: Geospatial Fundamentals (Optional)",
    "text": "Introduction :: Geospatial Fundamentals (Optional)\nDetailed self-paced lesson on Fundamentals of Geospatial Raster and Vector Data with Python from Data Carpentry\nThe end!"
  },
  {
    "objectID": "hackweek-links.html#getting-rs-data",
    "href": "hackweek-links.html#getting-rs-data",
    "title": "Hackweek Links",
    "section": "Getting RS data",
    "text": "Getting RS data\n\nUse sdmpredictors for the hackweek so you have some data. The spreadsheet with all the sdmpredictors variables\nFrom NASA Earthdata: In Home shared/earthaccess-gallery/notebooks/Sea_Level_Rise/SSL.ipynb\nR and Python examples: shared/incois-scripts\nZarr file\n\nshared/2023-Hackbook/python-notebooks/02_Using_Data.ipynb\nshared/2023-Hackbook/python-notebooks/02_Get_Points.ipynb\n\nCopernicus: Search the Slack for “Copenicus”. Someone got it working in R via a R package for Copenicus."
  },
  {
    "objectID": "hackweek-links.html#model-evaluation",
    "href": "hackweek-links.html#model-evaluation",
    "title": "Hackweek Links",
    "section": "Model evaluation",
    "text": "Model evaluation\n\nhttps://rspatial.org/raster/sdm/5_sdm_models.html\nBRTs with dismo https://rspatial.org/raster/sdm/9_sdm_brt.html\nNew link"
  },
  {
    "objectID": "overview.html",
    "href": "overview.html",
    "title": "Overview",
    "section": "",
    "text": "The the era of big data in the earth sciences is here and learning how to effectively use oceanographic remote-sensing data, both in the cloud and on your computer, is a core skill for modern fisheries science and management. Learning how to access cloud-based data, visualize these data, use these data in models, and use the tools of modern reproducible and collaborative science is the main goal of this course. Through the course, participants will gain experience with assessing remote-sensing data in the cloud, R and RStudio, Python and Jupyter notebooks, and collaborating with Git and GitHub.",
    "crumbs": [
      "Welcome",
      "Overview"
    ]
  },
  {
    "objectID": "overview.html#aims-and-objectives",
    "href": "overview.html#aims-and-objectives",
    "title": "Overview",
    "section": "Aims and Objectives",
    "text": "Aims and Objectives\n\nLearn how to discover and use oceanographic remote-sensing data for species distribution modeling and other fisheries applications\nFamiliarize participants with using remote-sensing data and geospatial tools in R and Python code.\nObtain hands-on experience in using remote-sensing data and other earth data in science workflows by working together on a group project.",
    "crumbs": [
      "Welcome",
      "Overview"
    ]
  },
  {
    "objectID": "overview.html#what-is-a-hackweek",
    "href": "overview.html#what-is-a-hackweek",
    "title": "Overview",
    "section": "What is a hackweek?",
    "text": "What is a hackweek?\nA hackweek is a participant-driven workshop that blends data science education, community building, and project work over a short period of time (one to two weeks). The events are highly immersive and allow participants to work directly with data science professionals to co-shape projects and educational outcomes. Hackweeks help individuals and teams engage more effectively in open and reproducible science. - eScience Institute, University of Washington, Seattle USA\nThe hackweek model has become a vital tool in the data science community, fostering idea exchange through modern data analysis workflow training. Unlike traditional academic events, hackweeks offer intensive, interactive learning, including tutorials on cutting-edge methods, peer-based learning, and collaborative on-site projects. Unlike hackathons, which emphasize software development, hackweeks prioritize education and open-ended projects, benefiting fields needing both expertise and efficient computational workflows for idea exchange and discovery. The hackweek model is now widely used in many fields: Astrohackweek, Neurohackweek, Geohackweek, OceanHackWeek, ICESat-2 Hackweek, SnowEx Hackweek, NASA Cloud Hackathon. The NOAA HackDays content and format is modeled off the University of Washington eScience Hackweek model.",
    "crumbs": [
      "Welcome",
      "Overview"
    ]
  },
  {
    "objectID": "overview.html#code-of-conduct",
    "href": "overview.html#code-of-conduct",
    "title": "Overview",
    "section": "Code of Conduct",
    "text": "Code of Conduct\nThe NOAA HackDays events are a safe learning space and all participants are required to abide by our Code of Conduct.",
    "crumbs": [
      "Welcome",
      "Overview"
    ]
  },
  {
    "objectID": "projects/index.html",
    "href": "projects/index.html",
    "title": "Projects Overview",
    "section": "",
    "text": "The following was borrowed and adapted from the excellent SnowEx Hackathon 2021\nThis section contains everything you need to know about hackweek projects.",
    "crumbs": [
      "Projects",
      "Projects Overview"
    ]
  },
  {
    "objectID": "projects/index.html#purpose-of-the-projects",
    "href": "projects/index.html#purpose-of-the-projects",
    "title": "Projects Overview",
    "section": "Purpose of the projects:",
    "text": "Purpose of the projects:\nDuring the course we will be facilitating team hacking sessions in the second half of each day. The purpose of these sessions is for participants to gain hands-on experience in working together on a well-defined problem, in a collaborative space where you can talk things through and get help.",
    "crumbs": [
      "Projects",
      "Projects Overview"
    ]
  },
  {
    "objectID": "projects/index.html#what-is-hacking",
    "href": "projects/index.html#what-is-hacking",
    "title": "Projects Overview",
    "section": "What is hacking?",
    "text": "What is hacking?\nHacking is a session of focused, highly collaborative work time – often involving coding – in which the group creates conditions for rapid absorption of new ideas and methods. The word “hack” or “hackathon” has many different interpretations, both positive and negative. Here our intention is to foster the idea of hacking as a fun, interactive and welcoming environment to explore and experiment with computer code.",
    "crumbs": [
      "Projects",
      "Projects Overview"
    ]
  },
  {
    "objectID": "projects/index.html#how-will-the-projects-be-conducted",
    "href": "projects/index.html#how-will-the-projects-be-conducted",
    "title": "Projects Overview",
    "section": "How will the projects be conducted?",
    "text": "How will the projects be conducted?\nParticipants are invited to start conversations about projects in the Slack channel 2023-hackweek-projects before and during week one of the course.\n\nIf you have a project idea brewing, please pitch it in this channel. You can tag your proposed teammates if you already have that worked out.\nStart a thread with “Project idea:” and then provide a few sentences. Include whether you are looking for teammates to join this project. Others who are interested can respond in a thread.\nWe welcome a broad range of project topics. People often use project time to dig deeper into concepts introduced in tutorials, to explore problems within their own research, or to advance community data sharing and software building efforts.\nThe course team is here to help you get clear on project ideas and decide on what is possible within 5 days.\n\nAt the end of week 1 of the course we will have a “Pitchfest”” where proposer(s) can pitch their idea. At this time we will finalize the project teams for the week — however we will provide times when team members will move and work with other teams. This kind of ‘cross-pollination’ helps everyone learn and solve problems.\nTeam hacktime will begin on week 2.\nEach team is encouraged to identify a project lead, likely the person who pitched the idea, who has knowledge of the datasets and the specific problem to be explored. But roles can be assigned as the group decides to best fit skills and needs.\nOn the final day of the course, each team will present their work in a series of lightning talks.",
    "crumbs": [
      "Projects",
      "Projects Overview"
    ]
  },
  {
    "objectID": "python-notebooks/00_NASA_Earthdata_Authentication.html#summary",
    "href": "python-notebooks/00_NASA_Earthdata_Authentication.html#summary",
    "title": "Authentication for NASA Earthdata",
    "section": "Summary",
    "text": "Summary\nThis notebook creates a hidden .netrc file (_netrc for Window OS) with Earthdata login credentials in your home directory. This file is needed to access NASA Earthdata assets from a scripting environment like Python.\n\nEarthdata Login\nAn Earthdata Login account is required to access data, as well as discover restricted data, from the NASA Earthdata system. Thus, to access NASA data, you need Earthdata Login. Please visit https://urs.earthdata.nasa.gov to register and manage your Earthdata Login account. This account is free to create and only takes a moment to set up.\n\n\nAuthentication via netrc File\nYou will need a netrc file containing your NASA Earthdata Login credentials in order to execute the notebooks. A netrc file can be created manually within text editor and saved to your home directory. An example of the required content is below.\nmachine urs.earthdata.nasa.gov\nlogin &lt;USERNAME&gt;\npassword &lt;PASSWORD&gt;\n&lt;USERNAME&gt; and &lt;PASSWORD&gt; would be replaced by your actual Earthdata Login username and password respectively.",
    "crumbs": [
      "Python Tutorials",
      "Data Access",
      "Authentication for NASA Earthdata"
    ]
  },
  {
    "objectID": "python-notebooks/00_NASA_Earthdata_Authentication.html#import-required-packages",
    "href": "python-notebooks/00_NASA_Earthdata_Authentication.html#import-required-packages",
    "title": "Authentication for NASA Earthdata",
    "section": "Import Required Packages",
    "text": "Import Required Packages\n\nfrom netrc import netrc\nfrom subprocess import Popen\nfrom platform import system\nfrom getpass import getpass\nimport os\n\nThe code below will:\n\ncheck what operating system (OS) is being used to determine which netrc file to check for/create (.netrc or _netrc)\ncheck if you have an netrc file, and if so, varify if those credentials are for the Earthdata endpoint\ncreate a netrc file if a netrc file is not present.\n\n\nurs = 'urs.earthdata.nasa.gov'    # Earthdata URL endpoint for authentication\nprompts = ['Enter NASA Earthdata Login Username: ',\n           'Enter NASA Earthdata Login Password: ']\n\n# Determine the OS (Windows machines usually use an '_netrc' file)\nnetrc_name = \"_netrc\" if system()==\"Windows\" else \".netrc\"\n\n# Determine if netrc file exists, and if so, if it includes NASA Earthdata Login Credentials\ntry:\n    netrcDir = os.path.expanduser(f\"~/{netrc_name}\")\n    netrc(netrcDir).authenticators(urs)[0]\n\n# Below, create a netrc file and prompt user for NASA Earthdata Login Username and Password\nexcept FileNotFoundError:\n    homeDir = os.path.expanduser(\"~\")\n    Popen('touch {0}{2} | echo machine {1} &gt;&gt; {0}{2}'.format(homeDir + os.sep, urs, netrc_name), shell=True)\n    Popen('echo login {} &gt;&gt; {}{}'.format(getpass(prompt=prompts[0]), homeDir + os.sep, netrc_name), shell=True)\n    Popen('echo \\'password {} \\'&gt;&gt; {}{}'.format(getpass(prompt=prompts[1]), homeDir + os.sep, netrc_name), shell=True)\n    # Set restrictive permissions\n    Popen('chmod 0600 {0}{1}'.format(homeDir + os.sep, netrc_name), shell=True)\n\n    # Determine OS and edit netrc file if it exists but is not set up for NASA Earthdata Login\nexcept TypeError:\n    homeDir = os.path.expanduser(\"~\")\n    Popen('echo machine {1} &gt;&gt; {0}{2}'.format(homeDir + os.sep, urs, netrc_name), shell=True)\n    Popen('echo login {} &gt;&gt; {}{}'.format(getpass(prompt=prompts[0]), homeDir + os.sep, netrc_name), shell=True)\n    Popen('echo \\'password {} \\'&gt;&gt; {}{}'.format(getpass(prompt=prompts[1]), homeDir + os.sep, netrc_name), shell=True)\n\n\nSee if the file was created\nIf the file was created, we’ll see a .netrc file (_netrc for Window OS) in the list printed below. To view the contents from a Jupyter environment, click File on the top toolbar, select Open from Path…, type .netrc, and click Open. The .netrc file will open within the text editor.\n\n!!! Beware, your password will be visible if the .netrc file is opened in the text editor.\n\n\n!",
    "crumbs": [
      "Python Tutorials",
      "Data Access",
      "Authentication for NASA Earthdata"
    ]
  },
  {
    "objectID": "python-notebooks/01_ERDDAP_Downloader.html#get-to-know-griddap-url-link-template",
    "href": "python-notebooks/01_ERDDAP_Downloader.html#get-to-know-griddap-url-link-template",
    "title": "Download data from NOAA ERDDAP using GRIDDAP",
    "section": "Get to know GRIDDAP URL link template",
    "text": "Get to know GRIDDAP URL link template\nOne of the datasets that we used in this project is Reanalysis Data ERA5 monthly 3d Wind velocities. For the default options, select file type as .nc (NetCDF3-binary file) and click ‘Just generate the URL,’ you can see the URL is then https://apdrc.soest.hawaii.edu/erddap/griddap/hawaii_soest_66d3_10d8_0f3c.nc?u[(2023-04-01T00:00:00Z):1:(2023-04-01T00:00:00Z)][(10.0):1:(1000.0)][(-90.0):1:(90.0)][(0.0):1:(359.75)],v[(2023-04-01T00:00:00Z):1:(2023-04-01T00:00:00Z)][(10.0):1:(1000.0)][(-90.0):1:(90.0)][(0.0):1:(359.75)] and it follows this template:\n\nhttps://apdrc.soest.hawaii.edu/erddap/griddap\n\n/DATASET_ID.FILE_EXTENSION\nfollowing by\nVARIABLE_NAME1 [(START_DIM1):STRIDE_DIM1:(STOP_DIM1)][(START_DIM2):STRIDE_DIM2:(STOP_DIM2)][(START_DIM3):STRIDE_DIM3:(STOP_DIM3)]…\nand then\nVARIABLE_NAME2[(START_DIM1):STRIDE_DIM1:(STOP_DIM1)][(START_DIM2):STRIDE_DIM2:(STOP_DIM2)][(START_DIM3):STRIDE_DIM3:(STOP_DIM3)]…\nand so on for all the variables. To illustrate this template, you can liken it to downloading every variable as an Xarray DataArray, and combine them together based on the coordinates. In the URL above, the order and specifications of the dimension slicing options must be the consistent for all variables, just like how Xarray combining by coords only works if the coordinates are so.\nThe template is also well-explained in GRIDDAP documentation.\nWe can all agree that trying to write down every template for different datasets is error-prone and tedious, so you are more than welcome to copy and modify your template using the original file generator Python formatted strings to help with generating URLs for downloads. Since URLs vary greatly form dataset to dataset, there is no ideal URL template to consider, but we will still develop a function to generalize the URL generator to the best of my knowledge.\nNOTE: for the URL template, we focus on downloading smaller chunks of the dataset, sliced temporally just like downloading from the EarthData database so that the remote server would not be broken handling the request. We also recommend you to download variables together in one NetCDF3 file (multiple variable names in one URL) instead of individual variables as it takes time piecing them all together.",
    "crumbs": [
      "Python Tutorials",
      "Data Access",
      "Download data from NOAA ERDDAP using GRIDDAP"
    ]
  },
  {
    "objectID": "python-notebooks/01_ERDDAP_Downloader.html#import-necessary-libraries",
    "href": "python-notebooks/01_ERDDAP_Downloader.html#import-necessary-libraries",
    "title": "Download data from NOAA ERDDAP using GRIDDAP",
    "section": "Import necessary libraries",
    "text": "Import necessary libraries\n\nimport xarray as xr\nimport numpy as np\nimport pandas as pd\nimport os, glob, shutil, urllib3",
    "crumbs": [
      "Python Tutorials",
      "Data Access",
      "Download data from NOAA ERDDAP using GRIDDAP"
    ]
  },
  {
    "objectID": "python-notebooks/01_ERDDAP_Downloader.html#download-data",
    "href": "python-notebooks/01_ERDDAP_Downloader.html#download-data",
    "title": "Download data from NOAA ERDDAP using GRIDDAP",
    "section": "Download data",
    "text": "Download data\nUnlike NASA Earthdata, we can slice dataset spatially directly for every server request.\n\ndef download_ERDDAP_URLs(var):\n    # get list of URLs\n    URLs, months = ERDDAP_URL_template_generator(var)\n    http = urllib3.PoolManager()\n    \n    MAIN_FOLDER = 'demonstrated data/ERDDAP'\n    TEMP_FOLDER = var['datasetID']\n    path_temp_folder = os.path.join(MAIN_FOLDER, TEMP_FOLDER)\n    \n    if not os.path.exists(path_temp_folder):\n        os.makedirs(path_temp_folder)\n    \n    for URL, month in zip(URLs, months):\n        fileout = os.path.join(path_temp_folder, f\"{month.strftime('%Y%m')}.nc\")\n        \n        # download\n        with http.request('GET', URL, preload_content=False) as resp, open(fileout, 'wb') as out_file:\n            shutil.copyfileobj(resp, out_file)\n        \n        resp.release_conn()\n        \n        print('Downloaded ' + fileout)\n\n\ndef ERDDAP_URL_template_generator(var) -&gt; list():\n    \"\"\"\n    Generates a list of ERDDAP URLs for downloading\n    \n    Example of parameters shown below in next code cell.\n        - datasetID: 'erdTAgeo1day_LonPM180'\n        - variable_name ['u', 'v']\n        - coordinates: {'time': '2000-01:1:2000-03', 'altitude': '0:1:0', 'latitude': '-12:1:12', 'longitude': '42:1:102'}\n    NOTE: 'time' dimension must be listed in order (typical of ERDDAP URLs)\n    \"\"\"\n\n    list_of_URLS = []\n    time_ele = dimension_boundaries_ordered['time'].split(':')\n    start_month = time_ele[0]\n    end_month = time_ele[2]\n    stride_time = time_ele[1]\n    months = pd.date_range(start_month, end_month, freq='M')\n    for month_idx in range(len(months)):\n        URL = f'https://apdrc.soest.hawaii.edu/erddap/griddap/{var_dict['datasetID']}.nc?'\n        for var in var_dict['variable_names']:\n            URL = URL + var + f\"[({months[month_idx].strftime('%Y-%m')}-01):{stride_time}:({months[month_idx].strftime('%Y-%m-%d')})]\"\n            for d in list(var_dict['coordinates'].values())[1:]:\n                d_ele = d.split(':')\n                URL = URL + f\"[({d_ele[0]}):{d_ele[1]}:({d_ele[2]})]\"\n            URL = URL + \",\"\n        \n        # eliminate last comma\n        list_of_URLS.append(URL[:-1])\n        \n        # example\n        # url = \"https://apdrc.soest.hawaii.edu/erddap/griddap/hawaii_soest_66d3_10d8_0f3c.nc?\"\\\n        # f\"u[({months[month_idx].strftime('%Y-%m')}-01):1:({months[month_idx+1].strftime('%Y-%m')})][(10):1:(10)][(-12):1:(32)][(42):1:(102)],\"\\\n        # f\"v[({months[month_idx].strftime('%Y-%m')}-01):1:({months[month_idx+1].strftime('%Y-%m')})][(10):1:(10)][(-12):1:(32)][(42):1:(102)]\"\n    return list_of_URLS, months\n\n\nvar = {\n    'datasetID': 'hawaii_soest_66d3_10d8_0f3c',\n    'variable_names': ['u', 'v'],\n    'coordinates': {'time': '2000-01:1:2000-10', # must be in order of the ERDDAP dataset dimension list\n                    'LEV': '10:1:10', \n                    'latitude': '-12:1:12', \n                    'longitude': '42:1:102'}\n}\n\nERDDAP_URL_template_generator(datasetID='hawaii_soest_66d3_10d8_0f3c', variable_names=['u', 'v'], dimension_boundaries_ordered={'time': '2000-01:1:2000-10', 'LEV': '10:1:10', 'latitude': '-12:1:12', 'longitude': '42:1:102'})\n\n['https://apdrc.soest.hawaii.edu/erddap/griddap/hawaii_soest_66d3_10d8_0f3c.nc?u[(2000-01-01):1:(2000-01-31)][(10):1:(10)][(-12):1:(12)][(42):1:(102)],v[(2000-01-01):1:(2000-01-31)][(10):1:(10)][(-12):1:(12)][(42):1:(102)]',\n 'https://apdrc.soest.hawaii.edu/erddap/griddap/hawaii_soest_66d3_10d8_0f3c.nc?u[(2000-02-01):1:(2000-02-29)][(10):1:(10)][(-12):1:(12)][(42):1:(102)],v[(2000-02-01):1:(2000-02-29)][(10):1:(10)][(-12):1:(12)][(42):1:(102)]',\n 'https://apdrc.soest.hawaii.edu/erddap/griddap/hawaii_soest_66d3_10d8_0f3c.nc?u[(2000-03-01):1:(2000-03-31)][(10):1:(10)][(-12):1:(12)][(42):1:(102)],v[(2000-03-01):1:(2000-03-31)][(10):1:(10)][(-12):1:(12)][(42):1:(102)]',\n 'https://apdrc.soest.hawaii.edu/erddap/griddap/hawaii_soest_66d3_10d8_0f3c.nc?u[(2000-04-01):1:(2000-04-30)][(10):1:(10)][(-12):1:(12)][(42):1:(102)],v[(2000-04-01):1:(2000-04-30)][(10):1:(10)][(-12):1:(12)][(42):1:(102)]',\n 'https://apdrc.soest.hawaii.edu/erddap/griddap/hawaii_soest_66d3_10d8_0f3c.nc?u[(2000-05-01):1:(2000-05-31)][(10):1:(10)][(-12):1:(12)][(42):1:(102)],v[(2000-05-01):1:(2000-05-31)][(10):1:(10)][(-12):1:(12)][(42):1:(102)]',\n 'https://apdrc.soest.hawaii.edu/erddap/griddap/hawaii_soest_66d3_10d8_0f3c.nc?u[(2000-06-01):1:(2000-06-30)][(10):1:(10)][(-12):1:(12)][(42):1:(102)],v[(2000-06-01):1:(2000-06-30)][(10):1:(10)][(-12):1:(12)][(42):1:(102)]',\n 'https://apdrc.soest.hawaii.edu/erddap/griddap/hawaii_soest_66d3_10d8_0f3c.nc?u[(2000-07-01):1:(2000-07-31)][(10):1:(10)][(-12):1:(12)][(42):1:(102)],v[(2000-07-01):1:(2000-07-31)][(10):1:(10)][(-12):1:(12)][(42):1:(102)]',\n 'https://apdrc.soest.hawaii.edu/erddap/griddap/hawaii_soest_66d3_10d8_0f3c.nc?u[(2000-08-01):1:(2000-08-31)][(10):1:(10)][(-12):1:(12)][(42):1:(102)],v[(2000-08-01):1:(2000-08-31)][(10):1:(10)][(-12):1:(12)][(42):1:(102)]',\n 'https://apdrc.soest.hawaii.edu/erddap/griddap/hawaii_soest_66d3_10d8_0f3c.nc?u[(2000-09-01):1:(2000-09-30)][(10):1:(10)][(-12):1:(12)][(42):1:(102)],v[(2000-09-01):1:(2000-09-30)][(10):1:(10)][(-12):1:(12)][(42):1:(102)]']",
    "crumbs": [
      "Python Tutorials",
      "Data Access",
      "Download data from NOAA ERDDAP using GRIDDAP"
    ]
  },
  {
    "objectID": "python-notebooks/01_OPeNDAP_Downloader.html#import-necessary-libraries",
    "href": "python-notebooks/01_OPeNDAP_Downloader.html#import-necessary-libraries",
    "title": "Download Data using OPeNDAP",
    "section": "Import necessary libraries",
    "text": "Import necessary libraries\n\nimport xarray as xr\nimport getpass\nfrom pydap.client import open_url\nfrom pydap.cas.get_cookies import setup_session\n\n\nUSERNAME = 'mphan'\nPASSWORD = getpass.getpass('Enter your password: ')\n\nEnter your password:  ········\n\n\nFor this notebook we want to stream data from the Copernicus Marine Environment Monitoring Service’s Global Ocean Physics Analysis\n\n# change your Dataset ID accordingly\nDATASET_ID = 'cmems_mod_glo_phy_my_0.083_P1D-m'\n\nIn the function below, we utilize a PydapDataStore, an Xarray store object used for accessing OpenDAP datasets. For this dataset, you need to log in your credentials using the name and password provided above. Note that not all OpenDAP datasets will require the same steps, so you should look up appropriate methods to access data.\n\ndef copernicusmarine_datastore(dataset, username, password):\n    cas_url = 'https://cmems-cas.cls.fr/cas/login'\n    session = setup_session(cas_url, username, password)\n    session.cookies.set(\"CASTGC\", session.cookies.get_dict()['CASTGC'])\n    database = ['my', 'nrt']\n    url = f'https://{database[0]}.cmems-du.eu/thredds/dodsC/{dataset}'\n    try:\n        data_store = xr.backends.PydapDataStore(open_url(url, session=session, user_charset='utf-8')) # needs PyDAP &gt;= v3.3.0 see https://github.com/pydap/pydap/pull/223/commits \n    except:\n        url = f'https://{database[1]}.cmems-du.eu/thredds/dodsC/{dataset}'\n        data_store = xr.backends.PydapDataStore(open_url(url, session=session, user_charset='utf-8')) # needs PyDAP &gt;= v3.3.0 see https://github.com/pydap/pydap/pull/223/commits\n    return data_store\n\n\ndata_store = copernicusmarine_datastore(DATASET_ID, USERNAME, PASSWORD)\n\nWhen you open a dataset from PyDAP’s Data Store object, only the “shell” of the dataset is streamed into our local machine. The rest of the data shall be remote, so if we want to process further outside of slicing/inspecting, we need to stream the additional data, which occupies lots of memory. The streaming/downloading speed is also very slow, so proceed with caution.\n\nDS = xr.open_dataset(data_store)\n\n\nDS\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt;\nDimensions:    (longitude: 4320, latitude: 2041, depth: 50, time: 10227)\nCoordinates:\n  * longitude  (longitude) float32 -180.0 -179.9 -179.8 ... 179.8 179.8 179.9\n  * latitude   (latitude) float32 -80.0 -79.92 -79.83 ... 89.83 89.92 90.0\n  * depth      (depth) float32 0.494 1.541 2.646 ... 5.275e+03 5.728e+03\n  * time       (time) datetime64[ns] 1993-01-01T12:00:00 ... 2020-12-31T12:00:00\nData variables:\n    mlotst     (time, latitude, longitude) float32 ...\n    zos        (time, latitude, longitude) float32 ...\n    bottomT    (time, latitude, longitude) float32 ...\n    sithick    (time, latitude, longitude) float32 ...\n    siconc     (time, latitude, longitude) float32 ...\n    usi        (time, latitude, longitude) float32 ...\n    vsi        (time, latitude, longitude) float32 ...\n    thetao     (time, depth, latitude, longitude) float32 ...\n    so         (time, depth, latitude, longitude) float32 ...\n    uo         (time, depth, latitude, longitude) float32 ...\n    vo         (time, depth, latitude, longitude) float32 ...\nAttributes: (12/24)\n    title:              daily mean fields from Global Ocean Physics Analysis ...\n    easting:            longitude\n    northing:           latitude\n    history:            2022/05/25 21:54:07 MERCATOR OCEAN Netcdf creation\n    source:             MERCATOR GLORYS12V1\n    institution:        MERCATOR OCEAN\n    ...                 ...\n    longitude_min:      -180.0\n    longitude_max:      179.91667\n    latitude_min:       -80.0\n    latitude_max:       90.0\n    z_min:              0.494025\n    z_max:              5727.917xarray.DatasetDimensions:longitude: 4320latitude: 2041depth: 50time: 10227Coordinates: (4)longitude(longitude)float32-180.0 -179.9 ... 179.8 179.9valid_min :-180.0valid_max :179.91667step :0.08332825units :degrees_eastunit_long :Degrees Eastlong_name :Longitudestandard_name :longitudeaxis :X_ChunkSizes :4320array([-180.     , -179.91667, -179.83333, ...,  179.75   ,  179.83333,\n        179.91667], dtype=float32)latitude(latitude)float32-80.0 -79.92 -79.83 ... 89.92 90.0valid_min :-80.0valid_max :90.0step :0.08333588units :degrees_northunit_long :Degrees Northlong_name :Latitudestandard_name :latitudeaxis :Y_ChunkSizes :2041array([-80.      , -79.916664, -79.833336, ...,  89.833336,  89.916664,\n        90.      ], dtype=float32)depth(depth)float320.494 1.541 ... 5.275e+03 5.728e+03valid_min :0.494025valid_max :5727.917units :mpositive :downunit_long :Meterslong_name :Depthstandard_name :depthaxis :Z_ChunkSizes :50array([4.940250e-01, 1.541375e+00, 2.645669e+00, 3.819495e+00, 5.078224e+00,\n       6.440614e+00, 7.929560e+00, 9.572997e+00, 1.140500e+01, 1.346714e+01,\n       1.581007e+01, 1.849556e+01, 2.159882e+01, 2.521141e+01, 2.944473e+01,\n       3.443415e+01, 4.034405e+01, 4.737369e+01, 5.576429e+01, 6.580727e+01,\n       7.785385e+01, 9.232607e+01, 1.097293e+02, 1.306660e+02, 1.558507e+02,\n       1.861256e+02, 2.224752e+02, 2.660403e+02, 3.181274e+02, 3.802130e+02,\n       4.539377e+02, 5.410889e+02, 6.435668e+02, 7.633331e+02, 9.023393e+02,\n       1.062440e+03, 1.245291e+03, 1.452251e+03, 1.684284e+03, 1.941893e+03,\n       2.225078e+03, 2.533336e+03, 2.865703e+03, 3.220820e+03, 3.597032e+03,\n       3.992484e+03, 4.405224e+03, 4.833291e+03, 5.274784e+03, 5.727917e+03],\n      dtype=float32)time(time)datetime64[ns]1993-01-01T12:00:00 ... 2020-12-...long_name :Time (hours since 1950-01-01)standard_name :timeaxis :T_ChunkSizes :1024array(['1993-01-01T12:00:00.000000000', '1993-01-02T12:00:00.000000000',\n       '1993-01-03T12:00:00.000000000', ..., '2020-12-29T12:00:00.000000000',\n       '2020-12-30T12:00:00.000000000', '2020-12-31T12:00:00.000000000'],\n      dtype='datetime64[ns]')Data variables: (11)mlotst(time, latitude, longitude)float32...long_name :Density ocean mixed layer thicknessstandard_name :ocean_mixed_layer_thickness_defined_by_sigma_thetaunits :munit_long :Metersvalid_min :1valid_max :6447cell_methods :area: mean_ChunkSizes :[1, 681, 1440][90172686240 values with dtype=float32]zos(time, latitude, longitude)float32...long_name :Sea surface heightstandard_name :sea_surface_height_above_geoidunits :munit_long :Metersvalid_min :-6181valid_max :5039cell_methods :area: mean_ChunkSizes :[1, 681, 1440][90172686240 values with dtype=float32]bottomT(time, latitude, longitude)float32...long_name :Sea floor potential temperaturestandard_name :sea_water_potential_temperature_at_sea_floorunits :degrees_Cunit_long :Degrees Celsiusvalid_min :-32757valid_max :21318cell_methods :area: mean_ChunkSizes :[1, 681, 1440][90172686240 values with dtype=float32]sithick(time, latitude, longitude)float32...long_name :Sea ice thicknessstandard_name :sea_ice_thicknessunits :munit_long :Metersvalid_min :1valid_max :4618cell_methods :area: mean where sea_ice_ChunkSizes :[1, 681, 1440][90172686240 values with dtype=float32]siconc(time, latitude, longitude)float32...long_name :Ice concentrationstandard_name :sea_ice_area_fractionunits :1unit_long :Fractionvalid_min :1valid_max :28060cell_methods :area: mean where sea_ice_ChunkSizes :[1, 681, 1440][90172686240 values with dtype=float32]usi(time, latitude, longitude)float32...long_name :Sea ice eastward velocitystandard_name :eastward_sea_ice_velocityunits :m s-1unit_long :Meters per secondvalid_min :-32707valid_max :32144cell_methods :area: mean where sea_ice_ChunkSizes :[1, 681, 1440][90172686240 values with dtype=float32]vsi(time, latitude, longitude)float32...long_name :Sea ice northward velocitystandard_name :northward_sea_ice_velocityunits :m s-1unit_long :Meters per secondvalid_min :-31907valid_max :31690cell_methods :area: mean where sea_ice_ChunkSizes :[1, 681, 1440][90172686240 values with dtype=float32]thetao(time, depth, latitude, longitude)float32...long_name :Temperaturestandard_name :sea_water_potential_temperatureunits :degrees_Cunit_long :Degrees Celsiusvalid_min :-32764valid_max :21298cell_methods :area: mean_ChunkSizes :[1, 7, 341, 720][4508634312000 values with dtype=float32]so(time, depth, latitude, longitude)float32...long_name :Salinitystandard_name :sea_water_salinityunits :1e-3unit_long :Practical Salinity Unitvalid_min :1valid_max :28336cell_methods :area: mean_ChunkSizes :[1, 7, 341, 720][4508634312000 values with dtype=float32]uo(time, depth, latitude, longitude)float32...long_name :Eastward velocitystandard_name :eastward_sea_water_velocityunits :m s-1unit_long :Meters per secondvalid_min :-3573valid_max :3559cell_methods :area: mean_ChunkSizes :[1, 7, 341, 720][4508634312000 values with dtype=float32]vo(time, depth, latitude, longitude)float32...long_name :Northward velocitystandard_name :northward_sea_water_velocityunits :m s-1unit_long :Meters per secondvalid_min :-3300valid_max :3399cell_methods :area: mean_ChunkSizes :[1, 7, 341, 720][4508634312000 values with dtype=float32]Indexes: (4)longitudePandasIndexPandasIndex(Index([            -180.0, -179.9166717529297, -179.8333282470703,\n                  -179.75, -179.6666717529297, -179.5833282470703,\n                   -179.5, -179.4166717529297, -179.3333282470703,\n                  -179.25,\n       ...\n        179.1666717529297,             179.25,  179.3333282470703,\n        179.4166717529297,              179.5,  179.5833282470703,\n        179.6666717529297,             179.75,  179.8333282470703,\n        179.9166717529297],\n      dtype='float32', name='longitude', length=4320))latitudePandasIndexPandasIndex(Index([             -80.0, -79.91666412353516, -79.83333587646484,\n                   -79.75, -79.66666412353516, -79.58333587646484,\n                    -79.5, -79.41666412353516, -79.33333587646484,\n                   -79.25,\n       ...\n                    89.25,  89.33333587646484,  89.41666412353516,\n                     89.5,  89.58333587646484,  89.66666412353516,\n                    89.75,  89.83333587646484,  89.91666412353516,\n                     90.0],\n      dtype='float32', name='latitude', length=2041))depthPandasIndexPandasIndex(Index([0.49402499198913574,  1.5413750410079956,  2.6456689834594727,\n        3.8194949626922607,   5.078224182128906,   6.440614223480225,\n          7.92956018447876,   9.572997093200684,  11.404999732971191,\n        13.467140197753906,  15.810070037841797,  18.495559692382812,\n        21.598819732666016,  25.211410522460938,  29.444730758666992,\n         34.43415069580078,  40.344051361083984,   47.37369155883789,\n         55.76428985595703,   65.80726623535156,   77.85385131835938,\n          92.3260726928711,  109.72930145263672,  130.66600036621094,\n        155.85069274902344,  186.12559509277344,  222.47520446777344,\n         266.0403137207031,   318.1274108886719,   380.2130126953125,\n         453.9377136230469,   541.0889282226562,   643.5667724609375,\n         763.3331298828125,   902.3392944335938,    1062.43994140625,\n            1245.291015625,     1452.2509765625,  1684.2840576171875,\n        1941.8929443359375,   2225.077880859375,        2533.3359375,\n         2865.702880859375,   3220.820068359375,   3597.031982421875,\n          3992.48388671875,    4405.22412109375,      4833.291015625,\n           5274.7841796875,     5727.9169921875],\n      dtype='float32', name='depth'))timePandasIndexPandasIndex(DatetimeIndex(['1993-01-01 12:00:00', '1993-01-02 12:00:00',\n               '1993-01-03 12:00:00', '1993-01-04 12:00:00',\n               '1993-01-05 12:00:00', '1993-01-06 12:00:00',\n               '1993-01-07 12:00:00', '1993-01-08 12:00:00',\n               '1993-01-09 12:00:00', '1993-01-10 12:00:00',\n               ...\n               '2020-12-22 12:00:00', '2020-12-23 12:00:00',\n               '2020-12-24 12:00:00', '2020-12-25 12:00:00',\n               '2020-12-26 12:00:00', '2020-12-27 12:00:00',\n               '2020-12-28 12:00:00', '2020-12-29 12:00:00',\n               '2020-12-30 12:00:00', '2020-12-31 12:00:00'],\n              dtype='datetime64[ns]', name='time', length=10227, freq=None))Attributes: (24)title :daily mean fields from Global Ocean Physics Analysis and Forecast updated Dailyeasting :longitudenorthing :latitudehistory :2022/05/25 21:54:07 MERCATOR OCEAN Netcdf creationsource :MERCATOR GLORYS12V1institution :MERCATOR OCEANreferences :http://www.mercator-ocean.frcomment :CMEMS productConventions :CF-1.4domain_name :GL12field_type :meanfield_date :2020-12-31 00:00:00field_julian_date :25932.0julian_day_unit :days since 1950-01-01 00:00:00forecast_range :forecast_type :bulletin_date :2021-01-06 00:00:00bulletin_type :operationallongitude_min :-180.0longitude_max :179.91667latitude_min :-80.0latitude_max :90.0z_min :0.494025z_max :5727.917\n\n\nWe can slice and get the portion of data we need. In consistency with the other notebooks, let’s get data from Jan-March 2003 for salinity at the most shallow level, within our region of interest (60-80 deg E, 5-25 deg N)\nDISCLAIMER: DO NOT LOAD DATASET IN ITS ENTIRETY BEFORE SLICING IF YOU DO NOT WANT TO OVERFLOW YOUR MEMORY. Actual data is only loaded when you slice the dataset.\n\nDS_sliced = DS['so'].isel(depth=0).sel(longitude=slice(60, 80), latitude=slice(5, 25), time=slice('2003-01', '2003-02'))\n\n\nDS_sliced\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray 'so' (time: 59, latitude: 241, longitude: 241)&gt;\narray([[[35.399944, 35.389263, ..., 33.65581 , 33.680225],\n        [35.425884, 35.407574, ..., 33.587147, 33.597828],\n        ...,\n        [36.52913 , 36.526077, ...,       nan,       nan],\n        [36.552017, 36.53981 , ...,       nan,       nan]],\n\n       [[35.384686, 35.380108, ..., 33.7611  , 33.78399 ],\n        [35.387737, 35.38316 , ..., 33.9259  , 34.067814],\n        ...,\n        [36.52913 , 36.527603, ...,       nan,       nan],\n        [36.545914, 36.54744 , ...,       nan,       nan]],\n\n       ...,\n\n       [[35.245827, 35.22599 , ..., 33.957947, 33.991516],\n        [35.254982, 35.24125 , ..., 33.898434, 33.935055],\n        ...,\n        [36.530655, 36.52913 , ...,       nan,       nan],\n        [36.552017, 36.57033 , ...,       nan,       nan]],\n\n       [[35.242775, 35.221413, ..., 34.06018 , 34.083073],\n        [35.26261 , 35.245827, ..., 34.0083  , 34.040344],\n        ...,\n        [36.562702, 36.57491 , ...,       nan,       nan],\n        [36.590168, 36.602375, ...,       nan,       nan]]], dtype=float32)\nCoordinates:\n  * longitude  (longitude) float32 60.0 60.08 60.17 60.25 ... 79.83 79.92 80.0\n  * latitude   (latitude) float32 5.0 5.083 5.167 5.25 ... 24.83 24.92 25.0\n    depth      float32 0.494\n  * time       (time) datetime64[ns] 2003-01-01T12:00:00 ... 2003-02-28T12:00:00\nAttributes:\n    long_name:      Salinity\n    standard_name:  sea_water_salinity\n    units:          1e-3\n    unit_long:      Practical Salinity Unit\n    valid_min:      1\n    valid_max:      28336\n    cell_methods:   area: mean\n    _ChunkSizes:    [1, 7, 341, 720]xarray.DataArray'so'time: 59latitude: 241longitude: 24135.4 35.39 35.38 35.37 35.37 35.37 35.36 ... nan nan nan nan nan nanarray([[[35.399944, 35.389263, ..., 33.65581 , 33.680225],\n        [35.425884, 35.407574, ..., 33.587147, 33.597828],\n        ...,\n        [36.52913 , 36.526077, ...,       nan,       nan],\n        [36.552017, 36.53981 , ...,       nan,       nan]],\n\n       [[35.384686, 35.380108, ..., 33.7611  , 33.78399 ],\n        [35.387737, 35.38316 , ..., 33.9259  , 34.067814],\n        ...,\n        [36.52913 , 36.527603, ...,       nan,       nan],\n        [36.545914, 36.54744 , ...,       nan,       nan]],\n\n       ...,\n\n       [[35.245827, 35.22599 , ..., 33.957947, 33.991516],\n        [35.254982, 35.24125 , ..., 33.898434, 33.935055],\n        ...,\n        [36.530655, 36.52913 , ...,       nan,       nan],\n        [36.552017, 36.57033 , ...,       nan,       nan]],\n\n       [[35.242775, 35.221413, ..., 34.06018 , 34.083073],\n        [35.26261 , 35.245827, ..., 34.0083  , 34.040344],\n        ...,\n        [36.562702, 36.57491 , ...,       nan,       nan],\n        [36.590168, 36.602375, ...,       nan,       nan]]], dtype=float32)Coordinates: (4)longitude(longitude)float3260.0 60.08 60.17 ... 79.92 80.0valid_min :-180.0valid_max :179.91667step :0.08332825units :degrees_eastunit_long :Degrees Eastlong_name :Longitudestandard_name :longitudeaxis :X_ChunkSizes :4320array([60.      , 60.083332, 60.166668, ..., 79.833336, 79.916664, 80.      ],\n      dtype=float32)latitude(latitude)float325.0 5.083 5.167 ... 24.92 25.0valid_min :-80.0valid_max :90.0step :0.08333588units :degrees_northunit_long :Degrees Northlong_name :Latitudestandard_name :latitudeaxis :Y_ChunkSizes :2041array([ 5.      ,  5.083333,  5.166667, ..., 24.833334, 24.916666, 25.      ],\n      dtype=float32)depth()float320.494valid_min :0.494025valid_max :5727.917units :mpositive :downunit_long :Meterslong_name :Depthstandard_name :depthaxis :Z_ChunkSizes :50array(0.494025, dtype=float32)time(time)datetime64[ns]2003-01-01T12:00:00 ... 2003-02-...long_name :Time (hours since 1950-01-01)standard_name :timeaxis :T_ChunkSizes :1024array(['2003-01-01T12:00:00.000000000', '2003-01-02T12:00:00.000000000',\n       '2003-01-03T12:00:00.000000000', '2003-01-04T12:00:00.000000000',\n       '2003-01-05T12:00:00.000000000', '2003-01-06T12:00:00.000000000',\n       '2003-01-07T12:00:00.000000000', '2003-01-08T12:00:00.000000000',\n       '2003-01-09T12:00:00.000000000', '2003-01-10T12:00:00.000000000',\n       '2003-01-11T12:00:00.000000000', '2003-01-12T12:00:00.000000000',\n       '2003-01-13T12:00:00.000000000', '2003-01-14T12:00:00.000000000',\n       '2003-01-15T12:00:00.000000000', '2003-01-16T12:00:00.000000000',\n       '2003-01-17T12:00:00.000000000', '2003-01-18T12:00:00.000000000',\n       '2003-01-19T12:00:00.000000000', '2003-01-20T12:00:00.000000000',\n       '2003-01-21T12:00:00.000000000', '2003-01-22T12:00:00.000000000',\n       '2003-01-23T12:00:00.000000000', '2003-01-24T12:00:00.000000000',\n       '2003-01-25T12:00:00.000000000', '2003-01-26T12:00:00.000000000',\n       '2003-01-27T12:00:00.000000000', '2003-01-28T12:00:00.000000000',\n       '2003-01-29T12:00:00.000000000', '2003-01-30T12:00:00.000000000',\n       '2003-01-31T12:00:00.000000000', '2003-02-01T12:00:00.000000000',\n       '2003-02-02T12:00:00.000000000', '2003-02-03T12:00:00.000000000',\n       '2003-02-04T12:00:00.000000000', '2003-02-05T12:00:00.000000000',\n       '2003-02-06T12:00:00.000000000', '2003-02-07T12:00:00.000000000',\n       '2003-02-08T12:00:00.000000000', '2003-02-09T12:00:00.000000000',\n       '2003-02-10T12:00:00.000000000', '2003-02-11T12:00:00.000000000',\n       '2003-02-12T12:00:00.000000000', '2003-02-13T12:00:00.000000000',\n       '2003-02-14T12:00:00.000000000', '2003-02-15T12:00:00.000000000',\n       '2003-02-16T12:00:00.000000000', '2003-02-17T12:00:00.000000000',\n       '2003-02-18T12:00:00.000000000', '2003-02-19T12:00:00.000000000',\n       '2003-02-20T12:00:00.000000000', '2003-02-21T12:00:00.000000000',\n       '2003-02-22T12:00:00.000000000', '2003-02-23T12:00:00.000000000',\n       '2003-02-24T12:00:00.000000000', '2003-02-25T12:00:00.000000000',\n       '2003-02-26T12:00:00.000000000', '2003-02-27T12:00:00.000000000',\n       '2003-02-28T12:00:00.000000000'], dtype='datetime64[ns]')Indexes: (3)longitudePandasIndexPandasIndex(Index([             60.0, 60.08333206176758, 60.16666793823242,\n                   60.25, 60.33333206176758, 60.41666793823242,\n                    60.5, 60.58333206176758, 60.66666793823242,\n                   60.75,\n       ...\n                   79.25, 79.33333587646484, 79.41666412353516,\n                    79.5, 79.58333587646484, 79.66666412353516,\n                   79.75, 79.83333587646484, 79.91666412353516,\n                    80.0],\n      dtype='float32', name='longitude', length=241))latitudePandasIndexPandasIndex(Index([              5.0, 5.083333492279053, 5.166666507720947,\n                    5.25, 5.333333492279053, 5.416666507720947,\n                     5.5, 5.583333492279053, 5.666666507720947,\n                    5.75,\n       ...\n                   24.25, 24.33333396911621, 24.41666603088379,\n                    24.5, 24.58333396911621, 24.66666603088379,\n                   24.75, 24.83333396911621, 24.91666603088379,\n                    25.0],\n      dtype='float32', name='latitude', length=241))timePandasIndexPandasIndex(DatetimeIndex(['2003-01-01 12:00:00', '2003-01-02 12:00:00',\n               '2003-01-03 12:00:00', '2003-01-04 12:00:00',\n               '2003-01-05 12:00:00', '2003-01-06 12:00:00',\n               '2003-01-07 12:00:00', '2003-01-08 12:00:00',\n               '2003-01-09 12:00:00', '2003-01-10 12:00:00',\n               '2003-01-11 12:00:00', '2003-01-12 12:00:00',\n               '2003-01-13 12:00:00', '2003-01-14 12:00:00',\n               '2003-01-15 12:00:00', '2003-01-16 12:00:00',\n               '2003-01-17 12:00:00', '2003-01-18 12:00:00',\n               '2003-01-19 12:00:00', '2003-01-20 12:00:00',\n               '2003-01-21 12:00:00', '2003-01-22 12:00:00',\n               '2003-01-23 12:00:00', '2003-01-24 12:00:00',\n               '2003-01-25 12:00:00', '2003-01-26 12:00:00',\n               '2003-01-27 12:00:00', '2003-01-28 12:00:00',\n               '2003-01-29 12:00:00', '2003-01-30 12:00:00',\n               '2003-01-31 12:00:00', '2003-02-01 12:00:00',\n               '2003-02-02 12:00:00', '2003-02-03 12:00:00',\n               '2003-02-04 12:00:00', '2003-02-05 12:00:00',\n               '2003-02-06 12:00:00', '2003-02-07 12:00:00',\n               '2003-02-08 12:00:00', '2003-02-09 12:00:00',\n               '2003-02-10 12:00:00', '2003-02-11 12:00:00',\n               '2003-02-12 12:00:00', '2003-02-13 12:00:00',\n               '2003-02-14 12:00:00', '2003-02-15 12:00:00',\n               '2003-02-16 12:00:00', '2003-02-17 12:00:00',\n               '2003-02-18 12:00:00', '2003-02-19 12:00:00',\n               '2003-02-20 12:00:00', '2003-02-21 12:00:00',\n               '2003-02-22 12:00:00', '2003-02-23 12:00:00',\n               '2003-02-24 12:00:00', '2003-02-25 12:00:00',\n               '2003-02-26 12:00:00', '2003-02-27 12:00:00',\n               '2003-02-28 12:00:00'],\n              dtype='datetime64[ns]', name='time', freq=None))Attributes: (8)long_name :Salinitystandard_name :sea_water_salinityunits :1e-3unit_long :Practical Salinity Unitvalid_min :1valid_max :28336cell_methods :area: mean_ChunkSizes :[1, 7, 341, 720]\n\n\nYou can get rid of the depth since it is an empty coordinate. Make sure to note the depth of the data sampled in your attribute when you work on the data, though!\n\nDS_sliced = DS_sliced.drop('depth')\n\n\nDS_sliced\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray 'so' (time: 59, latitude: 241, longitude: 241)&gt;\narray([[[35.399944, 35.389263, ..., 33.65581 , 33.680225],\n        [35.425884, 35.407574, ..., 33.587147, 33.597828],\n        ...,\n        [36.52913 , 36.526077, ...,       nan,       nan],\n        [36.552017, 36.53981 , ...,       nan,       nan]],\n\n       [[35.384686, 35.380108, ..., 33.7611  , 33.78399 ],\n        [35.387737, 35.38316 , ..., 33.9259  , 34.067814],\n        ...,\n        [36.52913 , 36.527603, ...,       nan,       nan],\n        [36.545914, 36.54744 , ...,       nan,       nan]],\n\n       ...,\n\n       [[35.245827, 35.22599 , ..., 33.957947, 33.991516],\n        [35.254982, 35.24125 , ..., 33.898434, 33.935055],\n        ...,\n        [36.530655, 36.52913 , ...,       nan,       nan],\n        [36.552017, 36.57033 , ...,       nan,       nan]],\n\n       [[35.242775, 35.221413, ..., 34.06018 , 34.083073],\n        [35.26261 , 35.245827, ..., 34.0083  , 34.040344],\n        ...,\n        [36.562702, 36.57491 , ...,       nan,       nan],\n        [36.590168, 36.602375, ...,       nan,       nan]]], dtype=float32)\nCoordinates:\n  * longitude  (longitude) float32 60.0 60.08 60.17 60.25 ... 79.83 79.92 80.0\n  * latitude   (latitude) float32 5.0 5.083 5.167 5.25 ... 24.83 24.92 25.0\n  * time       (time) datetime64[ns] 2003-01-01T12:00:00 ... 2003-02-28T12:00:00\nAttributes:\n    long_name:      Salinity\n    standard_name:  sea_water_salinity\n    units:          1e-3\n    unit_long:      Practical Salinity Unit\n    valid_min:      1\n    valid_max:      28336\n    cell_methods:   area: mean\n    _ChunkSizes:    [1, 7, 341, 720]xarray.DataArray'so'time: 59latitude: 241longitude: 24135.4 35.39 35.38 35.37 35.37 35.37 35.36 ... nan nan nan nan nan nanarray([[[35.399944, 35.389263, ..., 33.65581 , 33.680225],\n        [35.425884, 35.407574, ..., 33.587147, 33.597828],\n        ...,\n        [36.52913 , 36.526077, ...,       nan,       nan],\n        [36.552017, 36.53981 , ...,       nan,       nan]],\n\n       [[35.384686, 35.380108, ..., 33.7611  , 33.78399 ],\n        [35.387737, 35.38316 , ..., 33.9259  , 34.067814],\n        ...,\n        [36.52913 , 36.527603, ...,       nan,       nan],\n        [36.545914, 36.54744 , ...,       nan,       nan]],\n\n       ...,\n\n       [[35.245827, 35.22599 , ..., 33.957947, 33.991516],\n        [35.254982, 35.24125 , ..., 33.898434, 33.935055],\n        ...,\n        [36.530655, 36.52913 , ...,       nan,       nan],\n        [36.552017, 36.57033 , ...,       nan,       nan]],\n\n       [[35.242775, 35.221413, ..., 34.06018 , 34.083073],\n        [35.26261 , 35.245827, ..., 34.0083  , 34.040344],\n        ...,\n        [36.562702, 36.57491 , ...,       nan,       nan],\n        [36.590168, 36.602375, ...,       nan,       nan]]], dtype=float32)Coordinates: (3)longitude(longitude)float3260.0 60.08 60.17 ... 79.92 80.0valid_min :-180.0valid_max :179.91667step :0.08332825units :degrees_eastunit_long :Degrees Eastlong_name :Longitudestandard_name :longitudeaxis :X_ChunkSizes :4320array([60.      , 60.083332, 60.166668, ..., 79.833336, 79.916664, 80.      ],\n      dtype=float32)latitude(latitude)float325.0 5.083 5.167 ... 24.92 25.0valid_min :-80.0valid_max :90.0step :0.08333588units :degrees_northunit_long :Degrees Northlong_name :Latitudestandard_name :latitudeaxis :Y_ChunkSizes :2041array([ 5.      ,  5.083333,  5.166667, ..., 24.833334, 24.916666, 25.      ],\n      dtype=float32)time(time)datetime64[ns]2003-01-01T12:00:00 ... 2003-02-...long_name :Time (hours since 1950-01-01)standard_name :timeaxis :T_ChunkSizes :1024array(['2003-01-01T12:00:00.000000000', '2003-01-02T12:00:00.000000000',\n       '2003-01-03T12:00:00.000000000', '2003-01-04T12:00:00.000000000',\n       '2003-01-05T12:00:00.000000000', '2003-01-06T12:00:00.000000000',\n       '2003-01-07T12:00:00.000000000', '2003-01-08T12:00:00.000000000',\n       '2003-01-09T12:00:00.000000000', '2003-01-10T12:00:00.000000000',\n       '2003-01-11T12:00:00.000000000', '2003-01-12T12:00:00.000000000',\n       '2003-01-13T12:00:00.000000000', '2003-01-14T12:00:00.000000000',\n       '2003-01-15T12:00:00.000000000', '2003-01-16T12:00:00.000000000',\n       '2003-01-17T12:00:00.000000000', '2003-01-18T12:00:00.000000000',\n       '2003-01-19T12:00:00.000000000', '2003-01-20T12:00:00.000000000',\n       '2003-01-21T12:00:00.000000000', '2003-01-22T12:00:00.000000000',\n       '2003-01-23T12:00:00.000000000', '2003-01-24T12:00:00.000000000',\n       '2003-01-25T12:00:00.000000000', '2003-01-26T12:00:00.000000000',\n       '2003-01-27T12:00:00.000000000', '2003-01-28T12:00:00.000000000',\n       '2003-01-29T12:00:00.000000000', '2003-01-30T12:00:00.000000000',\n       '2003-01-31T12:00:00.000000000', '2003-02-01T12:00:00.000000000',\n       '2003-02-02T12:00:00.000000000', '2003-02-03T12:00:00.000000000',\n       '2003-02-04T12:00:00.000000000', '2003-02-05T12:00:00.000000000',\n       '2003-02-06T12:00:00.000000000', '2003-02-07T12:00:00.000000000',\n       '2003-02-08T12:00:00.000000000', '2003-02-09T12:00:00.000000000',\n       '2003-02-10T12:00:00.000000000', '2003-02-11T12:00:00.000000000',\n       '2003-02-12T12:00:00.000000000', '2003-02-13T12:00:00.000000000',\n       '2003-02-14T12:00:00.000000000', '2003-02-15T12:00:00.000000000',\n       '2003-02-16T12:00:00.000000000', '2003-02-17T12:00:00.000000000',\n       '2003-02-18T12:00:00.000000000', '2003-02-19T12:00:00.000000000',\n       '2003-02-20T12:00:00.000000000', '2003-02-21T12:00:00.000000000',\n       '2003-02-22T12:00:00.000000000', '2003-02-23T12:00:00.000000000',\n       '2003-02-24T12:00:00.000000000', '2003-02-25T12:00:00.000000000',\n       '2003-02-26T12:00:00.000000000', '2003-02-27T12:00:00.000000000',\n       '2003-02-28T12:00:00.000000000'], dtype='datetime64[ns]')Indexes: (3)longitudePandasIndexPandasIndex(Index([             60.0, 60.08333206176758, 60.16666793823242,\n                   60.25, 60.33333206176758, 60.41666793823242,\n                    60.5, 60.58333206176758, 60.66666793823242,\n                   60.75,\n       ...\n                   79.25, 79.33333587646484, 79.41666412353516,\n                    79.5, 79.58333587646484, 79.66666412353516,\n                   79.75, 79.83333587646484, 79.91666412353516,\n                    80.0],\n      dtype='float32', name='longitude', length=241))latitudePandasIndexPandasIndex(Index([              5.0, 5.083333492279053, 5.166666507720947,\n                    5.25, 5.333333492279053, 5.416666507720947,\n                     5.5, 5.583333492279053, 5.666666507720947,\n                    5.75,\n       ...\n                   24.25, 24.33333396911621, 24.41666603088379,\n                    24.5, 24.58333396911621, 24.66666603088379,\n                   24.75, 24.83333396911621, 24.91666603088379,\n                    25.0],\n      dtype='float32', name='latitude', length=241))timePandasIndexPandasIndex(DatetimeIndex(['2003-01-01 12:00:00', '2003-01-02 12:00:00',\n               '2003-01-03 12:00:00', '2003-01-04 12:00:00',\n               '2003-01-05 12:00:00', '2003-01-06 12:00:00',\n               '2003-01-07 12:00:00', '2003-01-08 12:00:00',\n               '2003-01-09 12:00:00', '2003-01-10 12:00:00',\n               '2003-01-11 12:00:00', '2003-01-12 12:00:00',\n               '2003-01-13 12:00:00', '2003-01-14 12:00:00',\n               '2003-01-15 12:00:00', '2003-01-16 12:00:00',\n               '2003-01-17 12:00:00', '2003-01-18 12:00:00',\n               '2003-01-19 12:00:00', '2003-01-20 12:00:00',\n               '2003-01-21 12:00:00', '2003-01-22 12:00:00',\n               '2003-01-23 12:00:00', '2003-01-24 12:00:00',\n               '2003-01-25 12:00:00', '2003-01-26 12:00:00',\n               '2003-01-27 12:00:00', '2003-01-28 12:00:00',\n               '2003-01-29 12:00:00', '2003-01-30 12:00:00',\n               '2003-01-31 12:00:00', '2003-02-01 12:00:00',\n               '2003-02-02 12:00:00', '2003-02-03 12:00:00',\n               '2003-02-04 12:00:00', '2003-02-05 12:00:00',\n               '2003-02-06 12:00:00', '2003-02-07 12:00:00',\n               '2003-02-08 12:00:00', '2003-02-09 12:00:00',\n               '2003-02-10 12:00:00', '2003-02-11 12:00:00',\n               '2003-02-12 12:00:00', '2003-02-13 12:00:00',\n               '2003-02-14 12:00:00', '2003-02-15 12:00:00',\n               '2003-02-16 12:00:00', '2003-02-17 12:00:00',\n               '2003-02-18 12:00:00', '2003-02-19 12:00:00',\n               '2003-02-20 12:00:00', '2003-02-21 12:00:00',\n               '2003-02-22 12:00:00', '2003-02-23 12:00:00',\n               '2003-02-24 12:00:00', '2003-02-25 12:00:00',\n               '2003-02-26 12:00:00', '2003-02-27 12:00:00',\n               '2003-02-28 12:00:00'],\n              dtype='datetime64[ns]', name='time', freq=None))Attributes: (8)long_name :Salinitystandard_name :sea_water_salinityunits :1e-3unit_long :Practical Salinity Unitvalid_min :1valid_max :28336cell_methods :area: mean_ChunkSizes :[1, 7, 341, 720]\n\n\n\nDS.isel(time=0)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt;\nDimensions:    (longitude: 4320, latitude: 2041, depth: 50)\nCoordinates:\n  * longitude  (longitude) float32 -180.0 -179.9 -179.8 ... 179.8 179.8 179.9\n  * latitude   (latitude) float32 -80.0 -79.92 -79.83 ... 89.83 89.92 90.0\n  * depth      (depth) float32 0.494 1.541 2.646 ... 5.275e+03 5.728e+03\n    time       datetime64[ns] 1993-01-01T12:00:00\nData variables:\n    mlotst     (latitude, longitude) float32 ...\n    zos        (latitude, longitude) float32 ...\n    bottomT    (latitude, longitude) float32 ...\n    sithick    (latitude, longitude) float32 ...\n    siconc     (latitude, longitude) float32 ...\n    usi        (latitude, longitude) float32 ...\n    vsi        (latitude, longitude) float32 ...\n    thetao     (depth, latitude, longitude) float32 ...\n    so         (depth, latitude, longitude) float32 ...\n    uo         (depth, latitude, longitude) float32 ...\n    vo         (depth, latitude, longitude) float32 ...\nAttributes: (12/24)\n    title:              daily mean fields from Global Ocean Physics Analysis ...\n    easting:            longitude\n    northing:           latitude\n    history:            2022/05/25 21:54:07 MERCATOR OCEAN Netcdf creation\n    source:             MERCATOR GLORYS12V1\n    institution:        MERCATOR OCEAN\n    ...                 ...\n    longitude_min:      -180.0\n    longitude_max:      179.91667\n    latitude_min:       -80.0\n    latitude_max:       90.0\n    z_min:              0.494025\n    z_max:              5727.917xarray.DatasetDimensions:longitude: 4320latitude: 2041depth: 50Coordinates: (4)longitude(longitude)float32-180.0 -179.9 ... 179.8 179.9valid_min :-180.0valid_max :179.91667step :0.08332825units :degrees_eastunit_long :Degrees Eastlong_name :Longitudestandard_name :longitudeaxis :X_ChunkSizes :4320array([-180.     , -179.91667, -179.83333, ...,  179.75   ,  179.83333,\n        179.91667], dtype=float32)latitude(latitude)float32-80.0 -79.92 -79.83 ... 89.92 90.0valid_min :-80.0valid_max :90.0step :0.08333588units :degrees_northunit_long :Degrees Northlong_name :Latitudestandard_name :latitudeaxis :Y_ChunkSizes :2041array([-80.      , -79.916664, -79.833336, ...,  89.833336,  89.916664,\n        90.      ], dtype=float32)depth(depth)float320.494 1.541 ... 5.275e+03 5.728e+03valid_min :0.494025valid_max :5727.917units :mpositive :downunit_long :Meterslong_name :Depthstandard_name :depthaxis :Z_ChunkSizes :50array([4.940250e-01, 1.541375e+00, 2.645669e+00, 3.819495e+00, 5.078224e+00,\n       6.440614e+00, 7.929560e+00, 9.572997e+00, 1.140500e+01, 1.346714e+01,\n       1.581007e+01, 1.849556e+01, 2.159882e+01, 2.521141e+01, 2.944473e+01,\n       3.443415e+01, 4.034405e+01, 4.737369e+01, 5.576429e+01, 6.580727e+01,\n       7.785385e+01, 9.232607e+01, 1.097293e+02, 1.306660e+02, 1.558507e+02,\n       1.861256e+02, 2.224752e+02, 2.660403e+02, 3.181274e+02, 3.802130e+02,\n       4.539377e+02, 5.410889e+02, 6.435668e+02, 7.633331e+02, 9.023393e+02,\n       1.062440e+03, 1.245291e+03, 1.452251e+03, 1.684284e+03, 1.941893e+03,\n       2.225078e+03, 2.533336e+03, 2.865703e+03, 3.220820e+03, 3.597032e+03,\n       3.992484e+03, 4.405224e+03, 4.833291e+03, 5.274784e+03, 5.727917e+03],\n      dtype=float32)time()datetime64[ns]1993-01-01T12:00:00long_name :Time (hours since 1950-01-01)standard_name :timeaxis :T_ChunkSizes :1024array('1993-01-01T12:00:00.000000000', dtype='datetime64[ns]')Data variables: (11)mlotst(latitude, longitude)float32...long_name :Density ocean mixed layer thicknessstandard_name :ocean_mixed_layer_thickness_defined_by_sigma_thetaunits :munit_long :Metersvalid_min :1valid_max :6447cell_methods :area: mean_ChunkSizes :[1, 681, 1440][8817120 values with dtype=float32]zos(latitude, longitude)float32...long_name :Sea surface heightstandard_name :sea_surface_height_above_geoidunits :munit_long :Metersvalid_min :-6181valid_max :5039cell_methods :area: mean_ChunkSizes :[1, 681, 1440][8817120 values with dtype=float32]bottomT(latitude, longitude)float32...long_name :Sea floor potential temperaturestandard_name :sea_water_potential_temperature_at_sea_floorunits :degrees_Cunit_long :Degrees Celsiusvalid_min :-32757valid_max :21318cell_methods :area: mean_ChunkSizes :[1, 681, 1440][8817120 values with dtype=float32]sithick(latitude, longitude)float32...long_name :Sea ice thicknessstandard_name :sea_ice_thicknessunits :munit_long :Metersvalid_min :1valid_max :4618cell_methods :area: mean where sea_ice_ChunkSizes :[1, 681, 1440][8817120 values with dtype=float32]siconc(latitude, longitude)float32...long_name :Ice concentrationstandard_name :sea_ice_area_fractionunits :1unit_long :Fractionvalid_min :1valid_max :28060cell_methods :area: mean where sea_ice_ChunkSizes :[1, 681, 1440][8817120 values with dtype=float32]usi(latitude, longitude)float32...long_name :Sea ice eastward velocitystandard_name :eastward_sea_ice_velocityunits :m s-1unit_long :Meters per secondvalid_min :-32707valid_max :32144cell_methods :area: mean where sea_ice_ChunkSizes :[1, 681, 1440][8817120 values with dtype=float32]vsi(latitude, longitude)float32...long_name :Sea ice northward velocitystandard_name :northward_sea_ice_velocityunits :m s-1unit_long :Meters per secondvalid_min :-31907valid_max :31690cell_methods :area: mean where sea_ice_ChunkSizes :[1, 681, 1440][8817120 values with dtype=float32]thetao(depth, latitude, longitude)float32...long_name :Temperaturestandard_name :sea_water_potential_temperatureunits :degrees_Cunit_long :Degrees Celsiusvalid_min :-32764valid_max :21298cell_methods :area: mean_ChunkSizes :[1, 7, 341, 720][440856000 values with dtype=float32]so(depth, latitude, longitude)float32...long_name :Salinitystandard_name :sea_water_salinityunits :1e-3unit_long :Practical Salinity Unitvalid_min :1valid_max :28336cell_methods :area: mean_ChunkSizes :[1, 7, 341, 720][440856000 values with dtype=float32]uo(depth, latitude, longitude)float32...long_name :Eastward velocitystandard_name :eastward_sea_water_velocityunits :m s-1unit_long :Meters per secondvalid_min :-3573valid_max :3559cell_methods :area: mean_ChunkSizes :[1, 7, 341, 720][440856000 values with dtype=float32]vo(depth, latitude, longitude)float32...long_name :Northward velocitystandard_name :northward_sea_water_velocityunits :m s-1unit_long :Meters per secondvalid_min :-3300valid_max :3399cell_methods :area: mean_ChunkSizes :[1, 7, 341, 720][440856000 values with dtype=float32]Indexes: (3)longitudePandasIndexPandasIndex(Index([            -180.0, -179.9166717529297, -179.8333282470703,\n                  -179.75, -179.6666717529297, -179.5833282470703,\n                   -179.5, -179.4166717529297, -179.3333282470703,\n                  -179.25,\n       ...\n        179.1666717529297,             179.25,  179.3333282470703,\n        179.4166717529297,              179.5,  179.5833282470703,\n        179.6666717529297,             179.75,  179.8333282470703,\n        179.9166717529297],\n      dtype='float32', name='longitude', length=4320))latitudePandasIndexPandasIndex(Index([             -80.0, -79.91666412353516, -79.83333587646484,\n                   -79.75, -79.66666412353516, -79.58333587646484,\n                    -79.5, -79.41666412353516, -79.33333587646484,\n                   -79.25,\n       ...\n                    89.25,  89.33333587646484,  89.41666412353516,\n                     89.5,  89.58333587646484,  89.66666412353516,\n                    89.75,  89.83333587646484,  89.91666412353516,\n                     90.0],\n      dtype='float32', name='latitude', length=2041))depthPandasIndexPandasIndex(Index([0.49402499198913574,  1.5413750410079956,  2.6456689834594727,\n        3.8194949626922607,   5.078224182128906,   6.440614223480225,\n          7.92956018447876,   9.572997093200684,  11.404999732971191,\n        13.467140197753906,  15.810070037841797,  18.495559692382812,\n        21.598819732666016,  25.211410522460938,  29.444730758666992,\n         34.43415069580078,  40.344051361083984,   47.37369155883789,\n         55.76428985595703,   65.80726623535156,   77.85385131835938,\n          92.3260726928711,  109.72930145263672,  130.66600036621094,\n        155.85069274902344,  186.12559509277344,  222.47520446777344,\n         266.0403137207031,   318.1274108886719,   380.2130126953125,\n         453.9377136230469,   541.0889282226562,   643.5667724609375,\n         763.3331298828125,   902.3392944335938,    1062.43994140625,\n            1245.291015625,     1452.2509765625,  1684.2840576171875,\n        1941.8929443359375,   2225.077880859375,        2533.3359375,\n         2865.702880859375,   3220.820068359375,   3597.031982421875,\n          3992.48388671875,    4405.22412109375,      4833.291015625,\n           5274.7841796875,     5727.9169921875],\n      dtype='float32', name='depth'))Attributes: (24)title :daily mean fields from Global Ocean Physics Analysis and Forecast updated Dailyeasting :longitudenorthing :latitudehistory :2022/05/25 21:54:07 MERCATOR OCEAN Netcdf creationsource :MERCATOR GLORYS12V1institution :MERCATOR OCEANreferences :http://www.mercator-ocean.frcomment :CMEMS productConventions :CF-1.4domain_name :GL12field_type :meanfield_date :2020-12-31 00:00:00field_julian_date :25932.0julian_day_unit :days since 1950-01-01 00:00:00forecast_range :forecast_type :bulletin_date :2021-01-06 00:00:00bulletin_type :operationallongitude_min :-180.0longitude_max :179.91667latitude_min :-80.0latitude_max :90.0z_min :0.494025z_max :5727.917\n\n\nFinally, we can export this dataset to combine with the rest of the data later\n\nDS_sliced.to_netcdf('demonstrated data/salinity_at_0_49m.nc')"
  },
  {
    "objectID": "python-notebooks/02_Get_Points.html",
    "href": "python-notebooks/02_Get_Points.html",
    "title": "Getting Environmental Variables’ Data at a Time-Space instance",
    "section": "",
    "text": "You have a pandas dataframe with time, lat, lon that you got from say reading a csv file.\nYou have a xarray dataset from reading in the zarr file\nGoal is a pandas data from with time, lat, lon, values for all the environmental varables at that time, lat, lon\n\n\nimport xarray as xr\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport holoviews as hv\nimport hvplot.xarray\n\n\n\n\n\n\n\n\n\n\n\n# Change the file path accordingly\nda = xr.open_zarr('/home/jovyan/shared/data/INDIAN_OCEAN_025GRID_DAILY.zarr/')\n\n\ndf = pd.read_csv('/home/jovyan/shared/data/species_occ/io-sea-turtles.csv')\n\n\ndf.head()\n\n\n\n\n\n\n\n\noccurrenceID\nscientificName\ndateIdentified\neventDate\ndecimalLatitude\ndecimalLongitude\ncoordinateUncertaintyInMeters\nindividualCount\nlifeStage\nsex\nbathymetry\nshoredistance\nsst\nsss\ndate\n\n\n\n\n0\n1014_8853\nCaretta caretta\n2012-03-30T08:55:10\n2012-03-30T08:55:10\n9.14804\n50.69448\n0.11\n1.0\nJuvenile\nNaN\n35\n4680\n26.21\n35.63\n2012-03-30\n\n\n1\n1014_9766\nCaretta caretta\n2011-04-12T19:12:41\n2011-04-12T19:12:41\n6.40193\n59.87883\n0.11\n1.0\nJuvenile\nNaN\n3051\n899682\n28.67\n35.74\n2011-04-12\n\n\n2\n2069_333\nChelonia mydas\n2016-07-24T15:18:00\n2016-07-24T15:18:00\n25.69900\n55.78900\n111.32\n1.0\nNaN\nNaN\n8\n-67\n28.19\n38.72\n2016-07-24\n\n\n3\n1014_8861\nCaretta caretta\n2012-03-31T02:43:31\n2012-03-31T02:43:31\n9.21307\n50.82211\n0.11\n1.0\nJuvenile\nNaN\n136\n12208\n26.21\n35.65\n2012-03-31\n\n\n4\n1014_15706\nCaretta caretta\n2011-05-14T17:53:55\n2011-05-14T17:53:55\n15.39172\n55.71303\n0.11\n1.0\nJuvenile\nNaN\n2784\n193567\n27.00\n36.09\n2011-05-14\n\n\n\n\n\n\n\n\ndf = df[:20]\n\n\ndf['occurrenceID']\n\n0      1014_8853\n1      1014_9766\n2       2069_333\n3      1014_8861\n4     1014_15706\n5      2069_4254\n6      2069_3631\n7      2069_4910\n8     1014_16785\n9      2069_3419\n10     2069_5265\n11      2069_442\n12     2069_4668\n13    1014_15473\n14    1014_16862\n15    1014_16860\n16     1014_2087\n17    1014_16484\n18    1014_16899\n19    1014_16669\nName: occurrenceID, dtype: object\n\n\n\nptime = xr.DataArray(df['date'], dims=['point']) # data is from 1979-01-01 to 2022-12-31\nplat = xr.DataArray(df['decimalLatitude'], dims=['point'])  # Points to select need common dimensions\nplon = xr.DataArray(df['decimalLongitude'], dims=['point'])\n\n\nda.sel(time=ptime, lat=plat, lon=plon, method='nearest').squeeze().to_pandas()\n\n\n\n\n\n\n\n\nCHL\nCHL_uncertainty\nadt\nair_temp\ncurr_dir\ncurr_speed\nlat\nlon\nmlotst\nsla\n...\ntime\ntopo\nu_curr\nu_wind\nug_curr\nv_curr\nv_wind\nvg_curr\nwind_dir\nwind_speed\n\n\npoint\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n0\n0.077778\n53.305443\nNaN\n299.421875\nNaN\nNaN\n9.25\n50.75\n13.046663\nNaN\n...\n2012-03-30\n-32.0\nNaN\n-2.179688\nNaN\nNaN\n1.361979\nNaN\n148.000763\n2.570219\n\n\n1\n0.087336\n38.892536\n0.842800\n301.322906\n164.178543\n0.183589\n6.50\n60.00\n21.820736\n0.097500\n...\n2011-04-12\n-2685.0\n-0.176634\n-4.575521\n-0.114507\n0.050054\n-2.950521\n0.011828\n-147.184082\n5.444352\n\n\n2\n1.637924\n53.872570\nNaN\n306.830719\nNaN\nNaN\n25.75\n55.75\n11.291849\nNaN\n...\n2016-07-24\n-9.0\nNaN\n-6.393229\nNaN\nNaN\n2.523438\nNaN\n158.460617\n6.873217\n\n\n3\n0.201034\n43.664085\nNaN\n299.718750\nNaN\nNaN\n9.25\n50.75\n12.817775\nNaN\n...\n2012-03-31\n-32.0\nNaN\n-2.481771\nNaN\nNaN\n1.101562\nNaN\n156.065399\n2.715258\n\n\n4\n0.099818\n38.205021\n0.754650\n301.520844\n49.900036\n0.530751\n15.50\n55.75\n11.520738\n0.164175\n...\n2011-05-14\n-3001.0\n0.341869\n-1.447917\n0.343384\n0.405983\n2.841146\n0.416438\n117.004478\n3.188820\n\n\n5\n0.689319\n28.379957\n0.462725\n298.252594\n70.057045\n0.139277\n24.50\n53.00\n7.171850\n-0.046750\n...\n2018-03-31\n-8.0\n0.047505\n3.419271\n0.053919\n0.130925\n-4.880208\n0.138531\n-54.983360\n5.958846\n\n\n6\n0.738169\n35.419315\n0.472050\n298.463531\n82.680542\n0.163747\n24.50\n53.25\n7.782220\n-0.038450\n...\n2018-03-31\n-10.0\n0.020862\n3.609375\n0.032643\n0.162413\n-4.356771\n0.158398\n-50.359882\n5.657653\n\n\n7\n2.898179\n48.014732\n0.678450\n306.979156\n83.276283\n0.146953\n24.75\n53.00\n11.978516\n0.166025\n...\n2019-08-16\n-14.0\n0.017206\n0.359375\n0.028725\n0.145943\n-2.687500\n0.140182\n-82.383530\n2.711421\n\n\n8\n0.139877\n31.482536\n0.752725\n298.158844\n132.076324\n0.093470\n15.50\n53.75\n11.291849\n0.220450\n...\n2011-04-05\n-2440.0\n-0.062636\n-0.677083\n-0.059925\n0.069378\n2.070312\n0.064436\n108.110016\n2.178218\n\n\n9\n0.922149\n21.962460\n0.450300\n300.229156\n-94.786385\n0.117982\n24.50\n53.00\n7.171850\n-0.059225\n...\n2017-04-27\n-8.0\n-0.009845\n2.500000\n0.023389\n-0.117570\n-2.994792\n-0.083020\n-50.145458\n3.901125\n\n\n10\nNaN\nNaN\nNaN\n305.799469\nNaN\nNaN\n24.25\n51.75\nNaN\n0.209875\n...\n2019-10-07\n20.0\nNaN\n-5.515625\nNaN\nNaN\n-0.151042\nNaN\n-178.431381\n5.517693\n\n\n11\nNaN\nNaN\nNaN\n304.533844\nNaN\nNaN\n25.75\n56.00\nNaN\nNaN\n...\n2016-09-24\n15.0\nNaN\n1.070312\nNaN\nNaN\n0.791667\nNaN\n36.488827\n1.331280\n\n\n12\n2.525344\n40.379536\n0.554250\n304.059906\n-166.844086\n0.089691\n24.50\n53.00\n7.171850\n0.044725\n...\n2019-05-28\n-8.0\n-0.087337\n-1.195312\n-0.069366\n-0.020414\n-1.648438\n-0.018889\n-125.946548\n2.036202\n\n\n13\n0.131018\n31.812517\n0.766250\n299.195312\n-24.765293\n0.264651\n16.00\n54.25\n19.074070\n0.230625\n...\n2011-04-11\n-2732.0\n0.240311\n1.335938\n0.209158\n-0.110863\n4.286458\n-0.100357\n72.689590\n4.489817\n\n\n14\n0.153894\n35.990002\n0.736325\n299.567719\n-132.773819\n0.464942\n14.50\n53.50\n15.182960\n0.152625\n...\n2011-04-13\n-2407.0\n-0.315745\n-1.312500\n-0.326198\n-0.341286\n2.192708\n-0.356120\n120.903671\n2.555509\n\n\n15\n0.153894\n35.990002\n0.736325\n299.567719\n-132.773819\n0.464942\n14.50\n53.50\n15.182960\n0.152625\n...\n2011-04-13\n-2407.0\n-0.315745\n-1.312500\n-0.326198\n-0.341286\n2.192708\n-0.356120\n120.903671\n2.555509\n\n\n16\n0.164712\n40.660110\n0.642800\n301.687500\n-17.193645\n0.117679\n16.75\n54.75\n12.588886\n0.092950\n...\n2012-05-13\n-2268.0\n0.112420\n6.015625\n0.092263\n-0.034786\n2.247396\n-0.001633\n20.485338\n6.421723\n\n\n17\n0.164154\n30.657511\n0.639750\n298.781250\n91.528801\n0.122125\n5.25\n54.00\n22.049625\n-0.079750\n...\n2011-02-10\n-4884.0\n-0.003258\n-4.299479\n0.094443\n0.122081\n-1.971354\n0.049081\n-155.368073\n4.729879\n\n\n18\n0.140080\n31.470005\n0.657150\n300.768219\n-103.054863\n0.217463\n13.50\n52.75\n13.809626\n0.037550\n...\n2011-04-16\n-2609.0\n-0.049121\n-2.177083\n-0.053068\n-0.211842\n0.380208\n-0.216044\n170.093689\n2.210034\n\n\n19\n0.182036\n30.130001\n0.736275\n298.018219\n151.549240\n0.114068\n16.50\n53.75\n16.174812\n0.193825\n...\n2011-03-24\n-2329.0\n-0.100291\n2.276042\n-0.100962\n0.054342\n2.364583\n0.063244\n46.093052\n3.282015\n\n\n\n\n20 rows × 22 columns\n\n\n\n\n# side check\n# add occurence ID for points (database)\n\npoints = pd.read_csv('/home/jovyan/shared-readwrite/data/species_occ/result.csv')\n\n\npoints\n\n\n\n\n\n\n\n\nCHL\nCHL_uncertainty\nadt\nair_temp\ncurr_dir\ncurr_speed\nlat\nlon\nmlotst\nsla\n...\ntopo\nu_curr\nu_wind\nug_curr\nv_curr\nv_wind\nvg_curr\nwind_dir\nwind_speed\noccurrenceID\n\n\n\n\n0\n0.077778\n53.305443\nNaN\n299.42188\nNaN\nNaN\n9.25\n50.75\n13.046663\nNaN\n...\n-32.0\nNaN\n-2.179688\nNaN\nNaN\n1.361979\nNaN\n148.000760\n2.570219\n1014_8853\n\n\n1\n0.087336\n38.892536\n0.842800\n301.32290\n164.178540\n0.183589\n6.50\n60.00\n21.820736\n0.097500\n...\n-2685.0\n-0.176634\n-4.575521\n-0.114507\n0.050054\n-2.950521\n0.011828\n-147.184080\n5.444352\n1014_9766\n\n\n2\n1.637924\n53.872570\nNaN\n306.83072\nNaN\nNaN\n25.75\n55.75\n11.291849\nNaN\n...\n-9.0\nNaN\n-6.393229\nNaN\nNaN\n2.523438\nNaN\n158.460620\n6.873217\n2069_333\n\n\n3\n0.201034\n43.664085\nNaN\n299.71875\nNaN\nNaN\n9.25\n50.75\n12.817775\nNaN\n...\n-32.0\nNaN\n-2.481771\nNaN\nNaN\n1.101562\nNaN\n156.065400\n2.715258\n1014_8861\n\n\n4\n0.099818\n38.205020\n0.754650\n301.52084\n49.900036\n0.530751\n15.50\n55.75\n11.520738\n0.164175\n...\n-3001.0\n0.341869\n-1.447917\n0.343384\n0.405983\n2.841146\n0.416438\n117.004480\n3.188820\n1014_15706\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n13205\n0.134301\n74.170000\n0.717500\n301.84116\n-91.168960\n0.037919\n19.75\n61.50\n36.469620\n0.063825\n...\n-3553.0\n-0.000774\n9.335938\n-0.130193\n-0.037911\n9.286458\n0.006756\n44.847763\n13.168069\n1014_2555\n\n\n13206\n0.705127\n28.212389\n0.620425\n300.71094\n108.626130\n0.072745\n24.75\n53.00\n11.139257\n0.108000\n...\n-14.0\n-0.023234\n2.289062\n-0.026993\n0.068935\n-2.375000\n0.052721\n-46.055580\n3.298550\n2069_7427\n\n\n13207\n0.974497\n32.285313\nNaN\n305.82553\nNaN\nNaN\n25.75\n55.75\n11.673330\nNaN\n...\n-9.0\nNaN\n1.078125\nNaN\nNaN\n-0.213542\nNaN\n-11.203434\n1.099069\n2069_587\n\n\n13208\n0.411613\n36.068120\n0.597650\n304.20572\n-107.023180\n0.112533\n24.75\n53.00\n12.741478\n0.085250\n...\n-14.0\n-0.032945\n2.906250\n-0.009166\n-0.107603\n-1.510417\n-0.094187\n-27.461494\n3.275309\n2069_272\n\n\n13209\n0.254070\n43.014880\n0.697400\n299.09897\n-94.746110\n0.627941\n15.50\n55.50\n12.207404\n0.109350\n...\n-2929.0\n-0.051956\n-1.369792\n-0.054724\n-0.625787\n0.901042\n-0.642169\n146.663280\n1.639575\n1014_1635\n\n\n\n\n13210 rows × 23 columns\n\n\n\n\npoints['occurrenceID'] = df['occurrenceID']\n\n\npoints.to_csv('/home/jovyan/shared-readwrite/data/species_occ/result2.csv', index=False)\n\n\npoints = points.drop(columns=['point'])"
  },
  {
    "objectID": "python-notebooks/02_Using_Data.html#about-our-product",
    "href": "python-notebooks/02_Using_Data.html#about-our-product",
    "title": "IO Zarr file",
    "section": "About our product",
    "text": "About our product\nOur final product INDIAN_OCEAN_025GRID_DAILY.zarr is a blended dataset formated as a .zarr file, containing daily cleaned and interpolated data from fifteen variables across multiple sources, mostly from processed NASA/NOAA and Copernicus collections: * adt: sea surface height above geoid (m) * air_temp: air temperature (K) * CHL: chlorophyll-a concentration (mg/m3) * CHL_uncertainty: chlorophyll-a concentration uncertainty (%) * mlotst: mean ocean mixed layer thickness (m) * sla: sea level anomaly (m) * so: sea salinity concentration (m-3 or PSL) * sst: sea surface temperature (K) * topo: topography (m) * u_curr: u-component of total surface currents (m/s) * v_curr: v-component of total surface currents (m/s) * ug_curr: u-component of geostrophic surface currents (m/s) * vg_curr: v-component of geostrophic surface currents (m/s) * u_wind: u-component of surface wind (m/s) * v_wind: v-component of surface wind (m/s)\nWe also computed four additional components, namely: * curr_speed: total current speed (m/s) * curr_dir: total current direction (degrees) * wind_speed: surface wind speed (m/s) * wind_dir: surface wind direction (degrees)\nAll variables have been broadcasted to all fit in the temporal range we have. Therefore, not all variable data are available at any time. Check out each individual variable before use.",
    "crumbs": [
      "Python Tutorials",
      "Using Data",
      "IO Zarr file"
    ]
  },
  {
    "objectID": "python-notebooks/02_Using_Data.html#import-necessary-libraries",
    "href": "python-notebooks/02_Using_Data.html#import-necessary-libraries",
    "title": "IO Zarr file",
    "section": "Import necessary libraries",
    "text": "Import necessary libraries\n\n! pip install cmocean\n\nCollecting cmocean\n  Using cached cmocean-3.0.3-py3-none-any.whl (222 kB)\nRequirement already satisfied: matplotlib in /srv/conda/envs/notebook/lib/python3.9/site-packages (from cmocean) (3.7.1)\nRequirement already satisfied: numpy in /srv/conda/envs/notebook/lib/python3.9/site-packages (from cmocean) (1.23.5)\nRequirement already satisfied: packaging in /srv/conda/envs/notebook/lib/python3.9/site-packages (from cmocean) (23.1)\nRequirement already satisfied: contourpy&gt;=1.0.1 in /srv/conda/envs/notebook/lib/python3.9/site-packages (from matplotlib-&gt;cmocean) (1.0.7)\nRequirement already satisfied: cycler&gt;=0.10 in /srv/conda/envs/notebook/lib/python3.9/site-packages (from matplotlib-&gt;cmocean) (0.11.0)\nRequirement already satisfied: fonttools&gt;=4.22.0 in /srv/conda/envs/notebook/lib/python3.9/site-packages (from matplotlib-&gt;cmocean) (4.39.4)\nRequirement already satisfied: kiwisolver&gt;=1.0.1 in /srv/conda/envs/notebook/lib/python3.9/site-packages (from matplotlib-&gt;cmocean) (1.4.4)\nRequirement already satisfied: pillow&gt;=6.2.0 in /srv/conda/envs/notebook/lib/python3.9/site-packages (from matplotlib-&gt;cmocean) (9.5.0)\nRequirement already satisfied: pyparsing&gt;=2.3.1 in /srv/conda/envs/notebook/lib/python3.9/site-packages (from matplotlib-&gt;cmocean) (3.0.9)\nRequirement already satisfied: python-dateutil&gt;=2.7 in /srv/conda/envs/notebook/lib/python3.9/site-packages (from matplotlib-&gt;cmocean) (2.8.2)\nRequirement already satisfied: importlib-resources&gt;=3.2.0 in /srv/conda/envs/notebook/lib/python3.9/site-packages (from matplotlib-&gt;cmocean) (5.12.0)\nRequirement already satisfied: zipp&gt;=3.1.0 in /srv/conda/envs/notebook/lib/python3.9/site-packages (from importlib-resources&gt;=3.2.0-&gt;matplotlib-&gt;cmocean) (3.15.0)\nRequirement already satisfied: six&gt;=1.5 in /srv/conda/envs/notebook/lib/python3.9/site-packages (from python-dateutil&gt;=2.7-&gt;matplotlib-&gt;cmocean) (1.16.0)\nInstalling collected packages: cmocean\nSuccessfully installed cmocean-3.0.3\n\n\n\nimport xarray as xr\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt # simple plotting\nimport holoviews as hv # simple plotting\nimport hvplot.xarray # simple plotting",
    "crumbs": [
      "Python Tutorials",
      "Using Data",
      "IO Zarr file"
    ]
  },
  {
    "objectID": "python-notebooks/02_Using_Data.html#read-data",
    "href": "python-notebooks/02_Using_Data.html#read-data",
    "title": "IO Zarr file",
    "section": "Read data",
    "text": "Read data\n\nds = xr.open_zarr('/home/jovyan/shared/data/INDIAN_OCEAN_025GRID_DAILY.zarr/')\n\n\nds\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt;\nDimensions:          (time: 16071, lat: 177, lon: 241)\nCoordinates:\n  * lat              (lat) float32 32.0 31.75 31.5 31.25 ... -11.5 -11.75 -12.0\n  * lon              (lon) float32 42.0 42.25 42.5 42.75 ... 101.5 101.8 102.0\n  * time             (time) datetime64[ns] 1979-01-01 1979-01-02 ... 2022-12-31\nData variables: (12/19)\n    CHL              (time, lat, lon) float32 dask.array&lt;chunksize=(100, 177, 241), meta=np.ndarray&gt;\n    CHL_uncertainty  (time, lat, lon) float32 dask.array&lt;chunksize=(100, 177, 241), meta=np.ndarray&gt;\n    adt              (time, lat, lon) float32 dask.array&lt;chunksize=(100, 177, 241), meta=np.ndarray&gt;\n    air_temp         (time, lat, lon) float32 dask.array&lt;chunksize=(100, 177, 241), meta=np.ndarray&gt;\n    curr_dir         (time, lat, lon) float32 dask.array&lt;chunksize=(100, 177, 241), meta=np.ndarray&gt;\n    curr_speed       (time, lat, lon) float32 dask.array&lt;chunksize=(100, 177, 241), meta=np.ndarray&gt;\n    ...               ...\n    ug_curr          (time, lat, lon) float32 dask.array&lt;chunksize=(100, 177, 241), meta=np.ndarray&gt;\n    v_curr           (time, lat, lon) float32 dask.array&lt;chunksize=(100, 177, 241), meta=np.ndarray&gt;\n    v_wind           (time, lat, lon) float32 dask.array&lt;chunksize=(100, 177, 241), meta=np.ndarray&gt;\n    vg_curr          (time, lat, lon) float32 dask.array&lt;chunksize=(100, 177, 241), meta=np.ndarray&gt;\n    wind_dir         (time, lat, lon) float32 dask.array&lt;chunksize=(100, 177, 241), meta=np.ndarray&gt;\n    wind_speed       (time, lat, lon) float32 dask.array&lt;chunksize=(100, 177, 241), meta=np.ndarray&gt;\nAttributes: (12/17)\n    creator_email:              minhphan@uw.edu\n    creator_name:               Minh Phan\n    creator_type:               person\n    date_created:               2023-07-19\n    geospatial_lat_max:         32.0\n    geospatial_lat_min:         -12.0\n    ...                         ...\n    geospatial_lon_units:       degrees_east\n    source:                     Earth & Space Research (ESR), Copernicus Clim...\n    summary:                    Daily mean of 0.25 x 0.25 degrees gridded dat...\n    time_coverage_end:          2022-12-31T23:59:59\n    time_coverage_start:        1979-01-01T00:00:00\n    title:                      Climate Data for Coastal Upwelling Machine Le...xarray.DatasetDimensions:time: 16071lat: 177lon: 241Coordinates: (3)lat(lat)float3232.0 31.75 31.5 ... -11.75 -12.0long_name :latitudestandard_name :latitudeunits :degrees_northarray([ 32.  ,  31.75,  31.5 ,  31.25,  31.  ,  30.75,  30.5 ,  30.25,  30.  ,\n        29.75,  29.5 ,  29.25,  29.  ,  28.75,  28.5 ,  28.25,  28.  ,  27.75,\n        27.5 ,  27.25,  27.  ,  26.75,  26.5 ,  26.25,  26.  ,  25.75,  25.5 ,\n        25.25,  25.  ,  24.75,  24.5 ,  24.25,  24.  ,  23.75,  23.5 ,  23.25,\n        23.  ,  22.75,  22.5 ,  22.25,  22.  ,  21.75,  21.5 ,  21.25,  21.  ,\n        20.75,  20.5 ,  20.25,  20.  ,  19.75,  19.5 ,  19.25,  19.  ,  18.75,\n        18.5 ,  18.25,  18.  ,  17.75,  17.5 ,  17.25,  17.  ,  16.75,  16.5 ,\n        16.25,  16.  ,  15.75,  15.5 ,  15.25,  15.  ,  14.75,  14.5 ,  14.25,\n        14.  ,  13.75,  13.5 ,  13.25,  13.  ,  12.75,  12.5 ,  12.25,  12.  ,\n        11.75,  11.5 ,  11.25,  11.  ,  10.75,  10.5 ,  10.25,  10.  ,   9.75,\n         9.5 ,   9.25,   9.  ,   8.75,   8.5 ,   8.25,   8.  ,   7.75,   7.5 ,\n         7.25,   7.  ,   6.75,   6.5 ,   6.25,   6.  ,   5.75,   5.5 ,   5.25,\n         5.  ,   4.75,   4.5 ,   4.25,   4.  ,   3.75,   3.5 ,   3.25,   3.  ,\n         2.75,   2.5 ,   2.25,   2.  ,   1.75,   1.5 ,   1.25,   1.  ,   0.75,\n         0.5 ,   0.25,   0.  ,  -0.25,  -0.5 ,  -0.75,  -1.  ,  -1.25,  -1.5 ,\n        -1.75,  -2.  ,  -2.25,  -2.5 ,  -2.75,  -3.  ,  -3.25,  -3.5 ,  -3.75,\n        -4.  ,  -4.25,  -4.5 ,  -4.75,  -5.  ,  -5.25,  -5.5 ,  -5.75,  -6.  ,\n        -6.25,  -6.5 ,  -6.75,  -7.  ,  -7.25,  -7.5 ,  -7.75,  -8.  ,  -8.25,\n        -8.5 ,  -8.75,  -9.  ,  -9.25,  -9.5 ,  -9.75, -10.  , -10.25, -10.5 ,\n       -10.75, -11.  , -11.25, -11.5 , -11.75, -12.  ], dtype=float32)lon(lon)float3242.0 42.25 42.5 ... 101.8 102.0long_name :longitudestandard_name :longitudeunits :degrees_eastarray([ 42.  ,  42.25,  42.5 , ..., 101.5 , 101.75, 102.  ], dtype=float32)time(time)datetime64[ns]1979-01-01 ... 2022-12-31axis :Tcomment :Data is averaged over the daylong_name :time centered on the daystandard_name :timetime_bounds :2000-01-01 00:00:00 to 2000-01-01 23:59:59array(['1979-01-01T00:00:00.000000000', '1979-01-02T00:00:00.000000000',\n       '1979-01-03T00:00:00.000000000', ..., '2022-12-29T00:00:00.000000000',\n       '2022-12-30T00:00:00.000000000', '2022-12-31T00:00:00.000000000'],\n      dtype='datetime64[ns]')Data variables: (19)CHL(time, lat, lon)float32dask.array&lt;chunksize=(100, 177, 241), meta=np.ndarray&gt;_ChunkSizes :[1, 256, 256]ancillary_variables :flags CHL_uncertaintycoverage_content_type :modelResultinput_files_reprocessings :Processors versions: MODIS R2022.0NRT/VIIRSN R2022.0NRT/OLCIA 07.02/VIIRSJ1 R2022.0NRT/OLCIB 07.02long_name :Chlorophyll-a concentration - Mean of the binned pixelsstandard_name :mass_concentration_of_chlorophyll_a_in_sea_watertype :surfaceunits :milligram m-3valid_max :1000.0valid_min :0.0\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n2.55 GiB\n16.27 MiB\n\n\nShape\n(16071, 177, 241)\n(100, 177, 241)\n\n\nDask graph\n161 chunks in 2 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\nCHL_uncertainty\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(100, 177, 241), meta=np.ndarray&gt;\n\n\n\n\n_ChunkSizes :\n\n[1, 256, 256]\n\ncoverage_content_type :\n\nqualityInformation\n\nlong_name :\n\nChlorophyll-a concentration - Uncertainty estimation\n\nunits :\n\n%\n\nvalid_max :\n\n32767\n\nvalid_min :\n\n0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n2.55 GiB\n16.27 MiB\n\n\nShape\n(16071, 177, 241)\n(100, 177, 241)\n\n\nDask graph\n161 chunks in 2 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nadt\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(100, 177, 241), meta=np.ndarray&gt;\n\n\n\n\ncomment :\n\nThe absolute dynamic topography is the sea surface height above geoid; the adt is obtained as follows: adt=sla+mdt where mdt is the mean dynamic topography; see the product user manual for details\n\ngrid_mapping :\n\ncrs\n\nlong_name :\n\nAbsolute dynamic topography\n\nstandard_name :\n\nsea_surface_height_above_geoid\n\nunits :\n\nm\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n2.55 GiB\n16.27 MiB\n\n\nShape\n(16071, 177, 241)\n(100, 177, 241)\n\n\nDask graph\n161 chunks in 2 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nair_temp\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(100, 177, 241), meta=np.ndarray&gt;\n\n\n\n\nlong_name :\n\n2 metre temperature\n\nnameCDM :\n\n2_metre_temperature_surface\n\nnameECMWF :\n\n2 metre temperature\n\nproduct_type :\n\nanalysis\n\nshortNameECMWF :\n\n2t\n\nstandard_name :\n\nair_temperature\n\nunits :\n\nK\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n2.55 GiB\n16.27 MiB\n\n\nShape\n(16071, 177, 241)\n(100, 177, 241)\n\n\nDask graph\n161 chunks in 2 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\ncurr_dir\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(100, 177, 241), meta=np.ndarray&gt;\n\n\n\n\ncomments :\n\nComputed from total surface current velocity elements. Velocities are an average over the top 30m of the mixed layer\n\ndepth :\n\n15m\n\nlong_name :\n\naverage direction of total surface currents\n\nunits :\n\ndegrees\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n2.55 GiB\n16.27 MiB\n\n\nShape\n(16071, 177, 241)\n(100, 177, 241)\n\n\nDask graph\n161 chunks in 2 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\ncurr_speed\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(100, 177, 241), meta=np.ndarray&gt;\n\n\n\n\ncomments :\n\nVelocities are an average over the top 30m of the mixed layer\n\ndepth :\n\n15m\n\nlong_name :\n\naverage total surface current speed\n\nunits :\n\nm s**-1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n2.55 GiB\n16.27 MiB\n\n\nShape\n(16071, 177, 241)\n(100, 177, 241)\n\n\nDask graph\n161 chunks in 2 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nmlotst\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(500, 177, 241), meta=np.ndarray&gt;\n\n\n\n\n_ChunkSizes :\n\n[1, 681, 1440]\n\ncell_methods :\n\narea: mean\n\nlong_name :\n\nDensity ocean mixed layer thickness\n\nstandard_name :\n\nocean_mixed_layer_thickness_defined_by_sigma_theta\n\nunit_long :\n\nMeters\n\nunits :\n\nm\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n2.55 GiB\n81.36 MiB\n\n\nShape\n(16071, 177, 241)\n(500, 177, 241)\n\n\nDask graph\n33 chunks in 2 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nsla\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(100, 177, 241), meta=np.ndarray&gt;\n\n\n\n\nancillary_variables :\n\nerr_sla\n\ncomment :\n\nThe sea level anomaly is the sea surface height above mean sea surface; it is referenced to the [1993, 2012] period; see the product user manual for details\n\ngrid_mapping :\n\ncrs\n\nlong_name :\n\nSea level anomaly\n\nstandard_name :\n\nsea_surface_height_above_sea_level\n\nunits :\n\nm\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n2.55 GiB\n16.27 MiB\n\n\nShape\n(16071, 177, 241)\n(100, 177, 241)\n\n\nDask graph\n161 chunks in 2 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nso\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(500, 177, 241), meta=np.ndarray&gt;\n\n\n\n\n_ChunkSizes :\n\n[1, 7, 341, 720]\n\ncell_methods :\n\narea: mean\n\nlong_name :\n\nmean sea water salinity at 0.49 metres below ocean surface\n\nstandard_name :\n\nsea_water_salinity\n\nunit_long :\n\nPractical Salinity Unit\n\nunits :\n\n1e-3\n\nvalid_max :\n\n28336\n\nvalid_min :\n\n1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n2.55 GiB\n81.36 MiB\n\n\nShape\n(16071, 177, 241)\n(500, 177, 241)\n\n\nDask graph\n33 chunks in 2 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nsst\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(100, 177, 241), meta=np.ndarray&gt;\n\n\n\n\nlong_name :\n\nSea surface temperature\n\nnameCDM :\n\nSea_surface_temperature_surface\n\nnameECMWF :\n\nSea surface temperature\n\nproduct_type :\n\nanalysis\n\nshortNameECMWF :\n\nsst\n\nstandard_name :\n\nsea_surface_temperature\n\nunits :\n\nK\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n2.55 GiB\n16.27 MiB\n\n\nShape\n(16071, 177, 241)\n(100, 177, 241)\n\n\nDask graph\n161 chunks in 2 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\ntopo\n\n\n(lat, lon)\n\n\nfloat64\n\n\ndask.array&lt;chunksize=(177, 241), meta=np.ndarray&gt;\n\n\n\n\ncolorBarMaximum :\n\n8000.0\n\ncolorBarMinimum :\n\n-8000.0\n\ncolorBarPalette :\n\nTopography\n\ngrid_mapping :\n\nGDAL_Geographics\n\nioos_category :\n\nLocation\n\nlong_name :\n\nTopography\n\nstandard_name :\n\naltitude\n\nunits :\n\nmeters\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n333.26 kiB\n333.26 kiB\n\n\nShape\n(177, 241)\n(177, 241)\n\n\nDask graph\n1 chunks in 2 graph layers\n\n\nData type\nfloat64 numpy.ndarray\n\n\n\n\n\n\n\n\n\nu_curr\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(100, 177, 241), meta=np.ndarray&gt;\n\n\n\n\ncomment :\n\nVelocities are an average over the top 30m of the mixed layer\n\ncoverage_content_type :\n\nmodelResult\n\ndepth :\n\n15m\n\nlong_name :\n\nzonal total surface current\n\nsource :\n\nSSH source: CMEMS SSALTO/DUACS SEALEVEL_GLO_PHY_L4_MY_008_047 DOI: 10.48670/moi-00148 ; WIND source: ECMWF ERA5 10m wind DOI: 10.24381/cds.adbb2d47 ; SST source: CMC 0.2 deg SST V2.0 DOI: 10.5067/GHCMC-4FM02\n\nstandard_name :\n\neastward_sea_water_velocity\n\nunits :\n\nm s-1\n\nvalid_max :\n\n3.0\n\nvalid_min :\n\n-3.0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n2.55 GiB\n16.27 MiB\n\n\nShape\n(16071, 177, 241)\n(100, 177, 241)\n\n\nDask graph\n161 chunks in 2 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nu_wind\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(100, 177, 241), meta=np.ndarray&gt;\n\n\n\n\nlong_name :\n\n10 metre U wind component\n\nnameCDM :\n\n10_metre_U_wind_component_surface\n\nnameECMWF :\n\n10 metre U wind component\n\nproduct_type :\n\nanalysis\n\nshortNameECMWF :\n\n10u\n\nstandard_name :\n\neastward_wind\n\nunits :\n\nm s**-1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n2.55 GiB\n16.27 MiB\n\n\nShape\n(16071, 177, 241)\n(100, 177, 241)\n\n\nDask graph\n161 chunks in 2 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nug_curr\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(100, 177, 241), meta=np.ndarray&gt;\n\n\n\n\ncomment :\n\nGeostrophic velocities calculated from absolute dynamic topography\n\ndepth :\n\n15m\n\nlong_name :\n\nzonal geostrophic surface current\n\nsource :\n\nSSH source: CMEMS SSALTO/DUACS SEALEVEL_GLO_PHY_L4_MY_008_047 DOI: 10.48670/moi-00148\n\nstandard_name :\n\ngeostrophic_eastward_sea_water_velocity\n\nunits :\n\nm s-1\n\nvalid_max :\n\n3.0\n\nvalid_min :\n\n-3.0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n2.55 GiB\n16.27 MiB\n\n\nShape\n(16071, 177, 241)\n(100, 177, 241)\n\n\nDask graph\n161 chunks in 2 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nv_curr\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(100, 177, 241), meta=np.ndarray&gt;\n\n\n\n\ncomment :\n\nVelocities are an average over the top 30m of the mixed layer\n\ncoverage_content_type :\n\nmodelResult\n\ndepth :\n\n15m\n\nlong_name :\n\nmeridional total surface current\n\nsource :\n\nSSH source: CMEMS SSALTO/DUACS SEALEVEL_GLO_PHY_L4_MY_008_047 DOI: 10.48670/moi-00148 ; WIND source: ECMWF ERA5 10m wind DOI: 10.24381/cds.adbb2d47 ; SST source: CMC 0.2 deg SST V2.0 DOI: 10.5067/GHCMC-4FM02\n\nstandard_name :\n\nnorthward_sea_water_velocity\n\nunits :\n\nm s-1\n\nvalid_max :\n\n3.0\n\nvalid_min :\n\n-3.0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n2.55 GiB\n16.27 MiB\n\n\nShape\n(16071, 177, 241)\n(100, 177, 241)\n\n\nDask graph\n161 chunks in 2 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nv_wind\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(100, 177, 241), meta=np.ndarray&gt;\n\n\n\n\nlong_name :\n\n10 metre V wind component\n\nnameCDM :\n\n10_metre_V_wind_component_surface\n\nnameECMWF :\n\n10 metre V wind component\n\nproduct_type :\n\nanalysis\n\nshortNameECMWF :\n\n10v\n\nstandard_name :\n\nnorthward_wind\n\nunits :\n\nm s**-1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n2.55 GiB\n16.27 MiB\n\n\nShape\n(16071, 177, 241)\n(100, 177, 241)\n\n\nDask graph\n161 chunks in 2 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nvg_curr\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(100, 177, 241), meta=np.ndarray&gt;\n\n\n\n\ncomment :\n\nGeostrophic velocities calculated from absolute dynamic topography\n\ndepth :\n\n15m\n\nlong_name :\n\nmeridional geostrophic surface current\n\nsource :\n\nSSH source: CMEMS SSALTO/DUACS SEALEVEL_GLO_PHY_L4_MY_008_047 DOI: 10.48670/moi-00148\n\nstandard_name :\n\ngeostrophic_northward_sea_water_velocity\n\nunits :\n\nm s-1\n\nvalid_max :\n\n3.0\n\nvalid_min :\n\n-3.0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n2.55 GiB\n16.27 MiB\n\n\nShape\n(16071, 177, 241)\n(100, 177, 241)\n\n\nDask graph\n161 chunks in 2 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nwind_dir\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(100, 177, 241), meta=np.ndarray&gt;\n\n\n\n\nlong_name :\n\n10 metre wind direction\n\nunits :\n\ndegrees\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n2.55 GiB\n16.27 MiB\n\n\nShape\n(16071, 177, 241)\n(100, 177, 241)\n\n\nDask graph\n161 chunks in 2 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nwind_speed\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(100, 177, 241), meta=np.ndarray&gt;\n\n\n\n\nlong_name :\n\n10 metre absolute speed\n\nunits :\n\nm s**-1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n2.55 GiB\n16.27 MiB\n\n\nShape\n(16071, 177, 241)\n(100, 177, 241)\n\n\nDask graph\n161 chunks in 2 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\nIndexes: (3)latPandasIndexPandasIndex(Index([  32.0,  31.75,   31.5,  31.25,   31.0,  30.75,   30.5,  30.25,   30.0,\n        29.75,\n       ...\n        -9.75,  -10.0, -10.25,  -10.5, -10.75,  -11.0, -11.25,  -11.5, -11.75,\n        -12.0],\n      dtype='float32', name='lat', length=177))lonPandasIndexPandasIndex(Index([  42.0,  42.25,   42.5,  42.75,   43.0,  43.25,   43.5,  43.75,   44.0,\n        44.25,\n       ...\n        99.75,  100.0, 100.25,  100.5, 100.75,  101.0, 101.25,  101.5, 101.75,\n        102.0],\n      dtype='float32', name='lon', length=241))timePandasIndexPandasIndex(DatetimeIndex(['1979-01-01', '1979-01-02', '1979-01-03', '1979-01-04',\n               '1979-01-05', '1979-01-06', '1979-01-07', '1979-01-08',\n               '1979-01-09', '1979-01-10',\n               ...\n               '2022-12-22', '2022-12-23', '2022-12-24', '2022-12-25',\n               '2022-12-26', '2022-12-27', '2022-12-28', '2022-12-29',\n               '2022-12-30', '2022-12-31'],\n              dtype='datetime64[ns]', name='time', length=16071, freq=None))Attributes: (17)creator_email :minhphan@uw.educreator_name :Minh Phancreator_type :persondate_created :2023-07-19geospatial_lat_max :32.0geospatial_lat_min :-12.0geospatial_lat_resolution :0.25geospatial_lat_units :degrees_northgeospatial_lon_max :102.0geospatial_lon_min :42.0geospatial_lon_resolution :0.25geospatial_lon_units :degrees_eastsource :Earth & Space Research (ESR), Copernicus Climate Change Service (C3S), Copernicus Marine Environment Monitoring Service (CMEMS), United States Geological Survey (USGS)summary :Daily mean of 0.25 x 0.25 degrees gridded data from multiple climate variables that may influence the patterns of coastal upwelling in the focused areatime_coverage_end :2022-12-31T23:59:59time_coverage_start :1979-01-01T00:00:00title :Climate Data for Coastal Upwelling Machine Learning Project in Indian Ocean\n\n\nWe can slice data by the dimensions (latitude, longitude, time) and data variables.\n\n# slice by latitude\n# notice how we specify the range in reverse\nds.sel(lat=slice(0, -12))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt;\nDimensions:          (time: 16071, lat: 49, lon: 241)\nCoordinates:\n  * lat              (lat) float32 0.0 -0.25 -0.5 -0.75 ... -11.5 -11.75 -12.0\n  * lon              (lon) float32 42.0 42.25 42.5 42.75 ... 101.5 101.8 102.0\n  * time             (time) datetime64[ns] 1979-01-01 1979-01-02 ... 2022-12-31\nData variables: (12/19)\n    CHL              (time, lat, lon) float32 dask.array&lt;chunksize=(100, 49, 241), meta=np.ndarray&gt;\n    CHL_uncertainty  (time, lat, lon) float32 dask.array&lt;chunksize=(100, 49, 241), meta=np.ndarray&gt;\n    adt              (time, lat, lon) float32 dask.array&lt;chunksize=(100, 49, 241), meta=np.ndarray&gt;\n    air_temp         (time, lat, lon) float32 dask.array&lt;chunksize=(100, 49, 241), meta=np.ndarray&gt;\n    curr_dir         (time, lat, lon) float32 dask.array&lt;chunksize=(100, 49, 241), meta=np.ndarray&gt;\n    curr_speed       (time, lat, lon) float32 dask.array&lt;chunksize=(100, 49, 241), meta=np.ndarray&gt;\n    ...               ...\n    ug_curr          (time, lat, lon) float32 dask.array&lt;chunksize=(100, 49, 241), meta=np.ndarray&gt;\n    v_curr           (time, lat, lon) float32 dask.array&lt;chunksize=(100, 49, 241), meta=np.ndarray&gt;\n    v_wind           (time, lat, lon) float32 dask.array&lt;chunksize=(100, 49, 241), meta=np.ndarray&gt;\n    vg_curr          (time, lat, lon) float32 dask.array&lt;chunksize=(100, 49, 241), meta=np.ndarray&gt;\n    wind_dir         (time, lat, lon) float32 dask.array&lt;chunksize=(100, 49, 241), meta=np.ndarray&gt;\n    wind_speed       (time, lat, lon) float32 dask.array&lt;chunksize=(100, 49, 241), meta=np.ndarray&gt;\nAttributes: (12/17)\n    creator_email:              minhphan@uw.edu\n    creator_name:               Minh Phan\n    creator_type:               person\n    date_created:               2023-07-19\n    geospatial_lat_max:         32.0\n    geospatial_lat_min:         -12.0\n    ...                         ...\n    geospatial_lon_units:       degrees_east\n    source:                     Earth & Space Research (ESR), Copernicus Clim...\n    summary:                    Daily mean of 0.25 x 0.25 degrees gridded dat...\n    time_coverage_end:          2022-12-31T23:59:59\n    time_coverage_start:        1979-01-01T00:00:00\n    title:                      Climate Data for Coastal Upwelling Machine Le...xarray.DatasetDimensions:time: 16071lat: 49lon: 241Coordinates: (3)lat(lat)float320.0 -0.25 -0.5 ... -11.75 -12.0long_name :latitudestandard_name :latitudeunits :degrees_northarray([  0.  ,  -0.25,  -0.5 ,  -0.75,  -1.  ,  -1.25,  -1.5 ,  -1.75,  -2.  ,\n        -2.25,  -2.5 ,  -2.75,  -3.  ,  -3.25,  -3.5 ,  -3.75,  -4.  ,  -4.25,\n        -4.5 ,  -4.75,  -5.  ,  -5.25,  -5.5 ,  -5.75,  -6.  ,  -6.25,  -6.5 ,\n        -6.75,  -7.  ,  -7.25,  -7.5 ,  -7.75,  -8.  ,  -8.25,  -8.5 ,  -8.75,\n        -9.  ,  -9.25,  -9.5 ,  -9.75, -10.  , -10.25, -10.5 , -10.75, -11.  ,\n       -11.25, -11.5 , -11.75, -12.  ], dtype=float32)lon(lon)float3242.0 42.25 42.5 ... 101.8 102.0long_name :longitudestandard_name :longitudeunits :degrees_eastarray([ 42.  ,  42.25,  42.5 , ..., 101.5 , 101.75, 102.  ], dtype=float32)time(time)datetime64[ns]1979-01-01 ... 2022-12-31axis :Tcomment :Data is averaged over the daylong_name :time centered on the daystandard_name :timetime_bounds :2000-01-01 00:00:00 to 2000-01-01 23:59:59array(['1979-01-01T00:00:00.000000000', '1979-01-02T00:00:00.000000000',\n       '1979-01-03T00:00:00.000000000', ..., '2022-12-29T00:00:00.000000000',\n       '2022-12-30T00:00:00.000000000', '2022-12-31T00:00:00.000000000'],\n      dtype='datetime64[ns]')Data variables: (19)CHL(time, lat, lon)float32dask.array&lt;chunksize=(100, 49, 241), meta=np.ndarray&gt;_ChunkSizes :[1, 256, 256]ancillary_variables :flags CHL_uncertaintycoverage_content_type :modelResultinput_files_reprocessings :Processors versions: MODIS R2022.0NRT/VIIRSN R2022.0NRT/OLCIA 07.02/VIIRSJ1 R2022.0NRT/OLCIB 07.02long_name :Chlorophyll-a concentration - Mean of the binned pixelsstandard_name :mass_concentration_of_chlorophyll_a_in_sea_watertype :surfaceunits :milligram m-3valid_max :1000.0valid_min :0.0\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n723.96 MiB\n4.50 MiB\n\n\nShape\n(16071, 49, 241)\n(100, 49, 241)\n\n\nDask graph\n161 chunks in 3 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\nCHL_uncertainty\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(100, 49, 241), meta=np.ndarray&gt;\n\n\n\n\n_ChunkSizes :\n\n[1, 256, 256]\n\ncoverage_content_type :\n\nqualityInformation\n\nlong_name :\n\nChlorophyll-a concentration - Uncertainty estimation\n\nunits :\n\n%\n\nvalid_max :\n\n32767\n\nvalid_min :\n\n0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n723.96 MiB\n4.50 MiB\n\n\nShape\n(16071, 49, 241)\n(100, 49, 241)\n\n\nDask graph\n161 chunks in 3 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nadt\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(100, 49, 241), meta=np.ndarray&gt;\n\n\n\n\ncomment :\n\nThe absolute dynamic topography is the sea surface height above geoid; the adt is obtained as follows: adt=sla+mdt where mdt is the mean dynamic topography; see the product user manual for details\n\ngrid_mapping :\n\ncrs\n\nlong_name :\n\nAbsolute dynamic topography\n\nstandard_name :\n\nsea_surface_height_above_geoid\n\nunits :\n\nm\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n723.96 MiB\n4.50 MiB\n\n\nShape\n(16071, 49, 241)\n(100, 49, 241)\n\n\nDask graph\n161 chunks in 3 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nair_temp\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(100, 49, 241), meta=np.ndarray&gt;\n\n\n\n\nlong_name :\n\n2 metre temperature\n\nnameCDM :\n\n2_metre_temperature_surface\n\nnameECMWF :\n\n2 metre temperature\n\nproduct_type :\n\nanalysis\n\nshortNameECMWF :\n\n2t\n\nstandard_name :\n\nair_temperature\n\nunits :\n\nK\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n723.96 MiB\n4.50 MiB\n\n\nShape\n(16071, 49, 241)\n(100, 49, 241)\n\n\nDask graph\n161 chunks in 3 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\ncurr_dir\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(100, 49, 241), meta=np.ndarray&gt;\n\n\n\n\ncomments :\n\nComputed from total surface current velocity elements. Velocities are an average over the top 30m of the mixed layer\n\ndepth :\n\n15m\n\nlong_name :\n\naverage direction of total surface currents\n\nunits :\n\ndegrees\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n723.96 MiB\n4.50 MiB\n\n\nShape\n(16071, 49, 241)\n(100, 49, 241)\n\n\nDask graph\n161 chunks in 3 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\ncurr_speed\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(100, 49, 241), meta=np.ndarray&gt;\n\n\n\n\ncomments :\n\nVelocities are an average over the top 30m of the mixed layer\n\ndepth :\n\n15m\n\nlong_name :\n\naverage total surface current speed\n\nunits :\n\nm s**-1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n723.96 MiB\n4.50 MiB\n\n\nShape\n(16071, 49, 241)\n(100, 49, 241)\n\n\nDask graph\n161 chunks in 3 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nmlotst\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(500, 49, 241), meta=np.ndarray&gt;\n\n\n\n\n_ChunkSizes :\n\n[1, 681, 1440]\n\ncell_methods :\n\narea: mean\n\nlong_name :\n\nDensity ocean mixed layer thickness\n\nstandard_name :\n\nocean_mixed_layer_thickness_defined_by_sigma_theta\n\nunit_long :\n\nMeters\n\nunits :\n\nm\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n723.96 MiB\n22.52 MiB\n\n\nShape\n(16071, 49, 241)\n(500, 49, 241)\n\n\nDask graph\n33 chunks in 3 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nsla\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(100, 49, 241), meta=np.ndarray&gt;\n\n\n\n\nancillary_variables :\n\nerr_sla\n\ncomment :\n\nThe sea level anomaly is the sea surface height above mean sea surface; it is referenced to the [1993, 2012] period; see the product user manual for details\n\ngrid_mapping :\n\ncrs\n\nlong_name :\n\nSea level anomaly\n\nstandard_name :\n\nsea_surface_height_above_sea_level\n\nunits :\n\nm\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n723.96 MiB\n4.50 MiB\n\n\nShape\n(16071, 49, 241)\n(100, 49, 241)\n\n\nDask graph\n161 chunks in 3 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nso\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(500, 49, 241), meta=np.ndarray&gt;\n\n\n\n\n_ChunkSizes :\n\n[1, 7, 341, 720]\n\ncell_methods :\n\narea: mean\n\nlong_name :\n\nmean sea water salinity at 0.49 metres below ocean surface\n\nstandard_name :\n\nsea_water_salinity\n\nunit_long :\n\nPractical Salinity Unit\n\nunits :\n\n1e-3\n\nvalid_max :\n\n28336\n\nvalid_min :\n\n1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n723.96 MiB\n22.52 MiB\n\n\nShape\n(16071, 49, 241)\n(500, 49, 241)\n\n\nDask graph\n33 chunks in 3 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nsst\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(100, 49, 241), meta=np.ndarray&gt;\n\n\n\n\nlong_name :\n\nSea surface temperature\n\nnameCDM :\n\nSea_surface_temperature_surface\n\nnameECMWF :\n\nSea surface temperature\n\nproduct_type :\n\nanalysis\n\nshortNameECMWF :\n\nsst\n\nstandard_name :\n\nsea_surface_temperature\n\nunits :\n\nK\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n723.96 MiB\n4.50 MiB\n\n\nShape\n(16071, 49, 241)\n(100, 49, 241)\n\n\nDask graph\n161 chunks in 3 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\ntopo\n\n\n(lat, lon)\n\n\nfloat64\n\n\ndask.array&lt;chunksize=(49, 241), meta=np.ndarray&gt;\n\n\n\n\ncolorBarMaximum :\n\n8000.0\n\ncolorBarMinimum :\n\n-8000.0\n\ncolorBarPalette :\n\nTopography\n\ngrid_mapping :\n\nGDAL_Geographics\n\nioos_category :\n\nLocation\n\nlong_name :\n\nTopography\n\nstandard_name :\n\naltitude\n\nunits :\n\nmeters\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n92.26 kiB\n92.26 kiB\n\n\nShape\n(49, 241)\n(49, 241)\n\n\nDask graph\n1 chunks in 3 graph layers\n\n\nData type\nfloat64 numpy.ndarray\n\n\n\n\n\n\n\n\n\nu_curr\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(100, 49, 241), meta=np.ndarray&gt;\n\n\n\n\ncomment :\n\nVelocities are an average over the top 30m of the mixed layer\n\ncoverage_content_type :\n\nmodelResult\n\ndepth :\n\n15m\n\nlong_name :\n\nzonal total surface current\n\nsource :\n\nSSH source: CMEMS SSALTO/DUACS SEALEVEL_GLO_PHY_L4_MY_008_047 DOI: 10.48670/moi-00148 ; WIND source: ECMWF ERA5 10m wind DOI: 10.24381/cds.adbb2d47 ; SST source: CMC 0.2 deg SST V2.0 DOI: 10.5067/GHCMC-4FM02\n\nstandard_name :\n\neastward_sea_water_velocity\n\nunits :\n\nm s-1\n\nvalid_max :\n\n3.0\n\nvalid_min :\n\n-3.0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n723.96 MiB\n4.50 MiB\n\n\nShape\n(16071, 49, 241)\n(100, 49, 241)\n\n\nDask graph\n161 chunks in 3 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nu_wind\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(100, 49, 241), meta=np.ndarray&gt;\n\n\n\n\nlong_name :\n\n10 metre U wind component\n\nnameCDM :\n\n10_metre_U_wind_component_surface\n\nnameECMWF :\n\n10 metre U wind component\n\nproduct_type :\n\nanalysis\n\nshortNameECMWF :\n\n10u\n\nstandard_name :\n\neastward_wind\n\nunits :\n\nm s**-1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n723.96 MiB\n4.50 MiB\n\n\nShape\n(16071, 49, 241)\n(100, 49, 241)\n\n\nDask graph\n161 chunks in 3 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nug_curr\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(100, 49, 241), meta=np.ndarray&gt;\n\n\n\n\ncomment :\n\nGeostrophic velocities calculated from absolute dynamic topography\n\ndepth :\n\n15m\n\nlong_name :\n\nzonal geostrophic surface current\n\nsource :\n\nSSH source: CMEMS SSALTO/DUACS SEALEVEL_GLO_PHY_L4_MY_008_047 DOI: 10.48670/moi-00148\n\nstandard_name :\n\ngeostrophic_eastward_sea_water_velocity\n\nunits :\n\nm s-1\n\nvalid_max :\n\n3.0\n\nvalid_min :\n\n-3.0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n723.96 MiB\n4.50 MiB\n\n\nShape\n(16071, 49, 241)\n(100, 49, 241)\n\n\nDask graph\n161 chunks in 3 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nv_curr\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(100, 49, 241), meta=np.ndarray&gt;\n\n\n\n\ncomment :\n\nVelocities are an average over the top 30m of the mixed layer\n\ncoverage_content_type :\n\nmodelResult\n\ndepth :\n\n15m\n\nlong_name :\n\nmeridional total surface current\n\nsource :\n\nSSH source: CMEMS SSALTO/DUACS SEALEVEL_GLO_PHY_L4_MY_008_047 DOI: 10.48670/moi-00148 ; WIND source: ECMWF ERA5 10m wind DOI: 10.24381/cds.adbb2d47 ; SST source: CMC 0.2 deg SST V2.0 DOI: 10.5067/GHCMC-4FM02\n\nstandard_name :\n\nnorthward_sea_water_velocity\n\nunits :\n\nm s-1\n\nvalid_max :\n\n3.0\n\nvalid_min :\n\n-3.0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n723.96 MiB\n4.50 MiB\n\n\nShape\n(16071, 49, 241)\n(100, 49, 241)\n\n\nDask graph\n161 chunks in 3 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nv_wind\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(100, 49, 241), meta=np.ndarray&gt;\n\n\n\n\nlong_name :\n\n10 metre V wind component\n\nnameCDM :\n\n10_metre_V_wind_component_surface\n\nnameECMWF :\n\n10 metre V wind component\n\nproduct_type :\n\nanalysis\n\nshortNameECMWF :\n\n10v\n\nstandard_name :\n\nnorthward_wind\n\nunits :\n\nm s**-1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n723.96 MiB\n4.50 MiB\n\n\nShape\n(16071, 49, 241)\n(100, 49, 241)\n\n\nDask graph\n161 chunks in 3 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nvg_curr\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(100, 49, 241), meta=np.ndarray&gt;\n\n\n\n\ncomment :\n\nGeostrophic velocities calculated from absolute dynamic topography\n\ndepth :\n\n15m\n\nlong_name :\n\nmeridional geostrophic surface current\n\nsource :\n\nSSH source: CMEMS SSALTO/DUACS SEALEVEL_GLO_PHY_L4_MY_008_047 DOI: 10.48670/moi-00148\n\nstandard_name :\n\ngeostrophic_northward_sea_water_velocity\n\nunits :\n\nm s-1\n\nvalid_max :\n\n3.0\n\nvalid_min :\n\n-3.0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n723.96 MiB\n4.50 MiB\n\n\nShape\n(16071, 49, 241)\n(100, 49, 241)\n\n\nDask graph\n161 chunks in 3 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nwind_dir\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(100, 49, 241), meta=np.ndarray&gt;\n\n\n\n\nlong_name :\n\n10 metre wind direction\n\nunits :\n\ndegrees\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n723.96 MiB\n4.50 MiB\n\n\nShape\n(16071, 49, 241)\n(100, 49, 241)\n\n\nDask graph\n161 chunks in 3 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nwind_speed\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(100, 49, 241), meta=np.ndarray&gt;\n\n\n\n\nlong_name :\n\n10 metre absolute speed\n\nunits :\n\nm s**-1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n723.96 MiB\n4.50 MiB\n\n\nShape\n(16071, 49, 241)\n(100, 49, 241)\n\n\nDask graph\n161 chunks in 3 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\nIndexes: (3)latPandasIndexPandasIndex(Index([   0.0,  -0.25,   -0.5,  -0.75,   -1.0,  -1.25,   -1.5,  -1.75,   -2.0,\n        -2.25,   -2.5,  -2.75,   -3.0,  -3.25,   -3.5,  -3.75,   -4.0,  -4.25,\n         -4.5,  -4.75,   -5.0,  -5.25,   -5.5,  -5.75,   -6.0,  -6.25,   -6.5,\n        -6.75,   -7.0,  -7.25,   -7.5,  -7.75,   -8.0,  -8.25,   -8.5,  -8.75,\n         -9.0,  -9.25,   -9.5,  -9.75,  -10.0, -10.25,  -10.5, -10.75,  -11.0,\n       -11.25,  -11.5, -11.75,  -12.0],\n      dtype='float32', name='lat'))lonPandasIndexPandasIndex(Index([  42.0,  42.25,   42.5,  42.75,   43.0,  43.25,   43.5,  43.75,   44.0,\n        44.25,\n       ...\n        99.75,  100.0, 100.25,  100.5, 100.75,  101.0, 101.25,  101.5, 101.75,\n        102.0],\n      dtype='float32', name='lon', length=241))timePandasIndexPandasIndex(DatetimeIndex(['1979-01-01', '1979-01-02', '1979-01-03', '1979-01-04',\n               '1979-01-05', '1979-01-06', '1979-01-07', '1979-01-08',\n               '1979-01-09', '1979-01-10',\n               ...\n               '2022-12-22', '2022-12-23', '2022-12-24', '2022-12-25',\n               '2022-12-26', '2022-12-27', '2022-12-28', '2022-12-29',\n               '2022-12-30', '2022-12-31'],\n              dtype='datetime64[ns]', name='time', length=16071, freq=None))Attributes: (17)creator_email :minhphan@uw.educreator_name :Minh Phancreator_type :persondate_created :2023-07-19geospatial_lat_max :32.0geospatial_lat_min :-12.0geospatial_lat_resolution :0.25geospatial_lat_units :degrees_northgeospatial_lon_max :102.0geospatial_lon_min :42.0geospatial_lon_resolution :0.25geospatial_lon_units :degrees_eastsource :Earth & Space Research (ESR), Copernicus Climate Change Service (C3S), Copernicus Marine Environment Monitoring Service (CMEMS), United States Geological Survey (USGS)summary :Daily mean of 0.25 x 0.25 degrees gridded data from multiple climate variables that may influence the patterns of coastal upwelling in the focused areatime_coverage_end :2022-12-31T23:59:59time_coverage_start :1979-01-01T00:00:00title :Climate Data for Coastal Upwelling Machine Learning Project in Indian Ocean\n\n\n\n# slice by longitude\nds.sel(lon=slice(42, 45))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt;\nDimensions:          (time: 16071, lat: 177, lon: 13)\nCoordinates:\n  * lat              (lat) float32 32.0 31.75 31.5 31.25 ... -11.5 -11.75 -12.0\n  * lon              (lon) float32 42.0 42.25 42.5 42.75 ... 44.5 44.75 45.0\n  * time             (time) datetime64[ns] 1979-01-01 1979-01-02 ... 2022-12-31\nData variables: (12/19)\n    CHL              (time, lat, lon) float32 dask.array&lt;chunksize=(100, 177, 13), meta=np.ndarray&gt;\n    CHL_uncertainty  (time, lat, lon) float32 dask.array&lt;chunksize=(100, 177, 13), meta=np.ndarray&gt;\n    adt              (time, lat, lon) float32 dask.array&lt;chunksize=(100, 177, 13), meta=np.ndarray&gt;\n    air_temp         (time, lat, lon) float32 dask.array&lt;chunksize=(100, 177, 13), meta=np.ndarray&gt;\n    curr_dir         (time, lat, lon) float32 dask.array&lt;chunksize=(100, 177, 13), meta=np.ndarray&gt;\n    curr_speed       (time, lat, lon) float32 dask.array&lt;chunksize=(100, 177, 13), meta=np.ndarray&gt;\n    ...               ...\n    ug_curr          (time, lat, lon) float32 dask.array&lt;chunksize=(100, 177, 13), meta=np.ndarray&gt;\n    v_curr           (time, lat, lon) float32 dask.array&lt;chunksize=(100, 177, 13), meta=np.ndarray&gt;\n    v_wind           (time, lat, lon) float32 dask.array&lt;chunksize=(100, 177, 13), meta=np.ndarray&gt;\n    vg_curr          (time, lat, lon) float32 dask.array&lt;chunksize=(100, 177, 13), meta=np.ndarray&gt;\n    wind_dir         (time, lat, lon) float32 dask.array&lt;chunksize=(100, 177, 13), meta=np.ndarray&gt;\n    wind_speed       (time, lat, lon) float32 dask.array&lt;chunksize=(100, 177, 13), meta=np.ndarray&gt;\nAttributes: (12/17)\n    creator_email:              minhphan@uw.edu\n    creator_name:               Minh Phan\n    creator_type:               person\n    date_created:               2023-07-19\n    geospatial_lat_max:         32.0\n    geospatial_lat_min:         -12.0\n    ...                         ...\n    geospatial_lon_units:       degrees_east\n    source:                     Earth & Space Research (ESR), Copernicus Clim...\n    summary:                    Daily mean of 0.25 x 0.25 degrees gridded dat...\n    time_coverage_end:          2022-12-31T23:59:59\n    time_coverage_start:        1979-01-01T00:00:00\n    title:                      Climate Data for Coastal Upwelling Machine Le...xarray.DatasetDimensions:time: 16071lat: 177lon: 13Coordinates: (3)lat(lat)float3232.0 31.75 31.5 ... -11.75 -12.0long_name :latitudestandard_name :latitudeunits :degrees_northarray([ 32.  ,  31.75,  31.5 ,  31.25,  31.  ,  30.75,  30.5 ,  30.25,  30.  ,\n        29.75,  29.5 ,  29.25,  29.  ,  28.75,  28.5 ,  28.25,  28.  ,  27.75,\n        27.5 ,  27.25,  27.  ,  26.75,  26.5 ,  26.25,  26.  ,  25.75,  25.5 ,\n        25.25,  25.  ,  24.75,  24.5 ,  24.25,  24.  ,  23.75,  23.5 ,  23.25,\n        23.  ,  22.75,  22.5 ,  22.25,  22.  ,  21.75,  21.5 ,  21.25,  21.  ,\n        20.75,  20.5 ,  20.25,  20.  ,  19.75,  19.5 ,  19.25,  19.  ,  18.75,\n        18.5 ,  18.25,  18.  ,  17.75,  17.5 ,  17.25,  17.  ,  16.75,  16.5 ,\n        16.25,  16.  ,  15.75,  15.5 ,  15.25,  15.  ,  14.75,  14.5 ,  14.25,\n        14.  ,  13.75,  13.5 ,  13.25,  13.  ,  12.75,  12.5 ,  12.25,  12.  ,\n        11.75,  11.5 ,  11.25,  11.  ,  10.75,  10.5 ,  10.25,  10.  ,   9.75,\n         9.5 ,   9.25,   9.  ,   8.75,   8.5 ,   8.25,   8.  ,   7.75,   7.5 ,\n         7.25,   7.  ,   6.75,   6.5 ,   6.25,   6.  ,   5.75,   5.5 ,   5.25,\n         5.  ,   4.75,   4.5 ,   4.25,   4.  ,   3.75,   3.5 ,   3.25,   3.  ,\n         2.75,   2.5 ,   2.25,   2.  ,   1.75,   1.5 ,   1.25,   1.  ,   0.75,\n         0.5 ,   0.25,   0.  ,  -0.25,  -0.5 ,  -0.75,  -1.  ,  -1.25,  -1.5 ,\n        -1.75,  -2.  ,  -2.25,  -2.5 ,  -2.75,  -3.  ,  -3.25,  -3.5 ,  -3.75,\n        -4.  ,  -4.25,  -4.5 ,  -4.75,  -5.  ,  -5.25,  -5.5 ,  -5.75,  -6.  ,\n        -6.25,  -6.5 ,  -6.75,  -7.  ,  -7.25,  -7.5 ,  -7.75,  -8.  ,  -8.25,\n        -8.5 ,  -8.75,  -9.  ,  -9.25,  -9.5 ,  -9.75, -10.  , -10.25, -10.5 ,\n       -10.75, -11.  , -11.25, -11.5 , -11.75, -12.  ], dtype=float32)lon(lon)float3242.0 42.25 42.5 ... 44.5 44.75 45.0long_name :longitudestandard_name :longitudeunits :degrees_eastarray([42.  , 42.25, 42.5 , 42.75, 43.  , 43.25, 43.5 , 43.75, 44.  , 44.25,\n       44.5 , 44.75, 45.  ], dtype=float32)time(time)datetime64[ns]1979-01-01 ... 2022-12-31axis :Tcomment :Data is averaged over the daylong_name :time centered on the daystandard_name :timetime_bounds :2000-01-01 00:00:00 to 2000-01-01 23:59:59array(['1979-01-01T00:00:00.000000000', '1979-01-02T00:00:00.000000000',\n       '1979-01-03T00:00:00.000000000', ..., '2022-12-29T00:00:00.000000000',\n       '2022-12-30T00:00:00.000000000', '2022-12-31T00:00:00.000000000'],\n      dtype='datetime64[ns]')Data variables: (19)CHL(time, lat, lon)float32dask.array&lt;chunksize=(100, 177, 13), meta=np.ndarray&gt;_ChunkSizes :[1, 256, 256]ancillary_variables :flags CHL_uncertaintycoverage_content_type :modelResultinput_files_reprocessings :Processors versions: MODIS R2022.0NRT/VIIRSN R2022.0NRT/OLCIA 07.02/VIIRSJ1 R2022.0NRT/OLCIB 07.02long_name :Chlorophyll-a concentration - Mean of the binned pixelsstandard_name :mass_concentration_of_chlorophyll_a_in_sea_watertype :surfaceunits :milligram m-3valid_max :1000.0valid_min :0.0\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n141.07 MiB\n898.83 kiB\n\n\nShape\n(16071, 177, 13)\n(100, 177, 13)\n\n\nDask graph\n161 chunks in 3 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\nCHL_uncertainty\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(100, 177, 13), meta=np.ndarray&gt;\n\n\n\n\n_ChunkSizes :\n\n[1, 256, 256]\n\ncoverage_content_type :\n\nqualityInformation\n\nlong_name :\n\nChlorophyll-a concentration - Uncertainty estimation\n\nunits :\n\n%\n\nvalid_max :\n\n32767\n\nvalid_min :\n\n0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n141.07 MiB\n898.83 kiB\n\n\nShape\n(16071, 177, 13)\n(100, 177, 13)\n\n\nDask graph\n161 chunks in 3 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nadt\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(100, 177, 13), meta=np.ndarray&gt;\n\n\n\n\ncomment :\n\nThe absolute dynamic topography is the sea surface height above geoid; the adt is obtained as follows: adt=sla+mdt where mdt is the mean dynamic topography; see the product user manual for details\n\ngrid_mapping :\n\ncrs\n\nlong_name :\n\nAbsolute dynamic topography\n\nstandard_name :\n\nsea_surface_height_above_geoid\n\nunits :\n\nm\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n141.07 MiB\n898.83 kiB\n\n\nShape\n(16071, 177, 13)\n(100, 177, 13)\n\n\nDask graph\n161 chunks in 3 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nair_temp\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(100, 177, 13), meta=np.ndarray&gt;\n\n\n\n\nlong_name :\n\n2 metre temperature\n\nnameCDM :\n\n2_metre_temperature_surface\n\nnameECMWF :\n\n2 metre temperature\n\nproduct_type :\n\nanalysis\n\nshortNameECMWF :\n\n2t\n\nstandard_name :\n\nair_temperature\n\nunits :\n\nK\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n141.07 MiB\n898.83 kiB\n\n\nShape\n(16071, 177, 13)\n(100, 177, 13)\n\n\nDask graph\n161 chunks in 3 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\ncurr_dir\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(100, 177, 13), meta=np.ndarray&gt;\n\n\n\n\ncomments :\n\nComputed from total surface current velocity elements. Velocities are an average over the top 30m of the mixed layer\n\ndepth :\n\n15m\n\nlong_name :\n\naverage direction of total surface currents\n\nunits :\n\ndegrees\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n141.07 MiB\n898.83 kiB\n\n\nShape\n(16071, 177, 13)\n(100, 177, 13)\n\n\nDask graph\n161 chunks in 3 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\ncurr_speed\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(100, 177, 13), meta=np.ndarray&gt;\n\n\n\n\ncomments :\n\nVelocities are an average over the top 30m of the mixed layer\n\ndepth :\n\n15m\n\nlong_name :\n\naverage total surface current speed\n\nunits :\n\nm s**-1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n141.07 MiB\n898.83 kiB\n\n\nShape\n(16071, 177, 13)\n(100, 177, 13)\n\n\nDask graph\n161 chunks in 3 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nmlotst\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(500, 177, 13), meta=np.ndarray&gt;\n\n\n\n\n_ChunkSizes :\n\n[1, 681, 1440]\n\ncell_methods :\n\narea: mean\n\nlong_name :\n\nDensity ocean mixed layer thickness\n\nstandard_name :\n\nocean_mixed_layer_thickness_defined_by_sigma_theta\n\nunit_long :\n\nMeters\n\nunits :\n\nm\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n141.07 MiB\n4.39 MiB\n\n\nShape\n(16071, 177, 13)\n(500, 177, 13)\n\n\nDask graph\n33 chunks in 3 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nsla\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(100, 177, 13), meta=np.ndarray&gt;\n\n\n\n\nancillary_variables :\n\nerr_sla\n\ncomment :\n\nThe sea level anomaly is the sea surface height above mean sea surface; it is referenced to the [1993, 2012] period; see the product user manual for details\n\ngrid_mapping :\n\ncrs\n\nlong_name :\n\nSea level anomaly\n\nstandard_name :\n\nsea_surface_height_above_sea_level\n\nunits :\n\nm\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n141.07 MiB\n898.83 kiB\n\n\nShape\n(16071, 177, 13)\n(100, 177, 13)\n\n\nDask graph\n161 chunks in 3 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nso\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(500, 177, 13), meta=np.ndarray&gt;\n\n\n\n\n_ChunkSizes :\n\n[1, 7, 341, 720]\n\ncell_methods :\n\narea: mean\n\nlong_name :\n\nmean sea water salinity at 0.49 metres below ocean surface\n\nstandard_name :\n\nsea_water_salinity\n\nunit_long :\n\nPractical Salinity Unit\n\nunits :\n\n1e-3\n\nvalid_max :\n\n28336\n\nvalid_min :\n\n1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n141.07 MiB\n4.39 MiB\n\n\nShape\n(16071, 177, 13)\n(500, 177, 13)\n\n\nDask graph\n33 chunks in 3 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nsst\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(100, 177, 13), meta=np.ndarray&gt;\n\n\n\n\nlong_name :\n\nSea surface temperature\n\nnameCDM :\n\nSea_surface_temperature_surface\n\nnameECMWF :\n\nSea surface temperature\n\nproduct_type :\n\nanalysis\n\nshortNameECMWF :\n\nsst\n\nstandard_name :\n\nsea_surface_temperature\n\nunits :\n\nK\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n141.07 MiB\n898.83 kiB\n\n\nShape\n(16071, 177, 13)\n(100, 177, 13)\n\n\nDask graph\n161 chunks in 3 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\ntopo\n\n\n(lat, lon)\n\n\nfloat64\n\n\ndask.array&lt;chunksize=(177, 13), meta=np.ndarray&gt;\n\n\n\n\ncolorBarMaximum :\n\n8000.0\n\ncolorBarMinimum :\n\n-8000.0\n\ncolorBarPalette :\n\nTopography\n\ngrid_mapping :\n\nGDAL_Geographics\n\nioos_category :\n\nLocation\n\nlong_name :\n\nTopography\n\nstandard_name :\n\naltitude\n\nunits :\n\nmeters\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n17.98 kiB\n17.98 kiB\n\n\nShape\n(177, 13)\n(177, 13)\n\n\nDask graph\n1 chunks in 3 graph layers\n\n\nData type\nfloat64 numpy.ndarray\n\n\n\n\n\n\n\n\n\nu_curr\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(100, 177, 13), meta=np.ndarray&gt;\n\n\n\n\ncomment :\n\nVelocities are an average over the top 30m of the mixed layer\n\ncoverage_content_type :\n\nmodelResult\n\ndepth :\n\n15m\n\nlong_name :\n\nzonal total surface current\n\nsource :\n\nSSH source: CMEMS SSALTO/DUACS SEALEVEL_GLO_PHY_L4_MY_008_047 DOI: 10.48670/moi-00148 ; WIND source: ECMWF ERA5 10m wind DOI: 10.24381/cds.adbb2d47 ; SST source: CMC 0.2 deg SST V2.0 DOI: 10.5067/GHCMC-4FM02\n\nstandard_name :\n\neastward_sea_water_velocity\n\nunits :\n\nm s-1\n\nvalid_max :\n\n3.0\n\nvalid_min :\n\n-3.0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n141.07 MiB\n898.83 kiB\n\n\nShape\n(16071, 177, 13)\n(100, 177, 13)\n\n\nDask graph\n161 chunks in 3 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nu_wind\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(100, 177, 13), meta=np.ndarray&gt;\n\n\n\n\nlong_name :\n\n10 metre U wind component\n\nnameCDM :\n\n10_metre_U_wind_component_surface\n\nnameECMWF :\n\n10 metre U wind component\n\nproduct_type :\n\nanalysis\n\nshortNameECMWF :\n\n10u\n\nstandard_name :\n\neastward_wind\n\nunits :\n\nm s**-1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n141.07 MiB\n898.83 kiB\n\n\nShape\n(16071, 177, 13)\n(100, 177, 13)\n\n\nDask graph\n161 chunks in 3 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nug_curr\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(100, 177, 13), meta=np.ndarray&gt;\n\n\n\n\ncomment :\n\nGeostrophic velocities calculated from absolute dynamic topography\n\ndepth :\n\n15m\n\nlong_name :\n\nzonal geostrophic surface current\n\nsource :\n\nSSH source: CMEMS SSALTO/DUACS SEALEVEL_GLO_PHY_L4_MY_008_047 DOI: 10.48670/moi-00148\n\nstandard_name :\n\ngeostrophic_eastward_sea_water_velocity\n\nunits :\n\nm s-1\n\nvalid_max :\n\n3.0\n\nvalid_min :\n\n-3.0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n141.07 MiB\n898.83 kiB\n\n\nShape\n(16071, 177, 13)\n(100, 177, 13)\n\n\nDask graph\n161 chunks in 3 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nv_curr\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(100, 177, 13), meta=np.ndarray&gt;\n\n\n\n\ncomment :\n\nVelocities are an average over the top 30m of the mixed layer\n\ncoverage_content_type :\n\nmodelResult\n\ndepth :\n\n15m\n\nlong_name :\n\nmeridional total surface current\n\nsource :\n\nSSH source: CMEMS SSALTO/DUACS SEALEVEL_GLO_PHY_L4_MY_008_047 DOI: 10.48670/moi-00148 ; WIND source: ECMWF ERA5 10m wind DOI: 10.24381/cds.adbb2d47 ; SST source: CMC 0.2 deg SST V2.0 DOI: 10.5067/GHCMC-4FM02\n\nstandard_name :\n\nnorthward_sea_water_velocity\n\nunits :\n\nm s-1\n\nvalid_max :\n\n3.0\n\nvalid_min :\n\n-3.0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n141.07 MiB\n898.83 kiB\n\n\nShape\n(16071, 177, 13)\n(100, 177, 13)\n\n\nDask graph\n161 chunks in 3 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nv_wind\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(100, 177, 13), meta=np.ndarray&gt;\n\n\n\n\nlong_name :\n\n10 metre V wind component\n\nnameCDM :\n\n10_metre_V_wind_component_surface\n\nnameECMWF :\n\n10 metre V wind component\n\nproduct_type :\n\nanalysis\n\nshortNameECMWF :\n\n10v\n\nstandard_name :\n\nnorthward_wind\n\nunits :\n\nm s**-1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n141.07 MiB\n898.83 kiB\n\n\nShape\n(16071, 177, 13)\n(100, 177, 13)\n\n\nDask graph\n161 chunks in 3 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nvg_curr\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(100, 177, 13), meta=np.ndarray&gt;\n\n\n\n\ncomment :\n\nGeostrophic velocities calculated from absolute dynamic topography\n\ndepth :\n\n15m\n\nlong_name :\n\nmeridional geostrophic surface current\n\nsource :\n\nSSH source: CMEMS SSALTO/DUACS SEALEVEL_GLO_PHY_L4_MY_008_047 DOI: 10.48670/moi-00148\n\nstandard_name :\n\ngeostrophic_northward_sea_water_velocity\n\nunits :\n\nm s-1\n\nvalid_max :\n\n3.0\n\nvalid_min :\n\n-3.0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n141.07 MiB\n898.83 kiB\n\n\nShape\n(16071, 177, 13)\n(100, 177, 13)\n\n\nDask graph\n161 chunks in 3 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nwind_dir\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(100, 177, 13), meta=np.ndarray&gt;\n\n\n\n\nlong_name :\n\n10 metre wind direction\n\nunits :\n\ndegrees\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n141.07 MiB\n898.83 kiB\n\n\nShape\n(16071, 177, 13)\n(100, 177, 13)\n\n\nDask graph\n161 chunks in 3 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nwind_speed\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(100, 177, 13), meta=np.ndarray&gt;\n\n\n\n\nlong_name :\n\n10 metre absolute speed\n\nunits :\n\nm s**-1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n141.07 MiB\n898.83 kiB\n\n\nShape\n(16071, 177, 13)\n(100, 177, 13)\n\n\nDask graph\n161 chunks in 3 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\nIndexes: (3)latPandasIndexPandasIndex(Index([  32.0,  31.75,   31.5,  31.25,   31.0,  30.75,   30.5,  30.25,   30.0,\n        29.75,\n       ...\n        -9.75,  -10.0, -10.25,  -10.5, -10.75,  -11.0, -11.25,  -11.5, -11.75,\n        -12.0],\n      dtype='float32', name='lat', length=177))lonPandasIndexPandasIndex(Index([ 42.0, 42.25,  42.5, 42.75,  43.0, 43.25,  43.5, 43.75,  44.0, 44.25,\n        44.5, 44.75,  45.0],\n      dtype='float32', name='lon'))timePandasIndexPandasIndex(DatetimeIndex(['1979-01-01', '1979-01-02', '1979-01-03', '1979-01-04',\n               '1979-01-05', '1979-01-06', '1979-01-07', '1979-01-08',\n               '1979-01-09', '1979-01-10',\n               ...\n               '2022-12-22', '2022-12-23', '2022-12-24', '2022-12-25',\n               '2022-12-26', '2022-12-27', '2022-12-28', '2022-12-29',\n               '2022-12-30', '2022-12-31'],\n              dtype='datetime64[ns]', name='time', length=16071, freq=None))Attributes: (17)creator_email :minhphan@uw.educreator_name :Minh Phancreator_type :persondate_created :2023-07-19geospatial_lat_max :32.0geospatial_lat_min :-12.0geospatial_lat_resolution :0.25geospatial_lat_units :degrees_northgeospatial_lon_max :102.0geospatial_lon_min :42.0geospatial_lon_resolution :0.25geospatial_lon_units :degrees_eastsource :Earth & Space Research (ESR), Copernicus Climate Change Service (C3S), Copernicus Marine Environment Monitoring Service (CMEMS), United States Geological Survey (USGS)summary :Daily mean of 0.25 x 0.25 degrees gridded data from multiple climate variables that may influence the patterns of coastal upwelling in the focused areatime_coverage_end :2022-12-31T23:59:59time_coverage_start :1979-01-01T00:00:00title :Climate Data for Coastal Upwelling Machine Learning Project in Indian Ocean\n\n\n\n# slice by time\nds.sel(time=slice('1998', '1999'))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt;\nDimensions:          (time: 730, lat: 177, lon: 241)\nCoordinates:\n  * lat              (lat) float32 32.0 31.75 31.5 31.25 ... -11.5 -11.75 -12.0\n  * lon              (lon) float32 42.0 42.25 42.5 42.75 ... 101.5 101.8 102.0\n  * time             (time) datetime64[ns] 1998-01-01 1998-01-02 ... 1999-12-31\nData variables: (12/19)\n    CHL              (time, lat, lon) float32 dask.array&lt;chunksize=(60, 177, 241), meta=np.ndarray&gt;\n    CHL_uncertainty  (time, lat, lon) float32 dask.array&lt;chunksize=(60, 177, 241), meta=np.ndarray&gt;\n    adt              (time, lat, lon) float32 dask.array&lt;chunksize=(60, 177, 241), meta=np.ndarray&gt;\n    air_temp         (time, lat, lon) float32 dask.array&lt;chunksize=(60, 177, 241), meta=np.ndarray&gt;\n    curr_dir         (time, lat, lon) float32 dask.array&lt;chunksize=(60, 177, 241), meta=np.ndarray&gt;\n    curr_speed       (time, lat, lon) float32 dask.array&lt;chunksize=(60, 177, 241), meta=np.ndarray&gt;\n    ...               ...\n    ug_curr          (time, lat, lon) float32 dask.array&lt;chunksize=(60, 177, 241), meta=np.ndarray&gt;\n    v_curr           (time, lat, lon) float32 dask.array&lt;chunksize=(60, 177, 241), meta=np.ndarray&gt;\n    v_wind           (time, lat, lon) float32 dask.array&lt;chunksize=(60, 177, 241), meta=np.ndarray&gt;\n    vg_curr          (time, lat, lon) float32 dask.array&lt;chunksize=(60, 177, 241), meta=np.ndarray&gt;\n    wind_dir         (time, lat, lon) float32 dask.array&lt;chunksize=(60, 177, 241), meta=np.ndarray&gt;\n    wind_speed       (time, lat, lon) float32 dask.array&lt;chunksize=(60, 177, 241), meta=np.ndarray&gt;\nAttributes: (12/17)\n    creator_email:              minhphan@uw.edu\n    creator_name:               Minh Phan\n    creator_type:               person\n    date_created:               2023-07-19\n    geospatial_lat_max:         32.0\n    geospatial_lat_min:         -12.0\n    ...                         ...\n    geospatial_lon_units:       degrees_east\n    source:                     Earth & Space Research (ESR), Copernicus Clim...\n    summary:                    Daily mean of 0.25 x 0.25 degrees gridded dat...\n    time_coverage_end:          2022-12-31T23:59:59\n    time_coverage_start:        1979-01-01T00:00:00\n    title:                      Climate Data for Coastal Upwelling Machine Le...xarray.DatasetDimensions:time: 730lat: 177lon: 241Coordinates: (3)lat(lat)float3232.0 31.75 31.5 ... -11.75 -12.0long_name :latitudestandard_name :latitudeunits :degrees_northarray([ 32.  ,  31.75,  31.5 ,  31.25,  31.  ,  30.75,  30.5 ,  30.25,  30.  ,\n        29.75,  29.5 ,  29.25,  29.  ,  28.75,  28.5 ,  28.25,  28.  ,  27.75,\n        27.5 ,  27.25,  27.  ,  26.75,  26.5 ,  26.25,  26.  ,  25.75,  25.5 ,\n        25.25,  25.  ,  24.75,  24.5 ,  24.25,  24.  ,  23.75,  23.5 ,  23.25,\n        23.  ,  22.75,  22.5 ,  22.25,  22.  ,  21.75,  21.5 ,  21.25,  21.  ,\n        20.75,  20.5 ,  20.25,  20.  ,  19.75,  19.5 ,  19.25,  19.  ,  18.75,\n        18.5 ,  18.25,  18.  ,  17.75,  17.5 ,  17.25,  17.  ,  16.75,  16.5 ,\n        16.25,  16.  ,  15.75,  15.5 ,  15.25,  15.  ,  14.75,  14.5 ,  14.25,\n        14.  ,  13.75,  13.5 ,  13.25,  13.  ,  12.75,  12.5 ,  12.25,  12.  ,\n        11.75,  11.5 ,  11.25,  11.  ,  10.75,  10.5 ,  10.25,  10.  ,   9.75,\n         9.5 ,   9.25,   9.  ,   8.75,   8.5 ,   8.25,   8.  ,   7.75,   7.5 ,\n         7.25,   7.  ,   6.75,   6.5 ,   6.25,   6.  ,   5.75,   5.5 ,   5.25,\n         5.  ,   4.75,   4.5 ,   4.25,   4.  ,   3.75,   3.5 ,   3.25,   3.  ,\n         2.75,   2.5 ,   2.25,   2.  ,   1.75,   1.5 ,   1.25,   1.  ,   0.75,\n         0.5 ,   0.25,   0.  ,  -0.25,  -0.5 ,  -0.75,  -1.  ,  -1.25,  -1.5 ,\n        -1.75,  -2.  ,  -2.25,  -2.5 ,  -2.75,  -3.  ,  -3.25,  -3.5 ,  -3.75,\n        -4.  ,  -4.25,  -4.5 ,  -4.75,  -5.  ,  -5.25,  -5.5 ,  -5.75,  -6.  ,\n        -6.25,  -6.5 ,  -6.75,  -7.  ,  -7.25,  -7.5 ,  -7.75,  -8.  ,  -8.25,\n        -8.5 ,  -8.75,  -9.  ,  -9.25,  -9.5 ,  -9.75, -10.  , -10.25, -10.5 ,\n       -10.75, -11.  , -11.25, -11.5 , -11.75, -12.  ], dtype=float32)lon(lon)float3242.0 42.25 42.5 ... 101.8 102.0long_name :longitudestandard_name :longitudeunits :degrees_eastarray([ 42.  ,  42.25,  42.5 , ..., 101.5 , 101.75, 102.  ], dtype=float32)time(time)datetime64[ns]1998-01-01 ... 1999-12-31axis :Tcomment :Data is averaged over the daylong_name :time centered on the daystandard_name :timetime_bounds :2000-01-01 00:00:00 to 2000-01-01 23:59:59array(['1998-01-01T00:00:00.000000000', '1998-01-02T00:00:00.000000000',\n       '1998-01-03T00:00:00.000000000', ..., '1999-12-29T00:00:00.000000000',\n       '1999-12-30T00:00:00.000000000', '1999-12-31T00:00:00.000000000'],\n      dtype='datetime64[ns]')Data variables: (19)CHL(time, lat, lon)float32dask.array&lt;chunksize=(60, 177, 241), meta=np.ndarray&gt;_ChunkSizes :[1, 256, 256]ancillary_variables :flags CHL_uncertaintycoverage_content_type :modelResultinput_files_reprocessings :Processors versions: MODIS R2022.0NRT/VIIRSN R2022.0NRT/OLCIA 07.02/VIIRSJ1 R2022.0NRT/OLCIB 07.02long_name :Chlorophyll-a concentration - Mean of the binned pixelsstandard_name :mass_concentration_of_chlorophyll_a_in_sea_watertype :surfaceunits :milligram m-3valid_max :1000.0valid_min :0.0\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n118.79 MiB\n16.27 MiB\n\n\nShape\n(730, 177, 241)\n(100, 177, 241)\n\n\nDask graph\n8 chunks in 3 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\nCHL_uncertainty\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(60, 177, 241), meta=np.ndarray&gt;\n\n\n\n\n_ChunkSizes :\n\n[1, 256, 256]\n\ncoverage_content_type :\n\nqualityInformation\n\nlong_name :\n\nChlorophyll-a concentration - Uncertainty estimation\n\nunits :\n\n%\n\nvalid_max :\n\n32767\n\nvalid_min :\n\n0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n118.79 MiB\n16.27 MiB\n\n\nShape\n(730, 177, 241)\n(100, 177, 241)\n\n\nDask graph\n8 chunks in 3 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nadt\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(60, 177, 241), meta=np.ndarray&gt;\n\n\n\n\ncomment :\n\nThe absolute dynamic topography is the sea surface height above geoid; the adt is obtained as follows: adt=sla+mdt where mdt is the mean dynamic topography; see the product user manual for details\n\ngrid_mapping :\n\ncrs\n\nlong_name :\n\nAbsolute dynamic topography\n\nstandard_name :\n\nsea_surface_height_above_geoid\n\nunits :\n\nm\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n118.79 MiB\n16.27 MiB\n\n\nShape\n(730, 177, 241)\n(100, 177, 241)\n\n\nDask graph\n8 chunks in 3 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nair_temp\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(60, 177, 241), meta=np.ndarray&gt;\n\n\n\n\nlong_name :\n\n2 metre temperature\n\nnameCDM :\n\n2_metre_temperature_surface\n\nnameECMWF :\n\n2 metre temperature\n\nproduct_type :\n\nanalysis\n\nshortNameECMWF :\n\n2t\n\nstandard_name :\n\nair_temperature\n\nunits :\n\nK\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n118.79 MiB\n16.27 MiB\n\n\nShape\n(730, 177, 241)\n(100, 177, 241)\n\n\nDask graph\n8 chunks in 3 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\ncurr_dir\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(60, 177, 241), meta=np.ndarray&gt;\n\n\n\n\ncomments :\n\nComputed from total surface current velocity elements. Velocities are an average over the top 30m of the mixed layer\n\ndepth :\n\n15m\n\nlong_name :\n\naverage direction of total surface currents\n\nunits :\n\ndegrees\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n118.79 MiB\n16.27 MiB\n\n\nShape\n(730, 177, 241)\n(100, 177, 241)\n\n\nDask graph\n8 chunks in 3 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\ncurr_speed\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(60, 177, 241), meta=np.ndarray&gt;\n\n\n\n\ncomments :\n\nVelocities are an average over the top 30m of the mixed layer\n\ndepth :\n\n15m\n\nlong_name :\n\naverage total surface current speed\n\nunits :\n\nm s**-1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n118.79 MiB\n16.27 MiB\n\n\nShape\n(730, 177, 241)\n(100, 177, 241)\n\n\nDask graph\n8 chunks in 3 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nmlotst\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(60, 177, 241), meta=np.ndarray&gt;\n\n\n\n\n_ChunkSizes :\n\n[1, 681, 1440]\n\ncell_methods :\n\narea: mean\n\nlong_name :\n\nDensity ocean mixed layer thickness\n\nstandard_name :\n\nocean_mixed_layer_thickness_defined_by_sigma_theta\n\nunit_long :\n\nMeters\n\nunits :\n\nm\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n118.79 MiB\n81.36 MiB\n\n\nShape\n(730, 177, 241)\n(500, 177, 241)\n\n\nDask graph\n3 chunks in 3 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nsla\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(60, 177, 241), meta=np.ndarray&gt;\n\n\n\n\nancillary_variables :\n\nerr_sla\n\ncomment :\n\nThe sea level anomaly is the sea surface height above mean sea surface; it is referenced to the [1993, 2012] period; see the product user manual for details\n\ngrid_mapping :\n\ncrs\n\nlong_name :\n\nSea level anomaly\n\nstandard_name :\n\nsea_surface_height_above_sea_level\n\nunits :\n\nm\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n118.79 MiB\n16.27 MiB\n\n\nShape\n(730, 177, 241)\n(100, 177, 241)\n\n\nDask graph\n8 chunks in 3 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nso\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(60, 177, 241), meta=np.ndarray&gt;\n\n\n\n\n_ChunkSizes :\n\n[1, 7, 341, 720]\n\ncell_methods :\n\narea: mean\n\nlong_name :\n\nmean sea water salinity at 0.49 metres below ocean surface\n\nstandard_name :\n\nsea_water_salinity\n\nunit_long :\n\nPractical Salinity Unit\n\nunits :\n\n1e-3\n\nvalid_max :\n\n28336\n\nvalid_min :\n\n1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n118.79 MiB\n81.36 MiB\n\n\nShape\n(730, 177, 241)\n(500, 177, 241)\n\n\nDask graph\n3 chunks in 3 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nsst\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(60, 177, 241), meta=np.ndarray&gt;\n\n\n\n\nlong_name :\n\nSea surface temperature\n\nnameCDM :\n\nSea_surface_temperature_surface\n\nnameECMWF :\n\nSea surface temperature\n\nproduct_type :\n\nanalysis\n\nshortNameECMWF :\n\nsst\n\nstandard_name :\n\nsea_surface_temperature\n\nunits :\n\nK\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n118.79 MiB\n16.27 MiB\n\n\nShape\n(730, 177, 241)\n(100, 177, 241)\n\n\nDask graph\n8 chunks in 3 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\ntopo\n\n\n(lat, lon)\n\n\nfloat64\n\n\ndask.array&lt;chunksize=(177, 241), meta=np.ndarray&gt;\n\n\n\n\ncolorBarMaximum :\n\n8000.0\n\ncolorBarMinimum :\n\n-8000.0\n\ncolorBarPalette :\n\nTopography\n\ngrid_mapping :\n\nGDAL_Geographics\n\nioos_category :\n\nLocation\n\nlong_name :\n\nTopography\n\nstandard_name :\n\naltitude\n\nunits :\n\nmeters\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n333.26 kiB\n333.26 kiB\n\n\nShape\n(177, 241)\n(177, 241)\n\n\nDask graph\n1 chunks in 2 graph layers\n\n\nData type\nfloat64 numpy.ndarray\n\n\n\n\n\n\n\n\n\nu_curr\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(60, 177, 241), meta=np.ndarray&gt;\n\n\n\n\ncomment :\n\nVelocities are an average over the top 30m of the mixed layer\n\ncoverage_content_type :\n\nmodelResult\n\ndepth :\n\n15m\n\nlong_name :\n\nzonal total surface current\n\nsource :\n\nSSH source: CMEMS SSALTO/DUACS SEALEVEL_GLO_PHY_L4_MY_008_047 DOI: 10.48670/moi-00148 ; WIND source: ECMWF ERA5 10m wind DOI: 10.24381/cds.adbb2d47 ; SST source: CMC 0.2 deg SST V2.0 DOI: 10.5067/GHCMC-4FM02\n\nstandard_name :\n\neastward_sea_water_velocity\n\nunits :\n\nm s-1\n\nvalid_max :\n\n3.0\n\nvalid_min :\n\n-3.0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n118.79 MiB\n16.27 MiB\n\n\nShape\n(730, 177, 241)\n(100, 177, 241)\n\n\nDask graph\n8 chunks in 3 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nu_wind\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(60, 177, 241), meta=np.ndarray&gt;\n\n\n\n\nlong_name :\n\n10 metre U wind component\n\nnameCDM :\n\n10_metre_U_wind_component_surface\n\nnameECMWF :\n\n10 metre U wind component\n\nproduct_type :\n\nanalysis\n\nshortNameECMWF :\n\n10u\n\nstandard_name :\n\neastward_wind\n\nunits :\n\nm s**-1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n118.79 MiB\n16.27 MiB\n\n\nShape\n(730, 177, 241)\n(100, 177, 241)\n\n\nDask graph\n8 chunks in 3 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nug_curr\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(60, 177, 241), meta=np.ndarray&gt;\n\n\n\n\ncomment :\n\nGeostrophic velocities calculated from absolute dynamic topography\n\ndepth :\n\n15m\n\nlong_name :\n\nzonal geostrophic surface current\n\nsource :\n\nSSH source: CMEMS SSALTO/DUACS SEALEVEL_GLO_PHY_L4_MY_008_047 DOI: 10.48670/moi-00148\n\nstandard_name :\n\ngeostrophic_eastward_sea_water_velocity\n\nunits :\n\nm s-1\n\nvalid_max :\n\n3.0\n\nvalid_min :\n\n-3.0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n118.79 MiB\n16.27 MiB\n\n\nShape\n(730, 177, 241)\n(100, 177, 241)\n\n\nDask graph\n8 chunks in 3 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nv_curr\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(60, 177, 241), meta=np.ndarray&gt;\n\n\n\n\ncomment :\n\nVelocities are an average over the top 30m of the mixed layer\n\ncoverage_content_type :\n\nmodelResult\n\ndepth :\n\n15m\n\nlong_name :\n\nmeridional total surface current\n\nsource :\n\nSSH source: CMEMS SSALTO/DUACS SEALEVEL_GLO_PHY_L4_MY_008_047 DOI: 10.48670/moi-00148 ; WIND source: ECMWF ERA5 10m wind DOI: 10.24381/cds.adbb2d47 ; SST source: CMC 0.2 deg SST V2.0 DOI: 10.5067/GHCMC-4FM02\n\nstandard_name :\n\nnorthward_sea_water_velocity\n\nunits :\n\nm s-1\n\nvalid_max :\n\n3.0\n\nvalid_min :\n\n-3.0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n118.79 MiB\n16.27 MiB\n\n\nShape\n(730, 177, 241)\n(100, 177, 241)\n\n\nDask graph\n8 chunks in 3 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nv_wind\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(60, 177, 241), meta=np.ndarray&gt;\n\n\n\n\nlong_name :\n\n10 metre V wind component\n\nnameCDM :\n\n10_metre_V_wind_component_surface\n\nnameECMWF :\n\n10 metre V wind component\n\nproduct_type :\n\nanalysis\n\nshortNameECMWF :\n\n10v\n\nstandard_name :\n\nnorthward_wind\n\nunits :\n\nm s**-1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n118.79 MiB\n16.27 MiB\n\n\nShape\n(730, 177, 241)\n(100, 177, 241)\n\n\nDask graph\n8 chunks in 3 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nvg_curr\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(60, 177, 241), meta=np.ndarray&gt;\n\n\n\n\ncomment :\n\nGeostrophic velocities calculated from absolute dynamic topography\n\ndepth :\n\n15m\n\nlong_name :\n\nmeridional geostrophic surface current\n\nsource :\n\nSSH source: CMEMS SSALTO/DUACS SEALEVEL_GLO_PHY_L4_MY_008_047 DOI: 10.48670/moi-00148\n\nstandard_name :\n\ngeostrophic_northward_sea_water_velocity\n\nunits :\n\nm s-1\n\nvalid_max :\n\n3.0\n\nvalid_min :\n\n-3.0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n118.79 MiB\n16.27 MiB\n\n\nShape\n(730, 177, 241)\n(100, 177, 241)\n\n\nDask graph\n8 chunks in 3 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nwind_dir\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(60, 177, 241), meta=np.ndarray&gt;\n\n\n\n\nlong_name :\n\n10 metre wind direction\n\nunits :\n\ndegrees\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n118.79 MiB\n16.27 MiB\n\n\nShape\n(730, 177, 241)\n(100, 177, 241)\n\n\nDask graph\n8 chunks in 3 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nwind_speed\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(60, 177, 241), meta=np.ndarray&gt;\n\n\n\n\nlong_name :\n\n10 metre absolute speed\n\nunits :\n\nm s**-1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n118.79 MiB\n16.27 MiB\n\n\nShape\n(730, 177, 241)\n(100, 177, 241)\n\n\nDask graph\n8 chunks in 3 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\nIndexes: (3)latPandasIndexPandasIndex(Index([  32.0,  31.75,   31.5,  31.25,   31.0,  30.75,   30.5,  30.25,   30.0,\n        29.75,\n       ...\n        -9.75,  -10.0, -10.25,  -10.5, -10.75,  -11.0, -11.25,  -11.5, -11.75,\n        -12.0],\n      dtype='float32', name='lat', length=177))lonPandasIndexPandasIndex(Index([  42.0,  42.25,   42.5,  42.75,   43.0,  43.25,   43.5,  43.75,   44.0,\n        44.25,\n       ...\n        99.75,  100.0, 100.25,  100.5, 100.75,  101.0, 101.25,  101.5, 101.75,\n        102.0],\n      dtype='float32', name='lon', length=241))timePandasIndexPandasIndex(DatetimeIndex(['1998-01-01', '1998-01-02', '1998-01-03', '1998-01-04',\n               '1998-01-05', '1998-01-06', '1998-01-07', '1998-01-08',\n               '1998-01-09', '1998-01-10',\n               ...\n               '1999-12-22', '1999-12-23', '1999-12-24', '1999-12-25',\n               '1999-12-26', '1999-12-27', '1999-12-28', '1999-12-29',\n               '1999-12-30', '1999-12-31'],\n              dtype='datetime64[ns]', name='time', length=730, freq=None))Attributes: (17)creator_email :minhphan@uw.educreator_name :Minh Phancreator_type :persondate_created :2023-07-19geospatial_lat_max :32.0geospatial_lat_min :-12.0geospatial_lat_resolution :0.25geospatial_lat_units :degrees_northgeospatial_lon_max :102.0geospatial_lon_min :42.0geospatial_lon_resolution :0.25geospatial_lon_units :degrees_eastsource :Earth & Space Research (ESR), Copernicus Climate Change Service (C3S), Copernicus Marine Environment Monitoring Service (CMEMS), United States Geological Survey (USGS)summary :Daily mean of 0.25 x 0.25 degrees gridded data from multiple climate variables that may influence the patterns of coastal upwelling in the focused areatime_coverage_end :2022-12-31T23:59:59time_coverage_start :1979-01-01T00:00:00title :Climate Data for Coastal Upwelling Machine Learning Project in Indian Ocean\n\n\n\n# slice by variable\nds[['u_curr', 'u_wind']]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt;\nDimensions:  (time: 16071, lat: 177, lon: 241)\nCoordinates:\n  * lat      (lat) float32 32.0 31.75 31.5 31.25 ... -11.25 -11.5 -11.75 -12.0\n  * lon      (lon) float32 42.0 42.25 42.5 42.75 ... 101.2 101.5 101.8 102.0\n  * time     (time) datetime64[ns] 1979-01-01 1979-01-02 ... 2022-12-31\nData variables:\n    u_curr   (time, lat, lon) float32 dask.array&lt;chunksize=(100, 177, 241), meta=np.ndarray&gt;\n    u_wind   (time, lat, lon) float32 dask.array&lt;chunksize=(100, 177, 241), meta=np.ndarray&gt;\nAttributes: (12/17)\n    creator_email:              minhphan@uw.edu\n    creator_name:               Minh Phan\n    creator_type:               person\n    date_created:               2023-07-19\n    geospatial_lat_max:         32.0\n    geospatial_lat_min:         -12.0\n    ...                         ...\n    geospatial_lon_units:       degrees_east\n    source:                     Earth & Space Research (ESR), Copernicus Clim...\n    summary:                    Daily mean of 0.25 x 0.25 degrees gridded dat...\n    time_coverage_end:          2022-12-31T23:59:59\n    time_coverage_start:        1979-01-01T00:00:00\n    title:                      Climate Data for Coastal Upwelling Machine Le...xarray.DatasetDimensions:time: 16071lat: 177lon: 241Coordinates: (3)lat(lat)float3232.0 31.75 31.5 ... -11.75 -12.0long_name :latitudestandard_name :latitudeunits :degrees_northarray([ 32.  ,  31.75,  31.5 ,  31.25,  31.  ,  30.75,  30.5 ,  30.25,  30.  ,\n        29.75,  29.5 ,  29.25,  29.  ,  28.75,  28.5 ,  28.25,  28.  ,  27.75,\n        27.5 ,  27.25,  27.  ,  26.75,  26.5 ,  26.25,  26.  ,  25.75,  25.5 ,\n        25.25,  25.  ,  24.75,  24.5 ,  24.25,  24.  ,  23.75,  23.5 ,  23.25,\n        23.  ,  22.75,  22.5 ,  22.25,  22.  ,  21.75,  21.5 ,  21.25,  21.  ,\n        20.75,  20.5 ,  20.25,  20.  ,  19.75,  19.5 ,  19.25,  19.  ,  18.75,\n        18.5 ,  18.25,  18.  ,  17.75,  17.5 ,  17.25,  17.  ,  16.75,  16.5 ,\n        16.25,  16.  ,  15.75,  15.5 ,  15.25,  15.  ,  14.75,  14.5 ,  14.25,\n        14.  ,  13.75,  13.5 ,  13.25,  13.  ,  12.75,  12.5 ,  12.25,  12.  ,\n        11.75,  11.5 ,  11.25,  11.  ,  10.75,  10.5 ,  10.25,  10.  ,   9.75,\n         9.5 ,   9.25,   9.  ,   8.75,   8.5 ,   8.25,   8.  ,   7.75,   7.5 ,\n         7.25,   7.  ,   6.75,   6.5 ,   6.25,   6.  ,   5.75,   5.5 ,   5.25,\n         5.  ,   4.75,   4.5 ,   4.25,   4.  ,   3.75,   3.5 ,   3.25,   3.  ,\n         2.75,   2.5 ,   2.25,   2.  ,   1.75,   1.5 ,   1.25,   1.  ,   0.75,\n         0.5 ,   0.25,   0.  ,  -0.25,  -0.5 ,  -0.75,  -1.  ,  -1.25,  -1.5 ,\n        -1.75,  -2.  ,  -2.25,  -2.5 ,  -2.75,  -3.  ,  -3.25,  -3.5 ,  -3.75,\n        -4.  ,  -4.25,  -4.5 ,  -4.75,  -5.  ,  -5.25,  -5.5 ,  -5.75,  -6.  ,\n        -6.25,  -6.5 ,  -6.75,  -7.  ,  -7.25,  -7.5 ,  -7.75,  -8.  ,  -8.25,\n        -8.5 ,  -8.75,  -9.  ,  -9.25,  -9.5 ,  -9.75, -10.  , -10.25, -10.5 ,\n       -10.75, -11.  , -11.25, -11.5 , -11.75, -12.  ], dtype=float32)lon(lon)float3242.0 42.25 42.5 ... 101.8 102.0long_name :longitudestandard_name :longitudeunits :degrees_eastarray([ 42.  ,  42.25,  42.5 , ..., 101.5 , 101.75, 102.  ], dtype=float32)time(time)datetime64[ns]1979-01-01 ... 2022-12-31axis :Tcomment :Data is averaged over the daylong_name :time centered on the daystandard_name :timetime_bounds :2000-01-01 00:00:00 to 2000-01-01 23:59:59array(['1979-01-01T00:00:00.000000000', '1979-01-02T00:00:00.000000000',\n       '1979-01-03T00:00:00.000000000', ..., '2022-12-29T00:00:00.000000000',\n       '2022-12-30T00:00:00.000000000', '2022-12-31T00:00:00.000000000'],\n      dtype='datetime64[ns]')Data variables: (2)u_curr(time, lat, lon)float32dask.array&lt;chunksize=(100, 177, 241), meta=np.ndarray&gt;comment :Velocities are an average over the top 30m of the mixed layercoverage_content_type :modelResultdepth :15mlong_name :zonal total surface currentsource :SSH source: CMEMS SSALTO/DUACS SEALEVEL_GLO_PHY_L4_MY_008_047 DOI: 10.48670/moi-00148 ; WIND source: ECMWF ERA5 10m wind DOI: 10.24381/cds.adbb2d47 ; SST source: CMC 0.2 deg SST V2.0 DOI: 10.5067/GHCMC-4FM02standard_name :eastward_sea_water_velocityunits :m s-1valid_max :3.0valid_min :-3.0\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n2.55 GiB\n16.27 MiB\n\n\nShape\n(16071, 177, 241)\n(100, 177, 241)\n\n\nDask graph\n161 chunks in 2 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\nu_wind\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(100, 177, 241), meta=np.ndarray&gt;\n\n\n\n\nlong_name :\n\n10 metre U wind component\n\nnameCDM :\n\n10_metre_U_wind_component_surface\n\nnameECMWF :\n\n10 metre U wind component\n\nproduct_type :\n\nanalysis\n\nshortNameECMWF :\n\n10u\n\nstandard_name :\n\neastward_wind\n\nunits :\n\nm s**-1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n2.55 GiB\n16.27 MiB\n\n\nShape\n(16071, 177, 241)\n(100, 177, 241)\n\n\nDask graph\n161 chunks in 2 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\nIndexes: (3)latPandasIndexPandasIndex(Index([  32.0,  31.75,   31.5,  31.25,   31.0,  30.75,   30.5,  30.25,   30.0,\n        29.75,\n       ...\n        -9.75,  -10.0, -10.25,  -10.5, -10.75,  -11.0, -11.25,  -11.5, -11.75,\n        -12.0],\n      dtype='float32', name='lat', length=177))lonPandasIndexPandasIndex(Index([  42.0,  42.25,   42.5,  42.75,   43.0,  43.25,   43.5,  43.75,   44.0,\n        44.25,\n       ...\n        99.75,  100.0, 100.25,  100.5, 100.75,  101.0, 101.25,  101.5, 101.75,\n        102.0],\n      dtype='float32', name='lon', length=241))timePandasIndexPandasIndex(DatetimeIndex(['1979-01-01', '1979-01-02', '1979-01-03', '1979-01-04',\n               '1979-01-05', '1979-01-06', '1979-01-07', '1979-01-08',\n               '1979-01-09', '1979-01-10',\n               ...\n               '2022-12-22', '2022-12-23', '2022-12-24', '2022-12-25',\n               '2022-12-26', '2022-12-27', '2022-12-28', '2022-12-29',\n               '2022-12-30', '2022-12-31'],\n              dtype='datetime64[ns]', name='time', length=16071, freq=None))Attributes: (17)creator_email :minhphan@uw.educreator_name :Minh Phancreator_type :persondate_created :2023-07-19geospatial_lat_max :32.0geospatial_lat_min :-12.0geospatial_lat_resolution :0.25geospatial_lat_units :degrees_northgeospatial_lon_max :102.0geospatial_lon_min :42.0geospatial_lon_resolution :0.25geospatial_lon_units :degrees_eastsource :Earth & Space Research (ESR), Copernicus Climate Change Service (C3S), Copernicus Marine Environment Monitoring Service (CMEMS), United States Geological Survey (USGS)summary :Daily mean of 0.25 x 0.25 degrees gridded data from multiple climate variables that may influence the patterns of coastal upwelling in the focused areatime_coverage_end :2022-12-31T23:59:59time_coverage_start :1979-01-01T00:00:00title :Climate Data for Coastal Upwelling Machine Learning Project in Indian Ocean\n\n\n\n# combine multiple slicing options all at once\nds[['u_curr', 'u_wind']].sel(time=slice('1998', '1999'), \n                             lat=slice(0, -12), \n                             lon=slice(42, 45))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt;\nDimensions:  (time: 730, lat: 49, lon: 13)\nCoordinates:\n  * lat      (lat) float32 0.0 -0.25 -0.5 -0.75 ... -11.25 -11.5 -11.75 -12.0\n  * lon      (lon) float32 42.0 42.25 42.5 42.75 43.0 ... 44.25 44.5 44.75 45.0\n  * time     (time) datetime64[ns] 1998-01-01 1998-01-02 ... 1999-12-31\nData variables:\n    u_curr   (time, lat, lon) float32 dask.array&lt;chunksize=(60, 49, 13), meta=np.ndarray&gt;\n    u_wind   (time, lat, lon) float32 dask.array&lt;chunksize=(60, 49, 13), meta=np.ndarray&gt;\nAttributes: (12/17)\n    creator_email:              minhphan@uw.edu\n    creator_name:               Minh Phan\n    creator_type:               person\n    date_created:               2023-07-19\n    geospatial_lat_max:         32.0\n    geospatial_lat_min:         -12.0\n    ...                         ...\n    geospatial_lon_units:       degrees_east\n    source:                     Earth & Space Research (ESR), Copernicus Clim...\n    summary:                    Daily mean of 0.25 x 0.25 degrees gridded dat...\n    time_coverage_end:          2022-12-31T23:59:59\n    time_coverage_start:        1979-01-01T00:00:00\n    title:                      Climate Data for Coastal Upwelling Machine Le...xarray.DatasetDimensions:time: 730lat: 49lon: 13Coordinates: (3)lat(lat)float320.0 -0.25 -0.5 ... -11.75 -12.0long_name :latitudestandard_name :latitudeunits :degrees_northarray([  0.  ,  -0.25,  -0.5 ,  -0.75,  -1.  ,  -1.25,  -1.5 ,  -1.75,  -2.  ,\n        -2.25,  -2.5 ,  -2.75,  -3.  ,  -3.25,  -3.5 ,  -3.75,  -4.  ,  -4.25,\n        -4.5 ,  -4.75,  -5.  ,  -5.25,  -5.5 ,  -5.75,  -6.  ,  -6.25,  -6.5 ,\n        -6.75,  -7.  ,  -7.25,  -7.5 ,  -7.75,  -8.  ,  -8.25,  -8.5 ,  -8.75,\n        -9.  ,  -9.25,  -9.5 ,  -9.75, -10.  , -10.25, -10.5 , -10.75, -11.  ,\n       -11.25, -11.5 , -11.75, -12.  ], dtype=float32)lon(lon)float3242.0 42.25 42.5 ... 44.5 44.75 45.0long_name :longitudestandard_name :longitudeunits :degrees_eastarray([42.  , 42.25, 42.5 , 42.75, 43.  , 43.25, 43.5 , 43.75, 44.  , 44.25,\n       44.5 , 44.75, 45.  ], dtype=float32)time(time)datetime64[ns]1998-01-01 ... 1999-12-31axis :Tcomment :Data is averaged over the daylong_name :time centered on the daystandard_name :timetime_bounds :2000-01-01 00:00:00 to 2000-01-01 23:59:59array(['1998-01-01T00:00:00.000000000', '1998-01-02T00:00:00.000000000',\n       '1998-01-03T00:00:00.000000000', ..., '1999-12-29T00:00:00.000000000',\n       '1999-12-30T00:00:00.000000000', '1999-12-31T00:00:00.000000000'],\n      dtype='datetime64[ns]')Data variables: (2)u_curr(time, lat, lon)float32dask.array&lt;chunksize=(60, 49, 13), meta=np.ndarray&gt;comment :Velocities are an average over the top 30m of the mixed layercoverage_content_type :modelResultdepth :15mlong_name :zonal total surface currentsource :SSH source: CMEMS SSALTO/DUACS SEALEVEL_GLO_PHY_L4_MY_008_047 DOI: 10.48670/moi-00148 ; WIND source: ECMWF ERA5 10m wind DOI: 10.24381/cds.adbb2d47 ; SST source: CMC 0.2 deg SST V2.0 DOI: 10.5067/GHCMC-4FM02standard_name :eastward_sea_water_velocityunits :m s-1valid_max :3.0valid_min :-3.0\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n1.77 MiB\n248.83 kiB\n\n\nShape\n(730, 49, 13)\n(100, 49, 13)\n\n\nDask graph\n8 chunks in 3 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\nu_wind\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(60, 49, 13), meta=np.ndarray&gt;\n\n\n\n\nlong_name :\n\n10 metre U wind component\n\nnameCDM :\n\n10_metre_U_wind_component_surface\n\nnameECMWF :\n\n10 metre U wind component\n\nproduct_type :\n\nanalysis\n\nshortNameECMWF :\n\n10u\n\nstandard_name :\n\neastward_wind\n\nunits :\n\nm s**-1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n1.77 MiB\n248.83 kiB\n\n\nShape\n(730, 49, 13)\n(100, 49, 13)\n\n\nDask graph\n8 chunks in 3 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\nIndexes: (3)latPandasIndexPandasIndex(Index([   0.0,  -0.25,   -0.5,  -0.75,   -1.0,  -1.25,   -1.5,  -1.75,   -2.0,\n        -2.25,   -2.5,  -2.75,   -3.0,  -3.25,   -3.5,  -3.75,   -4.0,  -4.25,\n         -4.5,  -4.75,   -5.0,  -5.25,   -5.5,  -5.75,   -6.0,  -6.25,   -6.5,\n        -6.75,   -7.0,  -7.25,   -7.5,  -7.75,   -8.0,  -8.25,   -8.5,  -8.75,\n         -9.0,  -9.25,   -9.5,  -9.75,  -10.0, -10.25,  -10.5, -10.75,  -11.0,\n       -11.25,  -11.5, -11.75,  -12.0],\n      dtype='float32', name='lat'))lonPandasIndexPandasIndex(Index([ 42.0, 42.25,  42.5, 42.75,  43.0, 43.25,  43.5, 43.75,  44.0, 44.25,\n        44.5, 44.75,  45.0],\n      dtype='float32', name='lon'))timePandasIndexPandasIndex(DatetimeIndex(['1998-01-01', '1998-01-02', '1998-01-03', '1998-01-04',\n               '1998-01-05', '1998-01-06', '1998-01-07', '1998-01-08',\n               '1998-01-09', '1998-01-10',\n               ...\n               '1999-12-22', '1999-12-23', '1999-12-24', '1999-12-25',\n               '1999-12-26', '1999-12-27', '1999-12-28', '1999-12-29',\n               '1999-12-30', '1999-12-31'],\n              dtype='datetime64[ns]', name='time', length=730, freq=None))Attributes: (17)creator_email :minhphan@uw.educreator_name :Minh Phancreator_type :persondate_created :2023-07-19geospatial_lat_max :32.0geospatial_lat_min :-12.0geospatial_lat_resolution :0.25geospatial_lat_units :degrees_northgeospatial_lon_max :102.0geospatial_lon_min :42.0geospatial_lon_resolution :0.25geospatial_lon_units :degrees_eastsource :Earth & Space Research (ESR), Copernicus Climate Change Service (C3S), Copernicus Marine Environment Monitoring Service (CMEMS), United States Geological Survey (USGS)summary :Daily mean of 0.25 x 0.25 degrees gridded data from multiple climate variables that may influence the patterns of coastal upwelling in the focused areatime_coverage_end :2022-12-31T23:59:59time_coverage_start :1979-01-01T00:00:00title :Climate Data for Coastal Upwelling Machine Learning Project in Indian Ocean\n\n\nWe can also graph the data right from slicing, especially heatmaps from 2D arrays, or line charts. This is especially useful when we want to inspect elements on the go.\n\n# make sure that the array you slice for a heatmap visualization is a 2D array\nheatmap_arr = ds['wind_speed'].sel(time='2000-01-02')\nheatmap_arr\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray 'wind_speed' (lat: 177, lon: 241)&gt;\ndask.array&lt;getitem, shape=(177, 241), dtype=float32, chunksize=(177, 241), chunktype=numpy.ndarray&gt;\nCoordinates:\n  * lat      (lat) float32 32.0 31.75 31.5 31.25 ... -11.25 -11.5 -11.75 -12.0\n  * lon      (lon) float32 42.0 42.25 42.5 42.75 ... 101.2 101.5 101.8 102.0\n    time     datetime64[ns] 2000-01-02\nAttributes:\n    long_name:  10 metre absolute speed\n    units:      m s**-1xarray.DataArray'wind_speed'lat: 177lon: 241dask.array&lt;chunksize=(177, 241), meta=np.ndarray&gt;\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n166.63 kiB\n166.63 kiB\n\n\nShape\n(177, 241)\n(177, 241)\n\n\nDask graph\n1 chunks in 3 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\nCoordinates: (3)lat(lat)float3232.0 31.75 31.5 ... -11.75 -12.0long_name :latitudestandard_name :latitudeunits :degrees_northarray([ 32.  ,  31.75,  31.5 ,  31.25,  31.  ,  30.75,  30.5 ,  30.25,  30.  ,\n        29.75,  29.5 ,  29.25,  29.  ,  28.75,  28.5 ,  28.25,  28.  ,  27.75,\n        27.5 ,  27.25,  27.  ,  26.75,  26.5 ,  26.25,  26.  ,  25.75,  25.5 ,\n        25.25,  25.  ,  24.75,  24.5 ,  24.25,  24.  ,  23.75,  23.5 ,  23.25,\n        23.  ,  22.75,  22.5 ,  22.25,  22.  ,  21.75,  21.5 ,  21.25,  21.  ,\n        20.75,  20.5 ,  20.25,  20.  ,  19.75,  19.5 ,  19.25,  19.  ,  18.75,\n        18.5 ,  18.25,  18.  ,  17.75,  17.5 ,  17.25,  17.  ,  16.75,  16.5 ,\n        16.25,  16.  ,  15.75,  15.5 ,  15.25,  15.  ,  14.75,  14.5 ,  14.25,\n        14.  ,  13.75,  13.5 ,  13.25,  13.  ,  12.75,  12.5 ,  12.25,  12.  ,\n        11.75,  11.5 ,  11.25,  11.  ,  10.75,  10.5 ,  10.25,  10.  ,   9.75,\n         9.5 ,   9.25,   9.  ,   8.75,   8.5 ,   8.25,   8.  ,   7.75,   7.5 ,\n         7.25,   7.  ,   6.75,   6.5 ,   6.25,   6.  ,   5.75,   5.5 ,   5.25,\n         5.  ,   4.75,   4.5 ,   4.25,   4.  ,   3.75,   3.5 ,   3.25,   3.  ,\n         2.75,   2.5 ,   2.25,   2.  ,   1.75,   1.5 ,   1.25,   1.  ,   0.75,\n         0.5 ,   0.25,   0.  ,  -0.25,  -0.5 ,  -0.75,  -1.  ,  -1.25,  -1.5 ,\n        -1.75,  -2.  ,  -2.25,  -2.5 ,  -2.75,  -3.  ,  -3.25,  -3.5 ,  -3.75,\n        -4.  ,  -4.25,  -4.5 ,  -4.75,  -5.  ,  -5.25,  -5.5 ,  -5.75,  -6.  ,\n        -6.25,  -6.5 ,  -6.75,  -7.  ,  -7.25,  -7.5 ,  -7.75,  -8.  ,  -8.25,\n        -8.5 ,  -8.75,  -9.  ,  -9.25,  -9.5 ,  -9.75, -10.  , -10.25, -10.5 ,\n       -10.75, -11.  , -11.25, -11.5 , -11.75, -12.  ], dtype=float32)lon(lon)float3242.0 42.25 42.5 ... 101.8 102.0long_name :longitudestandard_name :longitudeunits :degrees_eastarray([ 42.  ,  42.25,  42.5 , ..., 101.5 , 101.75, 102.  ], dtype=float32)time()datetime64[ns]2000-01-02axis :Tcomment :Data is averaged over the daylong_name :time centered on the daystandard_name :timetime_bounds :2000-01-01 00:00:00 to 2000-01-01 23:59:59array('2000-01-02T00:00:00.000000000', dtype='datetime64[ns]')Indexes: (2)latPandasIndexPandasIndex(Index([  32.0,  31.75,   31.5,  31.25,   31.0,  30.75,   30.5,  30.25,   30.0,\n        29.75,\n       ...\n        -9.75,  -10.0, -10.25,  -10.5, -10.75,  -11.0, -11.25,  -11.5, -11.75,\n        -12.0],\n      dtype='float32', name='lat', length=177))lonPandasIndexPandasIndex(Index([  42.0,  42.25,   42.5,  42.75,   43.0,  43.25,   43.5,  43.75,   44.0,\n        44.25,\n       ...\n        99.75,  100.0, 100.25,  100.5, 100.75,  101.0, 101.25,  101.5, 101.75,\n        102.0],\n      dtype='float32', name='lon', length=241))Attributes: (2)long_name :10 metre absolute speedunits :m s**-1\n\n\n\nheatmap_arr.plot.imshow()\n\n\n\n\n\n# contour map with no filling\nheatmap_arr.plot.contour()\n\n\n\n\n\n# contour map with color filling\nheatmap_arr.plot.contourf()\n\n\n\n\n\nheatmap_arr.plot.surface()\n\n\n\n\n\nLine plots\n\nds['wind_speed'].mean(dim=['lat', 'lon']).plot(figsize=(10, 5))\n\n\n\n\nWe can add in parameters to customize our graphs, as additional arguments are passed to the underlying matplotlib plot() function.\n\nds['air_temp'].mean(dim=['lat', 'lon']).sel(time=slice('2007', '2009')).plot.line('r-o', figsize=(10,5), markersize=1)\n\n\n\n\n\n\nHistogram\n\n# creating a new Axe object if there is no currently\n# available one\nax = plt.gca() \nds['wind_dir'].plot.hist(ax = ax)\nax.set_xlabel('10 metre wind direction (degrees east)')\nax.set_ylabel('frequency')\nax.set_title('Daily average wind direction distribution over covered area (1979-2022)')\n\nText(0.5, 1.0, 'Daily average wind direction distribution over covered area (1979-2022)')\n\n\n\n\n\n\n# scroll along the axes to stretch the graph\nheatmap_arr.hvplot().options(cmap='bgy', width=600, height=500)",
    "crumbs": [
      "Python Tutorials",
      "Using Data",
      "IO Zarr file"
    ]
  },
  {
    "objectID": "python-notebooks/02_Using_Data.html#resampling",
    "href": "python-notebooks/02_Using_Data.html#resampling",
    "title": "IO Zarr file",
    "section": "Resampling",
    "text": "Resampling\n\nWith xarray\nIf your xarray version works just fine with resample(), here’s the most straightforward way to resample your data temporally. It may take a long while for the data to finish resampling, especially if your dataset is big and your resampling frequency is small.\n\nds_resampled = ds['CHL'].resample(time='1M').mean()\n\n\n\nWithout xarray\nIf not, which was the case we encountered for a bit during the documentation, we developed a roundabout using Pandas’ equivalent function with the same name, which also happened to also be the underlying function that the library itself depends on (of course, more optimized), to help us perform resampling on our dataset.\n\n# resampling is having issues, so this is a temporary solution using Pandas \n# to perform resampling by month on one range...\n\n# choose a small range just in case overloadding happens...\n\nds_to_resample = ds['CHL'].sel(lat=slice(10, 5), lon=slice(75, 80)).to_dataframe()\n\n\ndf_resampled = ds_to_resample.groupby([pd.Grouper(freq='1M', level='time'),\n                        pd.Grouper(level='lon'),\n                        pd.Grouper(level='lat')]).mean()\n\n\nds_resampled = xr.Dataset.from_dataframe(df_resampled)\n\nWe can see that after resampling, our time dimension size is reduced from 8523 (days) to 280 (months). Resampling is successful!\n\n# original ds_to_resample object before converting to dataframe\nds['CHL'].sel(lat=slice(10, 5), lon=slice(75, 80))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray 'CHL' (time: 16071, lat: 21, lon: 21)&gt;\ndask.array&lt;getitem, shape=(16071, 21, 21), dtype=float32, chunksize=(100, 21, 21), chunktype=numpy.ndarray&gt;\nCoordinates:\n  * lat      (lat) float32 10.0 9.75 9.5 9.25 9.0 8.75 ... 6.0 5.75 5.5 5.25 5.0\n  * lon      (lon) float32 75.0 75.25 75.5 75.75 76.0 ... 79.25 79.5 79.75 80.0\n  * time     (time) datetime64[ns] 1979-01-01 1979-01-02 ... 2022-12-31\nAttributes:\n    _ChunkSizes:                [1, 256, 256]\n    ancillary_variables:        flags CHL_uncertainty\n    coverage_content_type:      modelResult\n    input_files_reprocessings:  Processors versions: MODIS R2022.0NRT/VIIRSN ...\n    long_name:                  Chlorophyll-a concentration - Mean of the bin...\n    standard_name:              mass_concentration_of_chlorophyll_a_in_sea_water\n    type:                       surface\n    units:                      milligram m-3\n    valid_max:                  1000.0\n    valid_min:                  0.0xarray.DataArray'CHL'time: 16071lat: 21lon: 21dask.array&lt;chunksize=(100, 21, 21), meta=np.ndarray&gt;\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n27.04 MiB\n172.27 kiB\n\n\nShape\n(16071, 21, 21)\n(100, 21, 21)\n\n\nDask graph\n161 chunks in 3 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\nCoordinates: (3)lat(lat)float3210.0 9.75 9.5 9.25 ... 5.5 5.25 5.0long_name :latitudestandard_name :latitudeunits :degrees_northarray([10.  ,  9.75,  9.5 ,  9.25,  9.  ,  8.75,  8.5 ,  8.25,  8.  ,  7.75,\n        7.5 ,  7.25,  7.  ,  6.75,  6.5 ,  6.25,  6.  ,  5.75,  5.5 ,  5.25,\n        5.  ], dtype=float32)lon(lon)float3275.0 75.25 75.5 ... 79.5 79.75 80.0long_name :longitudestandard_name :longitudeunits :degrees_eastarray([75.  , 75.25, 75.5 , 75.75, 76.  , 76.25, 76.5 , 76.75, 77.  , 77.25,\n       77.5 , 77.75, 78.  , 78.25, 78.5 , 78.75, 79.  , 79.25, 79.5 , 79.75,\n       80.  ], dtype=float32)time(time)datetime64[ns]1979-01-01 ... 2022-12-31axis :Tcomment :Data is averaged over the daylong_name :time centered on the daystandard_name :timetime_bounds :2000-01-01 00:00:00 to 2000-01-01 23:59:59array(['1979-01-01T00:00:00.000000000', '1979-01-02T00:00:00.000000000',\n       '1979-01-03T00:00:00.000000000', ..., '2022-12-29T00:00:00.000000000',\n       '2022-12-30T00:00:00.000000000', '2022-12-31T00:00:00.000000000'],\n      dtype='datetime64[ns]')Indexes: (3)latPandasIndexPandasIndex(Index([10.0, 9.75,  9.5, 9.25,  9.0, 8.75,  8.5, 8.25,  8.0, 7.75,  7.5, 7.25,\n        7.0, 6.75,  6.5, 6.25,  6.0, 5.75,  5.5, 5.25,  5.0],\n      dtype='float32', name='lat'))lonPandasIndexPandasIndex(Index([ 75.0, 75.25,  75.5, 75.75,  76.0, 76.25,  76.5, 76.75,  77.0, 77.25,\n        77.5, 77.75,  78.0, 78.25,  78.5, 78.75,  79.0, 79.25,  79.5, 79.75,\n        80.0],\n      dtype='float32', name='lon'))timePandasIndexPandasIndex(DatetimeIndex(['1979-01-01', '1979-01-02', '1979-01-03', '1979-01-04',\n               '1979-01-05', '1979-01-06', '1979-01-07', '1979-01-08',\n               '1979-01-09', '1979-01-10',\n               ...\n               '2022-12-22', '2022-12-23', '2022-12-24', '2022-12-25',\n               '2022-12-26', '2022-12-27', '2022-12-28', '2022-12-29',\n               '2022-12-30', '2022-12-31'],\n              dtype='datetime64[ns]', name='time', length=16071, freq=None))Attributes: (10)_ChunkSizes :[1, 256, 256]ancillary_variables :flags CHL_uncertaintycoverage_content_type :modelResultinput_files_reprocessings :Processors versions: MODIS R2022.0NRT/VIIRSN R2022.0NRT/OLCIA 07.02/VIIRSJ1 R2022.0NRT/OLCIB 07.02long_name :Chlorophyll-a concentration - Mean of the binned pixelsstandard_name :mass_concentration_of_chlorophyll_a_in_sea_watertype :surfaceunits :milligram m-3valid_max :1000.0valid_min :0.0\n\n\n\nds_resampled\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt;\nDimensions:  (time: 528, lon: 21, lat: 21)\nCoordinates:\n  * time     (time) datetime64[ns] 1979-01-31 1979-02-28 ... 2022-12-31\n  * lon      (lon) float32 75.0 75.25 75.5 75.75 76.0 ... 79.25 79.5 79.75 80.0\n  * lat      (lat) float32 5.0 5.25 5.5 5.75 6.0 6.25 ... 9.0 9.25 9.5 9.75 10.0\nData variables:\n    CHL      (time, lon, lat) float32 nan nan nan nan ... 3.221 2.768 nan 1.353xarray.DatasetDimensions:time: 528lon: 21lat: 21Coordinates: (3)time(time)datetime64[ns]1979-01-31 ... 2022-12-31array(['1979-01-31T00:00:00.000000000', '1979-02-28T00:00:00.000000000',\n       '1979-03-31T00:00:00.000000000', ..., '2022-10-31T00:00:00.000000000',\n       '2022-11-30T00:00:00.000000000', '2022-12-31T00:00:00.000000000'],\n      dtype='datetime64[ns]')lon(lon)float3275.0 75.25 75.5 ... 79.5 79.75 80.0array([75.  , 75.25, 75.5 , 75.75, 76.  , 76.25, 76.5 , 76.75, 77.  , 77.25,\n       77.5 , 77.75, 78.  , 78.25, 78.5 , 78.75, 79.  , 79.25, 79.5 , 79.75,\n       80.  ], dtype=float32)lat(lat)float325.0 5.25 5.5 5.75 ... 9.5 9.75 10.0array([ 5.  ,  5.25,  5.5 ,  5.75,  6.  ,  6.25,  6.5 ,  6.75,  7.  ,  7.25,\n        7.5 ,  7.75,  8.  ,  8.25,  8.5 ,  8.75,  9.  ,  9.25,  9.5 ,  9.75,\n       10.  ], dtype=float32)Data variables: (1)CHL(time, lon, lat)float32nan nan nan nan ... 2.768 nan 1.353array([[[       nan,        nan,        nan, ...,        nan,\n                nan,        nan],\n        [       nan,        nan,        nan, ...,        nan,\n                nan,        nan],\n        [       nan,        nan,        nan, ...,        nan,\n                nan,        nan],\n        ...,\n        [       nan,        nan,        nan, ...,        nan,\n                nan,        nan],\n        [       nan,        nan,        nan, ...,        nan,\n                nan,        nan],\n        [       nan,        nan,        nan, ...,        nan,\n                nan,        nan]],\n\n       [[       nan,        nan,        nan, ...,        nan,\n                nan,        nan],\n        [       nan,        nan,        nan, ...,        nan,\n                nan,        nan],\n        [       nan,        nan,        nan, ...,        nan,\n                nan,        nan],\n...\n        [0.2705628 , 0.25798783, 0.25561604, ..., 1.4041072 ,\n         1.4939387 , 1.8776367 ],\n        [0.32376617, 0.2919578 , 0.28656372, ..., 1.7322022 ,\n         1.8477138 , 1.8143107 ],\n        [0.30853522, 0.3436496 , 0.3281598 , ..., 2.5401766 ,\n                nan, 1.3396835 ]],\n\n       [[0.23880072, 0.24270938, 0.24283718, ..., 0.20560512,\n         0.22738725, 0.3117161 ],\n        [0.2311706 , 0.2247422 , 0.23030376, ..., 0.21133746,\n         0.24978474, 0.3144625 ],\n        [0.22994642, 0.2216043 , 0.21443008, ..., 0.23380966,\n         0.27335462, 0.33697778],\n        ...,\n        [0.2801102 , 0.27076766, 0.28496662, ..., 1.4887002 ,\n         1.4666241 , 1.7220622 ],\n        [0.3205243 , 0.2954306 , 0.31341806, ..., 1.9154898 ,\n         1.42821   , 1.6312736 ],\n        [0.45734957, 0.3265505 , 0.33142075, ..., 2.7675717 ,\n                nan, 1.3527689 ]]], dtype=float32)Indexes: (3)timePandasIndexPandasIndex(DatetimeIndex(['1979-01-31', '1979-02-28', '1979-03-31', '1979-04-30',\n               '1979-05-31', '1979-06-30', '1979-07-31', '1979-08-31',\n               '1979-09-30', '1979-10-31',\n               ...\n               '2022-03-31', '2022-04-30', '2022-05-31', '2022-06-30',\n               '2022-07-31', '2022-08-31', '2022-09-30', '2022-10-31',\n               '2022-11-30', '2022-12-31'],\n              dtype='datetime64[ns]', name='time', length=528, freq='M'))lonPandasIndexPandasIndex(Index([ 75.0, 75.25,  75.5, 75.75,  76.0, 76.25,  76.5, 76.75,  77.0, 77.25,\n        77.5, 77.75,  78.0, 78.25,  78.5, 78.75,  79.0, 79.25,  79.5, 79.75,\n        80.0],\n      dtype='float32', name='lon'))latPandasIndexPandasIndex(Index([ 5.0, 5.25,  5.5, 5.75,  6.0, 6.25,  6.5, 6.75,  7.0, 7.25,  7.5, 7.75,\n        8.0, 8.25,  8.5, 8.75,  9.0, 9.25,  9.5, 9.75, 10.0],\n      dtype='float32', name='lat'))Attributes: (0)\n\n\n… and as a matter of fact, we can graph this data, too!\n\nCHL_month = ds_resampled.mean(dim=['lat', 'lon']).hvplot(label='monthly resampling').options(color='red', )\n\n\nCHL_day = ds['CHL'].sel(lat=slice(10, 5), lon=slice(75, 80)).mean(dim=['lat', 'lon']).hvplot(label='daily resampling').options(color='blue')\n\n\n(CHL_day*CHL_month).options(title='Monthly vs Daily resampling of chlorophyll-a levels', xlabel='year')",
    "crumbs": [
      "Python Tutorials",
      "Using Data",
      "IO Zarr file"
    ]
  },
  {
    "objectID": "python-notebooks/02_Xarray_hvplot.html#why-do-we-need-xarray",
    "href": "python-notebooks/02_Xarray_hvplot.html#why-do-we-need-xarray",
    "title": "03. Introduction to xarray",
    "section": "Why do we need xarray?",
    "text": "Why do we need xarray?\nAs Geoscientists, we often work with time series of data with two or more dimensions: a time series of calibrated, orthorectified satellite images; two-dimensional grids of surface air temperature from an atmospheric reanalysis; or three-dimensional (level, x, y) cubes of ocean salinity from an ocean model. These data are often provided in GeoTIFF, NetCDF or HDF format with rich and useful metadata that we want to retain, or even use in our analysis. Common analyses include calculating means, standard deviations and anomalies over time or one or more spatial dimensions (e.g. zonal means). Model output often includes multiple variables that you want to apply similar analyses to.\n\n\n\nA schematic of multi-dimensional data\n\n\nThe schematic above shows a typical data structure for multi-dimensional data. There are two data cubes, one for temperature and one for precipitation. Common coordinate variables, in this case latitude, longitude and time are associated with each variable. Each variable, including coordinate variables, will have a set of attributes: name, units, missing value, etc. The file containing the data may also have attributes: source of the data, model name coordinate reference system if the data are projected. Writing code using low-level packages such as netcdf4 and numpy to read the data, then perform analysis, and write the results to file is time consuming and prone to errors.",
    "crumbs": [
      "Python Tutorials",
      "Using Data",
      "03. Introduction to `xarray`"
    ]
  },
  {
    "objectID": "python-notebooks/02_Xarray_hvplot.html#what-is-xarray",
    "href": "python-notebooks/02_Xarray_hvplot.html#what-is-xarray",
    "title": "03. Introduction to xarray",
    "section": "What is xarray",
    "text": "What is xarray\nxarray is an open-source project and python package to work with labelled multi-dimensional arrays. It is leverages numpy, pandas, matplotlib and dask to build Dataset and DataArray objects with built-in methods to subset, analyze, interpolate, and plot multi-dimensional data. It makes working with multi-dimensional data cubes efficient and fun. It will change your life for the better. You’ll be more attractive, more interesting, and better equiped to take on lifes challenges.",
    "crumbs": [
      "Python Tutorials",
      "Using Data",
      "03. Introduction to `xarray`"
    ]
  },
  {
    "objectID": "python-notebooks/02_Xarray_hvplot.html#what-you-will-learn-from-this-tutorial",
    "href": "python-notebooks/02_Xarray_hvplot.html#what-you-will-learn-from-this-tutorial",
    "title": "03. Introduction to xarray",
    "section": "What you will learn from this tutorial",
    "text": "What you will learn from this tutorial\nIn this tutorial you will learn how to:\n\nload a netcdf file into xarray\ninterrogate the Dataset and understand the difference between DataArray and Dataset\nsubset a Dataset\ncalculate annual and monthly mean fields\ncalculate a time series of zonal means\nplot these results\n\nAs always, we’ll start by importing xarray. We’ll follow convention by giving the module the shortname xr\n\nimport xarray as xr\nxr.set_options(keep_attrs=True)\nimport hvplot.xarray\n\n\n\n\n\n\n\n\n\n\nI’m going to use one of xarray’s tutorial datasets. In this case, air temperature from the NCEP reanalysis. I’ll assign the result of the open_dataset to ds. I may change this to access a dataset directly\n\nds = xr.tutorial.open_dataset(\"air_temperature\")\n\nAs we are in an interactive environment, we can just type ds to see what we have.\n\nds\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt;\nDimensions:  (lat: 25, time: 2920, lon: 53)\nCoordinates:\n  * lat      (lat) float32 75.0 72.5 70.0 67.5 65.0 ... 25.0 22.5 20.0 17.5 15.0\n  * lon      (lon) float32 200.0 202.5 205.0 207.5 ... 322.5 325.0 327.5 330.0\n  * time     (time) datetime64[ns] 2013-01-01 ... 2014-12-31T18:00:00\nData variables:\n    air      (time, lat, lon) float32 ...\nAttributes:\n    Conventions:  COARDS\n    title:        4x daily NMC reanalysis (1948)\n    description:  Data is from NMC initialized reanalysis\\n(4x/day).  These a...\n    platform:     Model\n    references:   http://www.esrl.noaa.gov/psd/data/gridded/data.ncep.reanaly...xarray.DatasetDimensions:lat: 25time: 2920lon: 53Coordinates: (3)lat(lat)float3275.0 72.5 70.0 ... 20.0 17.5 15.0standard_name :latitudelong_name :Latitudeunits :degrees_northaxis :Yarray([75. , 72.5, 70. , 67.5, 65. , 62.5, 60. , 57.5, 55. , 52.5, 50. , 47.5,\n       45. , 42.5, 40. , 37.5, 35. , 32.5, 30. , 27.5, 25. , 22.5, 20. , 17.5,\n       15. ], dtype=float32)lon(lon)float32200.0 202.5 205.0 ... 327.5 330.0standard_name :longitudelong_name :Longitudeunits :degrees_eastaxis :Xarray([200. , 202.5, 205. , 207.5, 210. , 212.5, 215. , 217.5, 220. , 222.5,\n       225. , 227.5, 230. , 232.5, 235. , 237.5, 240. , 242.5, 245. , 247.5,\n       250. , 252.5, 255. , 257.5, 260. , 262.5, 265. , 267.5, 270. , 272.5,\n       275. , 277.5, 280. , 282.5, 285. , 287.5, 290. , 292.5, 295. , 297.5,\n       300. , 302.5, 305. , 307.5, 310. , 312.5, 315. , 317.5, 320. , 322.5,\n       325. , 327.5, 330. ], dtype=float32)time(time)datetime64[ns]2013-01-01 ... 2014-12-31T18:00:00standard_name :timelong_name :Timearray(['2013-01-01T00:00:00.000000000', '2013-01-01T06:00:00.000000000',\n       '2013-01-01T12:00:00.000000000', ..., '2014-12-31T06:00:00.000000000',\n       '2014-12-31T12:00:00.000000000', '2014-12-31T18:00:00.000000000'],\n      dtype='datetime64[ns]')Data variables: (1)air(time, lat, lon)float32...long_name :4xDaily Air temperature at sigma level 995units :degKprecision :2GRIB_id :11GRIB_name :TMPvar_desc :Air temperaturedataset :NMC Reanalysislevel_desc :Surfacestatistic :Individual Obsparent_stat :Otheractual_range :[185.16 322.1 ][3869000 values with dtype=float32]Attributes: (5)Conventions :COARDStitle :4x daily NMC reanalysis (1948)description :Data is from NMC initialized reanalysis\n(4x/day).  These are the 0.9950 sigma level values.platform :Modelreferences :http://www.esrl.noaa.gov/psd/data/gridded/data.ncep.reanalysis.html\n\n\nFirst thing to notice is that ds is an xarray.Dataset object. It has dimensions, lat, lon, and time. It also has coordinate variables with the same names as these dimensions. These coordinate variables are 1-dimensional. This is a NetCDF convention. The Dataset contains one data variable, air. This has dimensions (time, lat, lon).\nClicking on the document icon reveals attributes for each variable. Clicking on the disk icon reveals a representation of the data.\nEach of the data and coordinate variables can be accessed and examined using the variable name as a key.\n\nds.air\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray 'air' (time: 2920, lat: 25, lon: 53)&gt;\n[3869000 values with dtype=float32]\nCoordinates:\n  * lat      (lat) float32 75.0 72.5 70.0 67.5 65.0 ... 25.0 22.5 20.0 17.5 15.0\n  * lon      (lon) float32 200.0 202.5 205.0 207.5 ... 322.5 325.0 327.5 330.0\n  * time     (time) datetime64[ns] 2013-01-01 ... 2014-12-31T18:00:00\nAttributes:\n    long_name:     4xDaily Air temperature at sigma level 995\n    units:         degK\n    precision:     2\n    GRIB_id:       11\n    GRIB_name:     TMP\n    var_desc:      Air temperature\n    dataset:       NMC Reanalysis\n    level_desc:    Surface\n    statistic:     Individual Obs\n    parent_stat:   Other\n    actual_range:  [185.16 322.1 ]xarray.DataArray'air'time: 2920lat: 25lon: 53...[3869000 values with dtype=float32]Coordinates: (3)lat(lat)float3275.0 72.5 70.0 ... 20.0 17.5 15.0standard_name :latitudelong_name :Latitudeunits :degrees_northaxis :Yarray([75. , 72.5, 70. , 67.5, 65. , 62.5, 60. , 57.5, 55. , 52.5, 50. , 47.5,\n       45. , 42.5, 40. , 37.5, 35. , 32.5, 30. , 27.5, 25. , 22.5, 20. , 17.5,\n       15. ], dtype=float32)lon(lon)float32200.0 202.5 205.0 ... 327.5 330.0standard_name :longitudelong_name :Longitudeunits :degrees_eastaxis :Xarray([200. , 202.5, 205. , 207.5, 210. , 212.5, 215. , 217.5, 220. , 222.5,\n       225. , 227.5, 230. , 232.5, 235. , 237.5, 240. , 242.5, 245. , 247.5,\n       250. , 252.5, 255. , 257.5, 260. , 262.5, 265. , 267.5, 270. , 272.5,\n       275. , 277.5, 280. , 282.5, 285. , 287.5, 290. , 292.5, 295. , 297.5,\n       300. , 302.5, 305. , 307.5, 310. , 312.5, 315. , 317.5, 320. , 322.5,\n       325. , 327.5, 330. ], dtype=float32)time(time)datetime64[ns]2013-01-01 ... 2014-12-31T18:00:00standard_name :timelong_name :Timearray(['2013-01-01T00:00:00.000000000', '2013-01-01T06:00:00.000000000',\n       '2013-01-01T12:00:00.000000000', ..., '2014-12-31T06:00:00.000000000',\n       '2014-12-31T12:00:00.000000000', '2014-12-31T18:00:00.000000000'],\n      dtype='datetime64[ns]')Attributes: (11)long_name :4xDaily Air temperature at sigma level 995units :degKprecision :2GRIB_id :11GRIB_name :TMPvar_desc :Air temperaturedataset :NMC Reanalysislevel_desc :Surfacestatistic :Individual Obsparent_stat :Otheractual_range :[185.16 322.1 ]\n\n\n\nds['air']\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray 'air' (time: 2920, lat: 25, lon: 53)&gt;\n[3869000 values with dtype=float32]\nCoordinates:\n  * lat      (lat) float32 75.0 72.5 70.0 67.5 65.0 ... 25.0 22.5 20.0 17.5 15.0\n  * lon      (lon) float32 200.0 202.5 205.0 207.5 ... 322.5 325.0 327.5 330.0\n  * time     (time) datetime64[ns] 2013-01-01 ... 2014-12-31T18:00:00\nAttributes:\n    long_name:     4xDaily Air temperature at sigma level 995\n    units:         degK\n    precision:     2\n    GRIB_id:       11\n    GRIB_name:     TMP\n    var_desc:      Air temperature\n    dataset:       NMC Reanalysis\n    level_desc:    Surface\n    statistic:     Individual Obs\n    parent_stat:   Other\n    actual_range:  [185.16 322.1 ]xarray.DataArray'air'time: 2920lat: 25lon: 53...[3869000 values with dtype=float32]Coordinates: (3)lat(lat)float3275.0 72.5 70.0 ... 20.0 17.5 15.0standard_name :latitudelong_name :Latitudeunits :degrees_northaxis :Yarray([75. , 72.5, 70. , 67.5, 65. , 62.5, 60. , 57.5, 55. , 52.5, 50. , 47.5,\n       45. , 42.5, 40. , 37.5, 35. , 32.5, 30. , 27.5, 25. , 22.5, 20. , 17.5,\n       15. ], dtype=float32)lon(lon)float32200.0 202.5 205.0 ... 327.5 330.0standard_name :longitudelong_name :Longitudeunits :degrees_eastaxis :Xarray([200. , 202.5, 205. , 207.5, 210. , 212.5, 215. , 217.5, 220. , 222.5,\n       225. , 227.5, 230. , 232.5, 235. , 237.5, 240. , 242.5, 245. , 247.5,\n       250. , 252.5, 255. , 257.5, 260. , 262.5, 265. , 267.5, 270. , 272.5,\n       275. , 277.5, 280. , 282.5, 285. , 287.5, 290. , 292.5, 295. , 297.5,\n       300. , 302.5, 305. , 307.5, 310. , 312.5, 315. , 317.5, 320. , 322.5,\n       325. , 327.5, 330. ], dtype=float32)time(time)datetime64[ns]2013-01-01 ... 2014-12-31T18:00:00standard_name :timelong_name :Timearray(['2013-01-01T00:00:00.000000000', '2013-01-01T06:00:00.000000000',\n       '2013-01-01T12:00:00.000000000', ..., '2014-12-31T06:00:00.000000000',\n       '2014-12-31T12:00:00.000000000', '2014-12-31T18:00:00.000000000'],\n      dtype='datetime64[ns]')Attributes: (11)long_name :4xDaily Air temperature at sigma level 995units :degKprecision :2GRIB_id :11GRIB_name :TMPvar_desc :Air temperaturedataset :NMC Reanalysislevel_desc :Surfacestatistic :Individual Obsparent_stat :Otheractual_range :[185.16 322.1 ]\n\n\nThese are xarray.DataArray objects. This is the basic building block for xarray.\nVariables can also be accessed as attributes of ds.\n\nds.time\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray 'time' (time: 2920)&gt;\narray(['2013-01-01T00:00:00.000000000', '2013-01-01T06:00:00.000000000',\n       '2013-01-01T12:00:00.000000000', ..., '2014-12-31T06:00:00.000000000',\n       '2014-12-31T12:00:00.000000000', '2014-12-31T18:00:00.000000000'],\n      dtype='datetime64[ns]')\nCoordinates:\n  * time     (time) datetime64[ns] 2013-01-01 ... 2014-12-31T18:00:00\nAttributes:\n    standard_name:  time\n    long_name:      Timexarray.DataArray'time'time: 29202013-01-01 2013-01-01T06:00:00 ... 2014-12-31T18:00:00array(['2013-01-01T00:00:00.000000000', '2013-01-01T06:00:00.000000000',\n       '2013-01-01T12:00:00.000000000', ..., '2014-12-31T06:00:00.000000000',\n       '2014-12-31T12:00:00.000000000', '2014-12-31T18:00:00.000000000'],\n      dtype='datetime64[ns]')Coordinates: (1)time(time)datetime64[ns]2013-01-01 ... 2014-12-31T18:00:00standard_name :timelong_name :Timearray(['2013-01-01T00:00:00.000000000', '2013-01-01T06:00:00.000000000',\n       '2013-01-01T12:00:00.000000000', ..., '2014-12-31T06:00:00.000000000',\n       '2014-12-31T12:00:00.000000000', '2014-12-31T18:00:00.000000000'],\n      dtype='datetime64[ns]')Attributes: (2)standard_name :timelong_name :Time\n\n\nA major difference between accessing a variable as an attribute versus using a key is that the attribute is read-only but the key method can be used to update the variable. For example, if I want to convert the units of air from Kelvin to degrees Celsius.\n\nds['air'] = ds.air - 273.15\n\nThis approach can also be used to add new variables\n\nds['air_kelvin'] = ds.air + 273.15\n\nIt is helpful to update attributes such as units, this saves time, confusion and mistakes, especially when you save the dataset.\n\nds['air'].attrs['units'] = 'degC'\n\n\nds\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt;\nDimensions:     (lat: 25, time: 2920, lon: 53)\nCoordinates:\n  * lat         (lat) float32 75.0 72.5 70.0 67.5 65.0 ... 22.5 20.0 17.5 15.0\n  * lon         (lon) float32 200.0 202.5 205.0 207.5 ... 325.0 327.5 330.0\n  * time        (time) datetime64[ns] 2013-01-01 ... 2014-12-31T18:00:00\nData variables:\n    air         (time, lat, lon) float32 -31.95 -30.65 -29.65 ... 23.04 22.54\n    air_kelvin  (time, lat, lon) float32 241.2 242.5 243.5 ... 296.5 296.2 295.7\nAttributes:\n    Conventions:  COARDS\n    title:        4x daily NMC reanalysis (1948)\n    description:  Data is from NMC initialized reanalysis\\n(4x/day).  These a...\n    platform:     Model\n    references:   http://www.esrl.noaa.gov/psd/data/gridded/data.ncep.reanaly...xarray.DatasetDimensions:lat: 25time: 2920lon: 53Coordinates: (3)lat(lat)float3275.0 72.5 70.0 ... 20.0 17.5 15.0standard_name :latitudelong_name :Latitudeunits :degrees_northaxis :Yarray([75. , 72.5, 70. , 67.5, 65. , 62.5, 60. , 57.5, 55. , 52.5, 50. , 47.5,\n       45. , 42.5, 40. , 37.5, 35. , 32.5, 30. , 27.5, 25. , 22.5, 20. , 17.5,\n       15. ], dtype=float32)lon(lon)float32200.0 202.5 205.0 ... 327.5 330.0standard_name :longitudelong_name :Longitudeunits :degrees_eastaxis :Xarray([200. , 202.5, 205. , 207.5, 210. , 212.5, 215. , 217.5, 220. , 222.5,\n       225. , 227.5, 230. , 232.5, 235. , 237.5, 240. , 242.5, 245. , 247.5,\n       250. , 252.5, 255. , 257.5, 260. , 262.5, 265. , 267.5, 270. , 272.5,\n       275. , 277.5, 280. , 282.5, 285. , 287.5, 290. , 292.5, 295. , 297.5,\n       300. , 302.5, 305. , 307.5, 310. , 312.5, 315. , 317.5, 320. , 322.5,\n       325. , 327.5, 330. ], dtype=float32)time(time)datetime64[ns]2013-01-01 ... 2014-12-31T18:00:00standard_name :timelong_name :Timearray(['2013-01-01T00:00:00.000000000', '2013-01-01T06:00:00.000000000',\n       '2013-01-01T12:00:00.000000000', ..., '2014-12-31T06:00:00.000000000',\n       '2014-12-31T12:00:00.000000000', '2014-12-31T18:00:00.000000000'],\n      dtype='datetime64[ns]')Data variables: (2)air(time, lat, lon)float32-31.95 -30.65 ... 23.04 22.54long_name :4xDaily Air temperature at sigma level 995units :degCprecision :2GRIB_id :11GRIB_name :TMPvar_desc :Air temperaturedataset :NMC Reanalysislevel_desc :Surfacestatistic :Individual Obsparent_stat :Otheractual_range :[185.16 322.1 ]array([[[-31.949997, -30.649994, -29.649994, ..., -40.350006,\n         -37.649994, -34.550003],\n        [-29.350006, -28.649994, -28.449997, ..., -40.350006,\n         -37.850006, -33.850006],\n        [-23.149994, -23.350006, -24.259995, ..., -39.949997,\n         -36.759995, -31.449997],\n        ...,\n        [ 23.450012,  23.049988,  23.25    , ...,  22.25    ,\n          21.950012,  21.549988],\n        [ 22.75    ,  23.049988,  23.640015, ...,  22.75    ,\n          22.75    ,  22.049988],\n        [ 23.140015,  23.640015,  23.950012, ...,  23.75    ,\n          23.640015,  23.450012]],\n\n       [[-31.050003, -30.449997, -30.050003, ..., -41.149994,\n         -39.550003, -37.350006],\n        [-29.550003, -29.050003, -28.949997, ..., -42.149994,\n         -40.649994, -37.449997],\n        [-19.949997, -20.259995, -21.050003, ..., -42.350006,\n         -39.759995, -34.649994],\n...\n        [ 20.540009,  20.73999 ,  22.23999 , ...,  21.940002,\n          21.540009,  21.140015],\n        [ 23.140015,  24.040009,  24.440002, ...,  22.140015,\n          21.940002,  21.23999 ],\n        [ 24.640015,  25.23999 ,  25.339996, ...,  22.540009,\n          22.339996,  22.040009]],\n\n       [[-28.059998, -28.86    , -29.86    , ..., -31.460007,\n         -31.660004, -31.36    ],\n        [-23.259995, -23.86    , -24.759995, ..., -33.559998,\n         -32.86    , -31.460007],\n        [-10.160004, -10.959991, -11.76001 , ..., -33.259995,\n         -30.559998, -26.86    ],\n        ...,\n        [ 20.640015,  20.540009,  21.940002, ...,  22.140015,\n          21.940002,  21.540009],\n        [ 22.940002,  23.73999 ,  24.040009, ...,  22.540009,\n          22.540009,  22.040009],\n        [ 24.540009,  24.940002,  24.940002, ...,  23.339996,\n          23.040009,  22.540009]]], dtype=float32)air_kelvin(time, lat, lon)float32241.2 242.5 243.5 ... 296.2 295.7long_name :4xDaily Air temperature at sigma level 995units :degKprecision :2GRIB_id :11GRIB_name :TMPvar_desc :Air temperaturedataset :NMC Reanalysislevel_desc :Surfacestatistic :Individual Obsparent_stat :Otheractual_range :[185.16 322.1 ]array([[[241.2    , 242.5    , 243.5    , ..., 232.79999, 235.5    ,\n         238.59999],\n        [243.79999, 244.5    , 244.7    , ..., 232.79999, 235.29999,\n         239.29999],\n        [250.     , 249.79999, 248.89   , ..., 233.2    , 236.39   ,\n         241.7    ],\n        ...,\n        [296.6    , 296.19998, 296.4    , ..., 295.4    , 295.1    ,\n         294.69998],\n        [295.9    , 296.19998, 296.79   , ..., 295.9    , 295.9    ,\n         295.19998],\n        [296.29   , 296.79   , 297.1    , ..., 296.9    , 296.79   ,\n         296.6    ]],\n\n       [[242.09999, 242.7    , 243.09999, ..., 232.     , 233.59999,\n         235.79999],\n        [243.59999, 244.09999, 244.2    , ..., 231.     , 232.5    ,\n         235.7    ],\n        [253.2    , 252.89   , 252.09999, ..., 230.79999, 233.39   ,\n         238.5    ],\n...\n        [293.69   , 293.88998, 295.38998, ..., 295.09   , 294.69   ,\n         294.29   ],\n        [296.29   , 297.19   , 297.59   , ..., 295.29   , 295.09   ,\n         294.38998],\n        [297.79   , 298.38998, 298.49   , ..., 295.69   , 295.49   ,\n         295.19   ]],\n\n       [[245.09   , 244.29   , 243.29   , ..., 241.68999, 241.48999,\n         241.79   ],\n        [249.89   , 249.29   , 248.39   , ..., 239.59   , 240.29   ,\n         241.68999],\n        [262.99   , 262.19   , 261.38998, ..., 239.89   , 242.59   ,\n         246.29   ],\n        ...,\n        [293.79   , 293.69   , 295.09   , ..., 295.29   , 295.09   ,\n         294.69   ],\n        [296.09   , 296.88998, 297.19   , ..., 295.69   , 295.69   ,\n         295.19   ],\n        [297.69   , 298.09   , 298.09   , ..., 296.49   , 296.19   ,\n         295.69   ]]], dtype=float32)Attributes: (5)Conventions :COARDStitle :4x daily NMC reanalysis (1948)description :Data is from NMC initialized reanalysis\n(4x/day).  These are the 0.9950 sigma level values.platform :Modelreferences :http://www.esrl.noaa.gov/psd/data/gridded/data.ncep.reanalysis.html",
    "crumbs": [
      "Python Tutorials",
      "Using Data",
      "03. Introduction to `xarray`"
    ]
  },
  {
    "objectID": "python-notebooks/02_Xarray_hvplot.html#subsetting-and-indexing",
    "href": "python-notebooks/02_Xarray_hvplot.html#subsetting-and-indexing",
    "title": "03. Introduction to xarray",
    "section": "Subsetting and Indexing",
    "text": "Subsetting and Indexing\nSubsetting and indexing methods depend on whether you are working with a Dataset or DataArray. A DataArray can be accessed using positional indexing just like a numpy array. To access the temperature field for the first time step, you do the following.\n\nds['air'][0,:,:]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray 'air' (lat: 25, lon: 53)&gt;\narray([[-31.949997, -30.649994, -29.649994, ..., -40.350006, -37.649994,\n        -34.550003],\n       [-29.350006, -28.649994, -28.449997, ..., -40.350006, -37.850006,\n        -33.850006],\n       [-23.149994, -23.350006, -24.259995, ..., -39.949997, -36.759995,\n        -31.449997],\n       ...,\n       [ 23.450012,  23.049988,  23.25    , ...,  22.25    ,  21.950012,\n         21.549988],\n       [ 22.75    ,  23.049988,  23.640015, ...,  22.75    ,  22.75    ,\n         22.049988],\n       [ 23.140015,  23.640015,  23.950012, ...,  23.75    ,  23.640015,\n         23.450012]], dtype=float32)\nCoordinates:\n  * lat      (lat) float32 75.0 72.5 70.0 67.5 65.0 ... 25.0 22.5 20.0 17.5 15.0\n  * lon      (lon) float32 200.0 202.5 205.0 207.5 ... 322.5 325.0 327.5 330.0\n    time     datetime64[ns] 2013-01-01\nAttributes:\n    long_name:     4xDaily Air temperature at sigma level 995\n    units:         degC\n    precision:     2\n    GRIB_id:       11\n    GRIB_name:     TMP\n    var_desc:      Air temperature\n    dataset:       NMC Reanalysis\n    level_desc:    Surface\n    statistic:     Individual Obs\n    parent_stat:   Other\n    actual_range:  [185.16 322.1 ]xarray.DataArray'air'lat: 25lon: 53-31.95 -30.65 -29.65 -29.15 -29.05 ... 24.64 24.45 23.75 23.64 23.45array([[-31.949997, -30.649994, -29.649994, ..., -40.350006, -37.649994,\n        -34.550003],\n       [-29.350006, -28.649994, -28.449997, ..., -40.350006, -37.850006,\n        -33.850006],\n       [-23.149994, -23.350006, -24.259995, ..., -39.949997, -36.759995,\n        -31.449997],\n       ...,\n       [ 23.450012,  23.049988,  23.25    , ...,  22.25    ,  21.950012,\n         21.549988],\n       [ 22.75    ,  23.049988,  23.640015, ...,  22.75    ,  22.75    ,\n         22.049988],\n       [ 23.140015,  23.640015,  23.950012, ...,  23.75    ,  23.640015,\n         23.450012]], dtype=float32)Coordinates: (3)lat(lat)float3275.0 72.5 70.0 ... 20.0 17.5 15.0standard_name :latitudelong_name :Latitudeunits :degrees_northaxis :Yarray([75. , 72.5, 70. , 67.5, 65. , 62.5, 60. , 57.5, 55. , 52.5, 50. , 47.5,\n       45. , 42.5, 40. , 37.5, 35. , 32.5, 30. , 27.5, 25. , 22.5, 20. , 17.5,\n       15. ], dtype=float32)lon(lon)float32200.0 202.5 205.0 ... 327.5 330.0standard_name :longitudelong_name :Longitudeunits :degrees_eastaxis :Xarray([200. , 202.5, 205. , 207.5, 210. , 212.5, 215. , 217.5, 220. , 222.5,\n       225. , 227.5, 230. , 232.5, 235. , 237.5, 240. , 242.5, 245. , 247.5,\n       250. , 252.5, 255. , 257.5, 260. , 262.5, 265. , 267.5, 270. , 272.5,\n       275. , 277.5, 280. , 282.5, 285. , 287.5, 290. , 292.5, 295. , 297.5,\n       300. , 302.5, 305. , 307.5, 310. , 312.5, 315. , 317.5, 320. , 322.5,\n       325. , 327.5, 330. ], dtype=float32)time()datetime64[ns]2013-01-01standard_name :timelong_name :Timearray('2013-01-01T00:00:00.000000000', dtype='datetime64[ns]')Attributes: (11)long_name :4xDaily Air temperature at sigma level 995units :degCprecision :2GRIB_id :11GRIB_name :TMPvar_desc :Air temperaturedataset :NMC Reanalysislevel_desc :Surfacestatistic :Individual Obsparent_stat :Otheractual_range :[185.16 322.1 ]\n\n\nNote this returns a DataArray with coordinates but not attributes.\nHowever, the real power is being able to access variables using coordinate variables. I can get the same subset using the following. (It’s also more explicit about what is being selected and robust in case I modify the DataArray and expect the same output.)\n\nds['air'].sel(time='2013-01-01').time\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray 'time' (time: 4)&gt;\narray(['2013-01-01T00:00:00.000000000', '2013-01-01T06:00:00.000000000',\n       '2013-01-01T12:00:00.000000000', '2013-01-01T18:00:00.000000000'],\n      dtype='datetime64[ns]')\nCoordinates:\n  * time     (time) datetime64[ns] 2013-01-01 ... 2013-01-01T18:00:00\nAttributes:\n    standard_name:  time\n    long_name:      Timexarray.DataArray'time'time: 42013-01-01 2013-01-01T06:00:00 2013-01-01T12:00:00 2013-01-01T18:00:00array(['2013-01-01T00:00:00.000000000', '2013-01-01T06:00:00.000000000',\n       '2013-01-01T12:00:00.000000000', '2013-01-01T18:00:00.000000000'],\n      dtype='datetime64[ns]')Coordinates: (1)time(time)datetime64[ns]2013-01-01 ... 2013-01-01T18:00:00standard_name :timelong_name :Timearray(['2013-01-01T00:00:00.000000000', '2013-01-01T06:00:00.000000000',\n       '2013-01-01T12:00:00.000000000', '2013-01-01T18:00:00.000000000'],\n      dtype='datetime64[ns]')Attributes: (2)standard_name :timelong_name :Time\n\n\n\nds.air.sel(time='2013-01-01')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray 'air' (time: 4, lat: 25, lon: 53)&gt;\narray([[[-31.949997, -30.649994, -29.649994, ..., -40.350006,\n         -37.649994, -34.550003],\n        [-29.350006, -28.649994, -28.449997, ..., -40.350006,\n         -37.850006, -33.850006],\n        [-23.149994, -23.350006, -24.259995, ..., -39.949997,\n         -36.759995, -31.449997],\n        ...,\n        [ 23.450012,  23.049988,  23.25    , ...,  22.25    ,\n          21.950012,  21.549988],\n        [ 22.75    ,  23.049988,  23.640015, ...,  22.75    ,\n          22.75    ,  22.049988],\n        [ 23.140015,  23.640015,  23.950012, ...,  23.75    ,\n          23.640015,  23.450012]],\n\n       [[-31.050003, -30.449997, -30.050003, ..., -41.149994,\n         -39.550003, -37.350006],\n        [-29.550003, -29.050003, -28.949997, ..., -42.149994,\n         -40.649994, -37.449997],\n        [-19.949997, -20.259995, -21.050003, ..., -42.350006,\n         -39.759995, -34.649994],\n...\n        [ 22.450012,  22.25    ,  22.25    , ...,  23.140015,\n          22.140015,  21.850006],\n        [ 23.049988,  23.350006,  23.140015, ...,  23.25    ,\n          22.850006,  22.450012],\n        [ 23.25    ,  23.140015,  23.25    , ...,  23.850006,\n          23.850006,  23.640015]],\n\n       [[-31.259995, -31.350006, -31.350006, ..., -38.759995,\n         -37.649994, -35.550003],\n        [-26.850006, -27.850006, -28.949997, ..., -42.259995,\n         -41.649994, -38.649994],\n        [-16.549988, -18.449997, -21.050003, ..., -42.449997,\n         -41.350006, -37.050003],\n        ...,\n        [ 23.450012,  23.25    ,  22.850006, ...,  23.350006,\n          22.640015,  22.140015],\n        [ 23.850006,  24.350006,  23.950012, ...,  23.640015,\n          23.450012,  23.140015],\n        [ 24.350006,  24.549988,  24.350006, ...,  24.640015,\n          24.850006,  24.75    ]]], dtype=float32)\nCoordinates:\n  * lat      (lat) float32 75.0 72.5 70.0 67.5 65.0 ... 25.0 22.5 20.0 17.5 15.0\n  * lon      (lon) float32 200.0 202.5 205.0 207.5 ... 322.5 325.0 327.5 330.0\n  * time     (time) datetime64[ns] 2013-01-01 ... 2013-01-01T18:00:00\nAttributes:\n    long_name:     4xDaily Air temperature at sigma level 995\n    units:         degC\n    precision:     2\n    GRIB_id:       11\n    GRIB_name:     TMP\n    var_desc:      Air temperature\n    dataset:       NMC Reanalysis\n    level_desc:    Surface\n    statistic:     Individual Obs\n    parent_stat:   Other\n    actual_range:  [185.16 322.1 ]xarray.DataArray'air'time: 4lat: 25lon: 53-31.95 -30.65 -29.65 -29.15 -29.05 ... 25.45 25.05 24.64 24.85 24.75array([[[-31.949997, -30.649994, -29.649994, ..., -40.350006,\n         -37.649994, -34.550003],\n        [-29.350006, -28.649994, -28.449997, ..., -40.350006,\n         -37.850006, -33.850006],\n        [-23.149994, -23.350006, -24.259995, ..., -39.949997,\n         -36.759995, -31.449997],\n        ...,\n        [ 23.450012,  23.049988,  23.25    , ...,  22.25    ,\n          21.950012,  21.549988],\n        [ 22.75    ,  23.049988,  23.640015, ...,  22.75    ,\n          22.75    ,  22.049988],\n        [ 23.140015,  23.640015,  23.950012, ...,  23.75    ,\n          23.640015,  23.450012]],\n\n       [[-31.050003, -30.449997, -30.050003, ..., -41.149994,\n         -39.550003, -37.350006],\n        [-29.550003, -29.050003, -28.949997, ..., -42.149994,\n         -40.649994, -37.449997],\n        [-19.949997, -20.259995, -21.050003, ..., -42.350006,\n         -39.759995, -34.649994],\n...\n        [ 22.450012,  22.25    ,  22.25    , ...,  23.140015,\n          22.140015,  21.850006],\n        [ 23.049988,  23.350006,  23.140015, ...,  23.25    ,\n          22.850006,  22.450012],\n        [ 23.25    ,  23.140015,  23.25    , ...,  23.850006,\n          23.850006,  23.640015]],\n\n       [[-31.259995, -31.350006, -31.350006, ..., -38.759995,\n         -37.649994, -35.550003],\n        [-26.850006, -27.850006, -28.949997, ..., -42.259995,\n         -41.649994, -38.649994],\n        [-16.549988, -18.449997, -21.050003, ..., -42.449997,\n         -41.350006, -37.050003],\n        ...,\n        [ 23.450012,  23.25    ,  22.850006, ...,  23.350006,\n          22.640015,  22.140015],\n        [ 23.850006,  24.350006,  23.950012, ...,  23.640015,\n          23.450012,  23.140015],\n        [ 24.350006,  24.549988,  24.350006, ...,  24.640015,\n          24.850006,  24.75    ]]], dtype=float32)Coordinates: (3)lat(lat)float3275.0 72.5 70.0 ... 20.0 17.5 15.0standard_name :latitudelong_name :Latitudeunits :degrees_northaxis :Yarray([75. , 72.5, 70. , 67.5, 65. , 62.5, 60. , 57.5, 55. , 52.5, 50. , 47.5,\n       45. , 42.5, 40. , 37.5, 35. , 32.5, 30. , 27.5, 25. , 22.5, 20. , 17.5,\n       15. ], dtype=float32)lon(lon)float32200.0 202.5 205.0 ... 327.5 330.0standard_name :longitudelong_name :Longitudeunits :degrees_eastaxis :Xarray([200. , 202.5, 205. , 207.5, 210. , 212.5, 215. , 217.5, 220. , 222.5,\n       225. , 227.5, 230. , 232.5, 235. , 237.5, 240. , 242.5, 245. , 247.5,\n       250. , 252.5, 255. , 257.5, 260. , 262.5, 265. , 267.5, 270. , 272.5,\n       275. , 277.5, 280. , 282.5, 285. , 287.5, 290. , 292.5, 295. , 297.5,\n       300. , 302.5, 305. , 307.5, 310. , 312.5, 315. , 317.5, 320. , 322.5,\n       325. , 327.5, 330. ], dtype=float32)time(time)datetime64[ns]2013-01-01 ... 2013-01-01T18:00:00standard_name :timelong_name :Timearray(['2013-01-01T00:00:00.000000000', '2013-01-01T06:00:00.000000000',\n       '2013-01-01T12:00:00.000000000', '2013-01-01T18:00:00.000000000'],\n      dtype='datetime64[ns]')Attributes: (11)long_name :4xDaily Air temperature at sigma level 995units :degCprecision :2GRIB_id :11GRIB_name :TMPvar_desc :Air temperaturedataset :NMC Reanalysislevel_desc :Surfacestatistic :Individual Obsparent_stat :Otheractual_range :[185.16 322.1 ]\n\n\nI can also do slices. I’ll extract temperatures for the state of Colorado. The bounding box for the state is [-109 E, -102 E, 37 N, 41 N].\nIn the code below, pay attention to both the order of the coordinates and the range of values. The first value of the lat coordinate variable is 41 N, the second value is 37 N. Unfortunately, xarray expects slices of coordinates to be in the same order as the coordinates. Note lon is 0 to 360 not -180 to 180, and I let python calculate it for me within the slice.\n\nds.air.sel(lat=slice(41.,37.), lon=slice(360-109,360-102))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray 'air' (time: 2920, lat: 2, lon: 3)&gt;\narray([[[-10.049988 ,  -9.25     ,  -8.75     ],\n        [ -6.25     ,  -6.549988 ,  -6.3599854]],\n\n       [[-18.149994 , -14.950012 ,  -9.950012 ],\n        [-13.649994 , -11.049988 ,  -7.25     ]],\n\n       [[-20.449997 , -18.649994 , -13.359985 ],\n        [-19.350006 , -16.950012 , -11.25     ]],\n\n       ...,\n\n       [[-24.460007 , -28.259995 , -25.759995 ],\n        [-16.959991 , -24.059998 , -24.059998 ]],\n\n       [[-24.36     , -26.160004 , -23.460007 ],\n        [-15.959991 , -22.86     , -22.960007 ]],\n\n       [[-17.559998 , -15.359985 , -13.660004 ],\n        [-13.76001  , -15.959991 , -14.459991 ]]], dtype=float32)\nCoordinates:\n  * lat      (lat) float32 40.0 37.5\n  * lon      (lon) float32 252.5 255.0 257.5\n  * time     (time) datetime64[ns] 2013-01-01 ... 2014-12-31T18:00:00\nAttributes:\n    long_name:     4xDaily Air temperature at sigma level 995\n    units:         degC\n    precision:     2\n    GRIB_id:       11\n    GRIB_name:     TMP\n    var_desc:      Air temperature\n    dataset:       NMC Reanalysis\n    level_desc:    Surface\n    statistic:     Individual Obs\n    parent_stat:   Other\n    actual_range:  [185.16 322.1 ]xarray.DataArray'air'time: 2920lat: 2lon: 3-10.05 -9.25 -8.75 -6.25 -6.55 ... -15.36 -13.66 -13.76 -15.96 -14.46array([[[-10.049988 ,  -9.25     ,  -8.75     ],\n        [ -6.25     ,  -6.549988 ,  -6.3599854]],\n\n       [[-18.149994 , -14.950012 ,  -9.950012 ],\n        [-13.649994 , -11.049988 ,  -7.25     ]],\n\n       [[-20.449997 , -18.649994 , -13.359985 ],\n        [-19.350006 , -16.950012 , -11.25     ]],\n\n       ...,\n\n       [[-24.460007 , -28.259995 , -25.759995 ],\n        [-16.959991 , -24.059998 , -24.059998 ]],\n\n       [[-24.36     , -26.160004 , -23.460007 ],\n        [-15.959991 , -22.86     , -22.960007 ]],\n\n       [[-17.559998 , -15.359985 , -13.660004 ],\n        [-13.76001  , -15.959991 , -14.459991 ]]], dtype=float32)Coordinates: (3)lat(lat)float3240.0 37.5standard_name :latitudelong_name :Latitudeunits :degrees_northaxis :Yarray([40. , 37.5], dtype=float32)lon(lon)float32252.5 255.0 257.5standard_name :longitudelong_name :Longitudeunits :degrees_eastaxis :Xarray([252.5, 255. , 257.5], dtype=float32)time(time)datetime64[ns]2013-01-01 ... 2014-12-31T18:00:00standard_name :timelong_name :Timearray(['2013-01-01T00:00:00.000000000', '2013-01-01T06:00:00.000000000',\n       '2013-01-01T12:00:00.000000000', ..., '2014-12-31T06:00:00.000000000',\n       '2014-12-31T12:00:00.000000000', '2014-12-31T18:00:00.000000000'],\n      dtype='datetime64[ns]')Attributes: (11)long_name :4xDaily Air temperature at sigma level 995units :degCprecision :2GRIB_id :11GRIB_name :TMPvar_desc :Air temperaturedataset :NMC Reanalysislevel_desc :Surfacestatistic :Individual Obsparent_stat :Otheractual_range :[185.16 322.1 ]\n\n\nWhat if we want temperature for a point, for example Denver, CO (39.72510678889283 N, -104.98785545855408 E). xarray can handle this! If we just want data from the nearest grid point, we can use sel and specify the method as “nearest”.\n\ndenver_lat, denver_lon = 39.72510678889283, -104.98785545855408\n\n\nds.air.sel(lat=denver_lat, lon=360+denver_lon, method='nearest').hvplot()\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\nIf we want to interpolate, we can use interp(). In this case I use linear or bilinear interpolation.\ninterp() can also be used to resample data to a new grid and even reproject data\n\nds.air.interp(lat=denver_lat, lon=360+denver_lon, method='linear')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray 'air' (time: 2920)&gt;\narray([ -8.95085077, -14.49752791, -18.43715163, ..., -27.78736503,\n       -25.78552388, -15.41780902])\nCoordinates:\n  * time     (time) datetime64[ns] 2013-01-01 ... 2014-12-31T18:00:00\n    lat      float64 39.73\n    lon      float64 255.0\nAttributes:\n    long_name:     4xDaily Air temperature at sigma level 995\n    units:         degC\n    precision:     2\n    GRIB_id:       11\n    GRIB_name:     TMP\n    var_desc:      Air temperature\n    dataset:       NMC Reanalysis\n    level_desc:    Surface\n    statistic:     Individual Obs\n    parent_stat:   Other\n    actual_range:  [185.16 322.1 ]xarray.DataArray'air'time: 2920-8.951 -14.5 -18.44 -11.33 -8.942 ... -22.4 -27.79 -25.79 -15.42array([ -8.95085077, -14.49752791, -18.43715163, ..., -27.78736503,\n       -25.78552388, -15.41780902])Coordinates: (3)time(time)datetime64[ns]2013-01-01 ... 2014-12-31T18:00:00standard_name :timelong_name :Timearray(['2013-01-01T00:00:00.000000000', '2013-01-01T06:00:00.000000000',\n       '2013-01-01T12:00:00.000000000', ..., '2014-12-31T06:00:00.000000000',\n       '2014-12-31T12:00:00.000000000', '2014-12-31T18:00:00.000000000'],\n      dtype='datetime64[ns]')lat()float6439.73standard_name :latitudelong_name :Latitudeunits :degrees_northaxis :Yarray(39.72510679)lon()float64255.0standard_name :longitudelong_name :Longitudeunits :degrees_eastaxis :Xarray(255.01214454)Attributes: (11)long_name :4xDaily Air temperature at sigma level 995units :degCprecision :2GRIB_id :11GRIB_name :TMPvar_desc :Air temperaturedataset :NMC Reanalysislevel_desc :Surfacestatistic :Individual Obsparent_stat :Otheractual_range :[185.16 322.1 ]\n\n\nsel() and interp() can also be used on Dataset objects.\n\nds.sel(lat=slice(41,37), lon=slice(360-109,360-102))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt;\nDimensions:     (lat: 2, time: 2920, lon: 3)\nCoordinates:\n  * lat         (lat) float32 40.0 37.5\n  * lon         (lon) float32 252.5 255.0 257.5\n  * time        (time) datetime64[ns] 2013-01-01 ... 2014-12-31T18:00:00\nData variables:\n    air         (time, lat, lon) float32 -10.05 -9.25 -8.75 ... -15.96 -14.46\n    air_kelvin  (time, lat, lon) float32 263.1 263.9 264.4 ... 259.4 257.2 258.7\nAttributes:\n    Conventions:  COARDS\n    title:        4x daily NMC reanalysis (1948)\n    description:  Data is from NMC initialized reanalysis\\n(4x/day).  These a...\n    platform:     Model\n    references:   http://www.esrl.noaa.gov/psd/data/gridded/data.ncep.reanaly...xarray.DatasetDimensions:lat: 2time: 2920lon: 3Coordinates: (3)lat(lat)float3240.0 37.5standard_name :latitudelong_name :Latitudeunits :degrees_northaxis :Yarray([40. , 37.5], dtype=float32)lon(lon)float32252.5 255.0 257.5standard_name :longitudelong_name :Longitudeunits :degrees_eastaxis :Xarray([252.5, 255. , 257.5], dtype=float32)time(time)datetime64[ns]2013-01-01 ... 2014-12-31T18:00:00standard_name :timelong_name :Timearray(['2013-01-01T00:00:00.000000000', '2013-01-01T06:00:00.000000000',\n       '2013-01-01T12:00:00.000000000', ..., '2014-12-31T06:00:00.000000000',\n       '2014-12-31T12:00:00.000000000', '2014-12-31T18:00:00.000000000'],\n      dtype='datetime64[ns]')Data variables: (2)air(time, lat, lon)float32-10.05 -9.25 ... -15.96 -14.46long_name :4xDaily Air temperature at sigma level 995units :degCprecision :2GRIB_id :11GRIB_name :TMPvar_desc :Air temperaturedataset :NMC Reanalysislevel_desc :Surfacestatistic :Individual Obsparent_stat :Otheractual_range :[185.16 322.1 ]array([[[-10.049988 ,  -9.25     ,  -8.75     ],\n        [ -6.25     ,  -6.549988 ,  -6.3599854]],\n\n       [[-18.149994 , -14.950012 ,  -9.950012 ],\n        [-13.649994 , -11.049988 ,  -7.25     ]],\n\n       [[-20.449997 , -18.649994 , -13.359985 ],\n        [-19.350006 , -16.950012 , -11.25     ]],\n\n       ...,\n\n       [[-24.460007 , -28.259995 , -25.759995 ],\n        [-16.959991 , -24.059998 , -24.059998 ]],\n\n       [[-24.36     , -26.160004 , -23.460007 ],\n        [-15.959991 , -22.86     , -22.960007 ]],\n\n       [[-17.559998 , -15.359985 , -13.660004 ],\n        [-13.76001  , -15.959991 , -14.459991 ]]], dtype=float32)air_kelvin(time, lat, lon)float32263.1 263.9 264.4 ... 257.2 258.7long_name :4xDaily Air temperature at sigma level 995units :degKprecision :2GRIB_id :11GRIB_name :TMPvar_desc :Air temperaturedataset :NMC Reanalysislevel_desc :Surfacestatistic :Individual Obsparent_stat :Otheractual_range :[185.16 322.1 ]array([[[263.1    , 263.9    , 264.4    ],\n        [266.9    , 266.6    , 266.79   ]],\n\n       [[255.     , 258.19998, 263.19998],\n        [259.5    , 262.1    , 265.9    ]],\n\n       [[252.7    , 254.5    , 259.79   ],\n        [253.79999, 256.19998, 261.9    ]],\n\n       ...,\n\n       [[248.68999, 244.89   , 247.39   ],\n        [256.19   , 249.09   , 249.09   ]],\n\n       [[248.79   , 246.98999, 249.68999],\n        [257.19   , 250.29   , 250.18999]],\n\n       [[255.59   , 257.79   , 259.49   ],\n        [259.38998, 257.19   , 258.69   ]]], dtype=float32)Attributes: (5)Conventions :COARDStitle :4x daily NMC reanalysis (1948)description :Data is from NMC initialized reanalysis\n(4x/day).  These are the 0.9950 sigma level values.platform :Modelreferences :http://www.esrl.noaa.gov/psd/data/gridded/data.ncep.reanalysis.html\n\n\n\nds.interp(lat=denver_lat, lon=360+denver_lon, method='linear')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt;\nDimensions:     (time: 2920)\nCoordinates:\n  * time        (time) datetime64[ns] 2013-01-01 ... 2014-12-31T18:00:00\n    lat         float64 39.73\n    lon         float64 255.0\nData variables:\n    air         (time) float64 -8.951 -14.5 -18.44 ... -27.79 -25.79 -15.42\n    air_kelvin  (time) float64 264.2 258.7 254.7 261.8 ... 245.4 247.4 257.7\nAttributes:\n    Conventions:  COARDS\n    title:        4x daily NMC reanalysis (1948)\n    description:  Data is from NMC initialized reanalysis\\n(4x/day).  These a...\n    platform:     Model\n    references:   http://www.esrl.noaa.gov/psd/data/gridded/data.ncep.reanaly...xarray.DatasetDimensions:time: 2920Coordinates: (3)time(time)datetime64[ns]2013-01-01 ... 2014-12-31T18:00:00standard_name :timelong_name :Timearray(['2013-01-01T00:00:00.000000000', '2013-01-01T06:00:00.000000000',\n       '2013-01-01T12:00:00.000000000', ..., '2014-12-31T06:00:00.000000000',\n       '2014-12-31T12:00:00.000000000', '2014-12-31T18:00:00.000000000'],\n      dtype='datetime64[ns]')lat()float6439.73standard_name :latitudelong_name :Latitudeunits :degrees_northaxis :Yarray(39.72510679)lon()float64255.0standard_name :longitudelong_name :Longitudeunits :degrees_eastaxis :Xarray(255.01214454)Data variables: (2)air(time)float64-8.951 -14.5 ... -25.79 -15.42long_name :4xDaily Air temperature at sigma level 995units :degCprecision :2GRIB_id :11GRIB_name :TMPvar_desc :Air temperaturedataset :NMC Reanalysislevel_desc :Surfacestatistic :Individual Obsparent_stat :Otheractual_range :[185.16 322.1 ]array([ -8.95085077, -14.49752791, -18.43715163, ..., -27.78736503,\n       -25.78552388, -15.41780902])air_kelvin(time)float64264.2 258.7 254.7 ... 247.4 257.7long_name :4xDaily Air temperature at sigma level 995units :degKprecision :2GRIB_id :11GRIB_name :TMPvar_desc :Air temperaturedataset :NMC Reanalysislevel_desc :Surfacestatistic :Individual Obsparent_stat :Otheractual_range :[185.16 322.1 ]array([264.19914312, 258.65246598, 254.71284227, ..., 245.36262886,\n       247.36447002, 257.73218487])Attributes: (5)Conventions :COARDStitle :4x daily NMC reanalysis (1948)description :Data is from NMC initialized reanalysis\n(4x/day).  These are the 0.9950 sigma level values.platform :Modelreferences :http://www.esrl.noaa.gov/psd/data/gridded/data.ncep.reanalysis.html",
    "crumbs": [
      "Python Tutorials",
      "Using Data",
      "03. Introduction to `xarray`"
    ]
  },
  {
    "objectID": "python-notebooks/02_Xarray_hvplot.html#analysis",
    "href": "python-notebooks/02_Xarray_hvplot.html#analysis",
    "title": "03. Introduction to xarray",
    "section": "Analysis",
    "text": "Analysis\nAs a simple example, let’s try to calculate a mean field for the whole time range.\n\nds.mean(dim='time').hvplot()\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\nWe can also calculate a zonal mean (averaging over longitude)\n\nds.mean(dim='lon').hvplot()\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\nOther aggregation methods include min(), max(), std(), along with others.\n\nds.std(dim='time').hvplot()\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\nThe data we have are in 6h timesteps. This can be resampled to daily or monthly. If you are familiar with pandas, xarray uses the same methods.\n\nds.resample(time='M').mean().hvplot()\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\n\nds_mon = ds.resample(time='M').mean()\nds_mon\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt;\nDimensions:     (time: 24, lat: 25, lon: 53)\nCoordinates:\n  * time        (time) datetime64[ns] 2013-01-31 2013-02-28 ... 2014-12-31\n  * lat         (lat) float32 75.0 72.5 70.0 67.5 65.0 ... 22.5 20.0 17.5 15.0\n  * lon         (lon) float32 200.0 202.5 205.0 207.5 ... 325.0 327.5 330.0\nData variables:\n    air         (time, lat, lon) float32 -28.68 -28.49 -28.48 ... 24.57 24.56\n    air_kelvin  (time, lat, lon) float32 244.5 244.7 244.7 ... 297.7 297.7 297.7\nAttributes:\n    Conventions:  COARDS\n    title:        4x daily NMC reanalysis (1948)\n    description:  Data is from NMC initialized reanalysis\\n(4x/day).  These a...\n    platform:     Model\n    references:   http://www.esrl.noaa.gov/psd/data/gridded/data.ncep.reanaly...xarray.DatasetDimensions:time: 24lat: 25lon: 53Coordinates: (3)time(time)datetime64[ns]2013-01-31 ... 2014-12-31array(['2013-01-31T00:00:00.000000000', '2013-02-28T00:00:00.000000000',\n       '2013-03-31T00:00:00.000000000', '2013-04-30T00:00:00.000000000',\n       '2013-05-31T00:00:00.000000000', '2013-06-30T00:00:00.000000000',\n       '2013-07-31T00:00:00.000000000', '2013-08-31T00:00:00.000000000',\n       '2013-09-30T00:00:00.000000000', '2013-10-31T00:00:00.000000000',\n       '2013-11-30T00:00:00.000000000', '2013-12-31T00:00:00.000000000',\n       '2014-01-31T00:00:00.000000000', '2014-02-28T00:00:00.000000000',\n       '2014-03-31T00:00:00.000000000', '2014-04-30T00:00:00.000000000',\n       '2014-05-31T00:00:00.000000000', '2014-06-30T00:00:00.000000000',\n       '2014-07-31T00:00:00.000000000', '2014-08-31T00:00:00.000000000',\n       '2014-09-30T00:00:00.000000000', '2014-10-31T00:00:00.000000000',\n       '2014-11-30T00:00:00.000000000', '2014-12-31T00:00:00.000000000'],\n      dtype='datetime64[ns]')lat(lat)float3275.0 72.5 70.0 ... 20.0 17.5 15.0standard_name :latitudelong_name :Latitudeunits :degrees_northaxis :Yarray([75. , 72.5, 70. , 67.5, 65. , 62.5, 60. , 57.5, 55. , 52.5, 50. , 47.5,\n       45. , 42.5, 40. , 37.5, 35. , 32.5, 30. , 27.5, 25. , 22.5, 20. , 17.5,\n       15. ], dtype=float32)lon(lon)float32200.0 202.5 205.0 ... 327.5 330.0standard_name :longitudelong_name :Longitudeunits :degrees_eastaxis :Xarray([200. , 202.5, 205. , 207.5, 210. , 212.5, 215. , 217.5, 220. , 222.5,\n       225. , 227.5, 230. , 232.5, 235. , 237.5, 240. , 242.5, 245. , 247.5,\n       250. , 252.5, 255. , 257.5, 260. , 262.5, 265. , 267.5, 270. , 272.5,\n       275. , 277.5, 280. , 282.5, 285. , 287.5, 290. , 292.5, 295. , 297.5,\n       300. , 302.5, 305. , 307.5, 310. , 312.5, 315. , 317.5, 320. , 322.5,\n       325. , 327.5, 330. ], dtype=float32)Data variables: (2)air(time, lat, lon)float32-28.68 -28.49 ... 24.57 24.56long_name :4xDaily Air temperature at sigma level 995units :degCprecision :2GRIB_id :11GRIB_name :TMPvar_desc :Air temperaturedataset :NMC Reanalysislevel_desc :Surfacestatistic :Individual Obsparent_stat :Otheractual_range :[185.16 322.1 ]array([[[-28.68323  , -28.486452 , -28.479755 , ..., -30.658554 ,\n         -29.743628 , -28.474194 ],\n        [-26.076784 , -26.127504 , -26.4225   , ..., -32.5679   ,\n         -31.105167 , -28.442825 ],\n        [-22.770565 , -23.31516  , -24.042498 , ..., -31.165657 ,\n         -28.38291  , -24.144924 ],\n        ...,\n        [ 22.688152 ,  22.00097  ,  21.773153 , ...,  22.218397 ,\n          21.734531 ,  21.118395 ],\n        [ 23.31952  ,  23.16702  ,  22.698233 , ...,  22.43775  ,\n          22.190727 ,  21.715578 ],\n        [ 23.903486 ,  23.89203  ,  23.585333 , ...,  23.154608 ,\n          22.947426 ,  22.889124 ]],\n\n       [[-32.41607  , -32.44866  , -32.738483 , ..., -31.54482  ,\n         -30.430185 , -29.205448 ],\n        [-31.216885 , -31.08063  , -31.236965 , ..., -32.135708 ,\n         -30.825186 , -28.42241  ],\n        [-27.826433 , -28.123934 , -28.78045  , ..., -29.734114 ,\n         -27.383936 , -23.491434 ],\n...\n        [ 24.899088 ,  24.200085 ,  24.072004 , ...,  24.861843 ,\n          24.510258 ,  23.995668 ],\n        [ 25.815008 ,  25.661922 ,  25.121607 , ...,  24.954088 ,\n          25.071083 ,  24.735588 ],\n        [ 26.023424 ,  26.06767  ,  25.74576  , ...,  25.566338 ,\n          25.591848 ,  25.630259 ]],\n\n       [[-26.348473 , -26.260897 , -26.380894 , ..., -33.07903  ,\n         -32.067986 , -30.868315 ],\n        [-25.419994 , -24.849277 , -24.405483 , ..., -34.531376 ,\n         -32.82783  , -30.179682 ],\n        [-23.181051 , -23.56476  , -23.574757 , ..., -35.446938 ,\n         -31.91259  , -26.923311 ],\n        ...,\n        [ 23.299198 ,  22.541454 ,  22.60839  , ...,  23.378307 ,\n          23.067505 ,  22.662996 ],\n        [ 24.295895 ,  24.286139 ,  24.031782 , ...,  23.80259  ,\n          23.908312 ,  23.579037 ],\n        [ 24.897346 ,  25.076134 ,  24.909689 , ...,  24.547583 ,\n          24.573233 ,  24.560413 ]]], dtype=float32)air_kelvin(time, lat, lon)float32244.5 244.7 244.7 ... 297.7 297.7long_name :4xDaily Air temperature at sigma level 995units :degKprecision :2GRIB_id :11GRIB_name :TMPvar_desc :Air temperaturedataset :NMC Reanalysislevel_desc :Surfacestatistic :Individual Obsparent_stat :Otheractual_range :[185.16 322.1 ]array([[[244.4667 , 244.66354, 244.67027, ..., 242.49142, 243.40633,\n         244.67577],\n        [247.07323, 247.02248, 246.7275 , ..., 240.58205, 242.04489,\n         244.70726],\n        [250.37941, 249.83484, 249.10748, ..., 241.98434, 244.76712,\n         249.00505],\n        ...,\n        [295.83795, 295.15085, 294.9229 , ..., 295.36826, 294.88437,\n         294.26828],\n        [296.46942, 296.31686, 295.84802, ..., 295.5876 , 295.34058,\n         294.86536],\n        [297.05316, 297.0418 , 296.73517, ..., 296.30438, 296.09732,\n         296.0389 ]],\n\n       [[240.73384, 240.7013 , 240.4115 , ..., 241.60518, 242.71988,\n         243.94455],\n        [241.93309, 242.06935, 241.913  , ..., 241.01428, 242.32481,\n         244.72758],\n        [245.32361, 245.0261 , 244.36955, ..., 243.41588, 245.7661 ,\n         249.65858],\n...\n        [298.04895, 297.35007, 297.22195, ..., 298.01172, 297.66013,\n         297.14554],\n        [298.96484, 298.81186, 298.27136, ..., 298.10403, 298.22104,\n         297.88547],\n        [299.17334, 299.2175 , 298.89566, ..., 298.71625, 298.74167,\n         298.7802 ]],\n\n       [[246.80156, 246.88907, 246.76907, ..., 240.07089, 241.08206,\n         242.2817 ],\n        [247.72998, 248.30064, 248.74443, ..., 238.61859, 240.3222 ,\n         242.97026],\n        [249.96893, 249.58516, 249.57521, ..., 237.70308, 241.23743,\n         246.22667],\n        ...,\n        [296.4491 , 295.6914 , 295.75824, ..., 296.52817, 296.21747,\n         295.8128 ],\n        [297.44586, 297.43613, 297.1817 , ..., 296.95242, 297.05823,\n         296.72897],\n        [298.0472 , 298.22598, 298.0595 , ..., 297.6975 , 297.72318,\n         297.71024]]], dtype=float32)Attributes: (5)Conventions :COARDStitle :4x daily NMC reanalysis (1948)description :Data is from NMC initialized reanalysis\n(4x/day).  These are the 0.9950 sigma level values.platform :Modelreferences :http://www.esrl.noaa.gov/psd/data/gridded/data.ncep.reanalysis.html\n\n\nThis is a really short time series but as an example, let’s calculate a monthly climatology (at least for 2 months). For this we can use groupby()\n\nds_clim = ds_mon.groupby(ds_mon.time.dt.month).mean()",
    "crumbs": [
      "Python Tutorials",
      "Using Data",
      "03. Introduction to `xarray`"
    ]
  },
  {
    "objectID": "python-notebooks/02_Xarray_hvplot.html#plot-results",
    "href": "python-notebooks/02_Xarray_hvplot.html#plot-results",
    "title": "03. Introduction to xarray",
    "section": "Plot results",
    "text": "Plot results\nFinally, let’s plot the results! This will plot the lat/lon axes of the original ds DataArray.\n\nds_clim.air.sel(month=10).plot()",
    "crumbs": [
      "Python Tutorials",
      "Using Data",
      "03. Introduction to `xarray`"
    ]
  },
  {
    "objectID": "python-notebooks/04_Appending_Data.html#load-in-data",
    "href": "python-notebooks/04_Appending_Data.html#load-in-data",
    "title": "Adding new data",
    "section": "Load in data",
    "text": "Load in data\n\nog_ds = xr.open_zarr('demonstrated data/final-sample-appending.zarr/')\nnew_ds = xr.open_zarr('demonstrated data/new-data-sample.zarr/')\n\nNote that our new dataset does not have any metadata. As shown in the previous notebooks, metadata is added at the last step, so now we are going to copy all metadata from the original dataset to our new one.",
    "crumbs": [
      "Python Tutorials",
      "IO Zarr Database",
      "Adding new data"
    ]
  },
  {
    "objectID": "python-notebooks/04_Appending_Data.html#add-metadata",
    "href": "python-notebooks/04_Appending_Data.html#add-metadata",
    "title": "Adding new data",
    "section": "Add metadata",
    "text": "Add metadata\n\n# copy dataset metadata\nnew_ds.attrs = og_ds.attrs\n\n# copy variables/dimensions metadata\n# make sure that all vars in new_ds exist in og_ds\nfor var in new_ds.variables:\n    new_ds[var].attrs = og_ds[var].attrs\n\n\n# double-check\nnew_ds\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt;\nDimensions:          (time: 2556, lat: 81, lon: 81)\nCoordinates:\n  * lat              (lat) float32 25.0 24.75 24.5 24.25 ... 5.75 5.5 5.25 5.0\n  * lon              (lon) float32 60.0 60.25 60.5 60.75 ... 79.5 79.75 80.0\n  * time             (time) datetime64[ns] 1993-01-01 1993-01-02 ... 1999-12-31\nData variables: (12/14)\n    CHL              (time, lat, lon) float32 dask.array&lt;chunksize=(100, 81, 81), meta=np.ndarray&gt;\n    CHL_uncertainty  (time, lat, lon) float32 dask.array&lt;chunksize=(100, 81, 81), meta=np.ndarray&gt;\n    adt              (time, lat, lon) float32 dask.array&lt;chunksize=(100, 81, 81), meta=np.ndarray&gt;\n    air_temp         (time, lat, lon) float32 dask.array&lt;chunksize=(100, 81, 81), meta=np.ndarray&gt;\n    direction        (time, lat, lon) float32 dask.array&lt;chunksize=(100, 81, 81), meta=np.ndarray&gt;\n    sla              (time, lat, lon) float32 dask.array&lt;chunksize=(100, 81, 81), meta=np.ndarray&gt;\n    ...               ...\n    u_curr           (time, lat, lon) float32 dask.array&lt;chunksize=(100, 81, 81), meta=np.ndarray&gt;\n    u_wind           (time, lat, lon) float32 dask.array&lt;chunksize=(100, 81, 81), meta=np.ndarray&gt;\n    ug_curr          (time, lat, lon) float32 dask.array&lt;chunksize=(100, 81, 81), meta=np.ndarray&gt;\n    v_curr           (time, lat, lon) float32 dask.array&lt;chunksize=(100, 81, 81), meta=np.ndarray&gt;\n    v_wind           (time, lat, lon) float32 dask.array&lt;chunksize=(100, 81, 81), meta=np.ndarray&gt;\n    vg_curr          (time, lat, lon) float32 dask.array&lt;chunksize=(100, 81, 81), meta=np.ndarray&gt;\nAttributes: (12/17)\n    creator_email:              minhphan@uw.edu\n    creator_name:               Minh Phan\n    creator_type:               person\n    date_created:               2023-11-11\n    geospatial_lat_max:         25.0\n    geospatial_lat_min:         5.0\n    ...                         ...\n    geospatial_lon_units:       degrees_east\n    source:                     OSCAR, ERA5 Reanalysis, Copernicus Climate Ch...\n    summary:                    Daily mean of 0.25 x 0.25 degrees gridded dat...\n    time_coverage_end:          2002-12-31T23:59:59\n    time_coverage_start:        2000-01-01T00:00:00\n    title:                      Sample of Climate Data for Coastal Upwelling ...xarray.DatasetDimensions:time: 2556lat: 81lon: 81Coordinates: (3)lat(lat)float3225.0 24.75 24.5 ... 5.5 5.25 5.0long_name :latitudestandard_name :latitudeunits :degrees_northarray([25.  , 24.75, 24.5 , 24.25, 24.  , 23.75, 23.5 , 23.25, 23.  , 22.75,\n       22.5 , 22.25, 22.  , 21.75, 21.5 , 21.25, 21.  , 20.75, 20.5 , 20.25,\n       20.  , 19.75, 19.5 , 19.25, 19.  , 18.75, 18.5 , 18.25, 18.  , 17.75,\n       17.5 , 17.25, 17.  , 16.75, 16.5 , 16.25, 16.  , 15.75, 15.5 , 15.25,\n       15.  , 14.75, 14.5 , 14.25, 14.  , 13.75, 13.5 , 13.25, 13.  , 12.75,\n       12.5 , 12.25, 12.  , 11.75, 11.5 , 11.25, 11.  , 10.75, 10.5 , 10.25,\n       10.  ,  9.75,  9.5 ,  9.25,  9.  ,  8.75,  8.5 ,  8.25,  8.  ,  7.75,\n        7.5 ,  7.25,  7.  ,  6.75,  6.5 ,  6.25,  6.  ,  5.75,  5.5 ,  5.25,\n        5.  ], dtype=float32)lon(lon)float3260.0 60.25 60.5 ... 79.5 79.75 80.0long_name :longitudestandard_name :longitudeunits :degrees_eastarray([60.  , 60.25, 60.5 , 60.75, 61.  , 61.25, 61.5 , 61.75, 62.  , 62.25,\n       62.5 , 62.75, 63.  , 63.25, 63.5 , 63.75, 64.  , 64.25, 64.5 , 64.75,\n       65.  , 65.25, 65.5 , 65.75, 66.  , 66.25, 66.5 , 66.75, 67.  , 67.25,\n       67.5 , 67.75, 68.  , 68.25, 68.5 , 68.75, 69.  , 69.25, 69.5 , 69.75,\n       70.  , 70.25, 70.5 , 70.75, 71.  , 71.25, 71.5 , 71.75, 72.  , 72.25,\n       72.5 , 72.75, 73.  , 73.25, 73.5 , 73.75, 74.  , 74.25, 74.5 , 74.75,\n       75.  , 75.25, 75.5 , 75.75, 76.  , 76.25, 76.5 , 76.75, 77.  , 77.25,\n       77.5 , 77.75, 78.  , 78.25, 78.5 , 78.75, 79.  , 79.25, 79.5 , 79.75,\n       80.  ], dtype=float32)time(time)datetime64[ns]1993-01-01 ... 1999-12-31_ChunkSizes :1_CoordinateAxisType :Timeaxis :Tcoverage_content_type :coordinatelong_name :reference timestandard_name :timevalid_max :401767.0valid_min :0.0array(['1993-01-01T00:00:00.000000000', '1993-01-02T00:00:00.000000000',\n       '1993-01-03T00:00:00.000000000', ..., '1999-12-29T00:00:00.000000000',\n       '1999-12-30T00:00:00.000000000', '1999-12-31T00:00:00.000000000'],\n      dtype='datetime64[ns]')Data variables: (14)CHL(time, lat, lon)float32dask.array&lt;chunksize=(100, 81, 81), meta=np.ndarray&gt;_ChunkSizes :[1, 256, 256]ancillary_variables :flags CHL_uncertaintycoverage_content_type :modelResultinput_files_reprocessings :Processors versions: MODIS R2022.0NRT/VIIRSN R2022.0NRT/OLCIA 07.02/VIIRSJ1 R2022.0NRT/OLCIB 07.02long_name :Chlorophyll-a concentration - Mean of the binned pixelsstandard_name :mass_concentration_of_chlorophyll_a_in_sea_watertype :surfaceunits :milligram m-3valid_max :1000.0valid_min :0.0\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n63.97 MiB\n2.50 MiB\n\n\nShape\n(2556, 81, 81)\n(100, 81, 81)\n\n\nDask graph\n26 chunks in 2 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\nCHL_uncertainty\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(100, 81, 81), meta=np.ndarray&gt;\n\n\n\n\n_ChunkSizes :\n\n[1, 256, 256]\n\ncoverage_content_type :\n\nqualityInformation\n\nlong_name :\n\nChlorophyll-a concentration - Uncertainty estimation\n\nunits :\n\n%\n\nvalid_max :\n\n32767\n\nvalid_min :\n\n0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n63.97 MiB\n2.50 MiB\n\n\nShape\n(2556, 81, 81)\n(100, 81, 81)\n\n\nDask graph\n26 chunks in 2 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nadt\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(100, 81, 81), meta=np.ndarray&gt;\n\n\n\n\n_ChunkSizes :\n\n[1, 50, 50]\n\ncomment :\n\nThe absolute dynamic topography is the sea surface height above geoid; the adt is obtained as follows: adt=sla+mdt where mdt is the mean dynamic topography; see the product user manual for details\n\ngrid_mapping :\n\ncrs\n\nlong_name :\n\nAbsolute dynamic topography\n\nstandard_name :\n\nsea_surface_height_above_geoid\n\nunits :\n\nm\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n63.97 MiB\n2.50 MiB\n\n\nShape\n(2556, 81, 81)\n(100, 81, 81)\n\n\nDask graph\n26 chunks in 2 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nair_temp\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(100, 81, 81), meta=np.ndarray&gt;\n\n\n\n\nlong_name :\n\n2 metre temperature\n\nnameCDM :\n\n2_metre_temperature_surface\n\nnameECMWF :\n\n2 metre temperature\n\nproduct_type :\n\nanalysis\n\nshortNameECMWF :\n\n2t\n\nstandard_name :\n\nair_temperature\n\nunits :\n\nK\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n63.97 MiB\n2.50 MiB\n\n\nShape\n(2556, 81, 81)\n(100, 81, 81)\n\n\nDask graph\n26 chunks in 2 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\ndirection\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(100, 81, 81), meta=np.ndarray&gt;\n\n\n\n\nlong_name :\n\n10 metre wind direction\n\nunits :\n\n°C\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n63.97 MiB\n2.50 MiB\n\n\nShape\n(2556, 81, 81)\n(100, 81, 81)\n\n\nDask graph\n26 chunks in 2 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nsla\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(100, 81, 81), meta=np.ndarray&gt;\n\n\n\n\n_ChunkSizes :\n\n[1, 50, 50]\n\nancillary_variables :\n\nerr_sla\n\ncomment :\n\nThe sea level anomaly is the sea surface height above mean sea surface; it is referenced to the [1993, 2012] period; see the product user manual for details\n\ngrid_mapping :\n\ncrs\n\nlong_name :\n\nSea level anomaly\n\nstandard_name :\n\nsea_surface_height_above_sea_level\n\nunits :\n\nm\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n63.97 MiB\n2.50 MiB\n\n\nShape\n(2556, 81, 81)\n(100, 81, 81)\n\n\nDask graph\n26 chunks in 2 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nspeed\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(100, 81, 81), meta=np.ndarray&gt;\n\n\n\n\nlong_name :\n\n10 metre absolute speed\n\nunits :\n\nm s**-1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n63.97 MiB\n2.50 MiB\n\n\nShape\n(2556, 81, 81)\n(100, 81, 81)\n\n\nDask graph\n26 chunks in 2 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nsst\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(100, 81, 81), meta=np.ndarray&gt;\n\n\n\n\nlong_name :\n\nSea surface temperature\n\nnameCDM :\n\nSea_surface_temperature_surface\n\nnameECMWF :\n\nSea surface temperature\n\nproduct_type :\n\nanalysis\n\nshortNameECMWF :\n\nsst\n\nstandard_name :\n\nsea_surface_temperature\n\nunits :\n\nK\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n63.97 MiB\n2.50 MiB\n\n\nShape\n(2556, 81, 81)\n(100, 81, 81)\n\n\nDask graph\n26 chunks in 2 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nu_curr\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(100, 81, 81), meta=np.ndarray&gt;\n\n\n\n\ncomment :\n\nVelocities are an average over the top 30m of the mixed layer\n\ncoverage_content_type :\n\nmodelResult\n\ndepth :\n\n15m\n\nlong_name :\n\nzonal total surface current\n\nsource :\n\nSSH source: CMEMS SSALTO/DUACS SEALEVEL_GLO_PHY_L4_MY_008_047 DOI: 10.48670/moi-00148 ; WIND source: ECMWF ERA5 10m wind DOI: 10.24381/cds.adbb2d47 ; SST source: CMC 0.2 deg SST V2.0 DOI: 10.5067/GHCMC-4FM02\n\nstandard_name :\n\neastward_sea_water_velocity\n\nunits :\n\nm s-1\n\nvalid_max :\n\n3.0\n\nvalid_min :\n\n-3.0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n63.97 MiB\n2.50 MiB\n\n\nShape\n(2556, 81, 81)\n(100, 81, 81)\n\n\nDask graph\n26 chunks in 2 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nu_wind\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(100, 81, 81), meta=np.ndarray&gt;\n\n\n\n\nlong_name :\n\n10 metre U wind component\n\nnameCDM :\n\n10_metre_U_wind_component_surface\n\nnameECMWF :\n\n10 metre U wind component\n\nproduct_type :\n\nanalysis\n\nshortNameECMWF :\n\n10u\n\nstandard_name :\n\neastward_wind\n\nunits :\n\nm s**-1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n63.97 MiB\n2.50 MiB\n\n\nShape\n(2556, 81, 81)\n(100, 81, 81)\n\n\nDask graph\n26 chunks in 2 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nug_curr\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(100, 81, 81), meta=np.ndarray&gt;\n\n\n\n\ncomment :\n\nGeostrophic velocities calculated from absolute dynamic topography\n\ndepth :\n\n15m\n\nlong_name :\n\nzonal geostrophic surface current\n\nsource :\n\nSSH source: CMEMS SSALTO/DUACS SEALEVEL_GLO_PHY_L4_MY_008_047 DOI: 10.48670/moi-00148\n\nstandard_name :\n\ngeostrophic_eastward_sea_water_velocity\n\nunits :\n\nm s-1\n\nvalid_max :\n\n3.0\n\nvalid_min :\n\n-3.0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n63.97 MiB\n2.50 MiB\n\n\nShape\n(2556, 81, 81)\n(100, 81, 81)\n\n\nDask graph\n26 chunks in 2 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nv_curr\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(100, 81, 81), meta=np.ndarray&gt;\n\n\n\n\ncomment :\n\nVelocities are an average over the top 30m of the mixed layer\n\ncoverage_content_type :\n\nmodelResult\n\ndepth :\n\n15m\n\nlong_name :\n\nmeridional total surface current\n\nsource :\n\nSSH source: CMEMS SSALTO/DUACS SEALEVEL_GLO_PHY_L4_MY_008_047 DOI: 10.48670/moi-00148 ; WIND source: ECMWF ERA5 10m wind DOI: 10.24381/cds.adbb2d47 ; SST source: CMC 0.2 deg SST V2.0 DOI: 10.5067/GHCMC-4FM02\n\nstandard_name :\n\nnorthward_sea_water_velocity\n\nunits :\n\nm s-1\n\nvalid_max :\n\n3.0\n\nvalid_min :\n\n-3.0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n63.97 MiB\n2.50 MiB\n\n\nShape\n(2556, 81, 81)\n(100, 81, 81)\n\n\nDask graph\n26 chunks in 2 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nv_wind\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(100, 81, 81), meta=np.ndarray&gt;\n\n\n\n\nlong_name :\n\n10 metre V wind component\n\nnameCDM :\n\n10_metre_V_wind_component_surface\n\nnameECMWF :\n\n10 metre V wind component\n\nproduct_type :\n\nanalysis\n\nshortNameECMWF :\n\n10v\n\nstandard_name :\n\nnorthward_wind\n\nunits :\n\nm s**-1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n63.97 MiB\n2.50 MiB\n\n\nShape\n(2556, 81, 81)\n(100, 81, 81)\n\n\nDask graph\n26 chunks in 2 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nvg_curr\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(100, 81, 81), meta=np.ndarray&gt;\n\n\n\n\ncomment :\n\nGeostrophic velocities calculated from absolute dynamic topography\n\ndepth :\n\n15m\n\nlong_name :\n\nmeridional geostrophic surface current\n\nsource :\n\nSSH source: CMEMS SSALTO/DUACS SEALEVEL_GLO_PHY_L4_MY_008_047 DOI: 10.48670/moi-00148\n\nstandard_name :\n\ngeostrophic_northward_sea_water_velocity\n\nunits :\n\nm s-1\n\nvalid_max :\n\n3.0\n\nvalid_min :\n\n-3.0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n63.97 MiB\n2.50 MiB\n\n\nShape\n(2556, 81, 81)\n(100, 81, 81)\n\n\nDask graph\n26 chunks in 2 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\nAttributes: (17)creator_email :minhphan@uw.educreator_name :Minh Phancreator_type :persondate_created :2023-11-11geospatial_lat_max :25.0geospatial_lat_min :5.0geospatial_lat_resolution :0.25geospatial_lat_units :degrees_northgeospatial_lon_max :80.0geospatial_lon_min :60.0geospatial_lon_resolution :0.25geospatial_lon_units :degrees_eastsource :OSCAR, ERA5 Reanalysis, Copernicus Climate Change Service (C3S), Copernicus Marine Environment Monitoring Service (CMEMS)summary :Daily mean of 0.25 x 0.25 degrees gridded data from multiple climate variables that may influence the patterns of coastal upwelling in the focused areatime_coverage_end :2002-12-31T23:59:59time_coverage_start :2000-01-01T00:00:00title :Sample of Climate Data for Coastal Upwelling Machine Learning Project in Indian Ocean",
    "crumbs": [
      "Python Tutorials",
      "IO Zarr Database",
      "Adding new data"
    ]
  },
  {
    "objectID": "python-notebooks/04_Appending_Data.html#appending-data",
    "href": "python-notebooks/04_Appending_Data.html#appending-data",
    "title": "Adding new data",
    "section": "Appending data",
    "text": "Appending data\n\nnew_ds.to_zarr('demonstrated data/final-sample-appending.zarr/', append_dim='time', mode='a')\n\n&lt;xarray.backends.zarr.ZarrStore at 0x7f1455c51dd0&gt;",
    "crumbs": [
      "Python Tutorials",
      "IO Zarr Database",
      "Adding new data"
    ]
  },
  {
    "objectID": "python-notebooks/04_Appending_Data.html#final-result",
    "href": "python-notebooks/04_Appending_Data.html#final-result",
    "title": "Adding new data",
    "section": "Final result",
    "text": "Final result\n\nxr.open_zarr('demonstrated data/final-sample-appending.zarr/')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt;\nDimensions:          (time: 3287, lat: 81, lon: 81)\nCoordinates:\n  * lat              (lat) float32 25.0 24.75 24.5 24.25 ... 5.75 5.5 5.25 5.0\n  * lon              (lon) float32 60.0 60.25 60.5 60.75 ... 79.5 79.75 80.0\n  * time             (time) datetime64[ns] 2000-01-01 2000-01-02 ... 1999-12-31\nData variables: (12/14)\n    CHL              (time, lat, lon) float32 dask.array&lt;chunksize=(100, 81, 81), meta=np.ndarray&gt;\n    CHL_uncertainty  (time, lat, lon) float32 dask.array&lt;chunksize=(100, 81, 81), meta=np.ndarray&gt;\n    adt              (time, lat, lon) float32 dask.array&lt;chunksize=(100, 81, 81), meta=np.ndarray&gt;\n    air_temp         (time, lat, lon) float32 dask.array&lt;chunksize=(100, 81, 81), meta=np.ndarray&gt;\n    direction        (time, lat, lon) float32 dask.array&lt;chunksize=(100, 81, 81), meta=np.ndarray&gt;\n    sla              (time, lat, lon) float32 dask.array&lt;chunksize=(100, 81, 81), meta=np.ndarray&gt;\n    ...               ...\n    u_curr           (time, lat, lon) float32 dask.array&lt;chunksize=(100, 81, 81), meta=np.ndarray&gt;\n    u_wind           (time, lat, lon) float32 dask.array&lt;chunksize=(100, 81, 81), meta=np.ndarray&gt;\n    ug_curr          (time, lat, lon) float32 dask.array&lt;chunksize=(100, 81, 81), meta=np.ndarray&gt;\n    v_curr           (time, lat, lon) float32 dask.array&lt;chunksize=(100, 81, 81), meta=np.ndarray&gt;\n    v_wind           (time, lat, lon) float32 dask.array&lt;chunksize=(100, 81, 81), meta=np.ndarray&gt;\n    vg_curr          (time, lat, lon) float32 dask.array&lt;chunksize=(100, 81, 81), meta=np.ndarray&gt;\nAttributes: (12/17)\n    creator_email:              minhphan@uw.edu\n    creator_name:               Minh Phan\n    creator_type:               person\n    date_created:               2023-11-11\n    geospatial_lat_max:         25.0\n    geospatial_lat_min:         5.0\n    ...                         ...\n    geospatial_lon_units:       degrees_east\n    source:                     OSCAR, ERA5 Reanalysis, Copernicus Climate Ch...\n    summary:                    Daily mean of 0.25 x 0.25 degrees gridded dat...\n    time_coverage_end:          2002-12-31T23:59:59\n    time_coverage_start:        2000-01-01T00:00:00\n    title:                      Sample of Climate Data for Coastal Upwelling ...xarray.DatasetDimensions:time: 3287lat: 81lon: 81Coordinates: (3)lat(lat)float3225.0 24.75 24.5 ... 5.5 5.25 5.0long_name :latitudestandard_name :latitudeunits :degrees_northarray([25.  , 24.75, 24.5 , 24.25, 24.  , 23.75, 23.5 , 23.25, 23.  , 22.75,\n       22.5 , 22.25, 22.  , 21.75, 21.5 , 21.25, 21.  , 20.75, 20.5 , 20.25,\n       20.  , 19.75, 19.5 , 19.25, 19.  , 18.75, 18.5 , 18.25, 18.  , 17.75,\n       17.5 , 17.25, 17.  , 16.75, 16.5 , 16.25, 16.  , 15.75, 15.5 , 15.25,\n       15.  , 14.75, 14.5 , 14.25, 14.  , 13.75, 13.5 , 13.25, 13.  , 12.75,\n       12.5 , 12.25, 12.  , 11.75, 11.5 , 11.25, 11.  , 10.75, 10.5 , 10.25,\n       10.  ,  9.75,  9.5 ,  9.25,  9.  ,  8.75,  8.5 ,  8.25,  8.  ,  7.75,\n        7.5 ,  7.25,  7.  ,  6.75,  6.5 ,  6.25,  6.  ,  5.75,  5.5 ,  5.25,\n        5.  ], dtype=float32)lon(lon)float3260.0 60.25 60.5 ... 79.5 79.75 80.0long_name :longitudestandard_name :longitudeunits :degrees_eastarray([60.  , 60.25, 60.5 , 60.75, 61.  , 61.25, 61.5 , 61.75, 62.  , 62.25,\n       62.5 , 62.75, 63.  , 63.25, 63.5 , 63.75, 64.  , 64.25, 64.5 , 64.75,\n       65.  , 65.25, 65.5 , 65.75, 66.  , 66.25, 66.5 , 66.75, 67.  , 67.25,\n       67.5 , 67.75, 68.  , 68.25, 68.5 , 68.75, 69.  , 69.25, 69.5 , 69.75,\n       70.  , 70.25, 70.5 , 70.75, 71.  , 71.25, 71.5 , 71.75, 72.  , 72.25,\n       72.5 , 72.75, 73.  , 73.25, 73.5 , 73.75, 74.  , 74.25, 74.5 , 74.75,\n       75.  , 75.25, 75.5 , 75.75, 76.  , 76.25, 76.5 , 76.75, 77.  , 77.25,\n       77.5 , 77.75, 78.  , 78.25, 78.5 , 78.75, 79.  , 79.25, 79.5 , 79.75,\n       80.  ], dtype=float32)time(time)datetime64[ns]2000-01-01 ... 1999-12-31_ChunkSizes :1_CoordinateAxisType :Timeaxis :Tcoverage_content_type :coordinatelong_name :reference timestandard_name :timevalid_max :401767.0valid_min :0.0array(['2000-01-01T00:00:00.000000000', '2000-01-02T00:00:00.000000000',\n       '2000-01-03T00:00:00.000000000', ..., '1999-12-29T00:00:00.000000000',\n       '1999-12-30T00:00:00.000000000', '1999-12-31T00:00:00.000000000'],\n      dtype='datetime64[ns]')Data variables: (14)CHL(time, lat, lon)float32dask.array&lt;chunksize=(100, 81, 81), meta=np.ndarray&gt;_ChunkSizes :[1, 256, 256]ancillary_variables :flags CHL_uncertaintycoverage_content_type :modelResultinput_files_reprocessings :Processors versions: MODIS R2022.0NRT/VIIRSN R2022.0NRT/OLCIA 07.02/VIIRSJ1 R2022.0NRT/OLCIB 07.02long_name :Chlorophyll-a concentration - Mean of the binned pixelsstandard_name :mass_concentration_of_chlorophyll_a_in_sea_watertype :surfaceunits :milligram m-3valid_max :1000.0valid_min :0.0\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n82.27 MiB\n2.50 MiB\n\n\nShape\n(3287, 81, 81)\n(100, 81, 81)\n\n\nDask graph\n33 chunks in 2 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\nCHL_uncertainty\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(100, 81, 81), meta=np.ndarray&gt;\n\n\n\n\n_ChunkSizes :\n\n[1, 256, 256]\n\ncoverage_content_type :\n\nqualityInformation\n\nlong_name :\n\nChlorophyll-a concentration - Uncertainty estimation\n\nunits :\n\n%\n\nvalid_max :\n\n32767\n\nvalid_min :\n\n0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n82.27 MiB\n2.50 MiB\n\n\nShape\n(3287, 81, 81)\n(100, 81, 81)\n\n\nDask graph\n33 chunks in 2 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nadt\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(100, 81, 81), meta=np.ndarray&gt;\n\n\n\n\n_ChunkSizes :\n\n[1, 50, 50]\n\ncomment :\n\nThe absolute dynamic topography is the sea surface height above geoid; the adt is obtained as follows: adt=sla+mdt where mdt is the mean dynamic topography; see the product user manual for details\n\ngrid_mapping :\n\ncrs\n\nlong_name :\n\nAbsolute dynamic topography\n\nstandard_name :\n\nsea_surface_height_above_geoid\n\nunits :\n\nm\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n82.27 MiB\n2.50 MiB\n\n\nShape\n(3287, 81, 81)\n(100, 81, 81)\n\n\nDask graph\n33 chunks in 2 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nair_temp\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(100, 81, 81), meta=np.ndarray&gt;\n\n\n\n\nlong_name :\n\n2 metre temperature\n\nnameCDM :\n\n2_metre_temperature_surface\n\nnameECMWF :\n\n2 metre temperature\n\nproduct_type :\n\nanalysis\n\nshortNameECMWF :\n\n2t\n\nstandard_name :\n\nair_temperature\n\nunits :\n\nK\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n82.27 MiB\n2.50 MiB\n\n\nShape\n(3287, 81, 81)\n(100, 81, 81)\n\n\nDask graph\n33 chunks in 2 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\ndirection\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(100, 81, 81), meta=np.ndarray&gt;\n\n\n\n\nlong_name :\n\n10 metre wind direction\n\nunits :\n\n°C\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n82.27 MiB\n2.50 MiB\n\n\nShape\n(3287, 81, 81)\n(100, 81, 81)\n\n\nDask graph\n33 chunks in 2 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nsla\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(100, 81, 81), meta=np.ndarray&gt;\n\n\n\n\n_ChunkSizes :\n\n[1, 50, 50]\n\nancillary_variables :\n\nerr_sla\n\ncomment :\n\nThe sea level anomaly is the sea surface height above mean sea surface; it is referenced to the [1993, 2012] period; see the product user manual for details\n\ngrid_mapping :\n\ncrs\n\nlong_name :\n\nSea level anomaly\n\nstandard_name :\n\nsea_surface_height_above_sea_level\n\nunits :\n\nm\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n82.27 MiB\n2.50 MiB\n\n\nShape\n(3287, 81, 81)\n(100, 81, 81)\n\n\nDask graph\n33 chunks in 2 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nspeed\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(100, 81, 81), meta=np.ndarray&gt;\n\n\n\n\nlong_name :\n\n10 metre absolute speed\n\nunits :\n\nm s**-1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n82.27 MiB\n2.50 MiB\n\n\nShape\n(3287, 81, 81)\n(100, 81, 81)\n\n\nDask graph\n33 chunks in 2 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nsst\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(100, 81, 81), meta=np.ndarray&gt;\n\n\n\n\nlong_name :\n\nSea surface temperature\n\nnameCDM :\n\nSea_surface_temperature_surface\n\nnameECMWF :\n\nSea surface temperature\n\nproduct_type :\n\nanalysis\n\nshortNameECMWF :\n\nsst\n\nstandard_name :\n\nsea_surface_temperature\n\nunits :\n\nK\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n82.27 MiB\n2.50 MiB\n\n\nShape\n(3287, 81, 81)\n(100, 81, 81)\n\n\nDask graph\n33 chunks in 2 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nu_curr\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(100, 81, 81), meta=np.ndarray&gt;\n\n\n\n\ncomment :\n\nVelocities are an average over the top 30m of the mixed layer\n\ncoverage_content_type :\n\nmodelResult\n\ndepth :\n\n15m\n\nlong_name :\n\nzonal total surface current\n\nsource :\n\nSSH source: CMEMS SSALTO/DUACS SEALEVEL_GLO_PHY_L4_MY_008_047 DOI: 10.48670/moi-00148 ; WIND source: ECMWF ERA5 10m wind DOI: 10.24381/cds.adbb2d47 ; SST source: CMC 0.2 deg SST V2.0 DOI: 10.5067/GHCMC-4FM02\n\nstandard_name :\n\neastward_sea_water_velocity\n\nunits :\n\nm s-1\n\nvalid_max :\n\n3.0\n\nvalid_min :\n\n-3.0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n82.27 MiB\n2.50 MiB\n\n\nShape\n(3287, 81, 81)\n(100, 81, 81)\n\n\nDask graph\n33 chunks in 2 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nu_wind\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(100, 81, 81), meta=np.ndarray&gt;\n\n\n\n\nlong_name :\n\n10 metre U wind component\n\nnameCDM :\n\n10_metre_U_wind_component_surface\n\nnameECMWF :\n\n10 metre U wind component\n\nproduct_type :\n\nanalysis\n\nshortNameECMWF :\n\n10u\n\nstandard_name :\n\neastward_wind\n\nunits :\n\nm s**-1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n82.27 MiB\n2.50 MiB\n\n\nShape\n(3287, 81, 81)\n(100, 81, 81)\n\n\nDask graph\n33 chunks in 2 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nug_curr\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(100, 81, 81), meta=np.ndarray&gt;\n\n\n\n\ncomment :\n\nGeostrophic velocities calculated from absolute dynamic topography\n\ndepth :\n\n15m\n\nlong_name :\n\nzonal geostrophic surface current\n\nsource :\n\nSSH source: CMEMS SSALTO/DUACS SEALEVEL_GLO_PHY_L4_MY_008_047 DOI: 10.48670/moi-00148\n\nstandard_name :\n\ngeostrophic_eastward_sea_water_velocity\n\nunits :\n\nm s-1\n\nvalid_max :\n\n3.0\n\nvalid_min :\n\n-3.0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n82.27 MiB\n2.50 MiB\n\n\nShape\n(3287, 81, 81)\n(100, 81, 81)\n\n\nDask graph\n33 chunks in 2 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nv_curr\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(100, 81, 81), meta=np.ndarray&gt;\n\n\n\n\ncomment :\n\nVelocities are an average over the top 30m of the mixed layer\n\ncoverage_content_type :\n\nmodelResult\n\ndepth :\n\n15m\n\nlong_name :\n\nmeridional total surface current\n\nsource :\n\nSSH source: CMEMS SSALTO/DUACS SEALEVEL_GLO_PHY_L4_MY_008_047 DOI: 10.48670/moi-00148 ; WIND source: ECMWF ERA5 10m wind DOI: 10.24381/cds.adbb2d47 ; SST source: CMC 0.2 deg SST V2.0 DOI: 10.5067/GHCMC-4FM02\n\nstandard_name :\n\nnorthward_sea_water_velocity\n\nunits :\n\nm s-1\n\nvalid_max :\n\n3.0\n\nvalid_min :\n\n-3.0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n82.27 MiB\n2.50 MiB\n\n\nShape\n(3287, 81, 81)\n(100, 81, 81)\n\n\nDask graph\n33 chunks in 2 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nv_wind\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(100, 81, 81), meta=np.ndarray&gt;\n\n\n\n\nlong_name :\n\n10 metre V wind component\n\nnameCDM :\n\n10_metre_V_wind_component_surface\n\nnameECMWF :\n\n10 metre V wind component\n\nproduct_type :\n\nanalysis\n\nshortNameECMWF :\n\n10v\n\nstandard_name :\n\nnorthward_wind\n\nunits :\n\nm s**-1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n82.27 MiB\n2.50 MiB\n\n\nShape\n(3287, 81, 81)\n(100, 81, 81)\n\n\nDask graph\n33 chunks in 2 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nvg_curr\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(100, 81, 81), meta=np.ndarray&gt;\n\n\n\n\ncomment :\n\nGeostrophic velocities calculated from absolute dynamic topography\n\ndepth :\n\n15m\n\nlong_name :\n\nmeridional geostrophic surface current\n\nsource :\n\nSSH source: CMEMS SSALTO/DUACS SEALEVEL_GLO_PHY_L4_MY_008_047 DOI: 10.48670/moi-00148\n\nstandard_name :\n\ngeostrophic_northward_sea_water_velocity\n\nunits :\n\nm s-1\n\nvalid_max :\n\n3.0\n\nvalid_min :\n\n-3.0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n82.27 MiB\n2.50 MiB\n\n\nShape\n(3287, 81, 81)\n(100, 81, 81)\n\n\nDask graph\n33 chunks in 2 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\nAttributes: (17)creator_email :minhphan@uw.educreator_name :Minh Phancreator_type :persondate_created :2023-11-11geospatial_lat_max :25.0geospatial_lat_min :5.0geospatial_lat_resolution :0.25geospatial_lat_units :degrees_northgeospatial_lon_max :80.0geospatial_lon_min :60.0geospatial_lon_resolution :0.25geospatial_lon_units :degrees_eastsource :OSCAR, ERA5 Reanalysis, Copernicus Climate Change Service (C3S), Copernicus Marine Environment Monitoring Service (CMEMS)summary :Daily mean of 0.25 x 0.25 degrees gridded data from multiple climate variables that may influence the patterns of coastal upwelling in the focused areatime_coverage_end :2002-12-31T23:59:59time_coverage_start :2000-01-01T00:00:00title :Sample of Climate Data for Coastal Upwelling Machine Learning Project in Indian Ocean",
    "crumbs": [
      "Python Tutorials",
      "IO Zarr Database",
      "Adding new data"
    ]
  },
  {
    "objectID": "r-tutorials/03-r-read-in-data.html#syntax-tips",
    "href": "r-tutorials/03-r-read-in-data.html#syntax-tips",
    "title": "R - Read in data",
    "section": "Syntax tips",
    "text": "Syntax tips\nxyz::fun() means function fun() in the xyz package. I will often use this when I don’t want to call library() to load the whole xyz package. Why? Because remembering to call library() slows me down and when I forget I get errors. Instead I tend to load the major packages (like library(tidyverse) and library(sf)) for which I need many functions.",
    "crumbs": [
      "Tutorials",
      "R and RStudio Basics",
      "Loading data in R"
    ]
  },
  {
    "objectID": "r-tutorials/03-r-read-in-data.html#file-not-found",
    "href": "r-tutorials/03-r-read-in-data.html#file-not-found",
    "title": "R - Read in data",
    "section": "File not found",
    "text": "File not found\nThe top error for new users is “File not found.” You know they file is there, but R says “File not found.” File locations are relative to your current location. Your current location if you are in a Quarto or Rmarkdown file is the folder with that file! This means that code that works on the command line, fails in a Quarto or RMarkdown document. Oh no!\nTry this in the Quarto document versus in the Console tab.\n\ngetwd()\n\nThis means that a normal file location can break:\n\n. Current location\n.. Back one folder\nfolder name Enter this folder\n~ Home directory\n\n./data/myfile.csv\nmeans current location &gt; data folder &gt; myfile.csv in that folder. But if your current location changes, that code breaks.\n\nSolution 1 - full file path\nAlways use the full file path:\n~/r-tutorials/data/myfile.csv\nHome &gt; r-tutorials folder &gt; data folder &gt; myfile.csv\nThis works until you move the r-tutorials folder somewhere else.\n\n\nSolution 2: the here package\nThis is the what experienced R users use. here() is the location of the base of your project. As long as you are in an RStudio project, it will return the base of that project.\nThis is what mine shows (I am on my laptop). It is the same inside a Quarto document, at the console and in scripts.\n\nhere::i_am(\"r-tutorials/03-r-read-in-data.qmd\")\n\nhere() starts at /Users/eli.holmes/Documents/GitHub/NOAAHackDays\n\nhere::here()\n\n[1] \"/Users/eli.holmes/Documents/GitHub/NOAAHackDays\"\n\n\nNow to specify a data file, I use\n\nfil &lt;- here::here(\"r-tutorials\", \"data\", \"myfile.csv\")\nfil\n\n[1] \"/Users/eli.holmes/Documents/GitHub/NOAAHackDays/r-tutorials/data/myfile.csv\"\n\n\nThis fully specifies the file location.\n\n\nDon’t use setwd()\nThis changes your working directory and although it seems to help in the short-run, it causes all kinds of problems later.",
    "crumbs": [
      "Tutorials",
      "R and RStudio Basics",
      "Loading data in R"
    ]
  },
  {
    "objectID": "r-tutorials/03-r-read-in-data.html#import-a-csv-file",
    "href": "r-tutorials/03-r-read-in-data.html#import-a-csv-file",
    "title": "R - Read in data",
    "section": "Import a csv file",
    "text": "Import a csv file\nWe will use the Import Dataset widget in the Environment tab.\n\nClick the Environment tag and then Import Dataset\nChoose “From Text (readr)”\nBrowse to obis_seamap_green.csv in the data folder.\nYou should now see a preview of the data.\n\n\nOk first thing we see is that we have 2 lines at the top of our data file. Put 2 in the Skip box to skip past those. Now the data and column headers should look right.\nNotice that it also shows us the R code. We can copy that and run it directly if we wanted. But we can also just click the Import button.\n\nR Code\nThe import box helpfully shows us the R code to use. But this will break inside a Quarto file because the file path is wrong. This works in the Console or a script but not in a Quarto or RMarkdown file.\nlibrary(readr)\nobis_seamap_green &lt;- read_csv(\"r-tutorials/data/obis_seamap_green.csv\", \n    skip = 2)\nLet’s make this code more robust:\n\nlibrary(readr)\nfil &lt;- here::here(\"r-tutorials\", \"data\", \"obis_seamap_green.csv\")\nobis_seamap_green &lt;- read_csv(fil, \n    skip = 2)\n\nRows: 5356 Columns: 15\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (7): scientific, common, provider, obs_date, date_time, ds_type, platform\ndbl (8): oid, id, dataset_id, tsn, latitude, longitude, lprecision, count\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\nShow the data\n\nobis_seamap_green\n\n# A tibble: 5,356 × 15\n       oid    id dataset_id    tsn scientific common provider latitude longitude\n     &lt;dbl&gt; &lt;dbl&gt;      &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;  &lt;chr&gt;       &lt;dbl&gt;     &lt;dbl&gt;\n 1  2.18e8   219       2069 173833 Chelonia … Green… Nicolas…     24.6      53.0\n 2  2.18e8     1       2069 173833 Chelonia … Green… Nicolas…     24.6      53.0\n 3  2.18e8   136       2069 173833 Chelonia … Green… Nicolas…     24.6      53.1\n 4  2.18e8   258       2069 173833 Chelonia … Green… Nicolas…     24.6      53.0\n 5  2.18e8   292       2069 173833 Chelonia … Green… Nicolas…     24.6      53.0\n 6  2.18e8   259       2069 173833 Chelonia … Green… Nicolas…     24.6      53.0\n 7  2.18e8   293       2069 173833 Chelonia … Green… Nicolas…     24.6      53.0\n 8  2.18e8     2       2069 173833 Chelonia … Green… Nicolas…     24.6      53.1\n 9  2.18e8   183       2069 173833 Chelonia … Green… Nicolas…     24.6      53.1\n10  2.18e8   184       2069 173833 Chelonia … Green… Nicolas…     24.6      53.1\n# ℹ 5,346 more rows\n# ℹ 6 more variables: lprecision &lt;dbl&gt;, obs_date &lt;chr&gt;, date_time &lt;chr&gt;,\n#   count &lt;dbl&gt;, ds_type &lt;chr&gt;, platform &lt;chr&gt;",
    "crumbs": [
      "Tutorials",
      "R and RStudio Basics",
      "Loading data in R"
    ]
  },
  {
    "objectID": "r-tutorials/03-r-read-in-data.html#get-the-columns",
    "href": "r-tutorials/03-r-read-in-data.html#get-the-columns",
    "title": "R - Read in data",
    "section": "Get the columns",
    "text": "Get the columns\nColumn names.\n\ncolnames(obis_seamap_green)\n\n [1] \"oid\"        \"id\"         \"dataset_id\" \"tsn\"        \"scientific\"\n [6] \"common\"     \"provider\"   \"latitude\"   \"longitude\"  \"lprecision\"\n[11] \"obs_date\"   \"date_time\"  \"count\"      \"ds_type\"    \"platform\"  \n\n\nWe select a column with $ like so. Notice how RStudio suggest the names.\n\nobis_seamap_green$date_time\n\n   [1] \"5/16/16 6:54\"   \"5/16/16 6:54\"   \"5/16/16 6:55\"   \"5/16/16 6:56\"  \n   [5] \"5/16/16 6:56\"   \"5/17/16 5:40\"   \"5/17/16 6:32\"   \"5/17/16 11:30\" \n   [9] \"5/17/16 11:32\"  \"5/17/16 11:32\"  \"5/17/16 13:23\"  \"5/18/16 6:16\"  \n  [13] \"5/18/16 6:17\"   \"5/18/16 9:40\"   \"5/18/16 9:40\"   \"5/18/16 9:41\"  \n  [17] \"5/18/16 11:15\"  \"5/18/16 11:24\"  \"5/18/16 11:25\"  \"5/18/16 12:33\" \n  [21] \"5/18/16 22:12\"  \"5/18/16 23:54\"  \"5/19/16 6:38\"   \"5/19/16 11:09\" \n  [25] \"5/19/16 11:09\"  \"5/19/16 11:12\"  \"5/19/16 11:13\"  \"5/19/16 11:15\" \n  [29] \"5/19/16 12:32\"  \"5/19/16 12:38\"  \"5/19/16 22:03\"  \"5/19/16 22:03\" \n  [33] \"5/19/16 22:05\"  \"5/19/16 23:40\"  \"5/20/16 10:53\"  \"5/20/16 10:54\" \n  [37] \"5/20/16 10:59\"  \"5/20/16 10:59\"  \"5/20/16 12:39\"  \"5/20/16 12:42\" \n  [41] \"5/20/16 13:54\"  \"5/20/16 19:11\"  \"5/20/16 23:30\"  \"5/20/16 23:33\" \n  [45] \"5/20/16 23:36\"  \"5/21/16 6:51\"   \"5/21/16 10:41\"  \"5/21/16 10:41\" \n  [49] \"5/21/16 10:45\"  \"5/21/16 10:53\"  \"5/21/16 12:26\"  \"5/21/16 12:26\" \n  [53] \"5/21/16 15:03\"  \"5/21/16 23:15\"  \"5/21/16 23:20\"  \"5/21/16 23:26\" \n  [57] \"5/22/16 1:52\"   \"5/22/16 10:29\"  \"5/22/16 10:35\"  \"5/22/16 10:35\" \n  [61] \"5/22/16 10:37\"  \"5/22/16 12:14\"  \"5/22/16 12:59\"  \"5/22/16 15:10\" \n  [65] \"5/22/16 23:04\"  \"5/23/16 6:53\"   \"5/23/16 6:57\"   \"5/23/16 10:22\" \n  [69] \"5/23/16 10:25\"  \"5/23/16 10:30\"  \"5/23/16 10:30\"  \"5/23/16 12:06\" \n  [73] \"5/23/16 12:06\"  \"5/23/16 12:31\"  \"5/23/16 22:51\"  \"5/23/16 23:00\" \n  [77] \"5/24/16 5:01\"   \"5/24/16 11:50\"  \"5/24/16 11:50\"  \"5/24/16 11:52\" \n  [81] \"5/24/16 11:55\"  \"5/24/16 11:57\"  \"5/24/16 11:57\"  \"5/24/16 13:48\" \n  [85] \"5/24/16 22:44\"  \"5/24/16 22:47\"  \"5/24/16 22:48\"  \"5/25/16 6:18\"  \n  [89] \"5/25/16 11:40\"  \"5/25/16 11:41\"  \"5/25/16 11:41\"  \"5/25/16 11:42\" \n  [93] \"5/25/16 11:43\"  \"5/25/16 13:21\"  \"5/25/16 13:22\"  \"5/25/16 19:09\" \n  [97] \"5/25/16 22:30\"  \"5/25/16 22:33\"  \"5/26/16 6:51\"   \"5/26/16 9:52\"  \n [101] \"5/26/16 11:27\"  \"5/26/16 11:31\"  \"5/26/16 11:31\"  \"5/26/16 11:35\" \n [105] \"5/26/16 11:36\"  \"5/26/16 13:04\"  \"5/26/16 22:21\"  \"5/26/16 23:59\" \n [109] \"5/27/16 1:26\"   \"5/27/16 6:29\"   \"5/27/16 9:39\"   \"5/27/16 9:39\"  \n [113] \"5/27/16 11:14\"  \"5/27/16 11:18\"  \"5/27/16 12:30\"  \"5/27/16 13:57\" \n [117] \"5/27/16 14:15\"  \"5/27/16 22:12\"  \"5/27/16 23:48\"  \"5/27/16 23:51\" \n [121] \"5/28/16 5:12\"   \"5/28/16 6:48\"   \"5/28/16 9:31\"   \"5/28/16 11:12\" \n [125] \"5/28/16 11:15\"  \"5/28/16 11:15\"  \"5/28/16 13:51\"  \"5/28/16 13:54\" \n [129] \"5/28/16 13:54\"  \"5/28/16 23:37\"  \"5/28/16 23:37\"  \"5/29/16 6:31\"  \n [133] \"5/29/16 6:39\"   \"5/29/16 10:50\"  \"5/29/16 10:50\"  \"5/29/16 11:00\" \n [137] \"5/29/16 11:00\"  \"5/29/16 12:39\"  \"5/29/16 13:21\"  \"5/29/16 13:28\" \n [141] \"5/29/16 19:26\"  \"5/29/16 23:31\"  \"5/30/16 4:30\"   \"5/30/16 7:55\"  \n [145] \"5/30/16 10:40\"  \"5/30/16 10:40\"  \"5/30/16 10:43\"  \"5/30/16 10:43\" \n [149] \"5/30/16 12:56\"  \"5/30/16 13:36\"  \"5/30/16 23:14\"  \"5/30/16 23:16\" \n [153] \"5/30/16 23:20\"  \"5/31/16 1:10\"   \"5/31/16 2:46\"   \"5/31/16 5:53\"  \n [157] \"5/31/16 6:49\"   \"5/31/16 10:31\"  \"5/31/16 10:31\"  \"5/31/16 10:34\" \n [161] \"5/31/16 10:34\"  \"5/31/16 12:13\"  \"5/31/16 12:34\"  \"5/31/16 13:33\" \n [165] \"6/1/16 2:14\"    \"6/1/16 2:43\"    \"6/1/16 5:29\"    \"6/1/16 10:22\"  \n [169] \"6/1/16 10:25\"   \"6/1/16 10:25\"   \"6/1/16 12:05\"   \"6/1/16 13:13\"  \n [173] \"6/1/16 13:47\"   \"6/1/16 14:39\"   \"6/2/16 1:49\"    \"6/2/16 6:49\"   \n [177] \"6/2/16 10:08\"   \"6/2/16 10:08\"   \"6/2/16 10:12\"   \"6/2/16 11:48\"  \n [181] \"6/2/16 11:50\"   \"6/2/16 11:58\"   \"6/2/16 16:27\"   \"6/2/16 22:40\"  \n [185] \"6/2/16 22:42\"   \"6/2/16 22:50\"   \"6/3/16 0:23\"    \"6/3/16 6:35\"   \n [189] \"6/3/16 9:58\"    \"6/3/16 11:36\"   \"6/3/16 11:36\"   \"6/3/16 11:42\"  \n [193] \"6/3/16 12:49\"   \"6/3/16 12:57\"   \"6/3/16 22:29\"   \"6/3/16 22:29\"  \n [197] \"6/4/16 7:05\"    \"6/4/16 11:24\"   \"6/4/16 11:30\"   \"6/4/16 11:32\"  \n [201] \"6/4/16 22:17\"   \"6/4/16 22:17\"   \"6/4/16 22:23\"   \"6/5/16 3:29\"   \n [205] \"6/5/16 9:35\"    \"6/5/16 11:12\"   \"6/5/16 13:50\"   \"6/5/16 14:16\"  \n [209] \"6/5/16 22:11\"   \"6/5/16 23:45\"   \"6/5/16 23:47\"   \"6/6/16 5:30\"   \n [213] \"6/6/16 5:32\"    \"6/6/16 7:09\"    \"6/6/16 11:01\"   \"6/6/16 11:04\"  \n [217] \"6/6/16 11:07\"   \"6/6/16 23:38\"   \"6/7/16 5:55\"    \"6/7/16 6:48\"   \n [221] \"6/7/16 10:53\"   \"6/7/16 12:38\"   \"6/7/16 13:45\"   \"6/8/16 10:39\"  \n [225] \"6/8/16 10:42\"   \"6/8/16 23:15\"   \"6/9/16 10:32\"   \"6/9/16 12:15\"  \n [229] \"6/9/16 23:11\"   \"6/10/16 10:15\"  \"6/10/16 15:04\"  \"6/10/16 22:52\" \n [233] \"6/11/16 6:21\"   \"6/11/16 11:46\"  \"6/11/16 14:31\"  \"6/11/16 22:40\" \n [237] \"6/12/16 1:28\"   \"6/12/16 12:51\"  \"6/12/16 16:13\"  \"6/12/16 22:35\" \n [241] \"6/13/16 15:07\"  \"6/13/16 22:19\"  \"6/14/16 9:39\"   \"6/14/16 23:42\" \n [245] \"6/15/16 1:40\"   \"6/15/16 11:05\"  \"6/15/16 18:37\"  \"6/16/16 1:02\"  \n [249] \"6/16/16 7:01\"   \"6/16/16 13:45\"  \"6/17/16 3:51\"   \"6/17/16 5:49\"  \n [253] \"6/17/16 10:40\"  \"6/17/16 16:10\"  \"6/18/16 8:00\"   \"6/18/16 10:30\" \n [257] \"6/18/16 23:00\"  \"6/19/16 7:40\"   \"6/19/16 11:58\"  \"6/20/16 1:41\"  \n [261] \"6/20/16 11:50\"  \"6/21/16 1:03\"   \"6/21/16 12:44\"  \"8/1/16 13:11\"  \n [265] \"8/2/16 6:36\"    \"8/2/16 6:36\"    \"8/2/16 12:00\"   \"8/2/16 12:03\"  \n [269] \"8/2/16 13:06\"   \"8/2/16 22:53\"   \"8/2/16 22:54\"   \"8/3/16 2:45\"   \n [273] \"8/3/16 6:17\"    \"8/3/16 6:24\"    \"8/3/16 11:52\"   \"8/3/16 11:57\"  \n [277] \"8/3/16 11:58\"   \"8/3/16 12:40\"   \"8/3/16 14:26\"   \"8/3/16 14:28\"  \n [281] \"8/3/16 16:37\"   \"8/3/16 22:41\"   \"8/3/16 22:42\"   \"8/3/16 22:42\"  \n [285] \"8/3/16 22:43\"   \"8/4/16 6:44\"    \"8/4/16 6:48\"    \"8/4/16 10:00\"  \n [289] \"8/4/16 11:35\"   \"8/4/16 11:38\"   \"8/4/16 13:50\"   \"8/4/16 13:56\"  \n [293] \"8/4/16 14:16\"   \"8/4/16 14:33\"   \"8/4/16 22:28\"   \"8/4/16 22:29\"  \n [297] \"8/4/16 22:29\"   \"8/4/16 22:30\"   \"8/4/16 22:31\"   \"8/5/16 6:20\"   \n [301] \"8/5/16 11:22\"   \"8/5/16 11:24\"   \"8/5/16 11:29\"   \"8/5/16 11:33\"  \n [305] \"8/5/16 11:35\"   \"8/5/16 14:07\"   \"8/5/16 14:10\"   \"8/5/16 19:16\"  \n [309] \"8/5/16 22:19\"   \"8/5/16 22:20\"   \"8/5/16 22:20\"   \"8/5/16 23:56\"  \n [313] \"8/5/16 23:58\"   \"8/6/16 6:02\"    \"8/6/16 11:11\"   \"8/6/16 11:12\"  \n [317] \"8/6/16 11:16\"   \"8/6/16 11:16\"   \"8/6/16 13:01\"   \"8/6/16 13:06\"  \n [321] \"8/6/16 15:17\"   \"8/6/16 22:05\"   \"8/6/16 23:45\"   \"8/6/16 23:46\"  \n [325] \"8/6/16 23:46\"   \"8/6/16 23:49\"   \"8/6/16 23:56\"   \"8/7/16 4:11\"   \n [329] \"8/7/16 6:39\"    \"8/7/16 11:03\"   \"8/7/16 11:06\"   \"8/7/16 11:06\"  \n [333] \"8/7/16 12:40\"   \"8/7/16 12:41\"   \"8/7/16 13:42\"   \"8/7/16 14:19\"  \n [337] \"8/7/16 22:02\"   \"8/7/16 23:35\"   \"8/7/16 23:37\"   \"8/7/16 23:39\"  \n [341] \"8/7/16 23:41\"   \"8/8/16 2:48\"    \"8/8/16 6:15\"    \"8/8/16 6:16\"   \n [345] \"8/8/16 6:20\"    \"8/8/16 10:52\"   \"8/8/16 12:33\"   \"8/8/16 12:34\"  \n [349] \"8/8/16 13:27\"   \"8/8/16 13:29\"   \"8/8/16 13:32\"   \"8/8/16 15:50\"  \n [353] \"8/8/16 23:22\"   \"8/8/16 23:24\"   \"8/8/16 23:26\"   \"8/9/16 3:57\"   \n [357] \"8/9/16 6:44\"    \"8/9/16 10:41\"   \"8/9/16 10:46\"   \"8/9/16 12:21\"  \n [361] \"8/9/16 12:25\"   \"8/9/16 12:25\"   \"8/9/16 13:43\"   \"8/9/16 23:09\"  \n [365] \"8/9/16 23:10\"   \"8/9/16 23:16\"   \"8/9/16 23:17\"   \"8/10/16 10:35\" \n [369] \"8/10/16 12:11\"  \"8/10/16 12:12\"  \"8/10/16 13:01\"  \"8/10/16 13:10\" \n [373] \"8/10/16 19:12\"  \"8/10/16 22:59\"  \"8/10/16 22:59\"  \"8/10/16 23:03\" \n [377] \"8/10/16 23:09\"  \"8/11/16 5:56\"   \"8/11/16 6:59\"   \"8/11/16 6:59\"  \n [381] \"8/11/16 10:22\"  \"8/11/16 12:04\"  \"8/11/16 12:07\"  \"8/11/16 12:55\" \n [385] \"8/11/16 14:40\"  \"8/11/16 22:50\"  \"8/11/16 22:55\"  \"8/12/16 3:14\"  \n [389] \"8/12/16 6:33\"   \"8/12/16 6:36\"   \"8/12/16 6:36\"   \"8/12/16 10:07\" \n [393] \"8/12/16 11:54\"  \"8/12/16 13:46\"  \"8/12/16 13:47\"  \"8/12/16 13:59\" \n [397] \"8/12/16 22:39\"  \"8/13/16 2:54\"   \"8/13/16 6:12\"   \"8/13/16 11:35\" \n [401] \"8/13/16 11:42\"  \"8/13/16 11:42\"  \"8/13/16 13:16\"  \"8/13/16 13:18\" \n [405] \"8/13/16 14:57\"  \"8/13/16 22:27\"  \"8/13/16 22:27\"  \"8/13/16 22:30\" \n [409] \"8/13/16 22:34\"  \"8/14/16 11:25\"  \"8/14/16 11:26\"  \"8/14/16 13:02\" \n [413] \"8/14/16 14:40\"  \"8/14/16 15:44\"  \"8/14/16 22:13\"  \"8/14/16 22:16\" \n [417] \"8/14/16 22:19\"  \"8/14/16 23:56\"  \"8/14/16 23:58\"  \"8/15/16 5:29\"  \n [421] \"8/15/16 11:10\"  \"8/15/16 11:11\"  \"8/15/16 13:54\"  \"8/15/16 13:57\" \n [425] \"8/15/16 13:58\"  \"8/15/16 22:07\"  \"8/15/16 23:45\"  \"8/15/16 23:48\" \n [429] \"8/15/16 23:50\"  \"8/15/16 23:51\"  \"8/16/16 4:18\"   \"8/16/16 5:52\"  \n [433] \"8/16/16 6:04\"   \"8/16/16 11:00\"  \"8/16/16 11:02\"  \"8/16/16 11:06\" \n [437] \"8/16/16 12:14\"  \"8/16/16 13:20\"  \"8/16/16 15:20\"  \"8/16/16 23:33\" \n [441] \"8/16/16 23:36\"  \"8/16/16 23:38\"  \"8/17/16 2:18\"   \"8/17/16 5:39\"  \n [445] \"8/17/16 10:49\"  \"8/17/16 12:34\"  \"8/17/16 12:35\"  \"8/17/16 14:29\" \n [449] \"8/17/16 14:37\"  \"8/17/16 16:12\"  \"8/17/16 23:20\"  \"8/17/16 23:26\" \n [453] \"8/17/16 23:29\"  \"8/17/16 23:30\"  \"8/18/16 2:02\"   \"8/18/16 3:39\"  \n [457] \"8/18/16 6:53\"   \"8/18/16 10:38\"  \"8/18/16 12:23\"  \"8/18/16 12:25\" \n [461] \"8/18/16 12:25\"  \"8/18/16 13:16\"  \"8/18/16 13:19\"  \"8/18/16 23:10\" \n [465] \"8/18/16 23:10\"  \"8/19/16 5:45\"   \"8/19/16 6:31\"   \"8/19/16 10:33\" \n [469] \"8/19/16 12:13\"  \"8/19/16 12:39\"  \"8/19/16 12:40\"  \"8/19/16 13:31\" \n [473] \"8/19/16 14:15\"  \"8/19/16 22:56\"  \"8/19/16 23:03\"  \"8/19/16 23:05\" \n [477] \"8/19/16 23:06\"  \"8/20/16 11:56\"  \"8/20/16 11:56\"  \"8/20/16 11:57\" \n [481] \"8/20/16 11:59\"  \"8/20/16 12:06\"  \"8/20/16 12:52\"  \"8/20/16 12:55\" \n [485] \"8/20/16 19:05\"  \"8/20/16 19:06\"  \"8/20/16 19:08\"  \"8/20/16 22:50\" \n [489] \"8/20/16 22:50\"  \"8/20/16 22:51\"  \"8/20/16 22:53\"  \"8/21/16 1:40\"  \n [493] \"8/21/16 5:48\"   \"8/21/16 5:55\"   \"8/21/16 6:49\"   \"8/21/16 11:44\" \n [497] \"8/21/16 11:51\"  \"8/21/16 11:55\"  \"8/21/16 12:39\"  \"8/21/16 13:29\" \n [501] \"8/21/16 14:08\"  \"8/21/16 14:18\"  \"8/21/16 22:35\"  \"8/21/16 22:39\" \n [505] \"8/21/16 22:43\"  \"8/22/16 1:57\"   \"8/22/16 6:25\"   \"8/22/16 9:54\"  \n [509] \"8/22/16 11:33\"  \"8/22/16 11:33\"  \"8/22/16 11:35\"  \"8/22/16 11:36\" \n [513] \"8/22/16 11:39\"  \"8/22/16 13:33\"  \"8/22/16 22:25\"  \"8/22/16 22:26\" \n [517] \"8/22/16 22:28\"  \"8/22/16 22:30\"  \"8/22/16 22:31\"  \"8/23/16 6:48\"  \n [521] \"8/23/16 11:21\"  \"8/23/16 11:23\"  \"8/23/16 11:26\"  \"8/23/16 11:28\" \n [525] \"8/23/16 11:32\"  \"8/23/16 13:59\"  \"8/23/16 14:26\"  \"8/23/16 22:13\" \n [529] \"8/23/16 22:16\"  \"8/23/16 23:51\"  \"8/23/16 23:52\"  \"8/23/16 23:53\" \n [533] \"8/23/16 23:59\"  \"8/24/16 11:13\"  \"8/24/16 11:14\"  \"8/24/16 11:14\" \n [537] \"8/24/16 11:16\"  \"8/24/16 11:17\"  \"8/24/16 13:55\"  \"8/24/16 14:01\" \n [541] \"8/24/16 22:03\"  \"8/24/16 22:06\"  \"8/24/16 23:40\"  \"8/24/16 23:40\" \n [545] \"8/24/16 23:41\"  \"8/24/16 23:44\"  \"8/24/16 23:46\"  \"8/25/16 5:23\"  \n [549] \"8/25/16 6:09\"   \"8/25/16 10:56\"  \"8/25/16 11:00\"  \"8/25/16 11:02\" \n [553] \"8/25/16 11:02\"  \"8/25/16 13:27\"  \"8/25/16 13:28\"  \"8/25/16 15:09\" \n [557] \"8/25/16 23:32\"  \"8/25/16 23:34\"  \"8/25/16 23:36\"  \"8/25/16 23:41\" \n [561] \"8/25/16 23:41\"  \"8/26/16 5:47\"   \"8/26/16 10:44\"  \"8/26/16 12:31\" \n [565] \"8/26/16 12:33\"  \"8/26/16 12:33\"  \"8/26/16 12:33\"  \"8/26/16 12:33\" \n [569] \"8/26/16 13:05\"  \"8/26/16 21:41\"  \"8/26/16 23:17\"  \"8/26/16 23:18\" \n [573] \"8/26/16 23:18\"  \"8/26/16 23:18\"  \"8/26/16 23:25\"  \"8/27/16 1:29\"  \n [577] \"8/27/16 3:35\"   \"8/27/16 5:36\"   \"8/27/16 10:46\"  \"8/27/16 12:20\" \n [581] \"8/27/16 12:20\"  \"8/27/16 12:39\"  \"8/27/16 13:14\"  \"8/27/16 14:17\" \n [585] \"8/27/16 14:49\"  \"8/27/16 23:07\"  \"8/27/16 23:07\"  \"8/27/16 23:12\" \n [589] \"8/27/16 23:15\"  \"8/28/16 6:02\"   \"8/28/16 12:11\"  \"8/28/16 12:11\" \n [593] \"8/28/16 13:00\"  \"8/28/16 13:01\"  \"8/28/16 13:44\"  \"8/28/16 13:51\" \n [597] \"8/28/16 13:56\"  \"8/28/16 22:55\"  \"8/28/16 22:59\"  \"8/28/16 22:59\" \n [601] \"8/28/16 23:02\"  \"8/29/16 10:11\"  \"8/29/16 10:14\"  \"8/29/16 10:17\" \n [605] \"8/29/16 11:58\"  \"8/29/16 12:00\"  \"8/29/16 12:02\"  \"8/29/16 13:16\" \n [609] \"8/29/16 19:24\"  \"8/29/16 22:48\"  \"8/29/16 22:50\"  \"8/29/16 22:51\" \n [613] \"8/29/16 22:52\"  \"8/30/16 2:01\"   \"8/30/16 2:56\"   \"8/30/16 6:08\"  \n [617] \"8/30/16 6:11\"   \"8/30/16 11:42\"  \"8/30/16 11:46\"  \"8/30/16 11:46\" \n [621] \"8/30/16 13:07\"  \"8/30/16 13:09\"  \"8/30/16 14:18\"  \"8/30/16 14:46\" \n [625] \"8/30/16 22:37\"  \"8/30/16 22:42\"  \"8/31/16 0:22\"   \"8/31/16 2:52\"  \n [629] \"8/31/16 5:43\"   \"8/31/16 11:31\"  \"8/31/16 11:36\"  \"8/31/16 11:36\" \n [633] \"8/31/16 11:38\"  \"8/31/16 11:40\"  \"8/31/16 13:53\"  \"8/31/16 22:25\" \n [637] \"8/31/16 22:27\"  \"8/31/16 22:30\"  \"9/1/16 6:25\"    \"9/1/16 11:19\"  \n [641] \"9/1/16 11:20\"   \"9/1/16 11:20\"   \"9/1/16 11:21\"   \"9/1/16 11:29\"  \n [645] \"9/1/16 13:57\"   \"9/1/16 23:49\"   \"9/1/16 23:50\"   \"9/1/16 23:58\"  \n [649] \"9/2/16 6:42\"    \"9/2/16 6:46\"    \"9/2/16 9:31\"    \"9/2/16 11:09\"  \n [653] \"9/2/16 11:12\"   \"9/2/16 11:12\"   \"9/2/16 11:12\"   \"9/2/16 11:16\"  \n [657] \"9/2/16 21:59\"   \"9/2/16 23:44\"   \"9/2/16 23:45\"   \"9/3/16 6:26\"   \n [661] \"9/3/16 11:01\"   \"9/3/16 12:42\"   \"9/3/16 13:34\"   \"9/3/16 14:01\"  \n [665] \"9/3/16 14:02\"   \"9/3/16 14:42\"   \"9/3/16 19:21\"   \"9/3/16 23:28\"  \n [669] \"9/3/16 23:29\"   \"9/3/16 23:33\"   \"9/4/16 3:46\"    \"9/4/16 6:58\"   \n [673] \"9/4/16 10:48\"   \"9/4/16 12:31\"   \"9/4/16 13:25\"   \"9/4/16 13:31\"  \n [677] \"9/4/16 14:19\"   \"9/4/16 18:13\"   \"9/4/16 19:00\"   \"9/4/16 19:01\"  \n [681] \"9/4/16 23:17\"   \"9/4/16 23:19\"   \"9/5/16 6:41\"    \"9/5/16 6:42\"   \n [685] \"9/5/16 10:39\"   \"9/5/16 12:17\"   \"9/5/16 12:19\"   \"9/5/16 12:21\"  \n [689] \"9/5/16 14:41\"   \"9/5/16 14:43\"   \"9/5/16 23:11\"   \"9/5/16 23:12\"  \n [693] \"9/6/16 5:29\"    \"9/6/16 6:15\"    \"9/6/16 10:22\"   \"9/6/16 10:25\"  \n [697] \"9/6/16 10:29\"   \"9/6/16 12:03\"   \"9/6/16 12:09\"   \"9/6/16 12:56\"  \n [701] \"9/6/16 22:59\"   \"9/6/16 23:00\"   \"9/7/16 3:10\"    \"9/7/16 6:39\"   \n [705] \"9/7/16 6:48\"    \"9/7/16 11:54\"   \"9/7/16 11:56\"   \"9/7/16 13:05\"  \n [709] \"9/7/16 13:39\"   \"9/7/16 13:40\"   \"9/7/16 17:53\"   \"9/7/16 22:49\"  \n [713] \"9/7/16 22:52\"   \"9/8/16 6:22\"    \"9/8/16 6:25\"    \"9/8/16 10:05\"  \n [717] \"9/8/16 11:45\"   \"9/8/16 12:40\"   \"9/8/16 13:06\"   \"9/8/16 14:17\"  \n [721] \"9/8/16 22:32\"   \"9/8/16 22:36\"   \"9/9/16 2:41\"    \"9/9/16 5:12\"   \n [725] \"9/9/16 6:03\"    \"9/9/16 6:03\"    \"9/9/16 6:57\"    \"9/9/16 11:32\"  \n [729] \"9/9/16 11:34\"   \"9/9/16 11:36\"   \"9/9/16 13:58\"   \"9/9/16 15:39\"  \n [733] \"9/10/16 2:56\"   \"9/10/16 3:04\"   \"9/10/16 6:38\"   \"9/10/16 6:39\"  \n [737] \"9/10/16 11:19\"  \"9/10/16 11:21\"  \"9/10/16 13:30\"  \"9/10/16 13:31\" \n [741] \"9/10/16 13:32\"  \"9/10/16 23:48\"  \"9/11/16 6:17\"   \"9/11/16 11:05\" \n [745] \"9/11/16 13:07\"  \"9/11/16 13:07\"  \"9/11/16 13:39\"  \"9/11/16 13:40\" \n [749] \"9/11/16 23:37\"  \"9/11/16 23:39\"  \"9/11/16 23:44\"  \"9/12/16 3:38\"  \n [753] \"9/12/16 5:53\"   \"9/12/16 10:53\"  \"9/12/16 10:55\"  \"9/12/16 12:40\" \n [757] \"9/12/16 13:25\"  \"9/12/16 13:28\"  \"9/12/16 14:23\"  \"9/12/16 19:32\" \n [761] \"9/12/16 21:50\"  \"9/12/16 23:34\"  \"9/13/16 4:42\"   \"9/13/16 10:51\" \n [765] \"9/13/16 12:27\"  \"9/13/16 13:14\"  \"9/13/16 14:58\"  \"9/13/16 19:09\" \n [769] \"9/13/16 23:23\"  \"9/14/16 5:58\"   \"9/14/16 6:01\"   \"9/14/16 6:01\"  \n [773] \"9/14/16 13:04\"  \"9/14/16 13:16\"  \"9/14/16 13:18\"  \"9/14/16 13:28\" \n [777] \"9/14/16 14:42\"  \"9/14/16 23:09\"  \"9/14/16 23:11\"  \"9/15/16 3:36\"  \n [781] \"9/15/16 5:44\"   \"9/15/16 6:32\"   \"9/15/16 6:34\"   \"9/15/16 10:25\" \n [785] \"9/15/16 12:04\"  \"9/15/16 13:04\"  \"9/15/16 13:07\"  \"9/15/16 14:29\" \n [789] \"9/15/16 22:57\"  \"9/16/16 6:54\"   \"9/16/16 6:58\"   \"9/16/16 10:13\" \n [793] \"9/16/16 11:49\"  \"9/16/16 12:41\"  \"9/16/16 14:28\"  \"9/16/16 22:43\" \n [797] \"9/16/16 22:44\"  \"9/16/16 22:49\"  \"9/17/16 2:41\"   \"9/17/16 6:33\"  \n [801] \"9/17/16 6:37\"   \"9/17/16 11:37\"  \"9/17/16 11:41\"  \"9/17/16 11:42\" \n [805] \"9/17/16 11:46\"  \"9/17/16 17:46\"  \"9/17/16 22:33\"  \"9/17/16 22:36\" \n [809] \"9/18/16 6:16\"   \"9/18/16 6:16\"   \"9/18/16 11:27\"  \"9/18/16 13:28\" \n [813] \"9/18/16 14:37\"  \"9/18/16 15:09\"  \"9/18/16 19:07\"  \"9/18/16 22:21\" \n [817] \"9/18/16 22:24\"  \"9/19/16 1:45\"   \"9/19/16 1:58\"   \"9/19/16 5:51\"  \n [821] \"9/19/16 5:52\"   \"9/19/16 11:13\"  \"9/19/16 11:14\"  \"9/19/16 13:50\" \n [825] \"9/19/16 14:07\"  \"9/19/16 15:46\"  \"9/20/16 1:06\"   \"9/20/16 6:26\"  \n [829] \"9/20/16 7:11\"   \"9/20/16 11:08\"  \"9/20/16 12:41\"  \"9/20/16 15:07\" \n [833] \"9/20/16 22:03\"  \"9/20/16 23:41\"  \"9/21/16 2:14\"   \"9/21/16 6:49\"  \n [837] \"9/21/16 10:56\"  \"9/21/16 12:41\"  \"9/21/16 13:22\"  \"9/21/16 13:54\" \n [841] \"9/21/16 16:24\"  \"9/21/16 23:24\"  \"9/22/16 4:02\"   \"9/22/16 6:30\"  \n [845] \"9/22/16 6:31\"   \"9/22/16 12:27\"  \"9/22/16 13:16\"  \"9/22/16 14:51\" \n [849] \"9/22/16 19:24\"  \"9/22/16 23:19\"  \"9/23/16 3:23\"   \"9/23/16 6:08\"  \n [853] \"9/23/16 12:11\"  \"9/23/16 12:59\"  \"9/23/16 13:07\"  \"9/23/16 14:36\" \n [857] \"9/23/16 19:02\"  \"9/24/16 3:16\"   \"9/24/16 6:42\"   \"9/24/16 6:47\"  \n [861] \"9/24/16 11:59\"  \"9/24/16 12:40\"  \"9/25/16 3:05\"   \"9/25/16 14:11\" \n [865] \"9/27/16 13:56\"  \"9/27/16 22:23\"  \"9/28/16 11:19\"  \"9/28/16 14:18\" \n [869] \"9/28/16 19:03\"  \"9/28/16 23:46\"  \"9/28/16 23:53\"  \"9/29/16 1:28\"  \n [873] \"9/29/16 5:50\"   \"9/29/16 11:04\"  \"9/29/16 11:04\"  \"9/29/16 23:40\" \n [877] \"9/30/16 6:23\"   \"9/30/16 10:52\"  \"10/1/16 3:46\"   \"10/1/16 12:25\" \n [881] \"10/1/16 14:30\"  \"10/2/16 10:33\"  \"10/2/16 13:57\"  \"10/2/16 19:16\" \n [885] \"10/2/16 23:01\"  \"10/3/16 12:03\"  \"10/3/16 15:07\"  \"10/3/16 19:00\" \n [889] \"10/3/16 22:55\"  \"10/4/16 11:50\"  \"10/4/16 15:56\"  \"10/4/16 22:46\" \n [893] \"10/5/16 6:21\"   \"10/5/16 11:39\"  \"10/6/16 5:07\"   \"10/6/16 11:27\" \n [897] \"10/6/16 14:23\"  \"10/7/16 15:18\"  \"10/7/16 15:19\"  \"10/7/16 23:49\" \n [901] \"10/7/16 23:51\"  \"10/8/16 11:05\"  \"10/8/16 15:55\"  \"10/8/16 23:32\" \n [905] \"10/8/16 23:35\"  \"10/9/16 14:50\"  \"10/9/16 14:59\"  \"10/9/16 23:24\" \n [909] \"10/9/16 23:27\"  \"10/10/16 5:22\"  \"10/10/16 14:55\" \"10/10/16 16:34\"\n [913] \"10/11/16 4:58\"  \"10/11/16 17:06\" \"10/12/16 13:30\" \"10/12/16 22:55\"\n [917] \"10/13/16 11:47\" \"10/13/16 17:08\" \"10/13/16 22:37\" \"10/13/16 22:43\"\n [921] \"10/14/16 3:41\"  \"10/14/16 7:18\"  \"10/14/16 13:55\" \"10/14/16 22:28\"\n [925] \"10/15/16 11:24\" \"10/15/16 14:03\" \"10/15/16 15:42\" \"10/15/16 22:15\"\n [929] \"10/15/16 23:51\" \"10/15/16 23:57\" \"10/16/16 6:31\"  \"10/16/16 14:58\"\n [933] \"10/16/16 15:03\" \"10/16/16 17:44\" \"10/16/16 22:04\" \"10/16/16 23:47\"\n [937] \"10/17/16 13:09\" \"10/17/16 13:14\" \"10/17/16 19:07\" \"10/17/16 23:32\"\n [941] \"10/17/16 23:32\" \"10/18/16 10:45\" \"10/18/16 14:24\" \"10/18/16 23:19\"\n [945] \"10/18/16 23:24\" \"10/18/16 23:27\" \"10/19/16 5:41\"  \"10/19/16 6:28\" \n [949] \"10/19/16 7:15\"  \"10/19/16 10:39\" \"10/19/16 10:42\" \"10/19/16 23:12\"\n [953] \"10/20/16 0:48\"  \"10/20/16 6:57\"  \"10/20/16 10:27\" \"10/20/16 23:02\"\n [957] \"10/21/16 6:32\"  \"10/21/16 11:57\" \"10/21/16 17:48\" \"10/21/16 22:44\"\n [961] \"10/22/16 10:04\" \"10/22/16 13:33\" \"10/22/16 22:38\" \"10/23/16 11:40\"\n [965] \"10/23/16 22:25\" \"10/23/16 22:26\" \"10/24/16 6:28\"  \"10/24/16 11:25\"\n [969] \"10/24/16 22:16\" \"10/25/16 1:25\"  \"10/25/16 6:47\"  \"10/25/16 13:29\"\n [973] \"10/26/16 14:31\" \"10/27/16 13:59\" \"10/27/16 23:16\" \"10/27/16 23:19\"\n [977] \"10/28/16 2:27\"  \"10/28/16 14:36\" \"10/28/16 14:43\" \"10/28/16 23:05\"\n [981] \"10/29/16 12:04\" \"10/29/16 14:30\" \"10/29/16 14:56\" \"10/29/16 22:55\"\n [985] \"10/30/16 6:49\"  \"10/30/16 14:24\" \"10/30/16 16:24\" \"10/30/16 22:43\"\n [989] \"10/30/16 22:46\" \"10/31/16 6:22\"  \"10/31/16 11:43\" \"10/31/16 13:56\"\n [993] \"10/31/16 15:36\" \"10/31/16 22:35\" \"10/31/16 22:36\" \"11/1/16 5:16\"  \n [997] \"11/1/16 11:27\"  \"11/1/16 11:31\"  \"11/1/16 13:26\"  \"11/1/16 22:20\" \n[1001] \"11/2/16 13:43\"  \"11/2/16 14:29\"  \"11/2/16 15:18\"  \"11/2/16 23:54\" \n[1005] \"11/2/16 23:58\"  \"11/3/16 3:56\"   \"11/3/16 5:26\"   \"11/3/16 11:03\" \n[1009] \"11/3/16 15:10\"  \"11/3/16 23:35\"  \"11/4/16 10:58\"  \"11/4/16 13:55\" \n[1013] \"11/4/16 23:29\"  \"11/5/16 7:12\"   \"11/5/16 14:44\"  \"11/5/16 14:44\" \n[1017] \"11/5/16 23:15\"  \"11/5/16 23:17\"  \"11/5/16 23:18\"  \"11/6/16 10:34\" \n[1021] \"11/6/16 13:10\"  \"11/6/16 14:03\"  \"11/6/16 23:04\"  \"11/6/16 23:06\" \n[1025] \"11/7/16 3:09\"   \"11/7/16 6:35\"   \"11/7/16 12:07\"  \"11/7/16 13:33\" \n[1029] \"11/7/16 14:20\"  \"11/7/16 22:53\"  \"11/8/16 0:39\"   \"11/8/16 1:12\"  \n[1033] \"11/8/16 10:17\"  \"11/8/16 11:54\"  \"11/8/16 14:08\"  \"11/9/16 11:37\" \n[1037] \"11/9/16 13:30\"  \"11/9/16 22:32\"  \"11/9/16 22:36\"  \"11/10/16 2:57\" \n[1041] \"11/10/16 11:25\" \"11/10/16 11:26\" \"11/10/16 11:32\" \"11/10/16 22:25\"\n[1045] \"11/10/16 22:25\" \"11/11/16 11:13\" \"11/11/16 11:14\" \"11/11/16 11:15\"\n[1049] \"11/11/16 22:08\" \"11/11/16 23:45\" \"11/11/16 23:45\" \"11/12/16 1:58\" \n[1053] \"11/12/16 1:58\"  \"11/12/16 11:00\" \"11/12/16 11:09\" \"11/12/16 15:00\"\n[1057] \"11/12/16 23:41\" \"11/13/16 1:24\"  \"11/13/16 10:55\" \"11/13/16 10:56\"\n[1061] \"11/13/16 12:37\" \"11/13/16 23:25\" \"11/13/16 23:35\" \"11/14/16 6:35\" \n[1065] \"11/14/16 12:25\" \"11/14/16 13:08\" \"11/14/16 13:13\" \"11/14/16 23:12\"\n[1069] \"11/14/16 23:13\" \"11/15/16 3:42\"  \"11/15/16 3:43\"  \"11/15/16 10:28\"\n[1073] \"11/15/16 12:12\" \"11/15/16 14:22\" \"11/15/16 23:00\" \"11/16/16 3:07\" \n[1077] \"11/16/16 6:54\"  \"11/16/16 14:07\" \"11/16/16 14:16\" \"11/16/16 15:32\"\n[1081] \"11/16/16 22:56\" \"11/17/16 5:35\"  \"11/17/16 5:37\"  \"11/17/16 13:25\"\n[1085] \"11/17/16 13:33\" \"11/17/16 13:36\" \"11/17/16 22:44\" \"11/18/16 4:15\" \n[1089] \"11/18/16 6:53\"  \"11/18/16 11:38\" \"11/18/16 13:12\" \"11/18/16 13:56\"\n[1093] \"11/18/16 22:30\" \"11/19/16 6:34\"  \"11/19/16 11:24\" \"11/19/16 13:45\"\n[1097] \"11/19/16 14:07\" \"11/19/16 22:19\" \"11/19/16 22:19\" \"11/20/16 11:19\"\n[1101] \"11/20/16 11:19\" \"11/20/16 11:20\" \"11/20/16 19:04\" \"11/20/16 19:04\"\n[1105] \"11/20/16 23:47\" \"11/21/16 2:15\"  \"11/21/16 10:58\" \"11/21/16 11:07\"\n[1109] \"11/21/16 13:22\" \"11/21/16 21:58\" \"11/21/16 23:42\" \"11/22/16 6:25\" \n[1113] \"11/22/16 6:29\"  \"11/22/16 10:52\" \"11/22/16 13:11\" \"11/23/16 6:51\" \n[1117] \"11/23/16 12:22\" \"11/23/16 12:24\" \"11/23/16 23:12\" \"11/24/16 6:33\" \n[1121] \"11/24/16 10:28\" \"11/24/16 14:05\" \"11/24/16 23:05\" \"11/25/16 6:12\" \n[1125] \"11/25/16 10:19\" \"11/25/16 12:04\" \"11/25/16 22:53\" \"11/26/16 6:46\" \n[1129] \"11/26/16 13:43\" \"11/26/16 15:22\" \"11/26/16 22:45\" \"11/27/16 4:15\" \n[1133] \"11/27/16 11:40\" \"11/27/16 16:37\" \"11/27/16 22:33\" \"11/28/16 6:46\" \n[1137] \"11/28/16 11:25\" \"11/28/16 13:38\" \"11/28/16 15:18\" \"11/28/16 22:16\"\n[1141] \"11/28/16 23:59\" \"11/29/16 11:14\" \"11/29/16 11:18\" \"11/29/16 13:31\"\n[1145] \"11/29/16 19:18\" \"11/29/16 19:19\" \"11/29/16 23:45\" \"11/30/16 3:47\" \n[1149] \"11/30/16 6:04\"  \"11/30/16 6:08\"  \"11/30/16 11:01\" \"11/30/16 12:42\"\n[1153] \"11/30/16 12:44\" \"12/1/16 3:31\"   \"12/1/16 6:39\"   \"12/1/16 12:34\" \n[1157] \"12/1/16 12:46\"  \"12/1/16 12:47\"  \"12/1/16 23:17\"  \"12/2/16 5:23\"  \n[1161] \"12/2/16 10:42\"  \"12/2/16 12:19\"  \"12/2/16 14:30\"  \"12/2/16 23:13\" \n[1165] \"12/2/16 23:17\"  \"12/3/16 6:40\"   \"12/3/16 6:42\"   \"12/3/16 12:10\" \n[1169] \"12/3/16 13:25\"  \"12/3/16 13:32\"  \"12/3/16 23:03\"  \"12/4/16 3:00\"  \n[1173] \"12/4/16 6:27\"   \"12/4/16 11:52\"  \"12/4/16 11:56\"  \"12/4/16 14:10\" \n[1177] \"12/4/16 22:51\"  \"12/5/16 3:22\"   \"12/5/16 6:04\"   \"12/5/16 6:59\"  \n[1181] \"12/5/16 11:43\"  \"12/5/16 11:46\"  \"12/5/16 12:48\"  \"12/6/16 6:37\"  \n[1185] \"12/6/16 11:37\"  \"12/6/16 11:39\"  \"12/6/16 13:44\"  \"12/6/16 22:23\" \n[1189] \"12/6/16 22:24\"  \"12/7/16 13:31\"  \"12/7/16 13:36\"  \"12/7/16 13:38\" \n[1193] \"12/7/16 23:51\"  \"12/7/16 23:55\"  \"12/8/16 5:51\"   \"12/8/16 11:08\" \n[1197] \"12/8/16 11:09\"  \"12/8/16 15:10\"  \"12/8/16 23:41\"  \"12/9/16 6:15\"  \n[1201] \"12/9/16 6:24\"   \"12/9/16 13:13\"  \"12/9/16 14:27\"  \"12/9/16 14:50\" \n[1205] \"12/9/16 23:32\"  \"12/10/16 6:49\"  \"12/10/16 12:30\" \"12/10/16 17:14\"\n[1209] \"12/10/16 23:25\" \"12/11/16 5:34\"  \"12/11/16 5:37\"  \"12/11/16 10:34\"\n[1213] \"12/11/16 10:39\" \"12/11/16 12:21\" \"12/11/16 23:09\" \"12/12/16 6:07\" \n[1217] \"12/12/16 6:53\"  \"12/12/16 10:33\" \"12/12/16 14:47\" \"12/12/16 14:51\"\n[1221] \"12/13/16 3:19\"  \"12/13/16 11:52\" \"12/13/16 14:03\" \"12/13/16 14:09\"\n[1225] \"12/13/16 22:50\" \"12/13/16 22:50\" \"12/14/16 1:15\"  \"12/14/16 4:36\" \n[1229] \"12/14/16 6:13\"  \"12/14/16 10:07\" \"12/14/16 11:40\" \"12/14/16 13:58\"\n[1233] \"12/15/16 3:13\"  \"12/15/16 4:11\"  \"12/15/16 5:57\"  \"12/15/16 11:28\"\n[1237] \"12/15/16 11:32\" \"12/15/16 13:55\" \"12/16/16 0:07\"  \"12/16/16 6:27\" \n[1241] \"12/16/16 6:31\"  \"12/16/16 11:22\" \"12/16/16 13:09\" \"12/16/16 15:04\"\n[1245] \"12/17/16 3:44\"  \"12/17/16 6:10\"  \"12/17/16 11:07\" \"12/17/16 11:08\"\n[1249] \"12/17/16 18:06\" \"12/17/16 23:45\" \"12/18/16 3:36\"  \"12/18/16 6:31\" \n[1253] \"12/18/16 10:56\" \"12/18/16 10:58\" \"12/18/16 23:30\" \"12/19/16 3:24\" \n[1257] \"12/19/16 12:25\" \"12/19/16 13:34\" \"12/19/16 15:18\" \"12/19/16 19:05\"\n[1261] \"12/20/16 3:15\"  \"12/20/16 5:55\"  \"12/20/16 10:35\" \"12/20/16 12:17\"\n[1265] \"12/20/16 12:19\" \"12/21/16 4:37\"  \"12/21/16 10:21\" \"12/21/16 12:46\"\n[1269] \"12/21/16 12:46\" \"12/21/16 22:56\" \"12/21/16 22:58\" \"12/22/16 2:52\" \n[1273] \"12/22/16 6:45\"  \"12/22/16 6:48\"  \"12/22/16 10:12\" \"12/22/16 11:57\"\n[1277] \"12/22/16 11:58\" \"12/23/16 4:50\"  \"12/23/16 9:58\"  \"12/23/16 11:39\"\n[1281] \"12/23/16 14:43\" \"12/23/16 22:34\" \"12/23/16 22:39\" \"12/24/16 3:26\" \n[1285] \"12/24/16 3:44\"  \"12/24/16 11:28\" \"12/24/16 11:28\" \"12/24/16 14:16\"\n[1289] \"12/24/16 22:18\" \"12/25/16 5:50\"  \"12/25/16 11:21\" \"12/25/16 13:38\"\n[1293] \"12/25/16 15:09\" \"12/25/16 23:55\" \"12/25/16 23:57\" \"12/26/16 5:26\" \n[1297] \"12/26/16 5:26\"  \"12/26/16 5:29\"  \"12/26/16 11:11\" \"12/26/16 13:18\"\n[1301] \"12/26/16 14:50\" \"12/27/16 3:27\"  \"12/27/16 10:58\" \"12/27/16 12:37\"\n[1305] \"12/27/16 13:32\" \"12/27/16 23:29\" \"12/27/16 23:34\" \"12/28/16 4:46\" \n[1309] \"12/28/16 6:21\"  \"12/28/16 6:32\"  \"12/28/16 12:24\" \"12/28/16 12:25\"\n[1313] \"12/28/16 14:37\" \"12/29/16 2:32\"  \"12/29/16 6:01\"  \"12/29/16 12:10\"\n[1317] \"12/29/16 12:13\" \"12/29/16 17:21\" \"12/29/16 23:09\" \"12/30/16 10:21\"\n[1321] \"12/30/16 10:21\" \"12/30/16 14:27\" \"12/30/16 22:50\" \"12/30/16 22:52\"\n[1325] \"12/31/16 5:21\"  \"12/31/16 5:22\"  \"12/31/16 11:46\" \"12/31/16 13:36\"\n[1329] \"12/31/16 14:03\" \"12/31/16 22:47\" \"1/1/17 0:23\"    \"1/1/17 2:39\"   \n[1333] \"1/1/17 2:42\"    \"1/1/17 11:41\"   \"1/1/17 13:12\"   \"1/2/17 3:20\"   \n[1337] \"1/2/17 3:22\"    \"1/2/17 4:00\"    \"1/2/17 11:25\"   \"1/2/17 11:29\"  \n[1341] \"1/2/17 11:30\"   \"1/3/17 2:52\"    \"1/3/17 3:52\"    \"1/3/17 12:57\"  \n[1345] \"1/3/17 13:59\"   \"1/3/17 23:43\"   \"1/4/17 2:44\"    \"1/4/17 5:45\"   \n[1349] \"1/4/17 12:47\"   \"1/4/17 14:49\"   \"1/4/17 18:33\"   \"1/4/17 21:56\"  \n[1353] \"1/5/17 3:33\"    \"1/5/17 5:22\"    \"1/5/17 10:50\"   \"1/5/17 12:37\"  \n[1357] \"1/5/17 14:40\"   \"1/5/17 23:31\"   \"1/6/17 3:19\"    \"1/6/17 10:40\"  \n[1361] \"1/6/17 14:11\"   \"1/6/17 17:52\"   \"1/6/17 23:15\"   \"1/6/17 23:18\"  \n[1365] \"1/7/17 7:15\"    \"1/7/17 12:13\"   \"1/7/17 22:59\"   \"1/7/17 23:08\"  \n[1369] \"1/8/17 0:46\"    \"1/8/17 6:01\"    \"1/8/17 11:59\"   \"1/8/17 12:01\"  \n[1373] \"1/9/17 4:24\"    \"1/9/17 6:35\"    \"1/9/17 10:05\"   \"1/9/17 11:52\"  \n[1377] \"1/10/17 2:55\"   \"1/10/17 11:38\"  \"1/10/17 13:44\"  \"1/10/17 22:33\" \n[1381] \"1/11/17 3:53\"   \"1/11/17 6:31\"   \"1/11/17 13:35\"  \"1/11/17 14:49\" \n[1385] \"1/12/17 2:24\"   \"1/12/17 11:11\"  \"1/12/17 11:13\"  \"1/12/17 22:09\" \n[1389] \"1/12/17 23:44\"  \"1/13/17 3:38\"   \"1/13/17 3:38\"   \"1/13/17 11:01\" \n[1393] \"1/13/17 18:01\"  \"1/14/17 6:28\"   \"1/14/17 10:50\"  \"1/14/17 10:54\" \n[1397] \"1/14/17 23:22\"  \"1/15/17 5:15\"   \"1/15/17 6:09\"   \"1/15/17 10:41\" \n[1401] \"1/15/17 12:19\"  \"1/16/17 4:52\"   \"1/16/17 10:28\"  \"1/16/17 12:08\" \n[1405] \"1/16/17 14:18\"  \"1/16/17 23:07\"  \"1/17/17 6:12\"   \"1/17/17 6:13\"  \n[1409] \"1/17/17 12:02\"  \"1/17/17 14:52\"  \"1/17/17 15:05\"  \"1/18/17 0:29\"  \n[1413] \"1/18/17 5:47\"   \"1/18/17 11:46\"  \"1/18/17 11:51\"  \"1/18/17 16:12\" \n[1417] \"1/19/17 2:53\"   \"1/19/17 4:06\"   \"1/19/17 5:32\"   \"1/19/17 13:41\" \n[1421] \"1/19/17 16:40\"  \"1/20/17 6:02\"   \"1/20/17 11:21\"  \"1/20/17 15:13\" \n[1425] \"1/20/17 22:17\"  \"1/21/17 11:09\"  \"1/21/17 11:18\"  \"1/21/17 22:04\" \n[1429] \"1/21/17 23:51\"  \"1/22/17 6:10\"   \"1/22/17 6:14\"   \"1/22/17 11:01\" \n[1433] \"1/22/17 14:05\"  \"1/23/17 12:33\"  \"1/23/17 14:07\"  \"1/23/17 23:21\" \n[1437] \"1/23/17 23:26\"  \"1/24/17 2:31\"   \"1/24/17 13:36\"  \"1/24/17 13:37\" \n[1441] \"1/24/17 23:05\"  \"1/25/17 5:57\"   \"1/25/17 13:16\"  \"1/25/17 14:51\" \n[1445] \"1/25/17 22:57\"  \"1/26/17 6:20\"   \"1/26/17 6:28\"   \"1/26/17 10:10\" \n[1449] \"1/26/17 14:24\"  \"1/27/17 11:40\"  \"1/27/17 17:13\"  \"1/27/17 22:35\" \n[1453] \"1/27/17 22:39\"  \"1/28/17 2:19\"   \"1/28/17 11:28\"  \"1/28/17 13:41\" \n[1457] \"1/29/17 11:21\"  \"1/29/17 11:23\"  \"1/29/17 15:28\"  \"1/29/17 23:52\" \n[1461] \"1/29/17 23:55\"  \"1/30/17 11:10\"  \"1/30/17 11:10\"  \"1/30/17 23:36\" \n[1465] \"1/30/17 23:45\"  \"1/31/17 4:45\"   \"1/31/17 6:22\"   \"1/31/17 11:03\" \n[1469] \"1/31/17 17:32\"  \"2/1/17 3:20\"    \"2/1/17 6:56\"    \"2/1/17 12:25\"  \n[1473] \"2/2/17 5:37\"    \"2/2/17 14:17\"   \"2/2/17 15:06\"   \"2/2/17 23:07\"  \n[1477] \"2/3/17 5:25\"    \"2/3/17 14:07\"   \"2/3/17 14:08\"   \"2/3/17 22:53\"  \n[1481] \"2/4/17 10:15\"   \"2/4/17 11:58\"   \"2/4/17 22:42\"   \"2/4/17 22:47\"  \n[1485] \"2/5/17 4:06\"    \"2/5/17 6:22\"    \"2/5/17 13:34\"   \"2/6/17 4:17\"   \n[1489] \"2/6/17 13:32\"   \"2/7/17 3:24\"    \"2/7/17 11:16\"   \"2/7/17 14:09\"  \n[1493] \"2/7/17 23:46\"   \"2/8/17 5:20\"    \"2/8/17 5:23\"    \"2/8/17 11:02\"  \n[1497] \"2/9/17 3:28\"    \"2/9/17 10:56\"   \"2/9/17 10:57\"   \"2/9/17 12:36\"  \n[1501] \"2/10/17 1:59\"   \"2/10/17 7:12\"   \"2/10/17 10:45\"  \"2/10/17 14:13\" \n[1505] \"2/11/17 1:40\"   \"2/11/17 3:19\"   \"2/11/17 10:30\"  \"2/11/17 15:23\" \n[1509] \"2/12/17 6:26\"   \"2/12/17 10:25\"  \"2/12/17 12:01\"  \"2/13/17 2:25\"  \n[1513] \"2/13/17 6:55\"   \"2/13/17 11:49\"  \"2/13/17 13:36\"  \"2/14/17 4:09\"  \n[1517] \"2/14/17 11:38\"  \"2/14/17 14:52\"  \"2/14/17 22:30\"  \"2/15/17 6:10\"  \n[1521] \"2/15/17 14:31\"  \"2/15/17 17:19\"  \"2/15/17 22:23\"  \"2/16/17 11:19\" \n[1525] \"2/16/17 23:51\"  \"2/16/17 23:54\"  \"2/17/17 6:28\"   \"2/17/17 10:59\" \n[1529] \"2/17/17 15:34\"  \"2/17/17 23:38\"  \"2/18/17 3:22\"   \"2/18/17 12:32\" \n[1533] \"2/18/17 23:29\"  \"2/19/17 10:38\"  \"2/19/17 23:11\"  \"2/19/17 23:16\" \n[1537] \"2/20/17 3:02\"   \"2/20/17 10:28\"  \"2/20/17 12:11\"  \"2/20/17 22:59\" \n[1541] \"2/21/17 2:46\"   \"2/21/17 2:48\"   \"2/21/17 13:33\"  \"2/21/17 18:38\" \n[1545] \"2/22/17 5:26\"   \"2/22/17 13:15\"  \"2/22/17 13:45\"  \"2/22/17 22:45\" \n[1549] \"2/23/17 5:04\"   \"2/23/17 9:55\"   \"2/23/17 11:41\"  \"2/23/17 22:26\" \n[1553] \"2/24/17 0:10\"   \"2/24/17 6:32\"   \"2/24/17 13:41\"  \"2/24/17 13:43\" \n[1557] \"2/24/17 23:56\"  \"2/25/17 11:09\"  \"2/25/17 11:17\"  \"2/25/17 22:05\" \n[1561] \"2/25/17 23:48\"  \"2/26/17 5:47\"   \"2/26/17 12:41\"  \"2/26/17 14:39\" \n[1565] \"2/26/17 23:40\"  \"2/27/17 5:22\"   \"2/27/17 10:46\"  \"2/27/17 10:57\" \n[1569] \"2/27/17 23:20\"  \"2/28/17 3:02\"   \"2/28/17 12:18\"  \"2/28/17 12:18\" \n[1573] \"2/28/17 23:17\"  \"3/1/17 1:59\"    \"3/1/17 6:18\"    \"3/1/17 12:11\"  \n[1577] \"3/1/17 14:27\"   \"3/2/17 6:08\"    \"3/2/17 10:18\"   \"3/2/17 11:58\"  \n[1581] \"3/2/17 22:55\"   \"3/3/17 4:03\"    \"3/3/17 5:47\"    \"3/3/17 12:46\"  \n[1585] \"3/3/17 14:27\"   \"3/4/17 6:15\"    \"3/4/17 11:41\"   \"3/4/17 13:36\"  \n[1589] \"3/5/17 3:13\"    \"3/5/17 6:41\"    \"3/5/17 11:22\"   \"3/5/17 13:37\"  \n[1593] \"3/6/17 6:20\"    \"3/6/17 12:53\"   \"3/7/17 11:07\"   \"3/8/17 10:50\"  \n[1597] \"3/8/17 23:21\"   \"3/8/17 23:23\"   \"3/9/17 2:33\"    \"3/9/17 10:40\"  \n[1601] \"3/9/17 15:15\"   \"3/9/17 23:14\"   \"3/10/17 6:38\"   \"3/10/17 13:18\" \n[1605] \"3/10/17 14:07\"  \"3/11/17 3:23\"   \"3/11/17 5:27\"   \"3/11/17 13:57\" \n[1609] \"3/11/17 14:07\"  \"3/12/17 5:55\"   \"3/12/17 11:46\"  \"3/12/17 15:24\" \n[1613] \"3/12/17 22:36\"  \"3/13/17 0:18\"   \"3/13/17 6:33\"   \"3/13/17 13:40\" \n[1617] \"3/13/17 13:42\"  \"3/13/17 22:29\"  \"3/14/17 13:03\"  \"3/14/17 23:55\" \n[1621] \"3/15/17 6:27\"   \"3/15/17 14:24\"  \"3/16/17 14:06\"  \"3/16/17 19:00\" \n[1625] \"3/17/17 12:28\"  \"3/17/17 14:18\"  \"3/17/17 15:17\"  \"3/17/17 23:14\" \n[1629] \"3/17/17 23:19\"  \"3/17/17 23:20\"  \"3/18/17 3:12\"   \"3/18/17 5:28\"  \n[1633] \"3/18/17 10:33\"  \"3/18/17 12:18\"  \"3/18/17 18:20\"  \"3/18/17 23:13\" \n[1637] \"3/19/17 6:44\"   \"3/19/17 6:47\"   \"3/19/17 12:07\"  \"3/19/17 14:57\" \n[1641] \"3/19/17 15:40\"  \"3/19/17 22:58\"  \"3/20/17 4:11\"   \"3/20/17 6:34\"  \n[1645] \"3/20/17 10:09\"  \"3/20/17 10:15\"  \"3/20/17 14:04\"  \"3/20/17 22:41\" \n[1649] \"3/21/17 4:03\"   \"3/21/17 6:14\"   \"3/21/17 11:36\"  \"3/21/17 13:33\" \n[1653] \"3/21/17 14:01\"  \"3/21/17 19:04\"  \"3/22/17 0:18\"   \"3/22/17 11:24\" \n[1657] \"3/22/17 13:13\"  \"3/22/17 13:14\"  \"3/22/17 22:21\"  \"3/22/17 23:58\" \n[1661] \"3/23/17 11:18\"  \"3/23/17 14:54\"  \"3/23/17 14:58\"  \"3/23/17 23:48\" \n[1665] \"3/23/17 23:53\"  \"3/23/17 23:54\"  \"3/24/17 3:32\"   \"3/24/17 6:51\"  \n[1669] \"3/24/17 11:05\"  \"3/24/17 12:49\"  \"3/24/17 15:45\"  \"3/24/17 23:40\" \n[1673] \"3/25/17 3:16\"   \"3/25/17 6:27\"   \"3/25/17 6:31\"   \"3/25/17 10:52\" \n[1677] \"3/25/17 12:35\"  \"3/25/17 15:12\"  \"3/26/17 4:27\"   \"3/26/17 13:13\" \n[1681] \"3/26/17 13:17\"  \"3/26/17 15:56\"  \"3/26/17 23:17\"  \"3/26/17 23:23\" \n[1685] \"3/27/17 3:20\"   \"3/27/17 6:39\"   \"3/27/17 10:30\"  \"3/27/17 12:19\" \n[1689] \"3/27/17 14:27\"  \"3/27/17 23:05\"  \"3/28/17 5:27\"   \"3/28/17 10:20\" \n[1693] \"3/28/17 11:59\"  \"3/28/17 12:01\"  \"3/28/17 22:50\"  \"3/28/17 22:54\" \n[1697] \"3/29/17 4:06\"   \"3/29/17 11:53\"  \"3/29/17 14:54\"  \"3/29/17 16:16\" \n[1701] \"3/29/17 22:42\"  \"3/30/17 4:39\"   \"3/30/17 9:56\"   \"3/30/17 9:59\"  \n[1705] \"3/30/17 14:22\"  \"3/30/17 22:30\"  \"3/30/17 22:31\"  \"3/31/17 3:42\"  \n[1709] \"3/31/17 13:18\"  \"3/31/17 13:22\"  \"3/31/17 22:18\"  \"3/31/17 23:56\" \n[1713] \"4/1/17 11:16\"   \"4/1/17 11:20\"   \"4/1/17 18:33\"   \"4/1/17 22:08\"  \n[1717] \"4/1/17 23:54\"   \"4/2/17 12:43\"   \"4/2/17 12:46\"   \"4/2/17 13:36\"  \n[1721] \"4/2/17 23:40\"   \"4/2/17 23:42\"   \"4/3/17 4:58\"    \"4/3/17 6:37\"   \n[1725] \"4/3/17 10:53\"   \"4/3/17 13:14\"   \"4/3/17 14:56\"   \"4/3/17 23:22\"  \n[1729] \"4/4/17 2:59\"    \"4/4/17 6:19\"    \"4/4/17 10:44\"   \"4/4/17 14:12\"  \n[1733] \"4/5/17 4:33\"    \"4/5/17 6:02\"    \"4/5/17 12:06\"   \"4/5/17 12:16\"  \n[1737] \"4/6/17 10:18\"   \"4/6/17 11:59\"   \"4/6/17 22:50\"   \"4/6/17 22:56\"  \n[1741] \"4/7/17 6:51\"    \"4/7/17 11:47\"   \"4/7/17 14:58\"   \"4/7/17 15:19\"  \n[1745] \"4/7/17 22:37\"   \"4/8/17 6:31\"    \"4/8/17 11:32\"   \"4/8/17 11:36\"  \n[1749] \"4/8/17 22:33\"   \"4/9/17 11:19\"   \"4/9/17 11:24\"   \"4/9/17 14:55\"  \n[1753] \"4/9/17 22:16\"   \"4/9/17 23:54\"   \"4/10/17 6:48\"   \"4/10/17 11:12\" \n[1757] \"4/10/17 14:52\"  \"4/10/17 15:16\"  \"4/10/17 23:45\"  \"4/11/17 10:58\" \n[1761] \"4/11/17 13:04\"  \"4/11/17 14:39\"  \"4/11/17 23:30\"  \"4/11/17 23:35\" \n[1765] \"4/12/17 6:48\"   \"4/12/17 10:48\"  \"4/12/17 10:51\"  \"4/12/17 18:05\" \n[1769] \"4/12/17 23:26\"  \"4/13/17 5:46\"   \"4/13/17 10:37\"  \"4/13/17 13:41\" \n[1773] \"4/13/17 23:17\"  \"4/14/17 12:15\"  \"4/14/17 13:09\"  \"4/14/17 23:04\" \n[1777] \"4/14/17 23:05\"  \"4/15/17 10:17\"  \"4/15/17 13:18\"  \"4/15/17 22:46\" \n[1781] \"4/15/17 22:51\"  \"4/16/17 3:21\"   \"4/16/17 6:22\"   \"4/16/17 13:34\" \n[1785] \"4/16/17 13:42\"  \"4/17/17 6:53\"   \"4/17/17 11:29\"  \"4/17/17 14:00\" \n[1789] \"4/17/17 22:25\"  \"4/18/17 4:06\"   \"4/18/17 11:24\"  \"4/18/17 13:44\" \n[1793] \"4/18/17 23:53\"  \"4/19/17 11:10\"  \"4/19/17 11:17\"  \"4/19/17 23:38\" \n[1797] \"4/19/17 23:40\"  \"4/20/17 5:52\"   \"4/20/17 10:54\"  \"4/20/17 13:20\" \n[1801] \"4/20/17 23:28\"  \"4/21/17 2:57\"   \"4/21/17 6:25\"   \"4/21/17 14:10\" \n[1805] \"4/21/17 14:22\"  \"4/22/17 6:42\"   \"4/22/17 10:37\"  \"4/22/17 13:38\" \n[1809] \"4/23/17 12:02\"  \"4/23/17 22:54\"  \"4/24/17 7:48\"   \"4/24/17 11:54\" \n[1813] \"4/24/17 11:54\"  \"4/24/17 13:41\"  \"4/24/17 19:00\"  \"4/24/17 22:41\" \n[1817] \"4/24/17 22:45\"  \"4/24/17 22:46\"  \"4/24/17 22:50\"  \"4/25/17 5:46\"  \n[1821] \"4/25/17 6:36\"   \"4/25/17 6:37\"   \"4/25/17 6:40\"   \"4/25/17 7:22\"  \n[1825] \"4/25/17 11:37\"  \"4/25/17 11:37\"  \"4/25/17 11:42\"  \"4/25/17 11:42\" \n[1829] \"4/25/17 11:43\"  \"4/25/17 11:43\"  \"4/25/17 11:45\"  \"4/25/17 11:48\" \n[1833] \"4/25/17 22:31\"  \"4/25/17 22:32\"  \"4/26/17 2:54\"   \"4/26/17 5:20\"  \n[1837] \"4/26/17 6:12\"   \"4/26/17 6:22\"   \"4/26/17 6:22\"   \"4/26/17 11:25\" \n[1841] \"4/26/17 11:26\"  \"4/26/17 11:26\"  \"4/26/17 11:27\"  \"4/26/17 11:27\" \n[1845] \"4/26/17 11:28\"  \"4/26/17 11:31\"  \"4/26/17 11:32\"  \"4/26/17 11:34\" \n[1849] \"4/26/17 13:19\"  \"4/26/17 16:41\"  \"4/26/17 22:23\"  \"4/26/17 23:58\" \n[1853] \"4/26/17 23:58\"  \"4/26/17 23:59\"  \"4/27/17 5:00\"   \"4/27/17 5:06\"  \n[1857] \"4/27/17 6:01\"   \"4/27/17 6:39\"   \"4/27/17 6:47\"   \"4/27/17 11:14\" \n[1861] \"4/27/17 11:18\"  \"4/27/17 11:20\"  \"4/27/17 11:22\"  \"4/27/17 11:24\" \n[1865] \"4/27/17 11:25\"  \"4/27/17 11:26\"  \"4/27/17 13:05\"  \"4/27/17 13:07\" \n[1869] \"4/27/17 13:12\"  \"4/27/17 22:11\"  \"4/27/17 23:49\"  \"4/27/17 23:52\" \n[1873] \"4/27/17 23:52\"  \"4/27/17 23:58\"  \"4/28/17 1:52\"   \"4/28/17 3:21\"  \n[1877] \"4/28/17 3:25\"   \"4/28/17 6:17\"   \"4/28/17 6:19\"   \"4/28/17 6:20\"  \n[1881] \"4/28/17 11:08\"  \"4/28/17 11:09\"  \"4/28/17 11:09\"  \"4/28/17 11:12\" \n[1885] \"4/28/17 11:14\"  \"4/28/17 12:48\"  \"4/28/17 12:49\"  \"4/28/17 12:50\" \n[1889] \"4/28/17 12:50\"  \"4/28/17 12:51\"  \"4/28/17 12:51\"  \"4/28/17 23:36\" \n[1893] \"4/28/17 23:37\"  \"4/28/17 23:44\"  \"4/28/17 23:45\"  \"4/28/17 23:45\" \n[1897] \"4/29/17 1:16\"   \"4/29/17 4:21\"   \"4/29/17 5:57\"   \"4/29/17 5:58\"  \n[1901] \"4/29/17 5:59\"   \"4/29/17 6:06\"   \"4/29/17 6:07\"   \"4/29/17 6:56\"  \n[1905] \"4/29/17 6:57\"   \"4/29/17 10:51\"  \"4/29/17 10:57\"  \"4/29/17 11:03\" \n[1909] \"4/29/17 12:37\"  \"4/29/17 12:38\"  \"4/29/17 12:38\"  \"4/29/17 12:39\" \n[1913] \"4/29/17 12:41\"  \"4/29/17 13:42\"  \"4/29/17 13:46\"  \"4/29/17 14:05\" \n[1917] \"4/29/17 23:29\"  \"4/29/17 23:31\"  \"4/30/17 5:35\"   \"4/30/17 5:39\"  \n[1921] \"4/30/17 5:44\"   \"4/30/17 5:45\"   \"4/30/17 6:36\"   \"4/30/17 6:37\"  \n[1925] \"4/30/17 6:38\"   \"4/30/17 10:44\"  \"4/30/17 12:21\"  \"4/30/17 12:24\" \n[1929] \"4/30/17 12:26\"  \"4/30/17 12:26\"  \"4/30/17 12:29\"  \"4/30/17 12:29\" \n[1933] \"4/30/17 12:30\"  \"4/30/17 13:38\"  \"4/30/17 13:38\"  \"4/30/17 23:13\" \n[1937] \"4/30/17 23:18\"  \"4/30/17 23:22\"  \"4/30/17 23:23\"  \"5/1/17 0:55\"   \n[1941] \"5/1/17 5:16\"    \"5/1/17 5:22\"    \"5/1/17 5:23\"    \"5/1/17 6:20\"   \n[1945] \"5/1/17 6:57\"    \"5/1/17 6:59\"    \"5/1/17 10:30\"   \"5/1/17 12:10\"  \n[1949] \"5/1/17 12:15\"   \"5/1/17 12:15\"   \"5/1/17 12:19\"   \"5/1/17 12:19\"  \n[1953] \"5/1/17 12:22\"   \"5/1/17 13:12\"   \"5/1/17 13:13\"   \"5/1/17 14:22\"  \n[1957] \"5/1/17 23:04\"   \"5/1/17 23:07\"   \"5/1/17 23:09\"   \"5/1/17 23:10\"  \n[1961] \"5/2/17 3:06\"    \"5/2/17 3:22\"    \"5/2/17 4:14\"    \"5/2/17 4:58\"   \n[1965] \"5/2/17 5:00\"    \"5/2/17 5:01\"    \"5/2/17 5:48\"    \"5/2/17 6:36\"   \n[1969] \"5/2/17 6:37\"    \"5/2/17 6:37\"    \"5/2/17 10:18\"   \"5/2/17 11:58\"  \n[1973] \"5/2/17 12:01\"   \"5/2/17 12:01\"   \"5/2/17 12:01\"   \"5/2/17 12:03\"  \n[1977] \"5/2/17 12:03\"   \"5/2/17 12:04\"   \"5/2/17 12:08\"   \"5/2/17 12:52\"  \n[1981] \"5/2/17 13:51\"   \"5/2/17 22:58\"   \"5/3/17 2:27\"    \"5/3/17 2:53\"   \n[1985] \"5/3/17 6:16\"    \"5/3/17 6:18\"    \"5/3/17 6:20\"    \"5/3/17 6:24\"   \n[1989] \"5/3/17 11:48\"   \"5/3/17 11:49\"   \"5/3/17 11:49\"   \"5/3/17 11:50\"  \n[1993] \"5/3/17 11:52\"   \"5/3/17 11:53\"   \"5/3/17 11:54\"   \"5/3/17 11:55\"  \n[1997] \"5/3/17 11:58\"   \"5/3/17 13:22\"   \"5/3/17 13:39\"   \"5/3/17 19:10\"  \n[2001] \"5/3/17 19:12\"   \"5/3/17 22:41\"   \"5/3/17 22:43\"   \"5/3/17 22:47\"  \n[2005] \"5/4/17 0:29\"    \"5/4/17 2:04\"    \"5/4/17 2:36\"    \"5/4/17 3:54\"   \n[2009] \"5/4/17 4:06\"    \"5/4/17 6:54\"    \"5/4/17 6:54\"    \"5/4/17 6:57\"   \n[2013] \"5/4/17 7:38\"    \"5/4/17 11:34\"   \"5/4/17 11:36\"   \"5/4/17 11:36\"  \n[2017] \"5/4/17 11:38\"   \"5/4/17 11:39\"   \"5/4/17 11:42\"   \"5/4/17 11:43\"  \n[2021] \"5/4/17 11:43\"   \"5/4/17 14:24\"   \"5/4/17 22:30\"   \"5/4/17 22:34\"  \n[2025] \"5/4/17 22:35\"   \"5/5/17 3:47\"    \"5/5/17 3:49\"    \"5/5/17 5:33\"   \n[2029] \"5/5/17 6:33\"    \"5/5/17 6:35\"    \"5/5/17 7:13\"    \"5/5/17 7:20\"   \n[2033] \"5/5/17 11:23\"   \"5/5/17 11:24\"   \"5/5/17 11:25\"   \"5/5/17 11:26\"  \n[2037] \"5/5/17 11:31\"   \"5/5/17 13:15\"   \"5/5/17 13:18\"   \"5/5/17 16:47\"  \n[2041] \"5/5/17 23:56\"   \"5/5/17 23:57\"   \"5/5/17 23:58\"   \"5/5/17 23:58\"  \n[2045] \"5/5/17 23:59\"   \"5/6/17 3:25\"    \"5/6/17 3:27\"    \"5/6/17 6:16\"   \n[2049] \"5/6/17 6:55\"    \"5/6/17 6:58\"    \"5/6/17 6:59\"    \"5/6/17 11:14\"  \n[2053] \"5/6/17 11:17\"   \"5/6/17 11:17\"   \"5/6/17 11:20\"   \"5/6/17 11:23\"  \n[2057] \"5/6/17 13:00\"   \"5/6/17 13:29\"   \"5/6/17 22:09\"   \"5/6/17 22:09\"  \n[2061] \"5/6/17 23:43\"   \"5/6/17 23:44\"   \"5/7/17 3:14\"    \"5/7/17 4:54\"   \n[2065] \"5/7/17 6:31\"    \"5/7/17 6:31\"    \"5/7/17 6:37\"    \"5/7/17 6:40\"   \n[2069] \"5/7/17 11:01\"   \"5/7/17 11:04\"   \"5/7/17 12:45\"   \"5/7/17 12:46\"  \n[2073] \"5/7/17 12:47\"   \"5/7/17 12:47\"   \"5/7/17 12:48\"   \"5/7/17 12:48\"  \n[2077] \"5/7/17 12:48\"   \"5/7/17 19:28\"   \"5/7/17 23:36\"   \"5/7/17 23:44\"  \n[2081] \"5/8/17 5:26\"    \"5/8/17 6:09\"    \"5/8/17 6:13\"    \"5/8/17 6:21\"   \n[2085] \"5/8/17 6:21\"    \"5/8/17 10:55\"   \"5/8/17 10:56\"   \"5/8/17 12:32\"  \n[2089] \"5/8/17 12:33\"   \"5/8/17 12:34\"   \"5/8/17 12:35\"   \"5/8/17 12:37\"  \n[2093] \"5/8/17 12:37\"   \"5/8/17 12:40\"   \"5/8/17 19:08\"   \"5/8/17 23:25\"  \n[2097] \"5/8/17 23:28\"   \"5/9/17 5:57\"    \"5/9/17 6:52\"    \"5/9/17 6:52\"   \n[2101] \"5/9/17 6:53\"    \"5/9/17 6:54\"    \"5/9/17 7:36\"    \"5/9/17 10:44\"  \n[2105] \"5/9/17 12:22\"   \"5/9/17 12:22\"   \"5/9/17 12:24\"   \"5/9/17 12:26\"  \n[2109] \"5/9/17 13:17\"   \"5/9/17 13:32\"   \"5/9/17 13:34\"   \"5/9/17 23:10\"  \n[2113] \"5/9/17 23:21\"   \"5/10/17 1:38\"   \"5/10/17 2:44\"   \"5/10/17 3:21\"  \n[2117] \"5/10/17 5:36\"   \"5/10/17 6:31\"   \"5/10/17 10:27\"  \"5/10/17 12:11\" \n[2121] \"5/10/17 12:11\"  \"5/10/17 12:15\"  \"5/10/17 12:15\"  \"5/10/17 12:51\" \n[2125] \"5/10/17 12:52\"  \"5/10/17 23:02\"  \"5/10/17 23:04\"  \"5/11/17 2:34\"  \n[2129] \"5/11/17 4:11\"   \"5/11/17 6:03\"   \"5/11/17 6:50\"   \"5/11/17 11:55\" \n[2133] \"5/11/17 11:56\"  \"5/11/17 11:59\"  \"5/11/17 12:00\"  \"5/11/17 12:00\" \n[2137] \"5/11/17 18:06\"  \"5/11/17 22:50\"  \"5/11/17 22:51\"  \"5/11/17 22:51\" \n[2141] \"5/12/17 3:59\"   \"5/12/17 6:29\"   \"5/12/17 6:36\"   \"5/12/17 6:39\"  \n[2145] \"5/12/17 11:46\"  \"5/12/17 11:50\"  \"5/12/17 11:51\"  \"5/12/17 11:51\" \n[2149] \"5/12/17 11:54\"  \"5/12/17 13:40\"  \"5/12/17 15:17\"  \"5/12/17 22:40\" \n[2153] \"5/13/17 0:24\"   \"5/13/17 0:49\"   \"5/13/17 3:54\"   \"5/13/17 6:08\"  \n[2157] \"5/13/17 6:16\"   \"5/13/17 11:35\"  \"5/13/17 11:38\"  \"5/13/17 11:38\" \n[2161] \"5/13/17 11:41\"  \"5/13/17 13:05\"  \"5/13/17 14:55\"  \"5/13/17 19:04\" \n[2165] \"5/14/17 0:12\"   \"5/14/17 3:39\"   \"5/14/17 5:52\"   \"5/14/17 6:50\"  \n[2169] \"5/14/17 11:20\"  \"5/14/17 11:22\"  \"5/14/17 11:25\"  \"5/14/17 11:25\" \n[2173] \"5/14/17 11:25\"  \"5/14/17 14:28\"  \"5/14/17 23:54\"  \"5/15/17 5:34\"  \n[2177] \"5/15/17 6:23\"   \"5/15/17 6:29\"   \"5/15/17 11:12\"  \"5/15/17 11:13\" \n[2181] \"5/15/17 12:54\"  \"5/15/17 17:32\"  \"5/15/17 23:49\"  \"5/15/17 23:50\" \n[2185] \"5/16/17 5:06\"   \"5/16/17 6:02\"   \"5/16/17 6:48\"   \"5/16/17 6:49\"  \n[2189] \"5/16/17 12:41\"  \"5/16/17 12:42\"  \"5/16/17 12:42\"  \"5/16/17 12:46\" \n[2193] \"5/16/17 14:54\"  \"5/16/17 23:41\"  \"5/17/17 4:46\"   \"5/17/17 4:52\"  \n[2197] \"5/17/17 6:33\"   \"5/17/17 10:49\"  \"5/17/17 12:33\"  \"5/17/17 12:34\" \n[2201] \"5/17/17 12:36\"  \"5/17/17 13:14\"  \"5/17/17 23:19\"  \"5/17/17 23:28\" \n[2205] \"5/18/17 4:29\"   \"5/18/17 6:09\"   \"5/18/17 6:10\"   \"5/18/17 10:38\" \n[2209] \"5/18/17 10:39\"  \"5/18/17 12:23\"  \"5/18/17 13:51\"  \"5/18/17 15:33\" \n[2213] \"5/19/17 2:38\"   \"5/19/17 5:43\"   \"5/19/17 6:44\"   \"5/19/17 10:29\" \n[2217] \"5/19/17 12:08\"  \"5/19/17 12:09\"  \"5/19/17 12:09\"  \"5/19/17 12:10\" \n[2221] \"5/19/17 23:02\"  \"5/19/17 23:03\"  \"5/20/17 2:37\"   \"5/20/17 5:25\"  \n[2225] \"5/20/17 5:27\"   \"5/20/17 6:26\"   \"5/20/17 11:52\"  \"5/20/17 11:56\" \n[2229] \"5/20/17 11:59\"  \"5/20/17 11:59\"  \"5/20/17 12:01\"  \"5/21/17 2:06\"  \n[2233] \"5/21/17 6:41\"   \"5/21/17 6:43\"   \"5/21/17 6:44\"   \"5/21/17 10:04\" \n[2237] \"5/21/17 11:46\"  \"5/21/17 11:47\"  \"5/21/17 11:48\"  \"5/22/17 1:05\"  \n[2241] \"5/22/17 6:26\"   \"5/22/17 6:27\"   \"5/22/17 6:27\"   \"5/22/17 11:29\" \n[2245] \"5/22/17 11:34\"  \"5/22/17 11:34\"  \"5/22/17 11:35\"  \"5/22/17 13:29\" \n[2249] \"5/22/17 19:20\"  \"5/23/17 3:32\"   \"5/23/17 6:05\"   \"5/23/17 11:20\" \n[2253] \"5/23/17 11:21\"  \"5/23/17 11:21\"  \"5/24/17 3:26\"   \"5/24/17 6:40\"  \n[2257] \"5/24/17 6:42\"   \"5/24/17 11:06\"  \"5/24/17 11:08\"  \"5/24/17 11:09\" \n[2261] \"5/24/17 11:11\"  \"5/24/17 23:45\"  \"5/24/17 23:46\"  \"5/25/17 1:29\"  \n[2265] \"5/25/17 2:10\"   \"5/25/17 5:19\"   \"5/25/17 6:20\"   \"5/25/17 6:20\"  \n[2269] \"5/25/17 10:58\"  \"5/25/17 10:59\"  \"5/25/17 11:01\"  \"5/25/17 12:40\" \n[2273] \"5/25/17 15:11\"  \"5/26/17 6:41\"   \"5/26/17 6:43\"   \"5/26/17 6:46\"  \n[2277] \"5/26/17 10:50\"  \"5/26/17 12:29\"  \"5/26/17 12:31\"  \"5/26/17 14:16\" \n[2281] \"5/26/17 14:31\"  \"5/26/17 23:23\"  \"5/27/17 6:16\"   \"5/27/17 6:18\"  \n[2285] \"5/27/17 6:21\"   \"5/27/17 10:40\"  \"5/27/17 10:41\"  \"5/27/17 12:18\" \n[2289] \"5/27/17 12:19\"  \"5/27/17 23:11\"  \"5/28/17 5:55\"   \"5/28/17 5:59\"  \n[2293] \"5/28/17 5:59\"   \"5/28/17 6:01\"   \"5/28/17 12:01\"  \"5/28/17 12:02\" \n[2297] \"5/28/17 12:02\"  \"5/28/17 12:06\"  \"5/29/17 6:38\"   \"5/29/17 6:38\"  \n[2301] \"5/29/17 11:55\"  \"5/29/17 11:55\"  \"5/29/17 12:01\"  \"5/30/17 6:16\"  \n[2305] \"5/30/17 6:57\"   \"5/30/17 6:59\"   \"5/30/17 10:04\"  \"5/30/17 11:43\" \n[2309] \"5/30/17 11:44\"  \"5/31/17 5:03\"   \"5/31/17 6:41\"   \"5/31/17 6:43\"  \n[2313] \"5/31/17 11:29\"  \"5/31/17 11:30\"  \"5/31/17 11:36\"  \"6/1/17 4:39\"   \n[2317] \"6/1/17 6:14\"    \"6/1/17 6:19\"    \"6/1/17 11:19\"   \"6/1/17 11:19\"  \n[2321] \"6/1/17 11:22\"   \"6/2/17 2:01\"    \"6/2/17 5:54\"    \"6/2/17 5:57\"   \n[2325] \"6/2/17 11:06\"   \"6/2/17 11:09\"   \"6/2/17 13:14\"   \"6/3/17 4:47\"   \n[2329] \"6/3/17 5:36\"    \"6/3/17 5:41\"    \"6/3/17 12:37\"   \"6/3/17 12:40\"  \n[2333] \"6/3/17 12:41\"   \"6/4/17 6:56\"    \"6/4/17 6:57\"    \"6/4/17 6:57\"   \n[2337] \"6/4/17 10:41\"   \"6/4/17 12:30\"   \"6/5/17 6:32\"    \"6/5/17 6:33\"   \n[2341] \"6/5/17 12:16\"   \"6/5/17 12:18\"   \"6/6/17 6:10\"    \"6/6/17 6:21\"   \n[2345] \"6/6/17 12:00\"   \"6/6/17 12:07\"   \"6/7/17 6:50\"    \"6/7/17 6:52\"   \n[2349] \"6/7/17 11:53\"   \"6/7/17 11:54\"   \"6/7/17 14:30\"   \"6/7/17 22:42\"  \n[2353] \"6/8/17 5:36\"    \"6/8/17 5:37\"    \"6/8/17 5:39\"    \"6/8/17 11:37\"  \n[2357] \"6/8/17 14:00\"   \"6/9/17 3:36\"    \"6/9/17 5:12\"    \"6/9/17 6:51\"   \n[2361] \"6/9/17 11:25\"   \"6/9/17 11:29\"   \"6/9/17 14:07\"   \"6/10/17 4:53\"  \n[2365] \"6/10/17 6:28\"   \"6/10/17 11:16\"  \"6/10/17 11:18\"  \"6/10/17 14:53\" \n[2369] \"6/10/17 23:56\"  \"6/11/17 2:23\"   \"6/11/17 4:31\"   \"6/11/17 6:11\"  \n[2373] \"6/11/17 11:07\"  \"6/11/17 12:49\"  \"6/12/17 5:55\"   \"6/12/17 10:55\" \n[2377] \"6/12/17 12:34\"  \"6/12/17 14:03\"  \"6/12/17 23:30\"  \"6/13/17 2:30\"  \n[2381] \"6/13/17 2:53\"   \"6/13/17 5:28\"   \"6/13/17 10:40\"  \"6/13/17 10:43\" \n[2385] \"6/13/17 10:45\"  \"6/14/17 6:50\"   \"6/14/17 6:55\"   \"6/14/17 10:28\" \n[2389] \"6/14/17 12:12\"  \"6/14/17 14:54\"  \"6/14/17 23:06\"  \"6/15/17 6:24\"  \n[2393] \"6/15/17 6:32\"   \"6/15/17 12:00\"  \"6/15/17 12:04\"  \"6/15/17 14:28\" \n[2397] \"6/16/17 3:53\"   \"6/16/17 6:06\"   \"6/16/17 6:12\"   \"6/16/17 11:45\" \n[2401] \"6/16/17 11:52\"  \"6/16/17 13:50\"  \"6/17/17 0:22\"   \"6/17/17 3:49\"  \n[2405] \"6/17/17 6:45\"   \"6/17/17 11:33\"  \"6/17/17 11:33\"  \"6/18/17 6:25\"  \n[2409] \"6/18/17 6:25\"   \"6/18/17 11:23\"  \"6/18/17 11:29\"  \"6/19/17 6:48\"  \n[2413] \"6/19/17 11:12\"  \"6/19/17 11:12\"  \"6/19/17 11:15\"  \"6/19/17 23:48\" \n[2417] \"6/20/17 3:09\"   \"6/20/17 6:21\"   \"6/20/17 12:44\"  \"6/20/17 14:05\" \n[2421] \"6/20/17 23:33\"  \"6/21/17 6:03\"   \"6/21/17 6:03\"   \"6/21/17 6:59\"  \n[2425] \"6/21/17 12:33\"  \"6/21/17 12:33\"  \"6/21/17 12:34\"  \"6/21/17 15:20\" \n[2429] \"6/21/17 23:28\"  \"6/22/17 5:41\"   \"6/22/17 5:44\"   \"6/22/17 6:42\"  \n[2433] \"6/22/17 6:42\"   \"6/22/17 12:21\"  \"6/22/17 12:23\"  \"6/22/17 12:25\" \n[2437] \"6/22/17 12:27\"  \"6/23/17 0:56\"   \"6/23/17 2:43\"   \"6/23/17 5:23\"  \n[2441] \"6/23/17 6:17\"   \"6/23/17 12:10\"  \"6/23/17 12:11\"  \"6/23/17 13:28\" \n[2445] \"6/23/17 15:32\"  \"6/24/17 5:59\"   \"6/24/17 6:47\"   \"6/24/17 10:20\" \n[2449] \"6/24/17 11:59\"  \"6/25/17 6:17\"   \"6/25/17 6:23\"   \"6/25/17 10:06\" \n[2453] \"6/25/17 11:47\"  \"6/26/17 3:48\"   \"6/26/17 6:01\"   \"6/26/17 11:36\" \n[2457] \"6/26/17 13:36\"  \"6/27/17 3:32\"   \"6/27/17 5:43\"   \"6/28/17 3:15\"  \n[2461] \"6/28/17 11:09\"  \"6/28/17 11:11\"  \"6/29/17 2:30\"   \"6/29/17 6:39\"  \n[2465] \"6/29/17 11:01\"  \"6/29/17 11:06\"  \"6/30/17 4:32\"   \"6/30/17 6:22\"  \n[2469] \"6/30/17 10:51\"  \"7/1/17 5:53\"    \"7/1/17 5:59\"    \"7/1/17 12:17\"  \n[2473] \"7/2/17 5:38\"    \"7/2/17 5:40\"    \"7/2/17 12:04\"   \"7/2/17 12:05\"  \n[2477] \"7/3/17 4:04\"    \"7/3/17 6:54\"    \"7/3/17 11:53\"   \"7/3/17 11:53\"  \n[2481] \"7/4/17 2:05\"    \"7/4/17 6:36\"    \"7/4/17 11:39\"   \"7/5/17 6:09\"   \n[2485] \"7/6/17 2:39\"    \"7/6/17 13:24\"   \"7/7/17 6:29\"    \"7/7/17 11:13\"  \n[2489] \"7/8/17 6:56\"    \"7/8/17 12:39\"   \"7/9/17 4:31\"    \"7/9/17 12:27\"  \n[2493] \"7/10/17 6:06\"   \"7/10/17 12:21\"  \"7/11/17 5:45\"   \"7/11/17 12:06\" \n[2497] \"7/12/17 6:28\"   \"7/12/17 11:57\"  \"7/13/17 3:41\"   \"7/14/17 6:33\"  \n[2501] \"7/15/17 6:13\"   \"7/15/17 13:42\"  \"7/16/17 4:52\"   \"7/17/17 5:31\"  \n[2505] \"7/17/17 12:34\"  \"7/18/17 4:25\"   \"7/18/17 10:43\"  \"7/19/17 2:33\"  \n[2509] \"7/19/17 12:13\"  \"7/20/17 6:08\"   \"7/20/17 12:03\"  \"7/21/17 5:39\"  \n[2513] \"7/21/17 13:56\"  \"7/22/17 5:21\"   \"7/22/17 11:37\"  \"7/23/17 2:36\"  \n[2517] \"7/23/17 11:23\"  \"7/24/17 6:25\"   \"7/24/17 11:14\"  \"7/25/17 4:43\"  \n[2521] \"7/25/17 12:43\"  \"7/26/17 5:45\"   \"7/26/17 10:55\"  \"7/27/17 6:17\"  \n[2525] \"7/27/17 7:00\"   \"7/28/17 6:37\"   \"7/28/17 12:13\"  \"7/29/17 11:58\" \n[2529] \"7/30/17 6:03\"   \"7/30/17 11:47\"  \"7/31/17 5:39\"   \"7/31/17 13:17\" \n[2533] \"8/1/17 5:15\"    \"8/1/17 11:23\"   \"8/2/17 3:18\"    \"8/2/17 11:18\"  \n[2537] \"8/3/17 2:57\"    \"8/5/17 12:25\"   \"8/6/17 6:51\"    \"8/6/17 12:12\"  \n[2541] \"8/7/17 6:33\"    \"8/8/17 6:12\"    \"8/9/17 11:34\"   \"8/10/17 5:30\"  \n[2545] \"8/10/17 13:34\"  \"8/11/17 6:50\"   \"8/12/17 6:30\"   \"8/12/17 12:41\" \n[2549] \"8/13/17 4:30\"   \"8/13/17 10:51\"  \"8/14/17 5:44\"   \"8/14/17 10:36\" \n[2553] \"8/15/17 3:58\"   \"8/15/17 12:09\"  \"8/16/17 6:43\"   \"8/16/17 11:56\" \n[2557] \"8/17/17 3:45\"   \"8/17/17 13:24\"  \"8/18/17 3:32\"   \"8/19/17 5:42\"  \n[2561] \"8/19/17 11:24\"  \"8/26/17 5:01\"   \"8/27/17 6:21\"   \"8/27/17 11:32\" \n[2565] \"8/28/17 11:16\"  \"8/29/17 5:40\"   \"8/30/17 5:19\"   \"8/30/17 10:54\" \n[2569] \"8/31/17 4:21\"   \"3/11/18 5:18\"   \"3/11/18 12:18\"  \"3/11/18 12:24\" \n[2573] \"3/12/18 6:35\"   \"3/12/18 6:36\"   \"3/12/18 6:36\"   \"3/12/18 6:37\"  \n[2577] \"3/12/18 6:38\"   \"3/12/18 6:40\"   \"3/12/18 12:07\"  \"3/12/18 12:13\" \n[2581] \"3/12/18 12:14\"  \"3/13/18 2:43\"   \"3/13/18 6:13\"   \"3/13/18 6:13\"  \n[2585] \"3/13/18 6:15\"   \"3/13/18 6:16\"   \"3/13/18 11:55\"  \"3/13/18 11:58\" \n[2589] \"3/13/18 12:02\"  \"3/13/18 12:04\"  \"3/13/18 13:25\"  \"3/13/18 13:30\" \n[2593] \"3/13/18 19:07\"  \"3/13/18 22:52\"  \"3/14/18 0:33\"   \"3/14/18 5:00\"  \n[2597] \"3/14/18 5:03\"   \"3/14/18 5:55\"   \"3/14/18 6:50\"   \"3/14/18 6:55\"  \n[2601] \"3/14/18 6:57\"   \"3/14/18 11:47\"  \"3/14/18 11:49\"  \"3/14/18 11:50\" \n[2605] \"3/14/18 11:52\"  \"3/14/18 11:56\"  \"3/14/18 14:35\"  \"3/15/18 4:53\"  \n[2609] \"3/15/18 5:31\"   \"3/15/18 5:32\"   \"3/15/18 5:34\"   \"3/15/18 5:36\"  \n[2613] \"3/15/18 5:39\"   \"3/15/18 6:36\"   \"3/15/18 11:34\"  \"3/15/18 11:36\" \n[2617] \"3/15/18 11:37\"  \"3/15/18 11:39\"  \"3/15/18 11:42\"  \"3/15/18 13:16\" \n[2621] \"3/15/18 13:20\"  \"3/16/18 4:11\"   \"3/16/18 5:16\"   \"3/16/18 6:13\"  \n[2625] \"3/16/18 6:14\"   \"3/16/18 6:56\"   \"3/16/18 6:57\"   \"3/16/18 11:22\" \n[2629] \"3/16/18 11:24\"  \"3/16/18 11:25\"  \"3/16/18 11:27\"  \"3/16/18 11:28\" \n[2633] \"3/16/18 13:12\"  \"3/16/18 15:17\"  \"3/16/18 23:55\"  \"3/17/18 2:24\"  \n[2637] \"3/17/18 4:25\"   \"3/17/18 6:29\"   \"3/17/18 6:32\"   \"3/17/18 6:33\"  \n[2641] \"3/17/18 6:35\"   \"3/17/18 6:40\"   \"3/17/18 11:16\"  \"3/17/18 11:16\" \n[2645] \"3/17/18 11:16\"  \"3/17/18 11:16\"  \"3/17/18 12:53\"  \"3/17/18 12:53\" \n[2649] \"3/17/18 13:00\"  \"3/17/18 14:41\"  \"3/17/18 23:53\"  \"3/18/18 6:10\"  \n[2653] \"3/18/18 6:11\"   \"3/18/18 6:12\"   \"3/18/18 6:15\"   \"3/18/18 6:15\"  \n[2657] \"3/18/18 6:16\"   \"3/18/18 6:17\"   \"3/18/18 6:19\"   \"3/18/18 11:02\" \n[2661] \"3/18/18 11:06\"  \"3/18/18 11:07\"  \"3/18/18 12:41\"  \"3/18/18 12:42\" \n[2665] \"3/18/18 12:43\"  \"3/19/18 4:07\"   \"3/19/18 4:29\"   \"3/19/18 5:51\"  \n[2669] \"3/19/18 5:58\"   \"3/19/18 6:52\"   \"3/19/18 6:53\"   \"3/19/18 10:49\" \n[2673] \"3/19/18 12:27\"  \"3/19/18 12:30\"  \"3/19/18 12:31\"  \"3/19/18 12:31\" \n[2677] \"3/19/18 12:31\"  \"3/19/18 12:34\"  \"3/19/18 12:38\"  \"3/19/18 23:27\" \n[2681] \"3/20/18 3:52\"   \"3/20/18 4:03\"   \"3/20/18 4:05\"   \"3/20/18 5:28\"  \n[2685] \"3/20/18 5:29\"   \"3/20/18 5:34\"   \"3/20/18 6:31\"   \"3/20/18 6:33\"  \n[2689] \"3/20/18 7:08\"   \"3/20/18 12:15\"  \"3/20/18 12:18\"  \"3/20/18 12:18\" \n[2693] \"3/20/18 12:19\"  \"3/20/18 12:20\"  \"3/20/18 12:23\"  \"3/21/18 3:47\"  \n[2697] \"3/21/18 5:12\"   \"3/21/18 6:08\"   \"3/21/18 6:45\"   \"3/21/18 6:52\"  \n[2701] \"3/21/18 6:54\"   \"3/21/18 6:55\"   \"3/21/18 10:29\"  \"3/21/18 10:29\" \n[2705] \"3/21/18 12:03\"  \"3/21/18 12:03\"  \"3/21/18 12:05\"  \"3/21/18 12:09\" \n[2709] \"3/21/18 12:10\"  \"3/21/18 22:59\"  \"3/22/18 3:04\"   \"3/22/18 3:22\"  \n[2713] \"3/22/18 3:24\"   \"3/22/18 6:25\"   \"3/22/18 6:25\"   \"3/22/18 6:26\"  \n[2717] \"3/22/18 6:27\"   \"3/22/18 6:31\"   \"3/22/18 12:00\"  \"3/22/18 13:40\" \n[2721] \"3/22/18 13:42\"  \"3/22/18 13:44\"  \"3/22/18 13:48\"  \"3/22/18 13:49\" \n[2725] \"3/23/18 3:14\"   \"3/23/18 4:55\"   \"3/23/18 6:04\"   \"3/23/18 6:11\"  \n[2729] \"3/23/18 6:12\"   \"3/23/18 6:13\"   \"3/23/18 11:40\"  \"3/23/18 11:49\" \n[2733] \"3/23/18 11:50\"  \"3/23/18 11:52\"  \"3/23/18 13:27\"  \"3/23/18 15:41\" \n[2737] \"3/23/18 19:02\"  \"3/23/18 19:05\"  \"3/24/18 4:49\"   \"3/24/18 5:44\"  \n[2741] \"3/24/18 5:45\"   \"3/24/18 5:50\"   \"3/24/18 6:47\"   \"3/24/18 6:49\"  \n[2745] \"3/24/18 6:52\"   \"3/24/18 11:32\"  \"3/24/18 11:34\"  \"3/24/18 11:35\" \n[2749] \"3/24/18 11:36\"  \"3/24/18 11:36\"  \"3/24/18 13:11\"  \"3/24/18 13:19\" \n[2753] \"3/25/18 4:29\"   \"3/25/18 5:29\"   \"3/25/18 6:25\"   \"3/25/18 6:25\"  \n[2757] \"3/25/18 6:27\"   \"3/25/18 6:30\"   \"3/25/18 6:30\"   \"3/25/18 6:31\"  \n[2761] \"3/25/18 11:18\"  \"3/25/18 11:25\"  \"3/25/18 11:26\"  \"3/25/18 13:00\" \n[2765] \"3/25/18 13:01\"  \"3/25/18 13:02\"  \"3/25/18 13:02\"  \"3/25/18 13:49\" \n[2769] \"3/26/18 5:08\"   \"3/26/18 6:44\"   \"3/26/18 6:46\"   \"3/26/18 6:48\"  \n[2773] \"3/26/18 11:06\"  \"3/26/18 11:11\"  \"3/26/18 11:13\"  \"3/26/18 11:14\" \n[2777] \"3/26/18 11:16\"  \"3/26/18 13:20\"  \"3/26/18 13:21\"  \"3/26/18 23:40\" \n[2781] \"3/26/18 23:47\"  \"3/26/18 23:48\"  \"3/27/18 2:03\"   \"3/27/18 4:31\"  \n[2785] \"3/27/18 4:48\"   \"3/27/18 5:46\"   \"3/27/18 6:22\"   \"3/27/18 12:35\" \n[2789] \"3/27/18 12:35\"  \"3/27/18 12:37\"  \"3/27/18 12:42\"  \"3/27/18 12:42\" \n[2793] \"3/27/18 12:45\"  \"3/27/18 14:02\"  \"3/27/18 23:29\"  \"3/27/18 23:32\" \n[2797] \"3/27/18 23:35\"  \"3/28/18 2:30\"   \"3/28/18 4:03\"   \"3/28/18 5:35\"  \n[2801] \"3/28/18 5:41\"   \"3/28/18 6:04\"   \"3/28/18 6:07\"   \"3/28/18 6:12\"  \n[2805] \"3/28/18 7:48\"   \"3/28/18 12:23\"  \"3/28/18 12:27\"  \"3/28/18 12:29\" \n[2809] \"3/28/18 12:31\"  \"3/28/18 12:33\"  \"3/28/18 13:35\"  \"3/28/18 13:36\" \n[2813] \"3/28/18 19:02\"  \"3/29/18 3:44\"   \"3/29/18 3:46\"   \"3/29/18 5:43\"  \n[2817] \"3/29/18 5:44\"   \"3/29/18 6:43\"   \"3/29/18 6:47\"   \"3/29/18 10:40\" \n[2821] \"3/29/18 12:18\"  \"3/29/18 12:19\"  \"3/29/18 12:19\"  \"3/29/18 13:10\" \n[2825] \"3/29/18 13:13\"  \"3/29/18 13:24\"  \"3/29/18 23:06\"  \"3/29/18 23:13\" \n[2829] \"3/30/18 4:56\"   \"3/30/18 5:12\"   \"3/30/18 5:16\"   \"3/30/18 5:21\"  \n[2833] \"3/30/18 5:28\"   \"3/30/18 6:24\"   \"3/30/18 10:25\"  \"3/30/18 10:25\" \n[2837] \"3/30/18 12:01\"  \"3/30/18 12:03\"  \"3/30/18 12:05\"  \"3/30/18 12:11\" \n[2841] \"3/30/18 16:29\"  \"3/30/18 22:55\"  \"3/31/18 4:59\"   \"3/31/18 5:07\"  \n[2845] \"3/31/18 6:41\"   \"3/31/18 6:44\"   \"3/31/18 6:45\"   \"3/31/18 11:56\" \n[2849] \"3/31/18 11:56\"  \"3/31/18 11:57\"  \"3/31/18 13:33\"  \"3/31/18 13:35\" \n[2853] \"4/1/18 4:45\"    \"4/1/18 6:18\"    \"4/1/18 6:25\"    \"4/1/18 6:26\"   \n[2857] \"4/1/18 6:29\"    \"4/1/18 11:43\"   \"4/1/18 11:46\"   \"4/1/18 11:48\"  \n[2861] \"4/1/18 13:25\"   \"4/1/18 13:34\"   \"4/1/18 19:14\"   \"4/1/18 19:15\"  \n[2865] \"4/2/18 2:19\"    \"4/2/18 4:38\"    \"4/2/18 4:39\"    \"4/2/18 4:44\"   \n[2869] \"4/2/18 4:44\"    \"4/2/18 5:56\"    \"4/2/18 5:58\"    \"4/2/18 11:27\"  \n[2873] \"4/2/18 11:30\"   \"4/2/18 11:33\"   \"4/2/18 11:33\"   \"4/2/18 11:34\"  \n[2877] \"4/2/18 11:34\"   \"4/2/18 23:57\"   \"4/3/18 4:30\"    \"4/3/18 4:32\"   \n[2881] \"4/3/18 5:37\"    \"4/3/18 5:47\"    \"4/3/18 6:42\"    \"4/3/18 11:18\"  \n[2885] \"4/3/18 11:19\"   \"4/3/18 11:20\"   \"4/3/18 12:57\"   \"4/3/18 12:59\"  \n[2889] \"4/3/18 13:02\"   \"4/3/18 13:03\"   \"4/3/18 23:51\"   \"4/4/18 5:56\"   \n[2893] \"4/4/18 5:57\"    \"4/4/18 5:59\"    \"4/4/18 6:24\"    \"4/4/18 6:24\"   \n[2897] \"4/4/18 6:59\"    \"4/4/18 11:04\"   \"4/4/18 11:13\"   \"4/4/18 12:45\"  \n[2901] \"4/4/18 12:48\"   \"4/4/18 12:48\"   \"4/4/18 12:49\"   \"4/4/18 12:55\"  \n[2905] \"4/4/18 23:41\"   \"4/5/18 5:53\"    \"4/5/18 6:03\"    \"4/5/18 6:36\"   \n[2909] \"4/5/18 6:40\"    \"4/5/18 6:46\"    \"4/5/18 12:32\"   \"4/5/18 12:33\"  \n[2913] \"4/5/18 12:35\"   \"4/5/18 12:35\"   \"4/5/18 12:40\"   \"4/5/18 12:41\"  \n[2917] \"4/5/18 12:41\"   \"4/5/18 23:28\"   \"4/5/18 23:28\"   \"4/5/18 23:33\"  \n[2921] \"4/6/18 3:55\"    \"4/6/18 5:35\"    \"4/6/18 6:14\"    \"4/6/18 6:15\"   \n[2925] \"4/6/18 6:21\"    \"4/6/18 12:20\"   \"4/6/18 12:21\"   \"4/6/18 12:21\"  \n[2929] \"4/6/18 12:21\"   \"4/6/18 12:26\"   \"4/6/18 12:28\"   \"4/6/18 19:09\"  \n[2933] \"4/6/18 19:15\"   \"4/7/18 5:55\"    \"4/7/18 5:58\"    \"4/7/18 5:59\"   \n[2937] \"4/7/18 6:00\"    \"4/7/18 6:00\"    \"4/7/18 6:01\"    \"4/7/18 6:56\"   \n[2941] \"4/7/18 7:40\"    \"4/7/18 12:10\"   \"4/7/18 12:10\"   \"4/7/18 12:16\"  \n[2945] \"4/7/18 12:17\"   \"4/7/18 14:30\"   \"4/8/18 2:54\"    \"4/8/18 5:15\"   \n[2949] \"4/8/18 5:33\"    \"4/8/18 5:43\"    \"4/8/18 5:44\"    \"4/8/18 5:45\"   \n[2953] \"4/8/18 6:39\"    \"4/8/18 6:41\"    \"4/8/18 11:57\"   \"4/8/18 11:59\"  \n[2957] \"4/8/18 12:04\"   \"4/8/18 12:06\"   \"4/8/18 12:06\"   \"4/8/18 13:12\"  \n[2961] \"4/8/18 13:43\"   \"4/9/18 5:05\"    \"4/9/18 5:14\"    \"4/9/18 6:13\"   \n[2965] \"4/9/18 6:19\"    \"4/9/18 6:21\"    \"4/9/18 6:56\"    \"4/9/18 6:56\"   \n[2969] \"4/9/18 6:57\"    \"4/9/18 7:02\"    \"4/9/18 11:46\"   \"4/9/18 11:51\"  \n[2973] \"4/9/18 11:52\"   \"4/9/18 11:52\"   \"4/9/18 11:53\"   \"4/9/18 11:57\"  \n[2977] \"4/10/18 4:51\"   \"4/10/18 4:56\"   \"4/10/18 6:00\"   \"4/10/18 6:32\"  \n[2981] \"4/10/18 6:34\"   \"4/10/18 6:39\"   \"4/10/18 6:40\"   \"4/10/18 6:42\"  \n[2985] \"4/10/18 11:34\"  \"4/10/18 11:37\"  \"4/10/18 11:40\"  \"4/10/18 11:41\" \n[2989] \"4/10/18 11:44\"  \"4/10/18 13:18\"  \"4/10/18 13:48\"  \"4/11/18 2:37\"  \n[2993] \"4/11/18 2:41\"   \"4/11/18 4:36\"   \"4/11/18 4:37\"   \"4/11/18 4:37\"  \n[2997] \"4/11/18 6:16\"   \"4/11/18 6:17\"   \"4/11/18 6:21\"   \"4/11/18 11:23\" \n[3001] \"4/11/18 11:26\"  \"4/11/18 11:27\"  \"4/11/18 11:29\"  \"4/11/18 11:31\" \n[3005] \"4/11/18 11:31\"  \"4/11/18 11:34\"  \"4/11/18 13:06\"  \"4/12/18 2:03\"  \n[3009] \"4/12/18 2:45\"   \"4/12/18 4:30\"   \"4/12/18 4:30\"   \"4/12/18 6:05\"  \n[3013] \"4/12/18 6:56\"   \"4/12/18 7:36\"   \"4/12/18 11:15\"  \"4/12/18 11:17\" \n[3017] \"4/12/18 11:17\"  \"4/12/18 11:18\"  \"4/12/18 12:58\"  \"4/12/18 12:58\" \n[3021] \"4/12/18 13:57\"  \"4/12/18 23:44\"  \"4/13/18 5:33\"   \"4/13/18 5:34\"  \n[3025] \"4/13/18 5:37\"   \"4/13/18 5:54\"   \"4/13/18 6:33\"   \"4/13/18 12:43\" \n[3029] \"4/13/18 12:44\"  \"4/13/18 12:44\"  \"4/13/18 12:47\"  \"4/13/18 12:48\" \n[3033] \"4/13/18 12:51\"  \"4/13/18 23:36\"  \"4/14/18 5:17\"   \"4/14/18 5:17\"  \n[3037] \"4/14/18 6:18\"   \"4/14/18 6:49\"   \"4/14/18 6:57\"   \"4/14/18 10:50\" \n[3041] \"4/14/18 12:28\"  \"4/14/18 12:30\"  \"4/14/18 12:30\"  \"4/14/18 12:32\" \n[3045] \"4/14/18 12:35\"  \"4/14/18 12:38\"  \"4/14/18 23:20\"  \"4/14/18 23:28\" \n[3049] \"4/15/18 3:45\"   \"4/15/18 5:28\"   \"4/15/18 5:54\"   \"4/15/18 6:29\"  \n[3053] \"4/15/18 6:30\"   \"4/15/18 6:37\"   \"4/15/18 12:16\"  \"4/15/18 12:17\" \n[3057] \"4/15/18 12:21\"  \"4/15/18 12:23\"  \"4/15/18 15:11\"  \"4/15/18 23:17\" \n[3061] \"4/16/18 1:44\"   \"4/16/18 2:53\"   \"4/16/18 3:21\"   \"4/16/18 6:11\"  \n[3065] \"4/16/18 6:12\"   \"4/16/18 6:14\"   \"4/16/18 6:17\"   \"4/16/18 10:30\" \n[3069] \"4/16/18 12:05\"  \"4/16/18 12:07\"  \"4/16/18 12:09\"  \"4/16/18 12:10\" \n[3073] \"4/16/18 14:00\"  \"4/17/18 2:48\"   \"4/17/18 5:47\"   \"4/17/18 5:51\"  \n[3077] \"4/17/18 5:55\"   \"4/17/18 5:57\"   \"4/17/18 5:57\"   \"4/17/18 6:50\"  \n[3081] \"4/17/18 11:53\"  \"4/17/18 11:59\"  \"4/17/18 11:59\"  \"4/17/18 12:00\" \n[3085] \"4/17/18 12:01\"  \"4/17/18 12:01\"  \"4/17/18 13:30\"  \"4/18/18 2:04\"  \n[3089] \"4/18/18 3:13\"   \"4/18/18 3:43\"   \"4/18/18 4:55\"   \"4/18/18 4:58\"  \n[3093] \"4/18/18 5:00\"   \"4/18/18 6:25\"   \"4/18/18 6:32\"   \"4/18/18 6:35\"  \n[3097] \"4/18/18 11:41\"  \"4/18/18 11:43\"  \"4/18/18 11:45\"  \"4/18/18 11:47\" \n[3101] \"4/18/18 11:53\"  \"4/18/18 11:53\"  \"4/18/18 13:09\"  \"4/18/18 13:26\" \n[3105] \"4/19/18 3:11\"   \"4/19/18 4:48\"   \"4/19/18 5:16\"   \"4/19/18 6:47\"  \n[3109] \"4/19/18 6:48\"   \"4/19/18 6:50\"   \"4/19/18 6:54\"   \"4/19/18 6:54\"  \n[3113] \"4/19/18 11:31\"  \"4/19/18 11:36\"  \"4/19/18 11:37\"  \"4/19/18 11:39\" \n[3117] \"4/19/18 11:41\"  \"4/19/18 13:17\"  \"4/20/18 2:51\"   \"4/20/18 4:46\"  \n[3121] \"4/20/18 5:52\"   \"4/20/18 6:25\"   \"4/20/18 6:28\"   \"4/20/18 6:30\"  \n[3125] \"4/20/18 6:33\"   \"4/20/18 11:22\"  \"4/20/18 11:25\"  \"4/20/18 11:27\" \n[3129] \"4/20/18 11:30\"  \"4/20/18 13:03\"  \"4/20/18 13:06\"  \"4/20/18 13:39\" \n[3133] \"4/20/18 14:00\"  \"4/20/18 23:57\"  \"4/21/18 4:29\"   \"4/21/18 5:57\"  \n[3137] \"4/21/18 6:01\"   \"4/21/18 6:02\"   \"4/21/18 6:05\"   \"4/21/18 6:05\"  \n[3141] \"4/21/18 6:10\"   \"4/21/18 6:13\"   \"4/21/18 6:14\"   \"4/21/18 11:12\" \n[3145] \"4/21/18 11:13\"  \"4/21/18 11:14\"  \"4/21/18 11:15\"  \"4/21/18 11:18\" \n[3149] \"4/21/18 11:19\"  \"4/21/18 12:57\"  \"4/22/18 4:02\"   \"4/22/18 5:42\"  \n[3153] \"4/22/18 5:48\"   \"4/22/18 5:49\"   \"4/22/18 5:49\"   \"4/22/18 5:51\"  \n[3157] \"4/22/18 6:49\"   \"4/22/18 7:30\"   \"4/22/18 12:36\"  \"4/22/18 12:41\" \n[3161] \"4/22/18 12:41\"  \"4/22/18 12:41\"  \"4/22/18 12:42\"  \"4/22/18 12:44\" \n[3165] \"4/22/18 23:32\"  \"4/23/18 3:52\"   \"4/23/18 3:57\"   \"4/23/18 5:24\"  \n[3169] \"4/23/18 5:25\"   \"4/23/18 5:34\"   \"4/23/18 5:36\"   \"4/23/18 5:37\"  \n[3173] \"4/23/18 6:26\"   \"4/23/18 6:30\"   \"4/23/18 10:51\"  \"4/23/18 12:25\" \n[3177] \"4/23/18 12:26\"  \"4/23/18 12:27\"  \"4/23/18 12:28\"  \"4/23/18 12:31\" \n[3181] \"4/23/18 12:32\"  \"4/23/18 12:33\"  \"4/23/18 13:48\"  \"4/24/18 3:46\"  \n[3185] \"4/24/18 5:22\"   \"4/24/18 5:27\"   \"4/24/18 6:42\"   \"4/24/18 6:43\"  \n[3189] \"4/24/18 6:44\"   \"4/24/18 6:46\"   \"4/24/18 6:48\"   \"4/24/18 6:49\"  \n[3193] \"4/24/18 12:13\"  \"4/24/18 12:14\"  \"4/24/18 12:15\"  \"4/24/18 12:15\" \n[3197] \"4/24/18 12:18\"  \"4/24/18 12:19\"  \"4/24/18 12:19\"  \"4/24/18 12:21\" \n[3201] \"4/24/18 12:21\"  \"4/25/18 2:25\"   \"4/25/18 3:38\"   \"4/25/18 4:44\"  \n[3205] \"4/25/18 4:46\"   \"4/25/18 4:50\"   \"4/25/18 5:48\"   \"4/25/18 6:22\"  \n[3209] \"4/25/18 6:26\"   \"4/25/18 6:31\"   \"4/25/18 7:28\"   \"4/25/18 12:04\" \n[3213] \"4/25/18 12:06\"  \"4/25/18 12:06\"  \"4/25/18 12:08\"  \"4/25/18 12:10\" \n[3217] \"4/26/18 3:09\"   \"4/26/18 3:43\"   \"4/26/18 5:21\"   \"4/26/18 6:08\"  \n[3221] \"4/26/18 6:12\"   \"4/26/18 11:50\"  \"4/26/18 11:59\"  \"4/26/18 11:59\" \n[3225] \"4/26/18 12:00\"  \"4/26/18 13:37\"  \"4/26/18 13:39\"  \"4/26/18 19:00\" \n[3229] \"4/27/18 2:40\"   \"4/27/18 2:44\"   \"4/27/18 3:16\"   \"4/27/18 4:52\"  \n[3233] \"4/27/18 5:42\"   \"4/27/18 5:47\"   \"4/27/18 6:45\"   \"4/27/18 6:46\"  \n[3237] \"4/27/18 6:46\"   \"4/27/18 11:43\"  \"4/27/18 11:43\"  \"4/27/18 11:44\" \n[3241] \"4/27/18 11:46\"  \"4/27/18 11:50\"  \"4/27/18 13:19\"  \"4/27/18 13:20\" \n[3245] \"4/27/18 13:23\"  \"4/28/18 2:55\"   \"4/28/18 4:36\"   \"4/28/18 5:21\"  \n[3249] \"4/28/18 6:25\"   \"4/28/18 6:27\"   \"4/28/18 6:27\"   \"4/28/18 6:29\"  \n[3253] \"4/28/18 6:58\"   \"4/28/18 11:27\"  \"4/28/18 11:31\"  \"4/28/18 11:31\" \n[3257] \"4/28/18 11:32\"  \"4/28/18 11:35\"  \"4/28/18 11:35\"  \"4/28/18 13:10\" \n[3261] \"4/28/18 13:19\"  \"4/29/18 4:32\"   \"4/29/18 5:04\"   \"4/29/18 5:59\"  \n[3265] \"4/29/18 6:42\"   \"4/29/18 6:46\"   \"4/29/18 6:47\"   \"4/29/18 11:16\" \n[3269] \"4/29/18 11:17\"  \"4/29/18 11:21\"  \"4/29/18 11:22\"  \"4/29/18 11:22\" \n[3273] \"4/29/18 11:22\"  \"4/29/18 11:24\"  \"4/29/18 11:24\"  \"4/29/18 23:56\" \n[3277] \"4/30/18 3:40\"   \"4/30/18 4:44\"   \"4/30/18 6:16\"   \"4/30/18 6:17\"  \n[3281] \"4/30/18 6:24\"   \"4/30/18 6:27\"   \"4/30/18 11:10\"  \"4/30/18 11:14\" \n[3285] \"4/30/18 12:44\"  \"4/30/18 12:45\"  \"4/30/18 12:46\"  \"4/30/18 12:51\" \n[3289] \"5/1/18 3:54\"    \"5/1/18 5:40\"    \"5/1/18 5:48\"    \"5/1/18 6:07\"   \n[3293] \"5/1/18 6:57\"    \"5/1/18 12:32\"   \"5/1/18 12:33\"   \"5/1/18 12:38\"  \n[3297] \"5/1/18 12:39\"   \"5/1/18 12:40\"   \"5/1/18 12:43\"   \"5/1/18 23:26\"  \n[3301] \"5/2/18 5:31\"    \"5/2/18 5:35\"    \"5/2/18 5:45\"    \"5/2/18 6:39\"   \n[3305] \"5/2/18 6:43\"    \"5/2/18 6:44\"    \"5/2/18 6:44\"    \"5/2/18 12:21\"  \n[3309] \"5/2/18 12:26\"   \"5/2/18 12:26\"   \"5/2/18 12:29\"   \"5/2/18 12:30\"  \n[3313] \"5/2/18 13:57\"   \"5/2/18 14:00\"   \"5/3/18 3:42\"    \"5/3/18 5:22\"   \n[3317] \"5/3/18 6:20\"    \"5/3/18 6:56\"    \"5/3/18 6:58\"    \"5/3/18 10:34\"  \n[3321] \"5/3/18 10:38\"   \"5/3/18 12:10\"   \"5/3/18 12:11\"   \"5/3/18 12:16\"  \n[3325] \"5/4/18 3:42\"    \"5/4/18 5:56\"    \"5/4/18 6:37\"    \"5/4/18 6:38\"   \n[3329] \"5/4/18 6:44\"    \"5/4/18 6:45\"    \"5/4/18 11:58\"   \"5/4/18 12:02\"  \n[3333] \"5/4/18 12:02\"   \"5/4/18 12:05\"   \"5/4/18 12:11\"   \"5/4/18 22:55\"  \n[3337] \"5/5/18 3:18\"    \"5/5/18 3:33\"    \"5/5/18 4:56\"    \"5/5/18 5:03\"   \n[3341] \"5/5/18 6:18\"    \"5/5/18 11:51\"   \"5/5/18 11:52\"   \"5/5/18 11:54\"  \n[3345] \"5/5/18 11:58\"   \"5/5/18 11:59\"   \"5/5/18 19:12\"   \"5/6/18 4:51\"   \n[3349] \"5/6/18 5:13\"    \"5/6/18 5:56\"    \"5/6/18 5:58\"    \"5/6/18 5:59\"   \n[3353] \"5/6/18 6:58\"    \"5/6/18 11:37\"   \"5/6/18 11:43\"   \"5/6/18 11:45\"  \n[3357] \"5/6/18 11:46\"   \"5/6/18 13:36\"   \"5/7/18 2:22\"    \"5/7/18 4:08\"   \n[3361] \"5/7/18 4:38\"    \"5/7/18 5:35\"    \"5/7/18 6:36\"    \"5/7/18 6:41\"   \n[3365] \"5/7/18 11:27\"   \"5/7/18 11:28\"   \"5/7/18 11:29\"   \"5/7/18 11:31\"  \n[3369] \"5/8/18 4:18\"    \"5/8/18 4:25\"    \"5/8/18 6:51\"    \"5/8/18 6:55\"   \n[3373] \"5/8/18 6:57\"    \"5/8/18 6:58\"    \"5/8/18 11:13\"   \"5/8/18 11:18\"  \n[3377] \"5/8/18 11:19\"   \"5/8/18 11:20\"   \"5/8/18 11:24\"   \"5/8/18 13:03\"  \n[3381] \"5/9/18 3:19\"    \"5/9/18 4:56\"    \"5/9/18 5:50\"    \"5/9/18 5:50\"   \n[3385] \"5/9/18 5:50\"    \"5/9/18 6:39\"    \"5/9/18 11:06\"   \"5/9/18 12:41\"  \n[3389] \"5/9/18 12:42\"   \"5/9/18 12:42\"   \"5/10/18 4:35\"   \"5/10/18 5:39\"  \n[3393] \"5/10/18 5:42\"   \"5/10/18 6:12\"   \"5/10/18 6:13\"   \"5/10/18 12:29\" \n[3397] \"5/10/18 12:33\"  \"5/10/18 12:36\"  \"5/10/18 12:37\"  \"5/10/18 13:59\" \n[3401] \"5/11/18 3:41\"   \"5/11/18 5:56\"   \"5/11/18 6:49\"   \"5/11/18 6:55\"  \n[3405] \"5/11/18 6:55\"   \"5/11/18 7:35\"   \"5/11/18 12:18\"  \"5/11/18 12:21\" \n[3409] \"5/11/18 12:27\"  \"5/12/18 3:15\"   \"5/12/18 5:30\"   \"5/12/18 5:35\"  \n[3413] \"5/12/18 5:37\"   \"5/12/18 5:40\"   \"5/12/18 6:34\"   \"5/12/18 12:07\" \n[3417] \"5/12/18 12:10\"  \"5/12/18 12:11\"  \"5/12/18 12:15\"  \"5/12/18 13:52\" \n[3421] \"5/12/18 13:57\"  \"5/13/18 5:03\"   \"5/13/18 5:08\"   \"5/13/18 6:10\"  \n[3425] \"5/13/18 6:49\"   \"5/13/18 6:50\"   \"5/13/18 6:57\"   \"5/13/18 10:21\" \n[3429] \"5/13/18 11:57\"  \"5/13/18 11:57\"  \"5/13/18 12:03\"  \"5/13/18 13:21\" \n[3433] \"5/14/18 5:55\"   \"5/14/18 6:28\"   \"5/14/18 6:29\"   \"5/14/18 6:31\"  \n[3437] \"5/14/18 6:37\"   \"5/14/18 11:46\"  \"5/14/18 11:47\"  \"5/14/18 11:47\" \n[3441] \"5/15/18 4:31\"   \"5/15/18 4:36\"   \"5/15/18 5:28\"   \"5/15/18 5:28\"  \n[3445] \"5/15/18 6:10\"   \"5/15/18 11:32\"  \"5/15/18 11:35\"  \"5/15/18 13:14\" \n[3449] \"5/15/18 13:17\"  \"5/15/18 14:01\"  \"5/16/18 3:39\"   \"5/16/18 3:45\"  \n[3453] \"5/16/18 4:35\"   \"5/16/18 5:49\"   \"5/16/18 6:51\"   \"5/16/18 11:23\" \n[3457] \"5/16/18 11:26\"  \"5/16/18 11:27\"  \"5/16/18 13:12\"  \"5/16/18 13:32\" \n[3461] \"5/16/18 13:32\"  \"5/17/18 5:29\"   \"5/17/18 5:32\"   \"5/17/18 5:34\"  \n[3465] \"5/17/18 5:55\"   \"5/17/18 5:56\"   \"5/17/18 6:28\"   \"5/17/18 7:08\"  \n[3469] \"5/17/18 11:16\"  \"5/17/18 11:20\"  \"5/17/18 12:51\"  \"5/17/18 12:54\" \n[3473] \"5/17/18 12:57\"  \"5/18/18 6:07\"   \"5/18/18 6:12\"   \"5/18/18 6:52\"  \n[3477] \"5/18/18 6:52\"   \"5/18/18 12:40\"  \"5/18/18 12:41\"  \"5/18/18 12:47\" \n[3481] \"5/18/18 14:00\"  \"5/19/18 5:35\"   \"5/19/18 5:52\"   \"5/19/18 5:53\"  \n[3485] \"5/19/18 6:27\"   \"5/19/18 6:32\"   \"5/19/18 12:32\"  \"5/19/18 12:35\" \n[3489] \"5/19/18 12:35\"  \"5/19/18 13:32\"  \"5/19/18 13:37\"  \"5/20/18 5:25\"  \n[3493] \"5/20/18 5:29\"   \"5/20/18 6:05\"   \"5/20/18 6:14\"   \"5/20/18 10:41\" \n[3497] \"5/20/18 12:17\"  \"5/20/18 12:19\"  \"5/20/18 12:20\"  \"5/20/18 12:22\" \n[3501] \"5/21/18 5:52\"   \"5/21/18 5:53\"   \"5/21/18 6:46\"   \"5/21/18 6:48\"  \n[3505] \"5/21/18 6:50\"   \"5/21/18 7:27\"   \"5/21/18 12:05\"  \"5/21/18 13:48\" \n[3509] \"5/21/18 13:49\"  \"5/21/18 13:50\"  \"5/22/18 5:04\"   \"5/22/18 5:26\"  \n[3513] \"5/22/18 5:31\"   \"5/22/18 5:32\"   \"5/22/18 6:29\"   \"5/22/18 7:07\"  \n[3517] \"5/22/18 11:51\"  \"5/22/18 11:53\"  \"5/22/18 11:53\"  \"5/22/18 11:56\" \n[3521] \"5/23/18 2:26\"   \"5/23/18 5:06\"   \"5/23/18 6:42\"   \"5/23/18 6:42\"  \n[3525] \"5/23/18 6:44\"   \"5/23/18 11:42\"  \"5/23/18 11:44\"  \"5/23/18 11:46\" \n[3529] \"5/23/18 11:46\"  \"5/24/18 2:02\"   \"5/24/18 6:21\"   \"5/24/18 6:24\"  \n[3533] \"5/24/18 6:26\"   \"5/24/18 6:32\"   \"5/24/18 11:30\"  \"5/24/18 11:33\" \n[3537] \"5/24/18 11:34\"  \"5/24/18 11:36\"  \"5/25/18 3:01\"   \"5/25/18 3:05\"  \n[3541] \"5/25/18 6:06\"   \"5/25/18 6:09\"   \"5/25/18 11:19\"  \"5/25/18 11:20\" \n[3545] \"5/25/18 11:26\"  \"5/25/18 13:04\"  \"5/26/18 4:09\"   \"5/26/18 5:46\"  \n[3549] \"5/26/18 6:42\"   \"5/26/18 6:46\"   \"5/26/18 7:27\"   \"5/26/18 12:50\" \n[3553] \"5/26/18 12:50\"  \"5/26/18 12:52\"  \"5/27/18 5:21\"   \"5/27/18 6:25\"  \n[3557] \"5/27/18 6:26\"   \"5/27/18 6:27\"   \"5/27/18 6:28\"   \"5/27/18 11:00\" \n[3561] \"5/27/18 12:41\"  \"5/27/18 12:44\"  \"5/28/18 6:00\"   \"5/28/18 6:00\"  \n[3565] \"5/28/18 6:38\"   \"5/28/18 6:39\"   \"5/28/18 6:40\"   \"5/28/18 12:22\" \n[3569] \"5/28/18 12:24\"  \"5/28/18 12:30\"  \"5/28/18 12:32\"  \"5/28/18 12:35\" \n[3573] \"5/29/18 5:39\"   \"5/29/18 6:18\"   \"5/29/18 6:21\"   \"5/29/18 6:24\"  \n[3577] \"5/29/18 6:26\"   \"5/29/18 12:12\"  \"5/29/18 12:18\"  \"5/29/18 12:19\" \n[3581] \"5/29/18 12:19\"  \"5/29/18 12:20\"  \"5/30/18 5:06\"   \"5/30/18 5:07\"  \n[3585] \"5/30/18 5:59\"   \"5/30/18 6:06\"   \"5/30/18 6:57\"   \"5/30/18 12:07\" \n[3589] \"5/30/18 13:45\"  \"5/30/18 13:47\"  \"5/30/18 13:58\"  \"5/31/18 5:37\"  \n[3593] \"5/31/18 5:37\"   \"5/31/18 5:45\"   \"5/31/18 5:46\"   \"5/31/18 6:45\"  \n[3597] \"5/31/18 11:48\"  \"5/31/18 11:53\"  \"5/31/18 11:57\"  \"5/31/18 14:04\" \n[3601] \"5/31/18 14:05\"  \"6/5/18 3:57\"    \"6/5/18 6:38\"    \"6/5/18 6:40\"   \n[3605] \"6/5/18 6:41\"    \"6/5/18 12:33\"   \"6/5/18 12:33\"   \"6/5/18 12:34\"  \n[3609] \"6/6/18 5:27\"    \"6/6/18 6:18\"    \"6/6/18 6:20\"    \"6/6/18 6:54\"   \n[3613] \"6/6/18 7:00\"    \"6/7/18 5:15\"    \"6/7/18 5:18\"    \"6/7/18 6:33\"   \n[3617] \"6/7/18 6:36\"    \"6/7/18 12:07\"   \"6/7/18 12:07\"   \"6/7/18 12:13\"  \n[3621] \"6/7/18 13:57\"   \"6/8/18 5:03\"    \"6/8/18 6:11\"    \"6/8/18 6:12\"   \n[3625] \"6/8/18 6:12\"    \"6/8/18 11:59\"   \"6/8/18 13:10\"   \"6/8/18 13:12\"  \n[3629] \"6/9/18 4:49\"    \"6/9/18 5:58\"    \"6/9/18 6:56\"    \"6/9/18 6:59\"   \n[3633] \"6/9/18 11:44\"   \"6/9/18 11:52\"   \"6/10/18 3:14\"   \"6/10/18 4:41\"  \n[3637] \"6/10/18 4:44\"   \"6/10/18 6:36\"   \"6/10/18 11:32\"  \"6/10/18 13:20\" \n[3641] \"6/10/18 13:20\"  \"6/11/18 5:16\"   \"6/11/18 6:14\"   \"6/11/18 6:53\"  \n[3645] \"6/11/18 6:55\"   \"6/11/18 11:25\"  \"6/11/18 11:25\"  \"6/11/18 11:25\" \n[3649] \"6/11/18 11:29\"  \"6/12/18 2:29\"   \"6/12/18 5:56\"   \"6/12/18 6:27\"  \n[3653] \"6/12/18 6:31\"   \"6/12/18 11:13\"  \"6/12/18 11:16\"  \"6/12/18 11:19\" \n[3657] \"6/12/18 13:30\"  \"6/13/18 6:06\"   \"6/13/18 6:08\"   \"6/13/18 6:11\"  \n[3661] \"6/13/18 6:14\"   \"6/13/18 7:13\"   \"6/13/18 11:01\"  \"6/13/18 11:02\" \n[3665] \"6/13/18 12:39\"  \"6/13/18 12:41\"  \"6/14/18 3:53\"   \"6/14/18 5:06\"  \n[3669] \"6/14/18 5:35\"   \"6/14/18 5:46\"   \"6/14/18 5:56\"   \"6/14/18 7:29\"  \n[3673] \"6/14/18 7:30\"   \"6/14/18 7:32\"   \"6/14/18 10:50\"  \"6/14/18 12:39\" \n[3677] \"6/24/18 4:03\"   \"6/24/18 4:06\"   \"6/24/18 5:47\"   \"6/24/18 5:48\"  \n[3681] \"6/24/18 6:47\"   \"6/24/18 12:13\"  \"6/24/18 12:17\"  \"6/24/18 13:31\" \n[3685] \"6/25/18 3:37\"   \"6/25/18 5:08\"   \"6/25/18 5:25\"   \"6/25/18 6:21\"  \n[3689] \"6/25/18 12:00\"  \"6/25/18 12:06\"  \"6/25/18 12:06\"  \"6/25/18 12:08\" \n[3693] \"6/25/18 13:49\"  \"6/26/18 5:57\"   \"6/26/18 6:39\"   \"6/26/18 6:42\"  \n[3697] \"6/26/18 6:47\"   \"6/26/18 6:47\"   \"6/26/18 11:50\"  \"6/26/18 11:50\" \n[3701] \"6/26/18 11:52\"  \"6/26/18 11:54\"  \"6/26/18 13:32\"  \"6/27/18 4:27\"  \n[3705] \"6/27/18 4:45\"   \"6/27/18 5:34\"   \"6/27/18 6:21\"   \"6/27/18 11:40\" \n[3709] \"6/27/18 11:41\"  \"6/27/18 11:43\"  \"6/27/18 11:44\"  \"6/27/18 13:16\" \n[3713] \"6/28/18 4:19\"   \"6/28/18 5:56\"   \"6/28/18 6:01\"   \"6/28/18 6:03\"  \n[3717] \"6/28/18 6:04\"   \"6/28/18 11:33\"  \"6/28/18 11:33\"  \"6/28/18 11:33\" \n[3721] \"6/28/18 13:09\"  \"6/28/18 13:13\"  \"6/29/18 4:22\"   \"6/29/18 5:36\"  \n[3725] \"6/29/18 5:43\"   \"6/29/18 5:45\"   \"6/29/18 6:43\"   \"6/29/18 11:17\" \n[3729] \"6/29/18 11:17\"  \"6/29/18 11:20\"  \"6/29/18 11:20\"  \"6/30/18 2:43\"  \n[3733] \"6/30/18 5:15\"   \"6/30/18 5:19\"   \"6/30/18 7:00\"   \"6/30/18 11:07\" \n[3737] \"7/1/18 5:39\"    \"7/1/18 6:01\"    \"7/1/18 6:36\"    \"7/1/18 10:53\"  \n[3741] \"7/1/18 10:59\"   \"7/1/18 13:56\"   \"7/2/18 4:03\"    \"7/2/18 6:12\"   \n[3745] \"7/2/18 6:16\"    \"7/2/18 7:16\"    \"7/3/18 2:49\"    \"7/3/18 5:56\"   \n[3749] \"7/3/18 12:10\"   \"7/3/18 12:11\"   \"7/3/18 13:33\"   \"7/4/18 2:15\"   \n[3753] \"7/4/18 2:26\"    \"7/4/18 3:24\"    \"7/8/18 5:52\"    \"7/8/18 6:55\"   \n[3757] \"7/8/18 11:15\"   \"7/8/18 11:20\"   \"7/9/18 2:51\"    \"7/9/18 7:10\"   \n[3761] \"7/9/18 13:46\"   \"7/10/18 6:50\"   \"7/10/18 6:53\"   \"7/10/18 12:30\" \n[3765] \"7/10/18 13:32\"  \"7/11/18 4:50\"   \"7/11/18 12:27\"  \"7/12/18 6:13\"  \n[3769] \"7/12/18 6:14\"   \"7/12/18 12:09\"  \"7/13/18 5:47\"   \"7/13/18 6:48\"  \n[3773] \"7/13/18 12:00\"  \"7/13/18 13:22\"  \"7/14/18 2:06\"   \"7/14/18 4:42\"  \n[3777] \"7/14/18 11:50\"  \"7/14/18 13:31\"  \"7/15/18 4:36\"   \"7/15/18 5:11\"  \n[3781] \"7/15/18 6:45\"   \"7/15/18 11:37\"  \"7/16/18 2:43\"   \"7/16/18 4:26\"  \n[3785] \"7/16/18 6:31\"   \"7/16/18 13:05\"  \"7/16/18 13:05\"  \"7/17/18 2:47\"  \n[3789] \"7/17/18 4:15\"   \"7/17/18 6:13\"   \"7/17/18 12:50\"  \"7/23/18 5:46\"  \n[3793] \"7/23/18 11:44\"  \"7/24/18 6:26\"   \"7/24/18 11:31\"  \"7/25/18 3:02\"  \n[3797] \"7/26/18 2:34\"   \"7/26/18 12:44\"  \"7/27/18 6:01\"   \"7/28/18 3:07\"  \n[3801] \"7/29/18 6:58\"   \"7/30/18 6:35\"   \"8/1/18 2:43\"    \"8/1/18 13:25\"  \n[3805] \"8/2/18 2:12\"    \"8/2/18 4:23\"    \"8/2/18 11:27\"   \"8/2/18 13:12\"  \n[3809] \"8/3/18 3:21\"    \"8/3/18 5:19\"    \"8/3/18 11:16\"   \"8/3/18 13:29\"  \n[3813] \"8/4/18 3:35\"    \"8/4/18 4:05\"    \"8/4/18 13:33\"   \"8/5/18 3:09\"   \n[3817] \"8/5/18 5:34\"    \"8/5/18 12:32\"   \"8/5/18 12:34\"   \"8/6/18 5:16\"   \n[3821] \"8/6/18 5:56\"    \"8/6/18 12:25\"   \"8/7/18 2:57\"    \"8/7/18 6:36\"   \n[3825] \"8/7/18 7:12\"    \"8/7/18 12:07\"   \"8/8/18 11:59\"   \"8/9/18 5:54\"   \n[3829] \"8/9/18 11:50\"   \"8/10/18 13:49\"  \"8/11/18 6:52\"   \"8/11/18 11:24\" \n[3833] \"8/12/18 4:11\"   \"8/13/18 6:46\"   \"8/13/18 13:52\"  \"8/14/18 6:24\"  \n[3837] \"8/14/18 13:53\"  \"8/15/18 2:07\"   \"8/15/18 6:04\"   \"8/15/18 13:28\" \n[3841] \"8/16/18 5:46\"   \"8/16/18 13:46\"  \"8/17/18 3:12\"   \"8/17/18 7:07\"  \n[3845] \"8/18/18 6:38\"   \"8/18/18 11:44\"  \"8/19/18 6:17\"   \"8/19/18 6:19\"  \n[3849] \"8/19/18 11:32\"  \"8/19/18 11:35\"  \"8/20/18 3:34\"   \"8/20/18 6:06\"  \n[3853] \"8/20/18 12:59\"  \"8/21/18 5:40\"   \"8/21/18 6:45\"   \"8/21/18 12:52\" \n[3857] \"8/22/18 2:47\"   \"8/22/18 5:26\"   \"8/22/18 12:34\"  \"8/22/18 12:37\" \n[3861] \"8/23/18 6:36\"   \"8/23/18 6:42\"   \"8/23/18 12:22\"  \"8/23/18 13:45\" \n[3865] \"8/24/18 6:17\"   \"8/24/18 6:22\"   \"8/24/18 12:09\"  \"8/24/18 13:53\" \n[3869] \"8/25/18 6:03\"   \"8/26/18 5:37\"   \"8/26/18 13:32\"  \"8/27/18 6:53\"  \n[3873] \"8/27/18 13:21\"  \"8/28/18 6:34\"   \"8/28/18 6:37\"   \"8/28/18 13:07\" \n[3877] \"8/29/18 6:19\"   \"8/30/18 5:53\"   \"8/30/18 12:47\"  \"8/31/18 5:37\"  \n[3881] \"8/31/18 12:29\"  \"9/1/18 6:18\"    \"9/1/18 12:25\"   \"9/2/18 3:27\"   \n[3885] \"9/2/18 12:15\"   \"9/3/18 2:14\"    \"9/3/18 13:39\"   \"9/4/18 6:55\"   \n[3889] \"9/4/18 13:27\"   \"9/5/18 5:36\"    \"9/5/18 11:36\"   \"9/6/18 2:18\"   \n[3893] \"9/6/18 11:25\"   \"9/6/18 13:02\"   \"9/7/18 4:47\"    \"9/7/18 5:52\"   \n[3897] \"9/7/18 11:16\"   \"9/8/18 5:41\"    \"9/8/18 6:11\"    \"9/8/18 12:38\"  \n[3901] \"9/8/18 12:41\"   \"9/9/18 3:44\"    \"9/9/18 6:49\"    \"9/9/18 12:28\"  \n[3905] \"9/9/18 12:33\"   \"9/10/18 2:02\"   \"9/10/18 12:18\"  \"9/11/18 5:05\"  \n[3909] \"9/11/18 12:10\"  \"9/12/18 5:50\"   \"9/12/18 11:55\"  \"9/13/18 3:30\"  \n[3913] \"9/13/18 6:06\"   \"9/13/18 11:46\"  \"9/14/18 5:49\"   \"9/14/18 6:47\"  \n[3917] \"9/14/18 7:27\"   \"9/14/18 11:28\"  \"9/15/18 4:12\"   \"9/15/18 4:18\"  \n[3921] \"9/15/18 13:22\"  \"9/15/18 13:50\"  \"9/16/18 2:08\"   \"9/16/18 6:40\"  \n[3925] \"9/16/18 13:30\"  \"9/17/18 6:19\"   \"9/17/18 7:25\"   \"9/18/18 3:05\"  \n[3929] \"9/18/18 10:44\"  \"9/20/18 2:28\"   \"9/20/18 12:06\"  \"9/21/18 4:47\"  \n[3933] \"9/21/18 11:53\"  \"9/22/18 6:13\"   \"9/22/18 13:21\"  \"9/23/18 11:29\" \n[3937] \"9/24/18 5:49\"   \"9/24/18 6:42\"   \"9/24/18 11:15\"  \"9/24/18 11:21\" \n[3941] \"9/24/18 13:01\"  \"9/25/18 5:15\"   \"9/25/18 5:41\"   \"9/25/18 12:49\" \n[3945] \"9/25/18 12:50\"  \"9/26/18 4:51\"   \"9/26/18 6:37\"   \"9/27/18 6:15\"  \n[3949] \"9/27/18 12:21\"  \"9/27/18 12:22\"  \"9/28/18 2:34\"   \"9/28/18 12:17\" \n[3953] \"9/29/18 13:39\"  \"9/30/18 3:13\"   \"9/30/18 11:48\"  \"9/30/18 11:52\" \n[3957] \"10/1/18 4:49\"   \"10/1/18 11:34\"  \"10/1/18 11:37\"  \"10/2/18 4:31\"  \n[3961] \"10/2/18 6:10\"   \"10/2/18 11:28\"  \"10/3/18 6:51\"   \"10/3/18 6:54\"  \n[3965] \"10/3/18 12:49\"  \"10/3/18 12:52\"  \"10/4/18 5:34\"   \"10/4/18 5:34\"  \n[3969] \"10/4/18 7:07\"   \"10/4/18 12:38\"  \"10/5/18 6:50\"   \"10/5/18 6:50\"  \n[3973] \"10/5/18 12:30\"  \"10/6/18 6:27\"   \"10/6/18 6:31\"   \"10/6/18 12:19\" \n[3977] \"10/6/18 12:23\"  \"10/7/18 6:10\"   \"10/7/18 12:04\"  \"10/7/18 13:50\" \n[3981] \"10/8/18 3:05\"   \"10/8/18 4:45\"   \"10/8/18 11:54\"  \"10/8/18 11:59\" \n[3985] \"10/8/18 13:38\"  \"10/9/18 6:32\"   \"10/9/18 13:25\"  \"10/9/18 13:25\" \n[3989] \"10/10/18 6:48\"  \"10/10/18 6:50\"  \"10/10/18 13:10\" \"10/10/18 13:44\"\n[3993] \"10/11/18 6:26\"  \"10/11/18 6:26\"  \"10/11/18 12:59\" \"10/12/18 5:42\" \n[3997] \"10/12/18 6:00\"  \"10/12/18 11:11\" \"10/12/18 11:11\" \"10/13/18 4:24\" \n[4001] \"10/13/18 6:48\"  \"10/13/18 12:33\" \"10/13/18 12:38\" \"10/14/18 5:17\" \n[4005] \"10/14/18 5:26\"  \"10/14/18 12:31\" \"10/15/18 5:00\"  \"10/15/18 6:40\" \n[4009] \"10/15/18 12:15\" \"10/15/18 12:20\" \"10/16/18 4:43\"  \"10/16/18 12:01\"\n[4013] \"10/16/18 12:04\" \"10/17/18 6:04\"  \"10/17/18 11:50\" \"10/17/18 11:51\"\n[4017] \"10/18/18 5:41\"  \"10/18/18 6:42\"  \"10/18/18 11:38\" \"10/18/18 11:41\"\n[4021] \"10/19/18 3:19\"  \"10/19/18 6:23\"  \"10/19/18 13:05\" \"10/20/18 6:36\" \n[4025] \"10/20/18 13:31\" \"10/21/18 6:13\"  \"10/21/18 6:14\"  \"10/21/18 12:41\"\n[4029] \"10/21/18 12:53\" \"10/22/18 5:24\"  \"10/22/18 6:00\"  \"10/22/18 7:02\" \n[4033] \"10/22/18 12:34\" \"10/23/18 5:37\"  \"10/23/18 5:38\"  \"10/23/18 7:16\" \n[4037] \"10/23/18 13:35\" \"10/24/18 3:25\"  \"10/24/18 6:57\"  \"10/24/18 12:06\"\n[4041] \"10/24/18 12:12\" \"10/25/18 5:59\"  \"10/25/18 6:34\"  \"10/25/18 11:58\"\n[4045] \"10/25/18 12:03\" \"10/26/18 2:17\"  \"10/26/18 6:16\"  \"10/26/18 11:48\"\n[4049] \"10/26/18 13:27\" \"10/27/18 3:35\"  \"10/27/18 6:09\"  \"10/27/18 11:34\"\n[4053] \"10/27/18 11:38\" \"10/28/18 5:34\"  \"10/28/18 5:34\"  \"10/28/18 11:22\"\n[4057] \"10/28/18 11:24\" \"10/29/18 6:17\"  \"10/29/18 6:49\"  \"10/29/18 11:11\"\n[4061] \"10/29/18 11:13\" \"10/30/18 6:27\"  \"10/30/18 6:33\"  \"10/30/18 12:37\"\n[4065] \"10/30/18 12:45\" \"10/31/18 5:23\"  \"10/31/18 5:25\"  \"10/31/18 7:15\" \n[4069] \"10/31/18 12:31\" \"11/1/18 3:12\"   \"11/1/18 5:44\"   \"11/1/18 13:58\" \n[4073] \"11/1/18 13:59\"  \"11/2/18 4:16\"   \"11/2/18 5:26\"   \"11/2/18 12:08\" \n[4077] \"11/3/18 5:05\"   \"11/3/18 5:07\"   \"11/3/18 11:59\"  \"11/3/18 13:33\" \n[4081] \"11/4/18 6:21\"   \"11/4/18 6:30\"   \"11/4/18 11:43\"  \"11/4/18 13:59\" \n[4085] \"11/5/18 3:03\"   \"11/5/18 4:20\"   \"11/5/18 11:34\"  \"11/5/18 11:36\" \n[4089] \"11/6/18 4:11\"   \"11/6/18 5:53\"   \"11/6/18 12:57\"  \"11/6/18 13:01\" \n[4093] \"11/7/18 5:37\"   \"11/7/18 5:42\"   \"11/7/18 12:50\"  \"11/7/18 12:51\" \n[4097] \"11/8/18 5:30\"   \"11/8/18 6:47\"   \"11/8/18 11:00\"  \"11/8/18 12:33\" \n[4101] \"11/9/18 6:26\"   \"11/9/18 6:29\"   \"11/9/18 12:21\"  \"11/9/18 12:21\" \n[4105] \"11/10/18 5:01\"  \"11/10/18 5:03\"  \"11/10/18 12:11\" \"11/10/18 13:45\"\n[4109] \"11/11/18 6:33\"  \"11/11/18 6:46\"  \"11/11/18 12:02\" \"11/12/18 4:39\" \n[4113] \"11/12/18 5:25\"  \"11/12/18 7:04\"  \"11/12/18 13:30\" \"11/13/18 3:08\" \n[4117] \"11/13/18 4:59\"  \"11/13/18 11:41\" \"11/14/18 4:40\"  \"11/14/18 4:40\" \n[4121] \"11/14/18 13:09\" \"11/15/18 13:02\" \"11/16/18 11:09\" \"11/17/18 6:24\" \n[4125] \"11/17/18 13:57\" \"11/18/18 2:38\"  \"11/18/18 5:59\"  \"11/18/18 12:21\"\n[4129] \"11/19/18 6:20\"  \"11/19/18 13:52\" \"11/19/18 13:53\" \"11/20/18 5:52\" \n[4133] \"11/20/18 5:56\"  \"11/22/18 6:54\"  \"11/22/18 13:02\" \"11/23/18 6:00\" \n[4137] \"11/23/18 14:07\" \"11/24/18 6:13\"  \"11/24/18 11:16\" \"11/25/18 4:39\" \n[4141] \"11/25/18 12:48\" \"11/26/18 2:41\"  \"11/29/18 6:09\"  \"11/29/18 11:57\"\n[4145] \"11/29/18 12:00\" \"11/30/18 2:35\"  \"11/30/18 5:52\"  \"11/30/18 11:44\"\n[4149] \"11/30/18 13:25\" \"12/1/18 4:18\"   \"12/1/18 6:01\"   \"12/1/18 13:08\" \n[4153] \"12/2/18 4:12\"   \"12/2/18 5:50\"   \"12/2/18 11:20\"  \"12/3/18 3:16\"  \n[4157] \"12/3/18 6:24\"   \"12/3/18 12:48\"  \"12/3/18 12:51\"  \"12/4/18 5:27\"  \n[4161] \"12/4/18 5:29\"   \"12/4/18 12:33\"  \"12/4/18 12:33\"  \"12/5/18 5:40\"  \n[4165] \"12/5/18 6:52\"   \"12/5/18 12:27\"  \"12/5/18 12:33\"  \"12/6/18 5:00\"  \n[4169] \"12/6/18 5:26\"   \"12/6/18 13:55\"  \"12/6/18 14:02\"  \"12/7/18 12:03\" \n[4173] \"12/7/18 13:41\"  \"12/8/18 2:20\"   \"12/8/18 5:49\"   \"12/9/18 4:23\"  \n[4177] \"12/9/18 7:10\"   \"12/10/18 4:21\"  \"12/10/18 13:10\" \"12/11/18 2:59\" \n[4181] \"12/11/18 12:59\" \"12/12/18 5:03\"  \"12/13/18 3:51\"  \"12/14/18 12:26\"\n[4185] \"12/15/18 5:42\"  \"12/15/18 6:44\"  \"12/15/18 12:10\" \"12/15/18 12:12\"\n[4189] \"12/16/18 3:06\"  \"12/16/18 11:57\" \"12/17/18 3:48\"  \"12/17/18 6:34\" \n[4193] \"12/17/18 11:44\" \"12/17/18 11:46\" \"12/18/18 4:23\"  \"12/18/18 11:39\"\n[4197] \"12/19/18 3:15\"  \"12/19/18 13:04\" \"12/19/18 14:03\" \"12/20/18 5:44\" \n[4201] \"12/21/18 3:46\"  \"12/21/18 12:42\" \"12/22/18 6:32\"  \"12/22/18 12:27\"\n[4205] \"12/24/18 13:46\" \"12/25/18 6:22\"  \"12/25/18 14:07\" \"12/26/18 6:48\" \n[4209] \"12/26/18 13:28\" \"12/27/18 6:30\"  \"12/28/18 11:21\" \"12/29/18 6:57\" \n[4213] \"12/30/18 5:26\"  \"12/30/18 13:16\" \"1/1/19 5:56\"    \"1/1/19 12:17\"  \n[4217] \"2/27/19 4:00\"   \"2/27/19 11:17\"  \"2/28/19 6:57\"   \"3/1/19 6:40\"   \n[4221] \"3/1/19 12:35\"   \"3/13/19 4:36\"   \"5/6/19 13:11\"   \"5/7/19 6:52\"   \n[4225] \"5/7/19 11:20\"   \"5/7/19 14:22\"   \"5/7/19 23:50\"   \"5/8/19 6:27\"   \n[4229] \"5/8/19 12:48\"   \"5/8/19 12:49\"   \"5/8/19 16:01\"   \"5/8/19 17:46\"  \n[4233] \"5/8/19 19:22\"   \"5/8/19 19:23\"   \"5/8/19 19:23\"   \"5/8/19 19:23\"  \n[4237] \"5/9/19 3:59\"    \"5/9/19 6:52\"    \"5/9/19 7:08\"    \"5/9/19 12:31\"  \n[4241] \"5/9/19 12:37\"   \"5/9/19 12:41\"   \"5/9/19 18:14\"   \"5/9/19 19:03\"  \n[4245] \"5/9/19 19:04\"   \"5/10/19 6:40\"   \"5/10/19 6:48\"   \"5/10/19 6:51\"  \n[4249] \"5/10/19 6:54\"   \"5/10/19 7:24\"   \"5/10/19 12:23\"  \"5/10/19 12:25\" \n[4253] \"5/10/19 12:30\"  \"5/10/19 18:05\"  \"5/11/19 6:29\"   \"5/11/19 6:31\"  \n[4257] \"5/11/19 6:33\"   \"5/11/19 6:33\"   \"5/11/19 7:11\"   \"5/11/19 12:12\" \n[4261] \"5/11/19 12:14\"  \"5/11/19 13:54\"  \"5/11/19 15:57\"  \"5/12/19 6:08\"  \n[4265] \"5/12/19 6:11\"   \"5/12/19 6:45\"   \"5/12/19 12:03\"  \"5/12/19 12:03\" \n[4269] \"5/12/19 15:33\"  \"5/12/19 17:13\"  \"5/12/19 17:32\"  \"5/12/19 19:00\" \n[4273] \"5/13/19 1:09\"   \"5/13/19 6:31\"   \"5/13/19 7:00\"   \"5/13/19 7:27\"  \n[4277] \"5/13/19 11:49\"  \"5/13/19 11:50\"  \"5/13/19 13:26\"  \"5/13/19 15:14\" \n[4281] \"5/13/19 19:18\"  \"5/14/19 5:24\"   \"5/14/19 6:08\"   \"5/14/19 6:42\"  \n[4285] \"5/14/19 11:38\"  \"5/14/19 11:41\"  \"5/14/19 13:20\"  \"5/14/19 13:20\" \n[4289] \"5/14/19 15:32\"  \"5/14/19 19:01\"  \"5/15/19 5:44\"   \"5/15/19 6:19\"  \n[4293] \"5/15/19 6:24\"   \"5/15/19 6:45\"   \"5/15/19 11:25\"  \"5/15/19 13:03\" \n[4297] \"5/15/19 14:06\"  \"5/15/19 14:09\"  \"5/15/19 15:19\"  \"5/15/19 23:59\" \n[4301] \"5/16/19 4:40\"   \"5/16/19 4:44\"   \"5/16/19 6:01\"   \"5/16/19 6:02\"  \n[4305] \"5/16/19 6:28\"   \"5/16/19 7:07\"   \"5/16/19 13:02\"  \"5/16/19 13:40\" \n[4309] \"5/16/19 15:08\"  \"5/16/19 17:11\"  \"5/17/19 3:59\"   \"5/17/19 4:02\"  \n[4313] \"5/17/19 6:41\"   \"5/17/19 6:50\"   \"5/17/19 6:58\"   \"5/17/19 7:21\"  \n[4317] \"5/17/19 12:44\"  \"5/17/19 12:47\"  \"5/17/19 13:32\"  \"5/17/19 15:11\" \n[4321] \"5/18/19 5:07\"   \"5/18/19 6:24\"   \"5/18/19 8:01\"   \"5/18/19 12:32\" \n[4325] \"5/18/19 12:32\"  \"5/18/19 14:16\"  \"5/18/19 15:56\"  \"5/18/19 19:16\" \n[4329] \"5/18/19 19:17\"  \"5/18/19 19:19\"  \"5/19/19 3:03\"   \"5/19/19 3:09\"  \n[4333] \"5/19/19 4:47\"   \"5/19/19 4:50\"   \"5/19/19 4:59\"   \"5/19/19 12:18\" \n[4337] \"5/19/19 12:21\"  \"5/19/19 15:26\"  \"5/19/19 17:10\"  \"5/20/19 0:58\"  \n[4341] \"5/20/19 2:47\"   \"5/20/19 6:12\"   \"5/20/19 6:19\"   \"5/20/19 12:07\" \n[4345] \"5/20/19 13:49\"  \"5/20/19 13:49\"  \"5/20/19 15:41\"  \"5/20/19 17:28\" \n[4349] \"5/21/19 5:23\"   \"5/21/19 6:11\"   \"5/21/19 6:13\"   \"5/21/19 6:17\"  \n[4353] \"5/21/19 6:22\"   \"5/21/19 7:02\"   \"5/21/19 7:02\"   \"5/21/19 11:55\" \n[4357] \"5/21/19 11:58\"  \"5/21/19 15:09\"  \"5/22/19 6:06\"   \"5/22/19 6:39\"  \n[4361] \"5/22/19 6:41\"   \"5/22/19 7:38\"   \"5/22/19 11:48\"  \"5/22/19 13:21\" \n[4365] \"5/22/19 13:22\"  \"5/22/19 16:11\"  \"5/22/19 19:34\"  \"5/23/19 6:20\"  \n[4369] \"5/23/19 6:27\"   \"5/23/19 7:17\"   \"5/23/19 13:13\"  \"5/23/19 15:01\" \n[4373] \"5/23/19 17:07\"  \"5/23/19 19:11\"  \"5/23/19 19:13\"  \"5/23/19 19:17\" \n[4377] \"5/24/19 5:17\"   \"5/24/19 5:20\"   \"5/24/19 5:58\"   \"5/24/19 11:23\" \n[4381] \"5/24/19 13:01\"  \"5/24/19 15:15\"  \"5/24/19 17:00\"  \"5/24/19 17:12\" \n[4385] \"5/24/19 23:56\"  \"5/25/19 5:35\"   \"5/25/19 5:39\"   \"5/25/19 6:09\"  \n[4389] \"5/25/19 6:40\"   \"5/25/19 7:21\"   \"5/25/19 7:23\"   \"5/25/19 12:45\" \n[4393] \"5/25/19 12:52\"  \"5/25/19 17:26\"  \"5/26/19 3:34\"   \"5/26/19 6:14\"  \n[4397] \"5/26/19 6:15\"   \"5/26/19 6:58\"   \"5/26/19 7:00\"   \"5/26/19 12:40\" \n[4401] \"5/26/19 12:41\"  \"5/26/19 15:10\"  \"5/26/19 17:00\"  \"5/27/19 6:44\"  \n[4405] \"5/27/19 6:46\"   \"5/27/19 7:10\"   \"5/27/19 12:28\"  \"5/27/19 14:08\" \n[4409] \"5/27/19 16:01\"  \"5/27/19 17:03\"  \"5/27/19 19:29\"  \"5/27/19 19:32\" \n[4413] \"5/28/19 6:18\"   \"5/28/19 6:19\"   \"5/28/19 6:54\"   \"5/28/19 7:13\"  \n[4417] \"5/28/19 12:15\"  \"5/28/19 12:18\"  \"5/28/19 16:43\"  \"5/28/19 17:28\" \n[4421] \"5/28/19 19:07\"  \"5/28/19 19:12\"  \"5/29/19 6:19\"   \"5/29/19 6:33\"  \n[4425] \"5/29/19 6:54\"   \"5/29/19 6:56\"   \"5/29/19 7:36\"   \"5/29/19 12:02\" \n[4429] \"5/29/19 12:05\"  \"5/29/19 12:06\"  \"5/29/19 17:07\"  \"5/30/19 4:53\"  \n[4433] \"5/30/19 6:07\"   \"5/30/19 6:07\"   \"5/30/19 6:35\"   \"5/30/19 11:58\" \n[4437] \"5/30/19 16:18\"  \"5/30/19 17:17\"  \"5/30/19 17:18\"  \"5/31/19 6:02\"  \n[4441] \"5/31/19 11:43\"  \"5/31/19 13:26\"  \"5/31/19 15:33\"  \"5/31/19 17:14\" \n[4445] \"5/31/19 17:17\"  \"5/31/19 19:04\"  \"5/31/19 19:05\"  \"5/31/19 19:06\" \n[4449] \"6/1/19 4:23\"    \"6/1/19 6:31\"    \"6/1/19 7:10\"    \"6/1/19 11:29\"  \n[4453] \"6/1/19 13:13\"   \"6/1/19 15:18\"   \"6/1/19 17:08\"   \"6/1/19 19:27\"  \n[4457] \"6/1/19 23:59\"   \"6/1/19 23:59\"   \"6/2/19 5:36\"    \"6/2/19 6:18\"   \n[4461] \"6/2/19 7:12\"    \"6/2/19 13:06\"   \"6/2/19 13:07\"   \"6/2/19 15:14\"  \n[4465] \"6/2/19 17:25\"   \"6/2/19 19:03\"   \"6/2/19 19:06\"   \"6/2/19 23:50\"  \n[4469] \"6/3/19 1:56\"    \"6/3/19 3:11\"    \"6/3/19 5:26\"    \"6/3/19 5:54\"   \n[4473] \"6/3/19 6:48\"    \"6/3/19 12:48\"   \"6/3/19 12:51\"   \"6/3/19 15:57\"  \n[4477] \"6/3/19 17:05\"   \"6/3/19 17:07\"   \"6/4/19 1:17\"    \"6/4/19 3:08\"   \n[4481] \"6/4/19 6:02\"    \"6/4/19 6:32\"    \"6/4/19 6:50\"    \"6/4/19 7:09\"   \n[4485] \"6/4/19 7:11\"    \"6/4/19 7:16\"    \"6/4/19 12:41\"   \"6/4/19 15:24\"  \n[4489] \"6/5/19 4:28\"    \"6/5/19 6:14\"    \"6/5/19 6:52\"    \"6/5/19 12:27\"  \n[4493] \"6/5/19 15:32\"   \"6/5/19 17:00\"   \"6/5/19 17:16\"   \"6/5/19 19:04\"  \n[4497] \"6/5/19 19:06\"   \"6/6/19 6:27\"    \"6/6/19 6:29\"    \"6/6/19 12:13\"  \n[4501] \"6/6/19 12:13\"   \"6/6/19 13:50\"   \"6/6/19 15:10\"   \"6/6/19 17:02\"  \n[4505] \"6/6/19 19:22\"   \"6/6/19 19:23\"   \"6/6/19 19:24\"   \"6/7/19 6:15\"   \n[4509] \"6/7/19 7:09\"    \"6/7/19 11:59\"   \"6/7/19 12:01\"   \"6/7/19 15:32\"  \n[4513] \"6/7/19 17:22\"   \"6/7/19 19:01\"   \"6/7/19 19:02\"   \"6/7/19 19:04\"  \n[4517] \"6/7/19 19:05\"   \"6/8/19 4:23\"    \"6/8/19 5:06\"    \"6/8/19 5:47\"   \n[4521] \"6/8/19 7:33\"    \"6/8/19 11:50\"   \"6/8/19 15:42\"   \"6/8/19 17:03\"  \n[4525] \"6/8/19 17:03\"   \"6/8/19 19:14\"   \"6/9/19 2:49\"    \"6/9/19 4:46\"   \n[4529] \"6/9/19 6:08\"    \"6/9/19 6:28\"    \"6/9/19 14:27\"   \"6/9/19 15:28\"  \n[4533] \"6/9/19 17:07\"   \"6/9/19 17:09\"   \"6/10/19 5:08\"   \"6/10/19 6:49\"  \n[4537] \"6/10/19 13:02\"  \"6/10/19 14:50\"  \"6/10/19 15:06\"  \"6/10/19 17:15\" \n[4541] \"6/10/19 17:15\"  \"6/10/19 23:57\"  \"6/10/19 23:59\"  \"6/10/19 23:59\" \n[4545] \"6/11/19 3:31\"   \"6/11/19 3:34\"   \"6/11/19 4:48\"   \"6/11/19 5:43\"  \n[4549] \"6/11/19 5:53\"   \"6/11/19 12:58\"  \"6/11/19 16:27\"  \"6/11/19 17:44\" \n[4553] \"6/11/19 17:44\"  \"6/12/19 4:46\"   \"6/12/19 5:29\"   \"6/12/19 5:30\"  \n[4557] \"6/12/19 6:07\"   \"6/12/19 7:03\"   \"6/12/19 7:05\"   \"6/12/19 12:45\" \n[4561] \"6/12/19 16:01\"  \"6/12/19 19:04\"  \"6/13/19 1:40\"   \"6/13/19 5:05\"  \n[4565] \"6/13/19 5:49\"   \"6/13/19 6:24\"   \"6/13/19 6:44\"   \"6/13/19 12:36\" \n[4569] \"6/13/19 15:33\"  \"6/13/19 17:02\"  \"6/13/19 17:28\"  \"6/14/19 5:25\"  \n[4573] \"6/14/19 5:57\"   \"6/14/19 6:03\"   \"6/14/19 6:23\"   \"6/14/19 12:21\" \n[4577] \"6/14/19 13:35\"  \"6/14/19 15:10\"  \"6/14/19 17:08\"  \"6/14/19 17:38\" \n[4581] \"6/15/19 3:35\"   \"6/15/19 5:05\"   \"6/15/19 5:07\"   \"6/15/19 6:26\"  \n[4585] \"6/15/19 12:08\"  \"6/15/19 13:46\"  \"6/15/19 15:36\"  \"6/15/19 17:11\" \n[4589] \"6/15/19 17:14\"  \"6/15/19 19:39\"  \"6/16/19 3:29\"   \"6/16/19 6:14\"  \n[4593] \"6/16/19 8:04\"   \"6/16/19 15:45\"  \"6/16/19 17:23\"  \"6/16/19 17:26\" \n[4597] \"6/16/19 19:15\"  \"6/16/19 19:18\"  \"6/17/19 2:58\"   \"6/17/19 4:26\"  \n[4601] \"6/17/19 11:43\"  \"6/17/19 13:26\"  \"6/17/19 15:18\"  \"6/17/19 17:09\" \n[4605] \"6/17/19 17:09\"  \"6/17/19 19:00\"  \"6/17/19 19:00\"  \"6/18/19 4:10\"  \n[4609] \"6/18/19 5:43\"   \"6/18/19 6:21\"   \"6/18/19 6:46\"   \"6/18/19 11:36\" \n[4613] \"6/18/19 13:15\"  \"6/18/19 15:06\"  \"6/18/19 17:04\"  \"6/18/19 17:05\" \n[4617] \"6/19/19 3:57\"   \"6/19/19 3:57\"   \"6/19/19 5:33\"   \"6/19/19 6:25\"  \n[4621] \"6/19/19 13:04\"  \"6/19/19 13:06\"  \"6/19/19 15:53\"  \"6/19/19 17:03\" \n[4625] \"6/19/19 17:04\"  \"6/20/19 3:12\"   \"6/20/19 5:40\"   \"6/20/19 6:46\"  \n[4629] \"6/20/19 12:51\"  \"6/20/19 12:54\"  \"6/20/19 15:28\"  \"6/20/19 17:06\" \n[4633] \"6/20/19 17:09\"  \"6/20/19 19:33\"  \"6/21/19 3:01\"   \"6/21/19 6:22\"  \n[4637] \"6/21/19 6:22\"   \"6/21/19 12:41\"  \"6/21/19 15:01\"  \"6/21/19 17:29\" \n[4641] \"6/21/19 17:33\"  \"6/21/19 19:10\"  \"6/21/19 19:16\"  \"6/22/19 3:43\"  \n[4645] \"6/22/19 4:55\"   \"6/22/19 4:57\"   \"6/22/19 6:40\"   \"6/22/19 7:04\"  \n[4649] \"6/22/19 12:29\"  \"6/22/19 12:30\"  \"6/22/19 15:10\"  \"6/22/19 17:08\" \n[4653] \"6/23/19 3:34\"   \"6/23/19 3:35\"   \"6/23/19 4:48\"   \"6/23/19 5:12\"  \n[4657] \"6/23/19 12:14\"  \"6/23/19 13:51\"  \"6/23/19 15:32\"  \"6/23/19 17:21\" \n[4661] \"6/23/19 17:25\"  \"6/24/19 3:11\"   \"6/24/19 4:38\"   \"6/24/19 5:18\"  \n[4665] \"6/24/19 6:59\"   \"6/24/19 13:45\"  \"6/24/19 15:07\"  \"6/24/19 17:01\" \n[4669] \"6/24/19 17:26\"  \"6/25/19 5:28\"   \"6/25/19 6:03\"   \"6/25/19 6:40\"  \n[4673] \"6/25/19 11:50\"  \"6/25/19 11:53\"  \"6/25/19 15:32\"  \"6/25/19 17:06\" \n[4677] \"6/25/19 17:06\"  \"6/25/19 19:30\"  \"6/26/19 5:59\"   \"6/26/19 7:17\"  \n[4681] \"6/26/19 11:43\"  \"6/26/19 13:20\"  \"6/26/19 15:03\"  \"6/26/19 17:11\" \n[4685] \"6/26/19 19:08\"  \"6/26/19 19:10\"  \"6/26/19 19:11\"  \"6/27/19 3:31\"  \n[4689] \"6/27/19 5:57\"   \"6/27/19 5:57\"   \"6/27/19 7:39\"   \"6/27/19 13:09\" \n[4693] \"6/27/19 13:12\"  \"6/27/19 15:07\"  \"6/27/19 17:02\"  \"6/27/19 23:59\" \n[4697] \"6/28/19 3:06\"   \"6/28/19 4:58\"   \"6/28/19 5:33\"   \"6/28/19 6:34\"  \n[4701] \"6/28/19 7:09\"   \"6/28/19 13:00\"  \"6/28/19 14:41\"  \"6/28/19 15:58\" \n[4705] \"6/28/19 17:17\"  \"6/29/19 6:13\"   \"6/29/19 6:17\"   \"6/29/19 12:44\" \n[4709] \"6/29/19 12:45\"  \"6/29/19 15:33\"  \"6/29/19 17:01\"  \"6/29/19 19:05\" \n[4713] \"6/29/19 19:08\"  \"6/30/19 1:21\"   \"6/30/19 3:56\"   \"6/30/19 4:02\"  \n[4717] \"6/30/19 6:39\"   \"6/30/19 12:30\"  \"6/30/19 12:33\"  \"6/30/19 12:38\" \n[4721] \"6/30/19 15:26\"  \"6/30/19 17:05\"  \"6/30/19 19:26\"  \"7/1/19 6:19\"   \n[4725] \"7/1/19 7:17\"    \"7/1/19 7:53\"    \"7/1/19 12:24\"   \"7/1/19 14:04\"  \n[4729] \"7/1/19 16:16\"   \"7/1/19 19:03\"   \"7/1/19 19:04\"   \"7/1/19 19:10\"  \n[4733] \"7/2/19 0:53\"    \"7/2/19 5:10\"    \"7/2/19 5:53\"    \"7/2/19 6:49\"   \n[4737] \"7/2/19 6:53\"    \"7/2/19 7:30\"    \"7/2/19 12:10\"   \"7/2/19 12:12\"  \n[4741] \"7/2/19 15:49\"   \"7/3/19 4:33\"    \"7/3/19 4:46\"    \"7/3/19 6:16\"   \n[4745] \"7/3/19 6:36\"    \"7/3/19 13:41\"   \"7/3/19 15:23\"   \"7/3/19 15:25\"  \n[4749] \"7/3/19 17:14\"   \"7/3/19 17:17\"   \"7/4/19 4:30\"    \"7/4/19 5:59\"   \n[4753] \"7/4/19 6:50\"    \"7/4/19 11:47\"   \"7/4/19 13:26\"   \"7/4/19 15:05\"  \n[4757] \"7/4/19 17:22\"   \"7/4/19 17:23\"   \"7/4/19 19:03\"   \"7/5/19 6:27\"   \n[4761] \"7/5/19 6:31\"    \"7/5/19 13:14\"   \"7/5/19 13:16\"   \"7/5/19 15:23\"  \n[4765] \"7/5/19 17:07\"   \"7/5/19 19:22\"   \"7/5/19 19:23\"   \"7/6/19 3:28\"   \n[4769] \"7/6/19 5:41\"    \"7/6/19 6:48\"    \"7/6/19 11:23\"   \"7/6/19 13:09\"  \n[4773] \"7/6/19 13:48\"   \"7/6/19 15:11\"   \"7/6/19 17:20\"   \"7/6/19 19:05\"  \n[4777] \"7/7/19 3:46\"    \"7/7/19 5:14\"    \"7/7/19 5:51\"    \"7/7/19 6:48\"   \n[4781] \"7/7/19 7:30\"    \"7/7/19 12:54\"   \"7/7/19 15:01\"   \"7/7/19 17:05\"  \n[4785] \"7/8/19 3:42\"    \"7/8/19 5:17\"    \"7/8/19 6:27\"    \"7/8/19 6:57\"   \n[4789] \"7/8/19 7:05\"    \"7/8/19 15:08\"   \"7/8/19 16:07\"   \"7/8/19 17:35\"  \n[4793] \"7/9/19 6:45\"    \"7/9/19 6:50\"    \"7/9/19 6:52\"    \"7/9/19 12:31\"  \n[4797] \"7/9/19 15:35\"   \"7/9/19 17:13\"   \"7/9/19 17:17\"   \"7/9/19 19:00\"  \n[4801] \"7/10/19 6:31\"   \"7/10/19 7:24\"   \"7/10/19 15:56\"  \"7/10/19 17:00\" \n[4805] \"7/10/19 19:18\"  \"7/10/19 19:20\"  \"7/10/19 19:21\"  \"7/11/19 6:10\"  \n[4809] \"7/11/19 6:43\"   \"7/11/19 7:05\"   \"7/11/19 13:45\"  \"7/11/19 15:26\" \n[4813] \"7/11/19 17:22\"  \"7/11/19 19:00\"  \"7/11/19 19:02\"  \"7/12/19 2:16\"  \n[4817] \"7/12/19 3:20\"   \"7/12/19 6:46\"   \"7/12/19 6:49\"   \"7/12/19 7:29\"  \n[4821] \"7/12/19 13:34\"  \"7/12/19 15:04\"  \"7/12/19 17:27\"  \"7/13/19 3:33\"  \n[4825] \"7/13/19 6:02\"   \"7/13/19 6:21\"   \"7/13/19 6:28\"   \"7/13/19 7:42\"  \n[4829] \"7/13/19 13:20\"  \"7/13/19 15:06\"  \"7/13/19 17:11\"  \"7/14/19 3:37\"  \n[4833] \"7/14/19 4:41\"   \"7/14/19 6:02\"   \"7/14/19 6:39\"   \"7/14/19 11:34\" \n[4837] \"7/14/19 15:35\"  \"7/14/19 17:04\"  \"7/15/19 3:24\"   \"7/15/19 12:58\" \n[4841] \"7/15/19 15:28\"  \"7/15/19 17:33\"  \"7/15/19 19:14\"  \"7/15/19 19:16\" \n[4845] \"7/15/19 19:17\"  \"7/16/19 2:16\"   \"7/16/19 2:52\"   \"7/16/19 2:53\"  \n[4849] \"7/16/19 6:40\"   \"7/16/19 13:41\"  \"7/16/19 15:06\"  \"7/16/19 17:10\" \n[4853] \"7/16/19 17:14\"  \"7/17/19 3:00\"   \"7/17/19 5:13\"   \"7/17/19 6:45\"  \n[4857] \"7/17/19 6:50\"   \"7/17/19 12:38\"  \"7/17/19 16:11\"  \"7/17/19 17:51\" \n[4861] \"7/17/19 17:51\"  \"7/18/19 3:31\"   \"7/18/19 3:34\"   \"7/18/19 6:39\"  \n[4865] \"7/18/19 6:42\"   \"7/18/19 7:03\"   \"7/18/19 12:26\"  \"7/18/19 15:54\" \n[4869] \"7/18/19 17:27\"  \"7/19/19 6:00\"   \"7/19/19 6:31\"   \"7/19/19 7:16\"  \n[4873] \"7/19/19 12:13\"  \"7/19/19 15:21\"  \"7/19/19 17:08\"  \"7/19/19 19:33\" \n[4877] \"7/19/19 19:34\"  \"7/20/19 4:40\"   \"7/20/19 6:26\"   \"7/20/19 15:08\" \n[4881] \"7/20/19 17:27\"  \"7/20/19 17:29\"  \"7/20/19 19:11\"  \"7/20/19 19:18\" \n[4885] \"7/21/19 3:38\"   \"7/21/19 3:41\"   \"7/21/19 6:33\"   \"7/21/19 6:35\"  \n[4889] \"7/21/19 7:38\"   \"7/21/19 16:20\"  \"7/21/19 17:10\"  \"7/22/19 1:58\"  \n[4893] \"7/22/19 4:06\"   \"7/22/19 5:41\"   \"7/22/19 6:35\"   \"7/22/19 7:23\"  \n[4897] \"7/22/19 13:23\"  \"7/22/19 15:27\"  \"7/22/19 17:08\"  \"7/23/19 2:42\"  \n[4901] \"7/23/19 5:21\"   \"7/23/19 6:21\"   \"7/23/19 6:59\"   \"7/23/19 13:03\" \n[4905] \"7/23/19 15:33\"  \"7/23/19 17:26\"  \"7/24/19 3:44\"   \"7/24/19 4:16\"  \n[4909] \"7/24/19 5:33\"   \"7/24/19 6:35\"   \"7/24/19 12:59\"  \"7/24/19 15:06\" \n[4913] \"7/24/19 17:03\"  \"7/25/19 3:31\"   \"7/25/19 6:51\"   \"7/25/19 12:49\" \n[4917] \"7/25/19 15:38\"  \"7/25/19 17:30\"  \"7/25/19 19:06\"  \"7/25/19 19:07\" \n[4921] \"7/26/19 6:32\"   \"7/26/19 6:42\"   \"7/26/19 6:45\"   \"7/26/19 12:27\" \n[4925] \"7/26/19 12:29\"  \"7/26/19 15:08\"  \"7/26/19 17:07\"  \"7/27/19 4:20\"  \n[4929] \"7/27/19 6:28\"   \"7/27/19 6:33\"   \"7/27/19 6:35\"   \"7/27/19 12:26\" \n[4933] \"7/27/19 15:26\"  \"7/27/19 17:45\"  \"7/28/19 3:54\"   \"7/28/19 6:14\"  \n[4937] \"7/28/19 6:51\"   \"7/28/19 6:55\"   \"7/28/19 12:08\"  \"7/28/19 12:11\" \n[4941] \"7/28/19 15:55\"  \"7/28/19 17:22\"  \"7/29/19 5:25\"   \"7/29/19 6:40\"  \n[4945] \"7/29/19 7:33\"   \"7/29/19 13:43\"  \"7/29/19 15:24\"  \"7/29/19 17:22\" \n[4949] \"7/29/19 19:25\"  \"7/29/19 19:26\"  \"7/30/19 5:30\"   \"7/30/19 7:14\"  \n[4953] \"7/30/19 7:16\"   \"7/30/19 13:32\"  \"7/30/19 15:32\"  \"7/30/19 19:05\" \n[4957] \"7/30/19 19:07\"  \"7/30/19 19:08\"  \"7/31/19 4:18\"   \"7/31/19 4:23\"  \n[4961] \"7/31/19 5:49\"   \"7/31/19 6:23\"   \"7/31/19 7:33\"   \"7/31/19 13:46\" \n[4965] \"7/31/19 15:28\"  \"7/31/19 17:37\"  \"8/1/19 3:53\"    \"8/1/19 3:55\"   \n[4969] \"8/1/19 6:06\"    \"8/1/19 6:12\"    \"8/1/19 7:11\"    \"8/1/19 7:15\"   \n[4973] \"8/1/19 15:02\"   \"8/1/19 17:14\"   \"8/2/19 3:29\"    \"8/2/19 5:11\"   \n[4977] \"8/2/19 5:12\"    \"8/2/19 6:49\"    \"8/2/19 12:47\"   \"8/2/19 16:21\"  \n[4981] \"8/2/19 18:05\"   \"8/2/19 18:18\"   \"8/3/19 1:28\"    \"8/3/19 6:34\"   \n[4985] \"8/3/19 12:39\"   \"8/3/19 12:42\"   \"8/3/19 15:56\"   \"8/3/19 17:38\"  \n[4989] \"8/3/19 19:22\"   \"8/4/19 6:42\"    \"8/4/19 12:24\"   \"8/4/19 12:28\"  \n[4993] \"8/4/19 17:28\"   \"8/4/19 19:00\"   \"8/4/19 19:03\"   \"8/5/19 6:26\"   \n[4997] \"8/5/19 6:31\"    \"8/5/19 6:32\"    \"8/5/19 12:16\"   \"8/5/19 12:18\"  \n[5001] \"8/5/19 17:31\"   \"8/6/19 4:35\"    \"8/6/19 6:06\"    \"8/6/19 6:14\"   \n[5005] \"8/6/19 7:07\"    \"8/6/19 12:06\"   \"8/6/19 15:50\"   \"8/6/19 17:33\"  \n[5009] \"8/7/19 3:03\"    \"8/7/19 5:45\"    \"8/7/19 6:46\"    \"8/7/19 13:35\"  \n[5013] \"8/7/19 15:39\"   \"8/7/19 17:15\"   \"8/7/19 17:20\"   \"8/8/19 6:23\"   \n[5017] \"8/8/19 6:28\"    \"8/8/19 8:08\"    \"8/8/19 13:18\"   \"8/8/19 15:25\"  \n[5021] \"8/8/19 17:05\"   \"8/8/19 19:20\"   \"8/9/19 5:48\"    \"8/9/19 6:39\"   \n[5025] \"8/9/19 7:50\"    \"8/9/19 13:10\"   \"8/9/19 15:17\"   \"8/9/19 17:18\"  \n[5029] \"8/9/19 19:00\"   \"8/10/19 3:27\"   \"8/10/19 6:43\"   \"8/10/19 6:46\"  \n[5033] \"8/10/19 7:27\"   \"8/10/19 12:54\"  \"8/10/19 16:19\"  \"8/10/19 17:26\" \n[5037] \"8/11/19 6:21\"   \"8/11/19 6:22\"   \"8/11/19 6:23\"   \"8/11/19 7:00\"  \n[5041] \"8/11/19 7:06\"   \"8/11/19 13:24\"  \"8/11/19 16:30\"  \"8/12/19 2:39\"  \n[5045] \"8/12/19 6:44\"   \"8/12/19 7:19\"   \"8/12/19 15:27\"  \"8/12/19 17:19\" \n[5049] \"8/13/19 6:39\"   \"8/13/19 6:58\"   \"8/13/19 7:20\"   \"8/13/19 12:27\" \n[5053] \"8/13/19 16:12\"  \"8/13/19 17:41\"  \"8/13/19 19:15\"  \"8/14/19 3:28\"  \n[5057] \"8/14/19 6:10\"   \"8/14/19 6:20\"   \"8/14/19 12:11\"  \"8/14/19 13:28\" \n[5061] \"8/14/19 17:13\"  \"8/15/19 5:41\"   \"8/15/19 5:47\"   \"8/15/19 6:20\"  \n[5065] \"8/15/19 7:22\"   \"8/15/19 14:41\"  \"8/15/19 15:53\"  \"8/16/19 4:19\"  \n[5069] \"8/16/19 6:00\"   \"8/16/19 6:58\"   \"8/16/19 7:02\"   \"8/16/19 11:51\" \n[5073] \"8/16/19 15:38\"  \"8/16/19 17:09\"  \"8/17/19 6:43\"   \"8/17/19 6:45\"  \n[5077] \"8/17/19 6:47\"   \"8/17/19 13:13\"  \"8/17/19 13:18\"  \"8/17/19 15:27\" \n[5081] \"8/17/19 17:08\"  \"8/18/19 6:55\"   \"8/18/19 13:06\"  \"8/18/19 13:07\" \n[5085] \"8/18/19 16:14\"  \"8/18/19 17:28\"  \"8/18/19 19:14\"  \"8/18/19 19:15\" \n[5089] \"8/19/19 5:24\"   \"8/19/19 6:54\"   \"8/19/19 6:55\"   \"8/19/19 12:51\" \n[5093] \"8/19/19 12:55\"  \"8/19/19 16:29\"  \"8/19/19 17:17\"  \"8/20/19 6:37\"  \n[5097] \"8/20/19 6:52\"   \"8/20/19 6:53\"   \"8/20/19 12:42\"  \"8/20/19 13:45\" \n[5101] \"8/20/19 16:29\"  \"8/20/19 17:23\"  \"8/21/19 4:41\"   \"8/21/19 6:43\"  \n[5105] \"8/21/19 6:57\"   \"8/21/19 12:38\"  \"8/21/19 13:13\"  \"8/21/19 15:47\" \n[5109] \"8/21/19 17:01\"  \"8/22/19 2:49\"   \"8/22/19 4:56\"   \"8/22/19 6:35\"  \n[5113] \"8/22/19 12:18\"  \"8/22/19 16:04\"  \"8/22/19 17:03\"  \"8/23/19 6:51\"  \n[5117] \"8/23/19 7:13\"   \"8/23/19 12:07\"  \"8/23/19 15:32\"  \"8/23/19 19:13\" \n[5121] \"8/24/19 6:52\"   \"8/24/19 6:56\"   \"8/24/19 13:40\"  \"8/24/19 15:25\" \n[5125] \"8/24/19 17:19\"  \"8/25/19 6:06\"   \"8/25/19 6:33\"   \"8/25/19 11:43\" \n[5129] \"8/25/19 15:03\"  \"8/25/19 17:14\"  \"8/26/19 5:39\"   \"8/26/19 5:49\"  \n[5133] \"8/26/19 6:52\"   \"8/26/19 11:36\"  \"8/26/19 13:14\"  \"8/26/19 15:36\" \n[5137] \"8/26/19 17:02\"  \"8/27/19 4:54\"   \"8/27/19 6:34\"   \"8/27/19 7:11\"  \n[5141] \"8/27/19 7:13\"   \"8/27/19 15:09\"  \"8/27/19 17:50\"  \"8/27/19 19:28\" \n[5145] \"8/28/19 5:20\"   \"8/28/19 6:48\"   \"8/28/19 7:50\"   \"8/28/19 14:33\" \n[5149] \"8/28/19 15:26\"  \"8/29/19 6:30\"   \"8/29/19 6:32\"   \"8/29/19 12:40\" \n[5153] \"8/29/19 12:41\"  \"8/29/19 15:40\"  \"8/29/19 17:06\"  \"8/30/19 5:29\"  \n[5157] \"8/30/19 5:34\"   \"8/30/19 6:26\"   \"8/30/19 7:16\"   \"8/30/19 12:30\" \n[5161] \"8/30/19 12:32\"  \"8/30/19 15:08\"  \"8/31/19 4:33\"   \"8/31/19 6:54\"  \n[5165] \"8/31/19 7:28\"   \"8/31/19 7:29\"   \"8/31/19 12:13\"  \"8/31/19 16:01\" \n[5169] \"8/31/19 19:45\"  \"9/1/19 5:54\"    \"9/1/19 6:36\"    \"9/1/19 7:08\"   \n[5173] \"9/1/19 12:01\"   \"9/1/19 12:06\"   \"9/1/19 15:27\"   \"9/1/19 19:24\"  \n[5177] \"9/2/19 6:12\"    \"9/2/19 6:49\"    \"9/2/19 7:05\"    \"9/2/19 11:52\"  \n[5181] \"9/2/19 13:35\"   \"9/2/19 15:40\"   \"9/2/19 19:06\"   \"9/3/19 6:28\"   \n[5185] \"9/3/19 6:51\"    \"9/3/19 6:52\"    \"9/3/19 7:30\"    \"9/3/19 13:08\"  \n[5189] \"9/3/19 15:05\"   \"9/4/19 5:30\"    \"9/4/19 6:04\"    \"9/4/19 6:31\"   \n[5193] \"9/4/19 7:11\"    \"9/4/19 13:05\"   \"9/4/19 13:15\"   \"9/4/19 16:43\"  \n[5197] \"9/5/19 4:15\"    \"9/5/19 6:03\"    \"9/5/19 6:09\"    \"9/5/19 12:53\"  \n[5201] \"9/5/19 13:03\"   \"9/5/19 15:40\"   \"9/5/19 18:23\"   \"9/6/19 6:32\"   \n[5205] \"9/6/19 12:45\"   \"9/6/19 13:14\"   \"9/6/19 16:01\"   \"9/6/19 17:00\"  \n[5209] \"9/6/19 19:20\"   \"9/6/19 19:20\"   \"9/7/19 5:27\"    \"9/7/19 7:46\"   \n[5213] \"9/7/19 12:34\"   \"9/7/19 16:18\"   \"9/7/19 19:01\"   \"9/8/19 6:20\"   \n[5217] \"9/8/19 6:48\"    \"9/8/19 12:25\"   \"9/8/19 12:26\"   \"9/9/19 5:27\"   \n[5221] \"9/9/19 6:28\"    \"9/9/19 12:09\"   \"9/9/19 12:13\"   \"9/9/19 15:52\"  \n[5225] \"9/10/19 6:40\"   \"9/10/19 6:46\"   \"9/10/19 11:59\"  \"9/10/19 13:39\" \n[5229] \"9/10/19 16:38\"  \"9/11/19 6:01\"   \"9/11/19 11:47\"  \"9/11/19 15:57\" \n[5233] \"9/11/19 19:17\"  \"9/12/19 6:09\"   \"9/12/19 6:40\"   \"9/12/19 11:37\" \n[5237] \"9/12/19 13:14\"  \"9/12/19 15:06\"  \"9/13/19 5:42\"   \"9/13/19 6:43\"  \n[5241] \"9/13/19 7:25\"   \"9/13/19 13:01\"  \"9/13/19 15:17\"  \"9/14/19 3:48\"  \n[5245] \"9/14/19 5:54\"   \"9/14/19 12:48\"  \"9/14/19 12:54\"  \"9/14/19 15:03\" \n[5249] \"9/15/19 5:38\"   \"9/15/19 6:44\"   \"9/15/19 12:44\"  \"9/15/19 13:29\" \n[5253] \"9/15/19 15:13\"  \"9/16/19 6:54\"   \"9/16/19 6:55\"   \"9/16/19 12:30\" \n[5257] \"9/16/19 15:51\"  \"9/17/19 5:57\"   \"9/17/19 6:59\"   \"9/17/19 12:22\" \n[5261] \"9/17/19 15:58\"  \"9/18/19 6:15\"   \"9/18/19 6:34\"   \"9/18/19 12:08\" \n[5265] \"9/18/19 15:46\"  \"9/19/19 5:20\"   \"9/19/19 6:21\"   \"9/19/19 7:01\"  \n[5269] \"9/19/19 15:38\"  \"9/20/19 6:42\"   \"9/20/19 13:22\"  \"9/20/19 15:46\" \n[5273] \"9/20/19 17:01\"  \"9/20/19 19:31\"  \"9/21/19 6:14\"   \"9/21/19 6:49\"  \n[5277] \"9/21/19 7:19\"   \"9/21/19 13:45\"  \"9/21/19 15:25\"  \"9/22/19 5:57\"  \n[5281] \"9/22/19 6:51\"   \"9/22/19 13:02\"  \"9/22/19 13:03\"  \"9/22/19 16:30\" \n[5285] \"9/23/19 6:34\"   \"9/23/19 6:58\"   \"9/23/19 12:45\"  \"9/23/19 12:49\" \n[5289] \"9/24/19 6:14\"   \"9/24/19 7:30\"   \"9/24/19 12:37\"  \"9/24/19 19:07\" \n[5293] \"9/25/19 6:34\"   \"9/25/19 7:10\"   \"9/25/19 7:30\"   \"9/25/19 15:06\" \n[5297] \"9/25/19 19:26\"  \"9/26/19 6:18\"   \"9/26/19 7:51\"   \"9/26/19 13:49\" \n[5301] \"9/26/19 19:10\"  \"9/27/19 6:54\"   \"9/27/19 12:04\"  \"9/28/19 6:29\"  \n[5305] \"9/28/19 14:03\"  \"9/28/19 15:05\"  \"9/29/19 15:25\"  \"9/29/19 19:01\" \n[5309] \"9/30/19 16:35\"  \"10/1/19 6:47\"   \"10/1/19 12:59\"  \"10/1/19 16:40\" \n[5313] \"10/2/19 6:55\"   \"10/2/19 15:49\"  \"10/3/19 6:40\"   \"10/3/19 12:35\" \n[5317] \"10/3/19 15:19\"  \"10/4/19 6:54\"   \"10/4/19 7:25\"   \"10/4/19 15:20\" \n[5321] \"10/5/19 6:21\"   \"10/5/19 16:27\"  \"10/6/19 5:20\"   \"10/6/19 15:40\" \n[5325] \"10/6/19 18:59\"  \"10/7/19 6:48\"   \"10/7/19 7:23\"   \"10/7/19 15:19\" \n[5329] \"10/8/19 5:29\"   \"10/8/19 7:01\"   \"10/8/19 16:57\"  \"10/9/19 6:42\"  \n[5333] \"10/9/19 7:07\"   \"10/9/19 15:35\"  \"10/10/19 15:30\" \"10/10/19 17:38\"\n[5337] \"10/10/19 19:16\" \"10/11/19 6:38\"  \"10/11/19 12:41\" \"10/11/19 15:04\"\n[5341] \"10/12/19 6:37\"  \"10/12/19 12:28\" \"10/12/19 16:10\" \"10/13/19 6:26\" \n[5345] \"10/13/19 16:08\" \"10/13/19 17:28\" \"10/14/19 15:09\" \"10/14/19 17:09\"\n[5349] \"10/14/19 19:34\" \"10/15/19 7:17\"  \"10/15/19 15:25\" \"10/15/19 19:17\"\n[5353] \"10/16/19 5:55\"  \"10/16/19 15:47\" \"10/16/19 17:07\" \"10/17/19 5:37\" \n\n\nLet’s get one value.\n\nobis_seamap_green$date_time[1]\n\n[1] \"5/16/16 6:54\"",
    "crumbs": [
      "Tutorials",
      "R and RStudio Basics",
      "Loading data in R"
    ]
  },
  {
    "objectID": "r-tutorials/03-r-read-in-data.html#get-the-size",
    "href": "r-tutorials/03-r-read-in-data.html#get-the-size",
    "title": "R - Read in data",
    "section": "Get the size",
    "text": "Get the size\n\ndim(obis_seamap_green)\n\n[1] 5356   15\n\n\nLet’s make it a bit smaller. Select the first 500 rows because it takes a long time to plot otherwise.\n\nobis_seamap_green &lt;- obis_seamap_green[1:500,]",
    "crumbs": [
      "Tutorials",
      "R and RStudio Basics",
      "Loading data in R"
    ]
  },
  {
    "objectID": "r-tutorials/03-r-read-in-data.html#look-at-the-top-of-the-file",
    "href": "r-tutorials/03-r-read-in-data.html#look-at-the-top-of-the-file",
    "title": "R - Read in data",
    "section": "Look at the top of the file",
    "text": "Look at the top of the file\n\nhead(obis_seamap_green)\n\n# A tibble: 6 × 15\n       oid    id dataset_id    tsn scientific common provider latitude longitude\n     &lt;dbl&gt; &lt;dbl&gt;      &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;  &lt;chr&gt;       &lt;dbl&gt;     &lt;dbl&gt;\n1   2.18e8   219       2069 173833 Chelonia … Green… Nicolas…     24.6      53.0\n2   2.18e8     1       2069 173833 Chelonia … Green… Nicolas…     24.6      53.0\n3   2.18e8   136       2069 173833 Chelonia … Green… Nicolas…     24.6      53.1\n4   2.18e8   258       2069 173833 Chelonia … Green… Nicolas…     24.6      53.0\n5   2.18e8   292       2069 173833 Chelonia … Green… Nicolas…     24.6      53.0\n6   2.18e8   259       2069 173833 Chelonia … Green… Nicolas…     24.6      53.0\n# ℹ 6 more variables: lprecision &lt;dbl&gt;, obs_date &lt;chr&gt;, date_time &lt;chr&gt;,\n#   count &lt;dbl&gt;, ds_type &lt;chr&gt;, platform &lt;chr&gt;",
    "crumbs": [
      "Tutorials",
      "R and RStudio Basics",
      "Loading data in R"
    ]
  },
  {
    "objectID": "r-tutorials/03-r-read-in-data.html#lets-load-new-data-from-robis",
    "href": "r-tutorials/03-r-read-in-data.html#lets-load-new-data-from-robis",
    "title": "R - Read in data",
    "section": "Let’s load new data from robis",
    "text": "Let’s load new data from robis\nSpecify a bounding box.\n\nlibrary(dplyr)\nlibrary(sf)\nextent_polygon &lt;- sf::st_bbox(c(xmin = 41.875, xmax = 65.125, \n                            ymax = -0.125, ymin = 32.125), \n                          #Assign reference system\n                          crs = sf::st_crs(4326)) %&gt;% st_as_sfc()\npol_geometry &lt;- sf::st_as_text(extent_polygon[[1]])\npol_geometry\n\n[1] \"POLYGON ((41.875 32.125, 65.125 32.125, 65.125 -0.125, 41.875 -0.125, 41.875 32.125))\"\n\n\nLet’s get some sharks and rays data. Sharks and rays in Bay of Bengal ref1 ref2 ref3\n\nfil &lt;- here::here(\"r-tutorials\", \"data\", \"sharks.csv\")\nif(!exists(fil)){\nsharks &lt;- c(\"Scoliodon laticaudus\", \"Sphrna lewin\", \"Rhizoprionodon acutus\", \"Chiloscyllium indicum\", \"Galeocerdo cuvier\", \"Carcharhinus melanopterus\", \"Carcharhinus falciformis\", \"Rhizoprionodon acutus\", \"Carcharhinus leucas\", \"Carcharhinus  sorrah\")\nobs &lt;- robis::occurrence(sharks, geometry = pol_geometry)\nreadr::write_csv(obs, file=here::here(\"r-tutorials\", \"data\", \"sharks.csv\"))\n}else{\n  obs &lt;- readr::read_csv(fil)\n}\n\n\nRetrieved 2259 records of approximately 2259 (100%)\n\n\nNow we can explore what kind of shark data we have for these species found in the Bay of Bengal study.\n\nlibrary(mapview)\nrequire(dplyr)\nlibrary(sf)\nobs_sf &lt;- obs[1:100,] %&gt;% as_tibble() %&gt;% \n  st_as_sf(coords = c(\"decimalLongitude\", \"decimalLatitude\"), crs = 4326)\nmapview(obs_sf, col.regions = \"gray\") + mapview(extent_polygon)",
    "crumbs": [
      "Tutorials",
      "R and RStudio Basics",
      "Loading data in R"
    ]
  },
  {
    "objectID": "r-tutorials/03-r-read-in-data.html#ask-chatgpt",
    "href": "r-tutorials/03-r-read-in-data.html#ask-chatgpt",
    "title": "R - Read in data",
    "section": "Ask ChatGPT",
    "text": "Ask ChatGPT\nWhen you are starting to code, you will not know how to do things. Obviously I can tell you but that won’t help you when I am not here. Instead I’ll have you ask a bot because the bot will be here when I am not. Also this is how people code now that AI is here to be our personal coding assistant.\nOpen https://chat.openai.com/ and login. It’s free.\nPaste this into the Chat box.\nI have a data.frame named obis_seamap_green with columns scientific, latitude, longitude, and platform. How do I make a histogram of the platform types? Only show the code. I want to use tidyverse.\nChatGPT returns this (or something similar)\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nobis_seamap_green %&gt;%\n  ggplot(aes(x = platform)) +\n  geom_bar() +\n  labs(title = \"Histogram of Platform Types\", x = \"Platform Type\", y = \"Frequency\")\n\n\n\n\nHmm that look weird? How many platform types are there? Let’s ask ChatGPT.\nHow do I find out the values for platform in my file\nChatGPT will return something like\n\nunique_platforms &lt;- unique(obis_seamap_green$platform)\n\nand we have to type\n\nunique_platforms\n\n[1] \"tag\"\n\n\nto see the values. Oh, there is only one value.",
    "crumbs": [
      "Tutorials",
      "R and RStudio Basics",
      "Loading data in R"
    ]
  },
  {
    "objectID": "r-tutorials/03-r-read-in-data.html#make-a-map",
    "href": "r-tutorials/03-r-read-in-data.html#make-a-map",
    "title": "R - Read in data",
    "section": "Make a map",
    "text": "Make a map\nLet’s get ChatGPT to help us map our points.\n“how can I make a map with my latitude longitude values on it” ChatGPT will most likely show you ggplot2 code like so\n\n# Install and load necessary packages (if not already installed)\n# install.packages(\"ggplot2\")\n# install.packages(\"maps\")\nlibrary(ggplot2)\nlibrary(maps)\n\n\nAttaching package: 'maps'\n\n\nThe following object is masked from 'package:purrr':\n\n    map\n\n# Assuming your data frame is named \"obis_seamap_green\"\n# Replace \"latitude\" and \"longitude\" with your actual column names\n\n# Create a basic map\nworld_map &lt;- map_data(\"world\")\n\n# Plot latitude and longitude points on the map\nggplot(obis_seamap_green, aes(x = longitude, y = latitude)) +\n  geom_polygon(data = world_map, aes(x = long, y = lat, group = group), fill = \"white\", color = \"black\") +\n  geom_point() +\n  labs(title = \"Map of Latitude and Longitude Points\")\n\n\n\n\n\nTell ChatGPT: “The map is the whole world and my points are in one small region. How can I fix that?”\n\n\n# Assuming your data frame is named \"obis_seamap_green\"\n# Replace \"latitude\" and \"longitude\" with your actual column names\n\n# Define the latitude and longitude limits for your region of interest\n# Adjust the values accordingly to zoom in on your specific region\nlat_limit &lt;- c(min(obis_seamap_green$latitude), max(obis_seamap_green$latitude))\nlon_limit &lt;- c(min(obis_seamap_green$longitude), max(obis_seamap_green$longitude))\n\n# Create a map focused on your region of interest\nggplot(obis_seamap_green, aes(x = longitude, y = latitude)) +\n  geom_polygon(data = world_map, aes(x = long, y = lat, group = group), fill = \"white\", color = \"black\") +\n  geom_point() +\n  labs(title = \"Map of Latitude and Longitude Points\") +\n  xlim(lon_limit) + ylim(lat_limit)\n\n\n\n\n\nThat looks terrible! At this point, you could describe the problem to ChatGPT and it will help you. But better yet, is to know about the mapview package.\nAsk ChatGPT: “Can I use mapview?”\n\n\n# Install and load the mapview package (if not already installed)\n# install.packages(\"mapview\")\nlibrary(mapview)\n\n# Assuming your data frame is named \"obis_seamap_green\"\n# Replace \"latitude\" and \"longitude\" with your actual column names\n\n# Create a map view\nmapview(obis_seamap_green, zcol = NULL, map.types = \"OpenStreetMap\")\n\n\nIt doesn’t work! Tell ChatGPT: “I get an error that says Either convert obis_seamap_green to a spatial object or provide xcol and ycol. oops! Arguments xcol and/or ycol are missing!” It apologizes and gives you a fix.\n\n\n# Install and load the required packages (if not already installed)\n# install.packages(\"mapview\")\n# install.packages(\"sf\")\nlibrary(mapview)\nlibrary(sf)\n\n# Assuming your data frame is named \"obis_seamap_green\"\n# Replace \"latitude\" and \"longitude\" with your actual column names\n\n# Convert your data frame to a spatial object\nobis_seamap_sf &lt;- st_as_sf(obis_seamap_green, coords = c(\"longitude\", \"latitude\"))\n\n# Create a map view\nmapview(obis_seamap_sf)\n\n\n\n\n\n\nTell ChatGPT: “Better! But the land is not shown”\n\n\n# Assuming your data frame is named \"obis_seamap_green\"\n# Replace \"latitude\" and \"longitude\" with your actual column names\nlibrary(mapview)\nlibrary(sf)\n\n# Convert your data frame to a spatial object\nobis_seamap_sf &lt;- st_as_sf(obis_seamap_green, coords = c(\"longitude\", \"latitude\"))\n\n# Create a map view with Esri.WorldImagery basemap\nmapview(obis_seamap_sf, map.types = \"Esri.WorldImagery\")\n\n\n\n\n\n\nTell ChatGPT: “I get another warning writing: substituting ENGCRS[”Undefined Cartesian SRS with unknown unit”] for missing CRS]”. It gives us a fix.\n\n\n# Assuming your data frame is named \"obis_seamap_green\"\n# Replace \"latitude\" and \"longitude\" with your actual column names\nlibrary(mapview)\nlibrary(sf)\n\n# Convert your data frame to a spatial object and set the CRS explicitly\nobis_seamap_sf &lt;- st_as_sf(obis_seamap_green, coords = c(\"longitude\", \"latitude\"), crs = 4326)\n\n# Create a map view with Esri.WorldImagery basemap\nmapview(obis_seamap_sf, map.types = \"Esri.WorldImagery\")",
    "crumbs": [
      "Tutorials",
      "R and RStudio Basics",
      "Loading data in R"
    ]
  },
  {
    "objectID": "r-tutorials/03-r-read-in-data.html#clean-up-the-date",
    "href": "r-tutorials/03-r-read-in-data.html#clean-up-the-date",
    "title": "R - Read in data",
    "section": "Clean up the date",
    "text": "Clean up the date\nIt is common that the date columns are in different formats and we need to change to a format that R will understand.\n\nobis_seamap_green$date_time[1]\n\n[1] \"5/16/16 6:54\"\n\n\nI don’t have about the time, I just want date in a standard format. Let’s ask ChatGPT.\n\n“My date_time column looks like this”5/16/16 6:54”. I need a date column in a standard format.”\n\n\n# Assuming your data frame is named \"obis_seamap_green\"\n# Replace \"date_time\" with your actual column name\nlibrary(mapview)\nlibrary(sf)\n\n# Convert the date_time column to a standard date format\nobis_seamap_green$date &lt;- as.Date(obis_seamap_green$date_time, format = \"%m/%d/%y %H:%M\")\n\n# Remove the original date_time column if needed\nobis_seamap_green &lt;- obis_seamap_green[, !names(obis_seamap_green) %in% \"date_time\"]\n\nLet’s check if it worked.\n\nobis_seamap_green$date[1]\n\nWarning: Unknown or uninitialised column: `date`.\n\n\nNULL\n\nclass(obis_seamap_green$date)\n\nWarning: Unknown or uninitialised column: `date`.\n\n\n[1] \"NULL\"",
    "crumbs": [
      "Tutorials",
      "R and RStudio Basics",
      "Loading data in R"
    ]
  },
  {
    "objectID": "r-tutorials/03-r-read-in-data.html#save-the-data-frame",
    "href": "r-tutorials/03-r-read-in-data.html#save-the-data-frame",
    "title": "R - Read in data",
    "section": "Save the data frame",
    "text": "Save the data frame\nMany times we do a bit of work on our data by cleaning up an such. Once we are done, we should save our data so we can load it in easily later.\nFirst we make a R script that shows how we created our data. We do this to be nice to our “future” selves. Create a R script and save in the data folder as create-obis-seamap-green.R.\nlibrary(readr)\nobis_seamap_green &lt;- read_csv(\"data/obis_seamap_green.csv\", \n    skip = 2)\nobis_seamap_green$date &lt;- as.Date(obis_seamap_green$date_time, format = \"%m/%d/%y %H:%M\")\n\n# Remove the original date_time column if needed\nobis_seamap_green &lt;- obis_seamap_green[, !names(obis_seamap_green) %in% \"date_time\"]\nNow that we have recorded how we made our file, we save our data to a file.\n\nsave(obis_seamap_green, file=\"data/obis_seamap_green.RData\")\n\nNext time we want to load these data we can run\n\nload(\"data/obis_seamap_green.RData\")",
    "crumbs": [
      "Tutorials",
      "R and RStudio Basics",
      "Loading data in R"
    ]
  },
  {
    "objectID": "r-tutorials/03-r-read-in-data.html#summary",
    "href": "r-tutorials/03-r-read-in-data.html#summary",
    "title": "R - Read in data",
    "section": "Summary",
    "text": "Summary\n\nYou learned how to import csv and Excel files using the Import widget. Tip: use NA for missing values!\nYou learned how to interact with ChatGPT as your personal coding helper. Tip: We are using tidyverse so let it know that you want that. Use “only show code” if you don’t want all the narrative.\nIn order to work with ChatGPT, you need to be able to tell it what your data look like. Core commands you need to know:\n\nClass class(object)\nColumn names colnames(object)\nSize dim(object)\n\nUse these commands to learn what to tell ChatGPT.\nYou learned how to ask ChatGPT to help you plot and how to tell it what the problems are so it can help you. You learned about the mapview R package. ChatGPT knows about it, but rarely will it suggest it on it’s own.\nYou made a (boring) plot with the ggplot2 plotting package. This is part of the tidyverse. It is 2023, we use ggplot2 for plotting not base R (plot()). We will cover much more plotting with ggplot2 in another tutorial.",
    "crumbs": [
      "Tutorials",
      "R and RStudio Basics",
      "Loading data in R"
    ]
  },
  {
    "objectID": "r-tutorials/03-r-read-in-data.html#your-turn",
    "href": "r-tutorials/03-r-read-in-data.html#your-turn",
    "title": "R - Read in data",
    "section": "Your turn!",
    "text": "Your turn!\nImport some of the other files in the data directory.",
    "crumbs": [
      "Tutorials",
      "R and RStudio Basics",
      "Loading data in R"
    ]
  },
  {
    "objectID": "r-tutorials/04-r-ggplot3.html",
    "href": "r-tutorials/04-r-ggplot3.html",
    "title": "R - ggplot2",
    "section": "",
    "text": "This tutorial will give an introduction to plotting with the ggplot2 package. Getting a basic familiarity with ggplot2 will really save you a lot of time.\nThis is oriented to those with little or no experience using ggplot2 or those who have tried it and gotten frustrated. The goal is to get you started. Once you have the basics, there are a so many ggplot tutorials online – though personally the basics are all I need.\nlibrary(ggplot2)\nlibrary(gridExtra)"
  },
  {
    "objectID": "r-tutorials/04-r-ggplot3.html#basic-x-versus-y-plot",
    "href": "r-tutorials/04-r-ggplot3.html#basic-x-versus-y-plot",
    "title": "R - ggplot2",
    "section": "Basic x versus y plot",
    "text": "Basic x versus y plot\n\nWorkflow\nLet’s say you wanted to make a simple xy plot with plot(). Here’s your workflow.\n\nDecide on x or use the default (1 to the number of data points).\nDecide on y.\nPlot with plot(x,y)\n\n\nval &lt;- mtcars$mpg\nx &lt;- mtcars$hp\nplot(x, val, type=\"p\")\n\n\n\n\nHere’s your ggplot() workflow.\n\nDecide on x or use the default (1 to the number of data points).\nDecide on y.\nMake a data frame with x and y.\nCreate the plot object with a call to ggplot() to tell it the data frame and then what the x and y to use: aes(x=..., y=...).\nAdd points or lines to the plot with geom_line() or geom_point().\n\n\ndf &lt;- data.frame(x=mtcars$hp, val=mtcars$mpg)\np1 &lt;- ggplot(df, aes(x=x, y=val)) + # set up\n        geom_point() # plot a line\np1 # plot it\n\n\n\n\nTypically you’d just write the call like so\nggplot(df, aes(x=x, y=val)) + geom_point()\np1 is a ggplot object.\n\nclass(p1)\n\n[1] \"gg\"     \"ggplot\"\n\n\nWe can add elements to a ggplot object\n\np1 + geom_line()\n\n\n\n\n\n\n\n\n\n\nAn x y plot\n\n\n\nggplot(df, aes(x=x, y=val)) +\n geom_point() +\n geom_line()"
  },
  {
    "objectID": "r-tutorials/04-r-ggplot3.html#modifying-your-plot",
    "href": "r-tutorials/04-r-ggplot3.html#modifying-your-plot",
    "title": "R - ggplot2",
    "section": "Modifying your plot",
    "text": "Modifying your plot\nThere are two different workflows that you need to decide on.\n\nDynamic colors etc: Let ggplot() pick your colors, points, line widths etc.\nFixed colors etc: Manually choose your colors, points, line widths etc, aka use a fixed value.\n\nGravitating to option 1 will make your life with ggplot() easier, but let’s start with option 2.\n\n\n\n\n\n\nImportant\n\n\n\nFixed lines, points attributes go outside of aes() in a geom_...() call. Dynamic attributes go inside of aes().\n\n\nLook at ?geom_point to see the attributes that you can pass in.\n\nggplot(df, aes(x=x, y=val)) + geom_point(col=\"blue\")\n\n\n\n\nThe length of the fixed attribute must be 1 or the length of the data.\n\nggplot(df, aes(x=x, y=val)) + \n  geom_point(shape=21, col=\"blue\", fill=mtcars$carb, size=6, alpha=.5, stroke=2)\n\n\n\n\nWays to set attributes that won’t work as you think:\nPutting color outside of aes() in ggplot() does nothing. ggplot() sets up the data to use, but information outside aes() doesn’t flow to the plotting functions like geom_point().\n\nggplot(df, aes(x=x, y=val), col=\"blue\") + geom_point()\n\n\n\n\nPutting color inside of aes() in ggplot() has a non-intuitive effect. Plot attributes, like color, in aes() are converted to factors and the colors (etc) will be chosen dynamically. The name “blue” is not a color is the a factor and ggplot() gives the first factor the color red in this case. Information in aes() will flow to the rest of the plot unless you tell the geom_point() otherwise).\n\nggplot(df, aes(x=x, y=val, color=\"blue\")) + geom_point()\n\n\n\n\nHow do we tell geom_point() not to use the color value in aes() in ggplot()? Tell it to use a fixed value by putting col=\"blue\" outside of an aes() call in geom_point().\n\nggplot(df, aes(x=x, y=val)) + geom_point(col=\"blue\")\n\n\n\n\nWhat happens if we put the color in aes() in geom_point()?\n\nggplot(df, aes(x=x, y=val)) + geom_point(aes(color=\"blue\"))\n\n\n\n\nLet’s make a plot with big red points and a thick blue line.\n\np1 &lt;- ggplot(df, aes(x=x, y=val)) + \n  geom_line(col=\"blue\", size=2) +\n  geom_point(col=\"red\", size=3)\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\np1\n\n\n\n\nLet’s add a column plot to that. I pass in alpha to add some transparency to the columns so they don’t wipe out the line.\n\np1 + geom_col(alpha=0.5, position=\"dodge\")\n\n\n\n\n\nLabels and limits\nWith ggplot, use xlab() ylab() and xlim() ylim(). Ignore the NA warning. I’ll show how to stop that later.\n\nggplot(df, aes(x=x, y=val)) +\n  geom_point() +\n  xlab(\"time\") + ylab(\"value\") +\n  ggtitle(\"ggplot plot\") +\n  xlim(c(100,200)) + ylim(c(0,40))\n\nWarning: Removed 16 rows containing missing values (`geom_point()`)."
  },
  {
    "objectID": "r-tutorials/04-r-ggplot3.html#changing-the-whole-look",
    "href": "r-tutorials/04-r-ggplot3.html#changing-the-whole-look",
    "title": "R - ggplot2",
    "section": "Changing the whole look",
    "text": "Changing the whole look\nggplot uses themes to set the look of your plot and you can change the whole look by setting a different theme. You can also just tweak one element of the plot’s existing theme. Note because we fixed the line and point colors, we override some theme elements (eg, line colors). See ?theme_bw to see all the themes. See ?theme to learn how to change one element of your plot design.\n\np1 + theme_classic()"
  },
  {
    "objectID": "r-tutorials/04-r-ggplot3.html#adding-lines-or-points",
    "href": "r-tutorials/04-r-ggplot3.html#adding-lines-or-points",
    "title": "R - ggplot2",
    "section": "Adding lines or points",
    "text": "Adding lines or points\nLet’s say you want to plot 2 lines.\n\nWorkflow\nIn ggplot() you have two possible workflows.\nOption 1. This workflow is kind of bad but often easier for beginners.\n\nMake a data frame for each line: df and df2.\nCall geom_line() with data=df or data=df and a aes()\n\n\ndf &lt;- data.frame(x=mtcars$hp, val=mtcars$mpg)\ndf2 &lt;- data.frame(x=df$x, val2=df$val+10)\nggplot() +\n  geom_point(data=df, aes(x=x, y=val)) +\n  geom_line(data=df2, aes(x=x, y=val2))\n\n\n\n\nOne issue with aes() and data is that it is inherited rightward. This inheriting feature is usually very handy, but when working with multiple data frames, it is often clearer if you keep the data and aes() with the points and lines.\nHere is another example of plotting data from two different data frames:\n\nggplot() +\n  geom_line(data=iris, aes(x = Sepal.Length, y=Petal.Length)) +\n  geom_bar(data=mtcars, aes(x=cyl), alpha=0.3) +\n  ylab(\"y\") + xlab(\"x\")\n\n\n\n\n\n\nAdding a legend\nggplot makes it rather hard to modify your legend if you create a plot this way. Creating a manual legend, as opposed to dynamically as ggplot is supposed to work, can be quite hard. First thing to know is that the color, linestyle, and/or shape must be in aes() to appear in the legend. If it’s not there you can’t control it in the legend.\nNote: What I am about to show is really hacky and not the way ggplot is intended to be used, but it comes up so often for new ggplot users that I want you to see a solution so you don’t give up on ggplot because of legends. Jump ahead to the correct ggplot workflow with long-form data frames to see how to avoid this.\nThis works. Ignore the warning about unknown aesthetics.\n\nggplot() +\n  geom_point(data=df, aes(x=x, y=val, col=\"df\", linetype=\"df\")) +\n  geom_line(data=df2, aes(x=x, y=val2, col=\"df2\", linetype=\"df2\")) +\n  scale_color_manual(\"name\", values=c(\"blue\", \"black\")) +\n  scale_linetype_manual(\"name\",values=c(0,1))\n\nWarning in geom_point(data = df, aes(x = x, y = val, col = \"df\", linetype =\n\"df\")): Ignoring unknown aesthetics: linetype"
  },
  {
    "objectID": "r-tutorials/04-r-ggplot3.html#correct-ggplot-workflow",
    "href": "r-tutorials/04-r-ggplot3.html#correct-ggplot-workflow",
    "title": "R - ggplot2",
    "section": "Correct ggplot workflow",
    "text": "Correct ggplot workflow\nThis is how ggplot() is intended to be used: one long data frame.\n\nMake data frames with df and df2 data and a “name” column.\nSet up the plot with ggplot()\nMake points or line different using the “name” column\n\n\ndf1 &lt;- data.frame(x=x, val=val, name=\"df1\")\ndf2 &lt;- data.frame(x=x, val=val+10, name=\"df2\")\ndf3 &lt;- rbind(df1, df2)\nggplot(df3, aes(x=x, y=val, col=name, shape=name)) + \n  geom_line() +\n  geom_point()\n\n\n\n\n\n\nIf the color, shape, linetype is not in an aes() it won’t appear in the legend.\nIf the color, shape, or linetype is in an aes() it will appear in the legend.\naes() info in ggplot() flows to the other elements. Put the aes() info in the individual geom_...() calls if you don’t want that. Use inherit.aes=FALSE to stop this inheriting behavior.\nWant to mix points and lines? You need to use scale_..._manual() to manually turn-off points or lines for some of the data.\nYou can always force colors, shapes, linetypes by passing in color, shape, size etc outside of aes() but it won’t appear in the legend. Only colors, shapes, etc, that appear in aes() will appear in a legend.\n\nExample, points are all black since the aes(col=name) only appears in the geom_line() call.\n\nggplot(df3, aes(x=x, y=val)) + \n  geom_line(aes(col=name)) +\n  geom_point(aes(shape=name))\n\n\n\n\n\n\nModifying the legend\nYou can control all aspects of the legend. Read up on it here.\nManually changing data points and other lines will require scale_...() and gets hacky.\n\nggplot(df3, aes(x=x, y=val)) + \n  geom_line(aes(col=name, linetype=name)) +\n  geom_point(aes(shape=name)) +\n  scale_color_manual(\"name\", values=c(\"blue\", \"black\")) +\n  scale_shape_manual(\"name\",values=c(NA,1)) +\n  scale_linetype_manual(\"name\",values=c(1,0))\n\nWarning: Removed 32 rows containing missing values (`geom_point()`).\n\n\n\n\n\n\n\nNA warnings\nAck all those NA warnings!\n\ndf4 &lt;- data.frame(x=1:10, y=c(1,NA,NA,4:10), name=rep(c(\"a\",\"b\"),5))\nggplot(df4, aes(x=x, y=y, col=name)) + \n  geom_point()\n\nWarning: Removed 2 rows containing missing values (`geom_point()`).\n\n\n\n\n\nGet rid of them using na.rm=TRUE.\n\nggplot(df4, aes(x=x, y=y, col=name)) + \n  geom_point(na.rm=TRUE)"
  },
  {
    "objectID": "r-tutorials/04-r-ggplot3.html#making-a-boxplot",
    "href": "r-tutorials/04-r-ggplot3.html#making-a-boxplot",
    "title": "R - ggplot2",
    "section": "Making a boxplot",
    "text": "Making a boxplot\nLet’s look at the mpg data frame.\n\nmpg\n\n# A tibble: 234 × 11\n   manufacturer model      displ  year   cyl trans drv     cty   hwy fl    class\n   &lt;chr&gt;        &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt;\n 1 audi         a4           1.8  1999     4 auto… f        18    29 p     comp…\n 2 audi         a4           1.8  1999     4 manu… f        21    29 p     comp…\n 3 audi         a4           2    2008     4 manu… f        20    31 p     comp…\n 4 audi         a4           2    2008     4 auto… f        21    30 p     comp…\n 5 audi         a4           2.8  1999     6 auto… f        16    26 p     comp…\n 6 audi         a4           2.8  1999     6 manu… f        18    26 p     comp…\n 7 audi         a4           3.1  2008     6 auto… f        18    27 p     comp…\n 8 audi         a4 quattro   1.8  1999     4 manu… 4        18    26 p     comp…\n 9 audi         a4 quattro   1.8  1999     4 auto… 4        16    25 p     comp…\n10 audi         a4 quattro   2    2008     4 manu… 4        20    28 p     comp…\n# ℹ 224 more rows\n\n\n\nggplot(mpg, aes(x=drv, y=hwy)) +\n  geom_boxplot()\n\n\n\n\n\nggplot(mpg, aes(x=drv, y=hwy)) +\n  geom_boxplot() +\n  facet_wrap(~class) +\n  xlab(\"\") + ylab(\"mpg\") +\n  ggtitle(\"highway miles per gallon by class\")"
  },
  {
    "objectID": "r-tutorials/04-r-ggplot3.html#arranging-plots-into-grids",
    "href": "r-tutorials/04-r-ggplot3.html#arranging-plots-into-grids",
    "title": "R - ggplot2",
    "section": "Arranging plots into grids",
    "text": "Arranging plots into grids"
  },
  {
    "objectID": "r-tutorials/04-r-ggplot3.html#dynamically",
    "href": "r-tutorials/04-r-ggplot3.html#dynamically",
    "title": "R - ggplot2",
    "section": "Dynamically",
    "text": "Dynamically\nggplot() will also dynamically break your data into plots for you.\n\ndf1 &lt;- data.frame(t=1:10, val=10:1, name=\"df1\")\ndf2 &lt;- data.frame(t=1:20, val=1:20+1, name=\"df2\")\ndf &lt;- rbind(df1, df2)\np1 &lt;- ggplot(df, aes(x=t, y=val, col=name)) + geom_line()\np1\n\n\n\n\n\np1 + facet_wrap(~name)\n\n\n\n\n\nhead(mpg)\n\n# A tibble: 6 × 11\n  manufacturer model displ  year   cyl trans      drv     cty   hwy fl    class \n  &lt;chr&gt;        &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt;      &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; \n1 audi         a4      1.8  1999     4 auto(l5)   f        18    29 p     compa…\n2 audi         a4      1.8  1999     4 manual(m5) f        21    29 p     compa…\n3 audi         a4      2    2008     4 manual(m6) f        20    31 p     compa…\n4 audi         a4      2    2008     4 auto(av)   f        21    30 p     compa…\n5 audi         a4      2.8  1999     6 auto(l5)   f        16    26 p     compa…\n6 audi         a4      2.8  1999     6 manual(m5) f        18    26 p     compa…\n\n\nLet’s plot city mpg versus number of cylinders.\n\npc &lt;- ggplot(mpg, aes(x=cyl, y=cty)) + geom_point()\npc\n\n\n\n\nNow we can divide this up by different factors in our the mpg data frame.\n\npc + facet_wrap(~class)\n\n\n\n\nWe can add some things to our plot and free the scales.\n\npc + facet_wrap(~class, scales=\"free\") + geom_smooth(method=\"lm\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\nManually\nNow let’s do this in with ggplot() in a similar workflow. The difference with ggplot() is that we save the plots and then arrange them into a grid with gridExtra::grid.arrange() (from the gridExtra package).\n\nlibrary(gridExtra)\ndf &lt;- data.frame(t = 1:10, val = rnorm(10))\np1 &lt;- ggplot(df, aes(x=t, y=val)) + geom_line() + geom_point()\np2 &lt;- ggplot(df, aes(x=t, y=val+100)) + geom_line()\np3 &lt;- ggplot(df, aes(x=t, y=100*val)) + geom_point()\ndf2 &lt;- data.frame(t = 1:4, val = 1:4, se=.1*(1:4))\np4 &lt;- ggplot(df2, aes(x=t, y=val)) + geom_col()\ngridExtra::grid.arrange(p1, p2, p3, p4)\n\n\n\n\nInstead of gridExtra and grid.arrange(), check out the patchwork package. It does similar jobs as grid.arrange() but is better.\nLet’s do two plots in one column but add new info to plot 1.\n\ngridExtra::grid.arrange(p1+geom_smooth(method=\"lm\"), \n                        p4+geom_errorbar(aes(ymin=val-se, ymax=val+se), width=0.3), ncol=1)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nWhen you don’t know how many plots you’ll be arranging. You can assemble ggplot objects into a list and pass that to grid.arrange().\n\ndf &lt;- data.frame(t = 1:10, val = rnorm(10))\nplist &lt;- list()\nn &lt;- 3\nfor(i in 1:n) plist[[i]] &lt;- ggplot(df, aes(x=t, y=val)) + geom_line() + geom_point()\ndf2 &lt;- data.frame(t = 1:4, val = 1:4, se=.1*(1:4))\nplist[[n+1]] &lt;- ggplot(df2, aes(x=t, y=val)) + geom_col()\ngridExtra::grid.arrange(grobs=plist)\n\n\n\n\nThe patchwork package gives you the same functionality.\npatchwork::wrap_plots(plist)\nWe can combine these facet wrapped figures into a grid because they are ggplot objects. Making this plot in base R would take you forever and another 2 forevers if you wanted to change it around or if the number of classes in your data changed.\n\npf &lt;- pc + facet_wrap(~class)\ngrid.arrange( p1, pf, ncol=2)"
  },
  {
    "objectID": "r-tutorials/04-r-ggplot3.html#notes",
    "href": "r-tutorials/04-r-ggplot3.html#notes",
    "title": "R - ggplot2",
    "section": "Notes",
    "text": "Notes\n\nfacet_wrap() often balks if you use different data frames in your plot construction, i.e. you doing something kind of hacky.\n\nneeds a column in your data frame factors (or characters it can coerce into factors). Might work with multiple data frames in your plot as long as each data frame has the same “name” column.\nwants all the data frame to be the same length. This is only when you use different data frames. Fine if you have all data in one data frame."
  },
  {
    "objectID": "r-tutorials/Lab-setup.html",
    "href": "r-tutorials/Lab-setup.html",
    "title": "Labs - Set up",
    "section": "",
    "text": "For initial set-up, you will need to create an folder sdm-labs and then create an RStudio project in that folder. Then every day, when you work on the tutorials, you will copy the tutorial into that folder. That copy is what you can alter, save and render (create html).",
    "crumbs": [
      "Tutorials",
      "Marine SDMs",
      "Lab Setup"
    ]
  },
  {
    "objectID": "r-tutorials/Lab-setup.html#set-up-a-new-project",
    "href": "r-tutorials/Lab-setup.html#set-up-a-new-project",
    "title": "Labs - Set up",
    "section": "Set up a new project",
    "text": "Set up a new project\nYou will need to create a directory called sdm-labs in your home directory. You home directory is called home/jovyan. Navigate there and then click the New Folder icon and create sdm-labs.\n\n\n\nCreate a sdm-labs folder if you have not already\n\n\nCreate an RStudio project at the sdm-labs directory. Go to\nFile &gt; New Project &gt; Existing directory\nThen navigate to the sdm-labs directory and create the project.\n\nCreate a data folder\nWithin the sdm-labs directory, also create a folder called data. You will store data there.",
    "crumbs": [
      "Tutorials",
      "Marine SDMs",
      "Lab Setup"
    ]
  },
  {
    "objectID": "r-tutorials/Lab-setup.html#returning-to-your-project",
    "href": "r-tutorials/Lab-setup.html#returning-to-your-project",
    "title": "Labs - Set up",
    "section": "Returning to your project",
    "text": "Returning to your project\nWhen you reopen RStudio, it should go to the last project that you had open. But if not you can select the sdm-labs project by click the “blue cube” dropdown in the top right and selecting the sdm-labs project.",
    "crumbs": [
      "Tutorials",
      "Marine SDMs",
      "Lab Setup"
    ]
  },
  {
    "objectID": "r-tutorials/Lab-setup.html#copy-the-lab-into-the-sdm-labs-directory",
    "href": "r-tutorials/Lab-setup.html#copy-the-lab-into-the-sdm-labs-directory",
    "title": "Labs - Set up",
    "section": "Copy the lab into the sdm-labs directory",
    "text": "Copy the lab into the sdm-labs directory\nThe R tutorials are in the folder r-tutorials in the shared/2023-Hackbook directory. You can open the lab we are working on in that directory and run the code, but you will not be able to save your work. There are 2 ways you can save your work.\n\nOpen file and save when ready\nYou can work in the file in the r-tutorials directory as long as you don’t “Render” the document. If you try to do that, it will complain that you do not have write permission.\nSo you work in the file and run code with the “Run” icon and then when you want to save, use File &gt; Save As. Then navigate to your \\home\\jovyan\\sdm-labs directory and save there.\n\n\nSave the file first\n\nNavigate to the r-notebooks folder\nSelect the file you will be using by clicking the check box\n\n * Click the Cog icon and select “Copy to” * Copy to to your \\home\\jovyan\\sdm-labs directory.\n\n\n\nCopy to\n\n\nNow navigate to the sdm-labs folder and open the file.",
    "crumbs": [
      "Tutorials",
      "Marine SDMs",
      "Lab Setup"
    ]
  },
  {
    "objectID": "r-tutorials/SDM-lab-spocc.html",
    "href": "r-tutorials/SDM-lab-spocc.html",
    "title": "SDM lab 1 - Downloading species data with spocc",
    "section": "",
    "text": "First tell R where you are in the file structure. Before running this code, the folder specified by dir_data if it does not exist already.\nhere::i_am(\"r-tutorials/SDM-lab-spocc.qmd\")\n\nhere() starts at /Users/eli.holmes/Documents/GitHub/NOAAHackDays\n\ndir_data &lt;- here::here(\"r-tutorials\", \"data\")\nSet the file paths.\n# data as csv file\nobs_csv &lt;- file.path(dir_data, \"obs.csv\")\n# data as geojson\nobs_geo &lt;- file.path(dir_data, \"obs.geojson\")\nLoad libraries.\nrequire(sf)\nrequire(spocc)\nrequire(knitr)\nrequire(dplyr)\nrequire(readr)\nrequire(mapview)",
    "crumbs": [
      "Tutorials",
      "Marine SDMs",
      "spocc"
    ]
  },
  {
    "objectID": "r-tutorials/SDM-lab-spocc.html#spocc-r-package",
    "href": "r-tutorials/SDM-lab-spocc.html#spocc-r-package",
    "title": "SDM lab 1 - Downloading species data with spocc",
    "section": "spocc R package",
    "text": "spocc R package\nThe spocc R package which allows us to query species occurrence data from a variety of sources:\nspocc: A programmatic interface to many species occurrence data sources, including GBIF, iNaturalist, Berkeley Ecoinformatics Engine, eBird, iDigBio, VertNet, OBIS, and ALA. Includes functionality for retrieving species occurrence data, and combining that data.\n\nBrown-throated sloth\nThis is a classic example for Species Distribution modeling. It is a sloth found in South America.\n\n\nif (!file.exists(obs_csv)){\n# get species occurrence data from GBIF with coordinates\nres &lt;- spocc::occ(\n    query = 'Bradypus variegatus', \n    from = 'gbif', has_coords = TRUE)\n  \n# extract data frame from result\ndf &lt;- res$gbif$data[[1]] \n# write data to a file\nreadr::write_csv(df, obs_csv)\n}else{\n  df &lt;- readr::read_csv(obs_csv)\n}\n\nRows: 500 Columns: 83\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (55): name, issues, prov, scientificName, datasetKey, publishingOrgKey,...\ndbl  (21): longitude, latitude, key, crawlId, taxonKey, kingdomKey, phylumKe...\nlgl   (1): isInCluster\ndttm  (5): lastCrawled, lastParsed, dateIdentified, modified, lastInterpreted\ndate  (1): eventDate\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\n\n\n\n\nLook at the objects\n\n\n\nType names(res) and then names(res$gbif). Then type df to see what is in the dataframe.\n\n\nMake a table of the first few lines of the dataframe.\n\nknitr::kable(df %&gt;% head)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nname\nlongitude\nlatitude\nissues\nprov\nkey\nscientificName\ndatasetKey\npublishingOrgKey\ninstallationKey\nhostingOrganizationKey\npublishingCountry\nprotocol\nlastCrawled\nlastParsed\ncrawlId\nbasisOfRecord\noccurrenceStatus\ntaxonKey\nkingdomKey\nphylumKey\nclassKey\norderKey\nfamilyKey\ngenusKey\nspeciesKey\nacceptedTaxonKey\nacceptedScientificName\nkingdom\nphylum\norder\nfamily\ngenus\nspecies\ngenericName\nspecificEpithet\ntaxonRank\ntaxonomicStatus\niucnRedListCategory\ndateIdentified\ncoordinateUncertaintyInMeters\ncontinent\nstateProvince\nyear\nmonth\nday\neventDate\nmodified\nlastInterpreted\nreferences\nlicense\nisInCluster\ndatasetName\nrecordedBy\nidentifiedBy\ngeodeticDatum\nclass\ncountryCode\ncountry\nrightsHolder\nidentifier\nhttp://unknown.org/nick\nverbatimEventDate\ncollectionCode\ngbifID\nverbatimLocality\noccurrenceID\ntaxonID\ncatalogNumber\ninstitutionCode\neventTime\nhttp://unknown.org/captive\nidentificationID\noccurrenceRemarks\nsex\nlifeStage\nindividualCount\nvernacularName\nlocality\nhigherClassification\ninformationWithheld\ninfraspecificEpithet\nidentificationRemarks\n\n\n\n\nBradypus variegatus Schinz, 1825\n-79.54650\n8.987403\ncdc,cdround\ngbif\n4011797263\nBradypus variegatus Schinz, 1825\n50c9509d-22c7-4a22-a47d-8c48425ef4a7\n28eb1a3f-1c15-4a95-931a-4af90ecb574d\n997448a8-f762-11e1-a439-00145eb45e9a\n28eb1a3f-1c15-4a95-931a-4af90ecb574d\nUS\nDWC_ARCHIVE\n2023-08-17 14:21:14\n2023-08-18 13:22:12\n390\nHUMAN_OBSERVATION\nPRESENT\n2436353\n1\n44\n359\n1494\n9418\n2436350\n2436353\n2436353\nBradypus variegatus Schinz, 1825\nAnimalia\nChordata\nPilosa\nBradypodidae\nBradypus\nBradypus variegatus\nBradypus\nvariegatus\nSPECIES\nACCEPTED\nLC\n2023-01-02 21:10:21\n61\nNORTH_AMERICA\nPanamá\n2023\n1\n2\n2023-01-02\n2023-03-09 20:50:19\n2023-08-18 13:22:12\nhttps://www.inaturalist.org/observations/145694886\nhttp://creativecommons.org/licenses/by/4.0/legalcode\nFALSE\niNaturalist research-grade observations\nKai Squires\nKai Squires\nWGS84\nMammalia\nPA\nPanama\nKai Squires\n145694886\nsquiresk\n2023/01/02 9:49 AM\nObservations\n4011797263\nPanama City, Panama\nhttps://www.inaturalist.org/observations/145694886\n47067\n145694886\niNaturalist\n09:49:00-05:00\nwild\n324468453\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nBradypus variegatus Schinz, 1825\n-84.68933\n10.519427\ncdc,cdround\ngbif\n4091593493\nBradypus variegatus Schinz, 1825\n50c9509d-22c7-4a22-a47d-8c48425ef4a7\n28eb1a3f-1c15-4a95-931a-4af90ecb574d\n997448a8-f762-11e1-a439-00145eb45e9a\n28eb1a3f-1c15-4a95-931a-4af90ecb574d\nUS\nDWC_ARCHIVE\n2023-08-17 14:21:14\n2023-08-18 13:41:19\n390\nHUMAN_OBSERVATION\nPRESENT\n2436353\n1\n44\n359\n1494\n9418\n2436350\n2436353\n2436353\nBradypus variegatus Schinz, 1825\nAnimalia\nChordata\nPilosa\nBradypodidae\nBradypus\nBradypus variegatus\nBradypus\nvariegatus\nSPECIES\nACCEPTED\nLC\n2023-01-31 13:22:05\n4\nNORTH_AMERICA\nAlajuela\n2023\n1\n2\n2023-01-02\n2023-04-13 01:01:22\n2023-08-18 13:41:19\nhttps://www.inaturalist.org/observations/145696593\nhttp://creativecommons.org/licenses/by-nc/4.0/legalcode\nFALSE\niNaturalist research-grade observations\nJessica Rae\nWren\nWGS84\nMammalia\nCR\nCosta Rica\nJessica Rae\n145696593\naltoidsboi\n2023/01/02 3:54 PM\nObservations\n4091593493\nAlajuela Province, San Carlos, Costa Rica\nhttps://www.inaturalist.org/observations/145696593\n47067\n145696593\niNaturalist\n15:54:00-06:00\nwild\n330896608\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nBradypus variegatus Schinz, 1825\n-82.15604\n9.316584\ncdc,cdround\ngbif\n4028722131\nBradypus variegatus Schinz, 1825\n50c9509d-22c7-4a22-a47d-8c48425ef4a7\n28eb1a3f-1c15-4a95-931a-4af90ecb574d\n997448a8-f762-11e1-a439-00145eb45e9a\n28eb1a3f-1c15-4a95-931a-4af90ecb574d\nUS\nDWC_ARCHIVE\n2023-08-17 14:21:14\n2023-08-18 13:41:57\n390\nHUMAN_OBSERVATION\nPRESENT\n2436353\n1\n44\n359\n1494\n9418\n2436350\n2436353\n2436353\nBradypus variegatus Schinz, 1825\nAnimalia\nChordata\nPilosa\nBradypodidae\nBradypus\nBradypus variegatus\nBradypus\nvariegatus\nSPECIES\nACCEPTED\nLC\n2023-01-03 18:24:03\n75\nNORTH_AMERICA\nBocas del Toro\n2023\n1\n3\n2023-01-03\n2023-02-02 01:16:10\n2023-08-18 13:41:57\nhttps://www.inaturalist.org/observations/145767431\nhttp://creativecommons.org/licenses/by-nc/4.0/legalcode\nFALSE\niNaturalist research-grade observations\nlucybrown19\nlucybrown19\nWGS84\nMammalia\nPA\nPanama\nlucybrown19\n145767431\nlucybrown19\nTue Jan 03 2023 11:36:24 GMT -0500 (EST)\nObservations\n4028722131\nBocas del Toro, PA-BC, PA\nhttps://www.inaturalist.org/observations/145767431\n47067\n145767431\niNaturalist\n11:36:24-05:00\nwild\n324686542\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nBradypus variegatus Schinz, 1825\n-84.06505\n10.449010\ncdc,cdround\ngbif\n4011771409\nBradypus variegatus Schinz, 1825\n50c9509d-22c7-4a22-a47d-8c48425ef4a7\n28eb1a3f-1c15-4a95-931a-4af90ecb574d\n997448a8-f762-11e1-a439-00145eb45e9a\n28eb1a3f-1c15-4a95-931a-4af90ecb574d\nUS\nDWC_ARCHIVE\n2023-08-17 14:21:14\n2023-08-18 13:41:03\n390\nHUMAN_OBSERVATION\nPRESENT\n2436353\n1\n44\n359\n1494\n9418\n2436350\n2436353\n2436353\nBradypus variegatus Schinz, 1825\nAnimalia\nChordata\nPilosa\nBradypodidae\nBradypus\nBradypus variegatus\nBradypus\nvariegatus\nSPECIES\nACCEPTED\nLC\n2023-01-03 19:44:25\n5\nNORTH_AMERICA\nHeredia\n2023\n1\n3\n2023-01-03\n2023-01-16 15:12:42\n2023-08-18 13:41:03\nhttps://www.inaturalist.org/observations/145772860\nhttp://creativecommons.org/licenses/by/4.0/legalcode\nFALSE\niNaturalist research-grade observations\nChris Harrison\nChris Harrison\nWGS84\nMammalia\nCR\nCosta Rica\nChris Harrison\n145772860\nsandboa\n2023-01-03 11:57:43\nObservations\n4011771409\nSarapiquí, CR-HE, CR\nhttps://www.inaturalist.org/observations/145772860\n47067\n145772860\niNaturalist\n11:57:43-06:00\nwild\n324705982\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nBradypus variegatus Schinz, 1825\n-79.64967\n9.061424\ncdc,cdround\ngbif\n4014910775\nBradypus variegatus Schinz, 1825\n50c9509d-22c7-4a22-a47d-8c48425ef4a7\n28eb1a3f-1c15-4a95-931a-4af90ecb574d\n997448a8-f762-11e1-a439-00145eb45e9a\n28eb1a3f-1c15-4a95-931a-4af90ecb574d\nUS\nDWC_ARCHIVE\n2023-08-17 14:21:14\n2023-08-18 13:21:15\n390\nHUMAN_OBSERVATION\nPRESENT\n2436353\n1\n44\n359\n1494\n9418\n2436350\n2436353\n2436353\nBradypus variegatus Schinz, 1825\nAnimalia\nChordata\nPilosa\nBradypodidae\nBradypus\nBradypus variegatus\nBradypus\nvariegatus\nSPECIES\nACCEPTED\nLC\n2023-01-04 02:04:32\n365\nNORTH_AMERICA\nPanamá\n2023\n1\n3\n2023-01-03\n2023-03-09 20:50:14\n2023-08-18 13:21:15\nhttps://www.inaturalist.org/observations/145797456\nhttp://creativecommons.org/licenses/by-nc/4.0/legalcode\nFALSE\niNaturalist research-grade observations\nMatt Cohen and Elizabeth Hargrave\nMatt Cohen and Elizabeth Hargrave\nWGS84\nMammalia\nPA\nPanama\nMatt Cohen and Elizabeth Hargrave\n145797456\nmattandeliz\n2023/01/03 9:02 AM\nObservations\n4014910775\nPanama\nhttps://www.inaturalist.org/observations/145797456\n47067\n145797456\niNaturalist\n09:02:00-05:00\nwild\n324778216\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nBradypus variegatus Schinz, 1825\n-80.14370\n8.602112\ncdc,cdround\ngbif\n4015257923\nBradypus variegatus Schinz, 1825\n50c9509d-22c7-4a22-a47d-8c48425ef4a7\n28eb1a3f-1c15-4a95-931a-4af90ecb574d\n997448a8-f762-11e1-a439-00145eb45e9a\n28eb1a3f-1c15-4a95-931a-4af90ecb574d\nUS\nDWC_ARCHIVE\n2023-08-17 14:21:14\n2023-08-18 13:40:25\n390\nHUMAN_OBSERVATION\nPRESENT\n2436353\n1\n44\n359\n1494\n9418\n2436350\n2436353\n2436353\nBradypus variegatus Schinz, 1825\nAnimalia\nChordata\nPilosa\nBradypodidae\nBradypus\nBradypus variegatus\nBradypus\nvariegatus\nSPECIES\nACCEPTED\nLC\n2023-01-05 00:14:33\n171\nNORTH_AMERICA\nCoclé\n2023\n1\n4\n2023-01-04\n2023-03-09 20:50:13\n2023-08-18 13:40:25\nhttps://www.inaturalist.org/observations/145862390\nhttp://creativecommons.org/licenses/by-nc/4.0/legalcode\nFALSE\niNaturalist research-grade observations\nMatt Cohen and Elizabeth Hargrave\nMatt Cohen and Elizabeth Hargrave\nWGS84\nMammalia\nPA\nPanama\nMatt Cohen and Elizabeth Hargrave\n145862390\nmattandeliz\n2023/01/04 2:49 PM\nObservations\n4015257923\nAnton Valley, Panama\nhttps://www.inaturalist.org/observations/145862390\n47067\n145862390\niNaturalist\n14:49:00-05:00\nwild\n324981107\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n\n\n\nConvert to points of observation from lon/lat columns in data frame. This part is using the sf R package to make our dataframe into a geospatial object. This way the mapping functions will be able to plot the points easily.\n\nobs &lt;- df %&gt;% \n  sf::st_as_sf(\n    coords = c(\"longitude\", \"latitude\"), # what columns have x and y\n    crs = sf::st_crs(4326)) %&gt;% # what projection to use\n    dplyr::select(prov, key) # save space (optional)\n# save the file\nsf::write_sf(obs, obs_geo, delete_dsn=TRUE)\nnrow(obs) # number of rows\n\n[1] 500\n\n\nThe format of a bounding box is [min-longitude, min-latitude, max-longitude, max-latitude]\nPlot using mapview.\n\nmapview::mapview(obs, col.regions = \"gray\")",
    "crumbs": [
      "Tutorials",
      "Marine SDMs",
      "spocc"
    ]
  },
  {
    "objectID": "r-tutorials/Turtle_maxnet.html#set-up",
    "href": "r-tutorials/Turtle_maxnet.html#set-up",
    "title": "Maxent SDM",
    "section": "Set-up",
    "text": "Set-up\nYou will need to install maxnet if you have not already. This checks if it is installed.\n\nif(!require(maxnet))\n   devtools::install_github(\"BigelowLab/maxnet\")\n\nLoad the necessary libraries. If you see errors that a library is not installed, you will need to install with install.packages(\"packagename\").\n\nsuppressPackageStartupMessages({\nlibrary(maxnet)\nlibrary(dplyr)\nlibrary(maxnet)\nlibrary(sf)\nlibrary(stars)\nlibrary(geodata)\nlibrary(dismo)\nlibrary(lubridate)\nlibrary(sdmpredictors)\nlibrary(ggplot2)\nlibrary(cmocean)\nlibrary(janitor)\nlibrary(DT)\nlibrary(here)\nlibrary(rnaturalearth)\nlibrary(rnaturalearthdata)\nlibrary(raster)\nlibrary(ggspatial)\nlibrary(tidyverse)\nlibrary(robis)\n})\n\nTell R that the root should be where this RMarkdown file resides. All our data files will be stored here.\n\nsdm_dir &lt;- \"r-tutorials\"\ndata_dir &lt;- \"data\"\nhere::i_am(paste0(sdm_dir,\"/Turtle_maxnet.qmd\"))\n\nhere() starts at /Users/eli.holmes/Documents/GitHub/NOAAHackDays",
    "crumbs": [
      "Tutorials",
      "Marine SDMs",
      "MaxEnt SDM"
    ]
  },
  {
    "objectID": "r-tutorials/Turtle_maxnet.html#set-up-the-spatial-region",
    "href": "r-tutorials/Turtle_maxnet.html#set-up-the-spatial-region",
    "title": "Maxent SDM",
    "section": "Set up the spatial region",
    "text": "Set up the spatial region\n\nCreate a bounding box\nWe create a bounding box using minimum and maximum coordinate pairs and assign a standared WGS 84 coordinate reference system. This creates a sfs_POLYGON.\n\nextent_polygon &lt;- sf::st_bbox(c(xmin = 41.875, xmax = 65.125, \n                            ymax = -0.125, ymin = 32.125), \n                          crs = sf::st_crs(4326)) %&gt;% \n  sf::st_as_sfc()\n\nSave the bounding box for future use.\n\nfil &lt;- here::here(sdm_dir, data_dir, \"BoundingBox.shp\")\nsf::write_sf(extent_polygon, fil)\n\nGet the polygon in text format.\n\npol_geometry &lt;- sf::st_as_text(extent_polygon[[1]])\npol_geometry\n\n[1] \"POLYGON ((41.875 32.125, 65.125 32.125, 65.125 -0.125, 41.875 -0.125, 41.875 32.125))\"\n\n\n\n\nCreate a world map with our region\nThis allows us to check our polygon of interest is located in the correct region.\n\n#Getting base map\nworld &lt;- rnaturalearth::ne_countries(scale = \"medium\", returnclass = \"sf\")\n\n#Plotting map\nworld_box &lt;- ggplot() + \n  #Adding base map\n  geom_sf(data = world) +\n  #Adding bounding box\n  geom_sf(data = extent_polygon, color = \"red\", fill = NA)+\n  #Setting theme of plots to not include a grey background\n  theme_bw()\n\nworld_box\n\n\n\n\nSave the plot.\n\nfil &lt;- here::here(sdm_dir,  data_dir, \"world_box.rda\")\nsave(world_box, file=fil)\n\n\n\nCreate a region map\nCreate a base map of our region and save it.\n\nbase_region_map &lt;- ggplot()+\n  #Adding base layer (world map)\n  geom_sf(data = world, fill = \"antiquewhite\")+\n  #Constraining map to original bounding box\n  lims(x = c(st_bbox(extent_polygon)$xmin, sf::st_bbox(extent_polygon)$xmax),\n       y = c(sf::st_bbox(extent_polygon)$ymin, sf::st_bbox(extent_polygon)$ymax))\nbase_region_map\n\n\n\n\nSave it\n\nfil &lt;- here::here(sdm_dir,  data_dir, \"base_region_map.rda\")\nsave(base_region_map, file=fil)\n\nWe will add some more features to our map: colors, scale and compass.\n\nregion_map &lt;- base_region_map +\n  #Add scale bar on the top right of the plot\n  annotation_scale(location = \"tr\", width_hint = 0.5)+\n  #Add north arrow on the top left of plot\n  annotation_north_arrow(location = \"tl\", which_north = \"true\",\n                         #Include small buffer from plot edge\n                         pad_x = unit(0.01, \"in\"), pad_y = unit(0.05, \"in\"),\n                         #Set style of north arrow\n                         style = north_arrow_fancy_orienteering) +\n  #Changing color, type and size of grid lines\n  theme(panel.grid.major = element_line(color = gray(.5), linetype = \"dashed\", size = 0.5), \n  #Change background of map\n  panel.background = element_rect(fill = \"aliceblue\")) +\n  labs(x = \"longitude\", y = \"latitude\")\n\nWarning: The `size` argument of `element_line()` is deprecated as of ggplot2 3.4.0.\nℹ Please use the `linewidth` argument instead.\n\nregion_map\n\nScale on map varies by more than 10%, scale bar may be inaccurate\n\n\n\n\n\nSave.\n\nfil &lt;- here::here(sdm_dir,  data_dir, \"region_map.rda\")\nsave(region_map, file=fil)\n\nWe add some labels for the countries.\n\n#Extracting labels for countries in base map\nworld_points &lt;- world %&gt;% \n  sf::st_make_valid(world) %&gt;%\n  #Getting centroids for all polygons in the world base map\n  sf::st_centroid(geometry) %&gt;% \n  #Getting coordinates for each centroid\n  sf::st_coordinates() %&gt;% \n  #Adding centroids to original base map\n  dplyr::bind_cols(world)\n\n#Do not use spherical geometry\nsf::sf_use_s2(FALSE)\n\n#Adding labels to map\nregion_map_label &lt;- region_map +\n  geom_text(data = world_points, \n            #Point to coordinates and column with country names\n            aes(x = X, y = Y, label = name),\n            #Changing color and size of labels\n            color = \"darkblue\", size = 3, \n            #Avoid label overlap\n            check_overlap = TRUE)\n# Save\nfil &lt;- here::here(sdm_dir,  data_dir,  \"region_map_label.rda\")\nsave(region_map_label, file=fil)\n\n#Checking final map\nregion_map_label\n\n\n\n\n\n\nLoading in the saved files\nLater when we need the extent polygon, we use\n\n#Loading bounding box for the area of interest\nfil &lt;- here::here(sdm_dir, data_dir, \"BoundingBox.shp\")\nextent_polygon &lt;- sf::read_sf(fil)\n\nWe often will need a sf bbox (bounding box object). To create that use\n\nbbox &lt;- sf::st_bbox(extent_polygon)\n\nWe load the maps as\n\nfil &lt;- here::here(sdm_dir, data_dir, \"region_map_label.rda\")\nload(fil)",
    "crumbs": [
      "Tutorials",
      "Marine SDMs",
      "MaxEnt SDM"
    ]
  },
  {
    "objectID": "r-tutorials/Turtle_maxnet.html#get-occurrence-data-from-robis",
    "href": "r-tutorials/Turtle_maxnet.html#get-occurrence-data-from-robis",
    "title": "Maxent SDM",
    "section": "Get occurrence data from robis",
    "text": "Get occurrence data from robis\npol_geometry is defined above.\n\npol_geometry\n\n[1] \"POLYGON ((41.875 32.125, 65.125 32.125, 65.125 -0.125, 41.875 -0.125, 41.875 32.125))\"\n\n\nGet the data. We use eval=redo so that we do not redownload data if we do not need to.\n\nredo &lt;- FALSE\n\nSet the species we want\n\nspp &lt;- c(\"Chelonia mydas\", \"Caretta caretta\", \"Eretmochelys imbricata\", \"Lepidochelys olivacea\", \"Natator depressus\", \"Dermochelys coriacea\")\n\nDownload the data.\n\nobs &lt;- robis::occurrence(spp, startdate = as.Date(\"2000-01-01\"), geometry = pol_geometry)\n\nThis has many columns that we don’t need. We reduce to fewer columns.\n\ncols.to.use &lt;- c(\"occurrenceID\", \"scientificName\", \n                 \"dateIdentified\", \"eventDate\", \n                 \"decimalLatitude\", \"decimalLongitude\", \"coordinateUncertaintyInMeters\",\n                 \"individualCount\",\"lifeStage\", \"sex\",\n                 \"bathymetry\",  \"shoredistance\", \"sst\", \"sss\")\nobs &lt;- obs[,cols.to.use]\n\nWe also add a cleaner date with YYYY-MM-DD format.\n\nobs$date &lt;- as.Date(obs$eventDate)\n\nSave our data.\n\nobs_csv &lt;- here::here(sdm_dir,  data_dir, \"occ_all.csv\")\nreadr::write_csv(obs, obs_csv)\n\n\nClean and prep data\nClean and prepare the data for our model and save to a new file name.\nLoad data in\n\n# presence data\nfil &lt;- here::here(sdm_dir, data_dir, \"occ_all.csv\")\nocc_all &lt;- read.csv(fil)\n\nWe will call the cleaned data occ.\n\n# subset the occurences to include just those in the water\nocc &lt;- occ_all %&gt;% \n  subset(bathymetry &gt; 0 & \n        shoredistance &gt; 0 & \n        coordinateUncertaintyInMeters &lt; 200)\n\n# seeing how often each species occurs\ntable(occ$scientificName) \n\n\nCaretta caretta  Chelonia mydas \n           5141            7060 \n\n\nAfter cleaning we discover that we only have loggerhead and green sea turtles. Also there are only juvenile loggerheads and we do not know the life-stage of the green turtles.\n\ntable(occ$lifeStage, occ$scientificName, useNA=\"ifany\")\n\n          \n           Caretta caretta Chelonia mydas\n  Juvenile            5141              0\n  &lt;NA&gt;                   0           7060\n\n\nSelect columns and add a common name column.\n\ncolnames(occ)\n\n [1] \"occurrenceID\"                  \"scientificName\"               \n [3] \"dateIdentified\"                \"eventDate\"                    \n [5] \"decimalLatitude\"               \"decimalLongitude\"             \n [7] \"coordinateUncertaintyInMeters\" \"individualCount\"              \n [9] \"lifeStage\"                     \"sex\"                          \n[11] \"bathymetry\"                    \"shoredistance\"                \n[13] \"sst\"                           \"sss\"                          \n[15] \"date\"                         \n\n\nWe want these. The last two are sea surface temperature and salinity.\n\ncols &lt;- c(\"scientificName\", \"date\", \"decimalLatitude\", \"decimalLongitude\", \"lifeStage\", \"bathymetry\", \"sst\", \"sss\")\n\nSubset the columns.\n\nocc.sub &lt;- occ %&gt;% dplyr::select(all_of(cols))\n\nChange the column names.\n\ncolnames(occ.sub) &lt;- c(\"sci.name\", \"date\", \"lat\", \"lon\", \"life.stage\", \"bathy\", \"SST\", \"SSS\")\n\nAdd common.name column.\n\nocc.sub &lt;- occ.sub %&gt;% \n  mutate(common.name = case_when(sci.name == \"Caretta caretta\" ~ \"Loggerhead\",\n                                 sci.name == \"Chelonia mydas\" ~ \"Green\"))\n\nSave the cleaned file\n\nfil &lt;- here::here(sdm_dir,  data_dir, \"occ_clean.csv\")\nreadr::write_csv(occ.sub, fil)",
    "crumbs": [
      "Tutorials",
      "Marine SDMs",
      "MaxEnt SDM"
    ]
  },
  {
    "objectID": "r-tutorials/Turtle_maxnet.html#create-background-data",
    "href": "r-tutorials/Turtle_maxnet.html#create-background-data",
    "title": "Maxent SDM",
    "section": "Create background data",
    "text": "Create background data\nWe will get random samples from our region.\n\nGet a marine raster layer\nWe just need one because we use this to sample lat/lons from the marine environment. sdmpredictors will download many files so we need to specify a directory.\n\n# set a default data directory\noptions(sdmpredictors_datadir = here::here(sdm_dir, data_dir))\n\n# choosing marine\nenv_datasets &lt;- sdmpredictors::list_datasets(terrestrial = FALSE, marine = TRUE)\nenv_stack &lt;- sdmpredictors::load_layers(\"MS_bathy_5m\")\nenv_stack &lt;- env_stack %&gt;% raster::crop(extent_polygon)\n\nPlot to check that the layer looks ok. This is bathymetry.\n\nplot(env_stack)\n\n\n\n\nLook at the raster to get some info on it.\n\nenv_stack\n\nclass      : RasterBrick \ndimensions : 388, 280, 108640, 1  (nrow, ncol, ncell, nlayers)\nresolution : 0.08333333, 0.08333333  (x, y)\nextent     : 41.83333, 65.16667, -0.1666667, 32.16667  (xmin, xmax, ymin, ymax)\ncrs        : +proj=longlat +datum=WGS84 +no_defs \nsource     : memory\nnames      : MS_bathy_5m \nmin values :       -5468 \nmax values :          -1 \n\n\n\n\nSample points from this\nIt returns a sf points object.\n\nnsamp &lt;- 1000\nabsence &lt;- dismo::randomPoints(env_stack[[1]], nsamp)\ncolnames(absence) &lt;- c(\"lon\", \"lat\")\n\nMake a plot.\n\nabsence_sf &lt;- absence %&gt;% \n    as_tibble() %&gt;% \n    sf::st_as_sf(coords = c(x=\"lon\", y=\"lat\"), crs = 4326)\nmapview::mapview(absence_sf, col.regions = \"gray\")\n\nWarning in cbind(`Feature ID` = fid, mat): number of rows of result is not a\nmultiple of vector length (arg 1)\n\n\n\n\n\n\nSave the absence locations to a file.\n\nfil &lt;- here::here(sdm_dir,  data_dir, \"absence.csv\")\nwrite.csv(absence, file=fil, row.names = FALSE)",
    "crumbs": [
      "Tutorials",
      "Marine SDMs",
      "MaxEnt SDM"
    ]
  },
  {
    "objectID": "r-tutorials/Turtle_maxnet.html#download-sdmpredictors-layers",
    "href": "r-tutorials/Turtle_maxnet.html#download-sdmpredictors-layers",
    "title": "Maxent SDM",
    "section": "Download sdmpredictors layers",
    "text": "Download sdmpredictors layers\nSet datasets to marine.\n\ndatasets &lt;- sdmpredictors::list_datasets(terrestrial = FALSE, marine = TRUE)\nlayers &lt;- list_layers(datasets)\n#View(layers) # if you want to view\n\nChoose layers.\n\nlayercodes = c(\"BO_sstmean\", \"BO_bathymean\", \"BO22_ph\", \"BO2_dissoxmean_bdmean\", \"BO2_salinitymean_ss\", \"BO2_chlomean_ss\", \"BO21_nitratemean_ss\")\n\nDownload layers. This step will take awhile and the files are large.\n\nenv &lt;- sdmpredictors::load_layers(layercodes, rasterstack = TRUE)\nenv_crop &lt;- env %&gt;% raster::crop(extent_polygon)\n\nLook at our layers.\n\nplot(env_crop)\n\n\n\n\nSave the raster brick for later reloading.\n\nenv.stars &lt;- stars::st_as_stars(env_crop) # convert to stars object\nfil &lt;- here::here(sdm_dir,  data_dir, \"env_stack.tif\")\nstars::write_stars(env.stars, fil)\n#y &lt;- stars::read_stars(fil)\n# We need to do this for sampling\nenv.stars &lt;- terra::split(env.stars)",
    "crumbs": [
      "Tutorials",
      "Marine SDMs",
      "MaxEnt SDM"
    ]
  },
  {
    "objectID": "r-tutorials/Turtle_maxnet.html#environmental-predictors-for-points",
    "href": "r-tutorials/Turtle_maxnet.html#environmental-predictors-for-points",
    "title": "Maxent SDM",
    "section": "Environmental predictors for points",
    "text": "Environmental predictors for points\nWe will use the stars package to sample from our raster layers.\nLoad in our point data as data frames.\n\n# presence data\nfil &lt;- here::here(sdm_dir, data_dir, \"occ_clean.csv\")\ndf.occ &lt;- read.csv(fil) \n\n# absence data\nfil &lt;- here::here(sdm_dir, data_dir, \"absence.csv\")\ndf.abs &lt;- read.csv(fil)\n\nConvert data frames to sf points objects. This is what stars needs.\n\ndf.abs &lt;- na.omit(df.abs) # just in case\nsf.abs &lt;- sf::st_as_sf(df.abs, coords = c(\"lon\", \"lat\"), crs = 4326)\nsf.occ &lt;- sf::st_as_sf(df.occ, coords = c(\"lon\", \"lat\"), crs = 4326)\n\nGet environment values for the absence points. Each row in sf.abs is a row in env.abs.\n\nenv.abs &lt;- stars::st_extract(env.stars, sf::st_coordinates(sf.abs)) %&gt;% \n  dplyr::as_tibble() %&gt;% \n  na.omit()\n\nhead(env.abs)\n\n# A tibble: 6 × 7\n  BO_sstmean BO_bathymean BO22_ph BO2_dissoxmean_bdmean BO2_salinitymean_ss\n       &lt;dbl&gt;        &lt;dbl&gt;   &lt;dbl&gt;                 &lt;dbl&gt;               &lt;dbl&gt;\n1       28.0        -4277    8.19                 154.                 36.3\n2       29.0        -4043    8.18                 176.                 35.5\n3       26.9        -2084    8.14                  85.7                36.6\n4       27.5        -3581    8.15                 132.                 36.5\n5       27.0        -3003    8.13                 115.                 36.5\n6       27.9        -5009    8.19                 181.                 35.5\n# ℹ 2 more variables: BO2_chlomean_ss &lt;dbl&gt;, BO21_nitratemean_ss &lt;dbl&gt;\n\n\nGet environment values for the occurence points. Each row in sf.occ is a row in env.occ.\n\nenv.occ &lt;- stars::st_extract(env.stars, sf::st_coordinates(sf.occ)) %&gt;% \n  dplyr::as_tibble() %&gt;% \n  na.omit()\n\nhead(env.occ)\n\n# A tibble: 6 × 7\n  BO_sstmean BO_bathymean BO22_ph BO2_dissoxmean_bdmean BO2_salinitymean_ss\n       &lt;dbl&gt;        &lt;dbl&gt;   &lt;dbl&gt;                 &lt;dbl&gt;               &lt;dbl&gt;\n1       26.4          -59    8.17                  183.                35.6\n2       28.6        -3158    8.18                  154.                35.7\n3       26.4          -92    8.17                  145.                35.6\n4       26.8        -2764    8.15                  130.                36.1\n5       27.9           -4    8.13                  198.                38.7\n6       27.6           -8    8.13                  198.                38.7\n# ℹ 2 more variables: BO2_chlomean_ss &lt;dbl&gt;, BO21_nitratemean_ss &lt;dbl&gt;\n\n\nNow make this into one data frame with a pa column for 1 is a occurrence row and 0 if an absence row.\n\npres &lt;- c(rep(1, nrow(env.occ)), rep(0, nrow(env.abs)))\nsdm_data &lt;- data.frame(pa = pres, rbind(env.occ, env.abs))\nhead(sdm_data)\n\n  pa BO_sstmean BO_bathymean BO22_ph BO2_dissoxmean_bdmean BO2_salinitymean_ss\n1  1     26.431          -59   8.173              182.6581            35.63274\n2  1     28.648        -3158   8.181              154.3245            35.73521\n3  1     26.428          -92   8.173              145.4299            35.64719\n4  1     26.822        -2764   8.148              130.0710            36.09258\n5  1     27.851           -4   8.133              197.8163            38.67305\n6  1     27.585           -8   8.133              197.6590            38.71139\n  BO2_chlomean_ss BO21_nitratemean_ss\n1        0.337094            2.043796\n2        0.107879            0.132562\n3        0.343290            2.086616\n4        0.217175            0.615217\n5        0.140721            0.000003\n6        0.141303            0.000003\n\n\nSave to a file. We will use for other models.\n\nfil &lt;- here::here(sdm_dir,  data_dir,  \"sdm_data.csv\")\nwrite.csv(sdm_data, row.names = FALSE, file=fil)",
    "crumbs": [
      "Tutorials",
      "Marine SDMs",
      "MaxEnt SDM"
    ]
  },
  {
    "objectID": "r-tutorials/Turtle_maxnet.html#fit-maxnet-model",
    "href": "r-tutorials/Turtle_maxnet.html#fit-maxnet-model",
    "title": "Maxent SDM",
    "section": "Fit Maxnet model",
    "text": "Fit Maxnet model\nmaxnet::maxnet(pres, environ)\n\npres string of 1s and 0s for whether the row is a occurrence or a absence.\nenviron a data frame of the environmental variables only\n\n\npres &lt;- sdm_data$pa\nenviron &lt;- sdm_data %&gt;% dplyr::select(-pa)\nsdm.model &lt;- maxnet::maxnet(pres, environ)\n\n\nModel metrics\n\nresponses &lt;- plot(sdm.model, type = \"cloglog\")\n\n\n\n\nWe have some bathymetry values &gt; 0 which might be a problem.\n\ntable(environ$BO_bathymean&gt;0)\n\n\nFALSE  TRUE \n12620   277",
    "crumbs": [
      "Tutorials",
      "Marine SDMs",
      "MaxEnt SDM"
    ]
  },
  {
    "objectID": "r-tutorials/Turtle_maxnet.html#predicting",
    "href": "r-tutorials/Turtle_maxnet.html#predicting",
    "title": "Maxent SDM",
    "section": "Predicting",
    "text": "Predicting\n\nclamp &lt;- TRUE       # see ?predict.maxnet for details\ntype &lt;- \"cloglog\"\nbb &lt;- sf::st_bbox(extent_polygon) # make a sf bounding box \npredicted &lt;- predict(sdm.model, \n                     env.stars %&gt;% sf::st_crop(bb), \n                     clamp = clamp, type = type)\n\nalthough coordinates are longitude/latitude, st_intersects assumes that they\nare planar\n\npredicted\n\nstars object with 2 dimensions and 1 attribute\nattribute(s):\n             Min.   1st Qu.    Median      Mean  3rd Qu. Max.  NA's\npred  0.001771407 0.1089345 0.1669926 0.1756772 0.216899    1 52802\ndimension(s):\n  from  to offset    delta                       refsys x/y\nx    1 280  41.83  0.08333 +proj=longlat +datum=WGS8... [x]\ny    1 388  32.17 -0.08333 +proj=longlat +datum=WGS8... [y]",
    "crumbs": [
      "Tutorials",
      "Marine SDMs",
      "MaxEnt SDM"
    ]
  },
  {
    "objectID": "r-tutorials/Turtle_maxnet.html#visualization",
    "href": "r-tutorials/Turtle_maxnet.html#visualization",
    "title": "Maxent SDM",
    "section": "Visualization",
    "text": "Visualization\nWe can plot the predictions like so, but the default palette is not great and our map is distorted.\n\nggplot() +\n    geom_stars(data = predicted)\n\n\n\n\nWe can try the cmocean palette and fix the coordinates.\n\nggplot() +\n  geom_stars(data = predicted) +\n  scale_fill_cmocean() +\n  coord_equal()\n\n\n\n\nOr other palettes and annotation.\n\npredplot &lt;- ggplot() +\n  geom_stars(data = predicted) +\n  scale_fill_cmocean(name = \"ice\", direction = -1, guide = guide_colorbar(barwidth = 1, barheight = 10, ticks = FALSE, nbin = 1000, frame.colour = \"black\"), limits = c(0, 1)) +\n  theme_linedraw() +\n  coord_equal() +\n  theme(panel.background = element_blank(),\n        panel.grid.major = element_blank(),\n        panel.grid.minor = element_blank()) +\n  labs(title = \"Loggerhead and green sea turtle SDM in the Arabian Sea\",\n       x = \"Longitude\",\n       y = \"Latitude\",\n       fill = \"Probability\",\n       shape = \"Species (presence)\",\n       subtitle = \"Environmental predictors: mean SS temp, mean SS salinity, mean bathymetry, \\nmean pH, mean DO, mean SS chlorophyll-a, mean SS nitrate\")\n\npredplot\n\n\n\n# To save to PDF use\n# ggsave(\"SDM_loggerhead_green_w points.pdf\", height = 6, width = 8.5)\n\nCreate a plot with the occurrence and absense data points.\n\npredplot +\n  geom_point(sf.occ, mapping = aes(shape = common.name, geometry = geometry), stat = \"sf_coordinates\", alpha = 0.3, color = \"purple\") +\n  geom_point(sf.abs, mapping = aes(geometry = geometry), stat = \"sf_coordinates\", alpha = 0.3, color = \"green\")\n\n\n\n# To save to PDF use\n# ggsave(\"SDM_loggerhead_green.pdf\", height = 6, width = 8.5)",
    "crumbs": [
      "Tutorials",
      "Marine SDMs",
      "MaxEnt SDM"
    ]
  },
  {
    "objectID": "r-tutorials/Turtle_maxnet.html#discussion",
    "href": "r-tutorials/Turtle_maxnet.html#discussion",
    "title": "Maxent SDM",
    "section": "Discussion",
    "text": "Discussion\nWe did not do much cleaning of the data (e.g. bathymetry &gt; 0) and we combined loggerheads and green sea turtles. We should separate these. Also some of the data are clearly tagging data and we should subsample that data to remove some of the temporal autocorrelation. We should also experiment with higher and lower numbers of background points.",
    "crumbs": [
      "Tutorials",
      "Marine SDMs",
      "MaxEnt SDM"
    ]
  },
  {
    "objectID": "r-tutorials/Turtle_maxnet.html#references",
    "href": "r-tutorials/Turtle_maxnet.html#references",
    "title": "Maxent SDM",
    "section": "References",
    "text": "References\n\nJane Elith, Steven J. Phillips, Trevor Hastie, Miroslav Dudík, Yung En Chee, and Colin J. Yates. 2011. A statistical explanation of MaxEnt for ecologists. Diversity and Distributions, 17:43-57. PDF\nCorey Merow, Matthew J. Smith, and John A. Silander, Jr. 2013. A practical guide to MaxEnt for modeling species’ distributions: what it does, and why inputs and settings matter. Ecography, 36: 1058–1069. PDF\nAleksandar Radosavljevic and Robert P. Anderson. 2014. Making better Maxent models of species distributions: complexity, overfitting, and evaluation. Journal of Biogeography, 41: 629–643. PDF",
    "crumbs": [
      "Tutorials",
      "Marine SDMs",
      "MaxEnt SDM"
    ]
  },
  {
    "objectID": "schedule.html",
    "href": "schedule.html",
    "title": "Schedule",
    "section": "",
    "text": "11-22 September 2023"
  },
  {
    "objectID": "support.html#thank-you-for-inspiration-and-content",
    "href": "support.html#thank-you-for-inspiration-and-content",
    "title": "Acknowledgements",
    "section": "Thank you for inspiration and content!",
    "text": "Thank you for inspiration and content!\nThank you to the open science community that has created software, teaching resources, and workflows that we have been able to build off of and be inspired by. These include: NASA Openscapes • OceanHackWeek • SnowEx Hackweek • eScience Institute, University of Washington • ICESat-2 Hackweek • Project Jupyter • Pangeo Project • CryoCloud   Thanks to OceanHackWeek for content, inspiration, and the opportunity to test out ideas.",
    "crumbs": [
      "Welcome",
      "Acknowledgements"
    ]
  }
]